$(document).ready(function () {indexDict['en'] = [{ "title" : "Overview", 
"url" : "overview.html", 
"breadcrumbs" : "Home \/ Overview", 
"snippet" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, ...", 
"body" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, collecting data from every pipeline, every job, wherever those workloads are running. Unravel creates a correlated data model that provides the full context you need on your apps and resources to properly plan, manage, and improve performance. Unravel enables you to: Get a unified view of your entire stack. Unravel collects performance data from every platform, system, and application, then uses agentless technologies and machine learning to model your data pipelines from end to end. : See how your data pipelines are performing from end to end. Uncover Explore, correlate, and analyze everything in your big data environment. Unravel's data model reveals dependencies, issues, and opportunities, how data and resources are being used, what's working and what's not. : Correlate and analyze all your applications, users and resource data. Understand Don't just monitor performance – quickly troubleshoot and rapidly remediate issues. Leverage AI-powered recommendations to automate performance improvements, lower costs, and prepare for what's next : Troubleshoot, tune, and optimize your ecosystem with AI-powered insights and recommendations. Unravel Unravel goes beyond raw visibility to provide concrete, providing AI-driven advice such as: Code you can use. Specific settings you can tweak. Recommendations you can immediately implement or automate to fix issues and optimize performance. The Unravel Data Operations Platform helps operations engineers, application developers, and enterprise architects reduce the complexity of delivering reliable application performance – providing unified visibility and operational intelligence to optimize your entire ecosystem. In summary, problems solved by Unravel: APM for Big Data AI for DataOps Cloud Migration Resource and Cost Optimization Troubleshooting and Root Cause Analysis " }, 
{ "title" : "Where Does Unravel Reside", 
"url" : "overview.html#UUID-728573d3-f2c2-af13-d857-e1b63c9492f2_id_Overview-WhereDoesUnravelReside", 
"breadcrumbs" : "Home \/ Overview \/ Where Does Unravel Reside", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services ...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS. Unravel analyzes this information and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Installing OnDemand for report generation. " }, 
{ "title" : "Features", 
"url" : "overview.html#UUID-728573d3-f2c2-af13-d857-e1b63c9492f2_id_Overview-Features", 
"breadcrumbs" : "Home \/ Overview \/ Features", 
"snippet" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. - provide an overall picture of your cluster's resources, usage, and activities. Dashboards - organize and display your application's in...", 
"body" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. - provide an overall picture of your cluster's resources, usage, and activities. Dashboards - organize and display your application's information allowing you to analyze and locate problems from a single interface and allows you to compare data between various runs. Application Program Manager (APM) - define automatic actions to be taken based upon your criteria, anyway from an alert\/notification to killing a rogue app. Auto-Actions & Alerts - a wide variety of reports analyzing everything from your file structure to cluster optimization to migrating to the cloud. Reporting - Unravel AI's engine helps you fine tune your cluster, applications, and tables to help you optimize for reliability and performance. Insights and Recommendations - you can control the access\/visibility users have within the Unravel UI based on their role. Support Role Based Access Control See here " }, 
{ "title" : "", 
"url" : "current.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.5.0.5...", 
"body" : " Unravel 4.5.0.5 " }, 
{ "title" : "Installation Guides", 
"url" : "current/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cloudera", 
"url" : "current/install/install-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Cloudera", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM)....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). " }, 
{ "title" : "Cloudera Pre-Installation Check", 
"url" : "current/install/install-cdh/install-cdh-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Cloudera \/ Cloudera Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.6-7.5 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH If Spark2 service is installed, Unravel host should be a client\/gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not NTP should be running and in-sync with the cluster MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions Local user unravel:unravel is created during installation, but can be changed later If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) For Impala support, a read-only account for Cloudera Manager API is needed Network The following ports must be open on the Unravel edge node: Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI If you plan to use Cloudera Manager to install Unravel Sensors, the Cloudera Manager service must also be able to reach the Unravel edge node on port 3000. 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "current/install/install-cdh/install-cdh-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Cloudera \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "This topic explains how to deploy Unravel Server on a CDH gateway\/edge node. Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive, ...", 
"body" : "This topic explains how to deploy Unravel Server on a CDH gateway\/edge node. Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive, and optionally, Spark2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. For details, see Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM. sudo rpm -U unravel-4.*.x86_64.rpm*\n\/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh externally managed MySQL Do host-specific post-installation actions. For CDH, there are no host-specific post-installation actions. Update site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties \/\/ Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/user\/history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/tmp\/logs\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/user\/spark\/applicationHistory,hdfs:\/\/user\/spark\/spark2applicationHistory Configure Unravel Server (Basic\/Core Optional for CDH) Enable optional daemons. Depending on your workload volume or kind of activity, you can enable optional daemons at this point. For more information, see Creating Multiple Workers for High Volume Data If Kerberos is enabled, add authentication for HDFS: or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Create Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE unravel Run Unravel Daemons with Custom User If Sentry is enabled, add these permissions: Define your own alt principal with narrow privileges. The alt principal can be unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal read+execute Spark event log hdfs:\/\/user\/spark\/spark2ApplicationHistory Your alt principal read+execute Spark 2 event log hdfs:\/\/user\/history Your alt principal read+execute MapReduce logs hdfs:\/\/tmp\/logs Your alt principal read+execute YARN aggregation folder hdfs:\/\/user\/hive\/warehouse Your alt principal read+execute Obtain table partition sizes with \"stat\" only For information on how to configure permissions for unravel with a Sentry enforced cluster, see Configure Permission for Unravel daemons on CDH Sentry Secured Cluster You can find the principal by running the klist -kt KEYTAB_FILE If you are using KMS and HDFS encryption and are using the hdfs principal, you might need to adjust kms-acls.xml If you are using \"JNI\" based groups for HDFS (a setting in CM), you need to add export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native \/usr\/local\/unravel\/etc\/unravel.ext.sh Switch user. Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Configure Hive metastore access. Hive metastore is accessed by Unravel server to analyze table usage in conjunction with Hive job instrumentation. Information is gathered using a Hive API that works very much like beeline connections which leverage the jdbc database connection protocol. As a quick-start approach, you can set Unravel to use the already-defined 'hive' user that is also used by HiveServer2. Alternatively, you can define a read-only metastore database user. If you want a custom user, then do the following steps for the particular kind of database that is used for Hive metastore: Connect to the Hive metastore using the normal conversational interface (mysql or psql, etc.) as an admin that can create new users. Create a user, unravel Grant select on all table in the hive database. As the new user, use the conversational interface (mysql or psql, etc.) from the Unravel server to verify their access. To complete the integration of the Hive metastore with the user, follow the steps in Hive Metastore Access Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60 Run the echo # echo \"http:\/\/( UNRAVEL_HOST_IP This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor and Configure Impala " }, 
{ "title" : "Step 2: Install Unravel Sensor and Configure Impala", 
"url" : "current/install/install-cdh/install-cdh-part2-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Cloudera \/ Step 2: Install Unravel Sensor and Configure Impala", 
"snippet" : "This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance....", 
"body" : "This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 When Active Directory Kerberos is used, UNRAVEL_HOST_IP Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ is the version of CDH version you are running X.Y is the host name or LAN IP address of Unravel Server where UNRAVEL_HOST_IP unravel_lr If you are running more than one version of CDH (multiple clusters) in Cloudera Manager, you can add more than one parcel directory from the UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, deactivate it now. Click Activate Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is enabled, grant privileges on the JAR files to the Sentry roles that run Hive queries. Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT ROLE # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE ROLE Deploy the Hive Hook Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use the \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip In Cloudera Manager, go to the Hive service. Select the Configuration Search for hive-site.xml Add the snippet to Hive Client Advanced Configuration Snippet for hive-site.xml To edit, click View as XML If cluster has been configured with Cloudera Navigator, the hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook The Hive Client Advanced Configuration Snippet for hive-site.xml Add the snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml To edit, click View as XML Like the step above, if the hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook The HiveServer2 Advanced Configuration Snippet for hive-site.xml Save the changes with optional comment Unravel snippet in hive-site.xml Deploy the Hive Client configuration by clicking the deploy glyph ( Actions Restart the Hive service. Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. Check Unravel UI to see if all Hive queries are running. If queries are running fine and appearing in Unravel UI, you are done. If queries are failing with a class not found Undo the hive-site.xml Deploy the hive client configuration. Restart the Hive service. Follow the steps in Troubleshooting Deploy the Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for spark-defaults In Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use host2's FQDN or logical hostname UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) spark-conf\/spark-defaults.conf Modify the value of UNRAVEL_HOST_IP SPARK_VERSION-X.Y has the following possible values: SPARK_VERSION-X.Y for Spark 1.3.x spark-1.3 for Spark 1.5.x spark-1.5 for Spark 1.6.x spark-1.6 for Spark 2.0.x spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true Save changes. Deploy the client configuration by clicking the deploy glyph ( Actions Enable Spark Streaming. The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark- X.Y Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited. Check Unravel UI to see if all Spark jobs are running. If jobs are running fine and appearing in Unravel UI, you are done. If queries are failing with a class not found Undo the spark-defaults.conf Deploy the client configuration. Investigate and fix the issue. Follow the steps in Troubleshooting In the case of yarn-client mode applications, the default Spark configuration is not sufficient, because the driver JVM starts before the configuration set through the SparkConf is applied. For more information, see Apache Spark Configuration per-application profiling cluster-wide profiling Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide For instructions, see CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) Enable Impala APM with CM as the Data Source Configure Unravel Server to retrieve Impala query data from Cloudera Manager (CM) as follows: Add com.unraveldata.data.source=cm \/usr\/local\/unravel\/etc\/unravel.properties Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties Cloudera Manager Properties Name\/Description Set By User Unit Default com.unraveldata.cloudera.manager.url URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8083 If the Cloudera Manager URL does not contain a port you must define port below Required string - com.unraveldata.cloudera.manager.username CM username Required string - com.unraveldata.cloudera.manager.password This is required only for Cloudera when port is missing from the CM password Required string - com.unraveldata.cloudera.cloudera.manager.port You only need to specify this if your Cloudera Manager is not on port 7180. Optional integer - com.unraveldata.cloudera.cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Optional integer - For example: com.unraveldata.data.source=cm \ncom.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret \ncom.unraveldata.cloudera.manager.port=9997 \ncom.unraveldata.cloudera.manager.username= mycmname mycmpassword Make sure that the CM user in com.unraveldata.cloudera.manager.username You can verify this by running a curl curl --user mycmname mycmpassword mycmname mycmpassword Cluster_Name By default, the ImpalaSensor task is enabled. To disable it, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw (Optional) Change the Impala lookback window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open \/usr\/local\/unravel\/etc\/unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, to set the lookback to 7 days: com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart the unravel_us (Optional) Advanced Configuration Configure Unravel Server for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Enabling LDAP Authentication for Unravel Web UI Troubleshooting Symptom Problem Remedy indicates that the directory does not exist hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed Install Unravel RPM on Unravel service host: sudo rpm -U unravel*.rpm* OR Verify that unravel \/user\/unravel\/ error for ClassNotFound com.unraveldata.dataflow.hive.hook.UnravelHiveHook Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST cd \/usr\/local\/unravel\/hive-hook\/;\ncp unravel-hive-HIVE_VER*hook.jar JAR_DEST References For more information on creating permanent functions, see http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path", 
"url" : "current/install/install-cdh/install-cdh-part2-impala.html#UUID-b7a067b9-94ce-3425-d6e5-be3a0e3bc0d3_step3", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Cloudera \/ Step 2: Install Unravel Sensor and Configure Impala \/ For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path", 
"snippet" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath...", 
"body" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath " }, 
{ "title" : "Hortonworks", 
"url" : "current/install/install-hdp.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Hortonworks", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP)....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). " }, 
{ "title" : "HDP Pre-Installation Check", 
"url" : "current/install/install-hdp/install-hdp-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Hortonworks \/ HDP Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache version equivalent) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache version equivalent) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway Verify that all clients are installed by checking that RPMs are installed; Unravel utilizes clients and associated libraries You need to register edge node to Ambari LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active If you plan to use the Auto Actions feature (move \/ kill), you'll need to add the Unravel username to YARN yarn.admin.acl JDBC access to the Hive Metastore (read-only user is sufficient) Application Timeline Server (ATS) read-only Network The following ports must be open on Unravel's host(s): Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "current/install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Hortonworks \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "This topic explains how to deploy Unravel Server on Hortonworks Data Platform (HDP). Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari UI to create a gateway node configuration. Install the Unravel Server RPM on the Host Machine Get the Unravel Serve...", 
"body" : "This topic explains how to deploy Unravel Server on Hortonworks Data Platform (HDP). Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari UI to create a gateway node configuration. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. For download location, see Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or run the mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM sudo rpm -U unravel-4.*.x86_64.rpm*\n\/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh Done The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do host-specific post-installation actions. For HDP, there are no host-specific post-installation actions. Configure Unravel Server (Basic\/Core Options) Enable optional daemons. Depending on your workload volume or kind of activity, you can enable optional daemons at this point. For more information, see Creating Multiple Workers for High Volume Data Edit \/usr\/local\/unravel\/etc\/unravel.properties General Property\/Description Set By User Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting purposes. Optional - com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Required string - com.unraveldata.tmpdir Location where Unravel's temp file resides. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer - com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer - Property\/Descripton Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Unravel UI admin. Set during installation. string admin com.unraveldata.s3.batch.monitoring.interval.sec Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. sec 300 com.unraveldata.spark.eventlog.location maprfs:\/\/\/apps\/spark [empty] 500 yarn.resourcemanager.webapp.address YARN resource manager web address URL directory String (path) oozie.server.url The Oozie server URL to be monitored by Unravel Required path - If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using properties klist -kt KEYTAB_FILE The keytab file should have chmod bits 500 and be owned by unravel Run Unravel Daemons with Custom User If Ranger is Enabled, Add These Permissions: Define your own alt principal with narrow prvileges. The alt user can be unravel rpm X in the switch user section below. Resource Principal Access Purpose hdfs:\/\/spark-history Your alt principal read+execute Spark event log hdfs:\/\/spark2-history Your alt principal read+execute Spark2 event log hdfs:\/\/mr-history\/done Your alt principal read+execute MapReduce logs hdfs:\/\/app-logs Your alt principal read+execute YARN aggregation folder hdfs:\/\/apps\/hive\/warehouse (Default value of hive.metastore.warehouse.dir Your alt principal read+execute Obtain table partition sizes Hive Metastore database GRANT hive read+execute Hive table information You must set yarn properties in Ambari. Log into Ambari and from the dashboard select YARN Configs Advanced Set yarn.acl.enable Add the Unravel user specified in com.unraveldata.kerberos.principal above yarn.admin.acl Save your changes. Disable Impala Sensor Impala is not officially supported on HDP clusters. Therefore you should disable the Impala Sensor by setting\/adding com.unraveldata.sensor.tasks.disabled=iw \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw Convert Your Unravel Installation to HDP Run the following commands on Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh This change is persistent through subsequent RPM upgrades; it does not need to be done each time. Switch \"user\" Depending on your cluster security configuration, you will need to run the switch_to_user sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y where X Y switch_to_user Restart Unravel Server. After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. Log into Unravel UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "current/install/install-hdp/install-hdp-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Hortonworks \/ Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "This topic explains how to configure Unravel Sensor for Tez ( unravel_us must be an FQDN or an IP address. UNRAVEL_HOST_IP Convert Your Unravel Installation to HDP This change is persistent after RPM upgrades. sudo \/etc\/init.d\/unravel_all.sh stop sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh ...", 
"body" : "This topic explains how to configure Unravel Sensor for Tez ( unravel_us must be an FQDN or an IP address. UNRAVEL_HOST_IP Convert Your Unravel Installation to HDP This change is persistent after RPM upgrades. sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties Add these properties to \/usr\/local\/unravel\/etc\/unravel.properties Property Description Default Value com.unraveldata.yarn.timeline-service.webapp.address The http address of the Timeline service web application http:\/\/localhost com.unraveldata.yarn.timeline-service.port Timeline service port 8188 For example, com.unraveldata.yarn.timeline-service.webapp.address=http:\/\/172.16.1.101 \ncom.unraveldata.yarn.timeline-service.port=8188 If the Application Timeline Server requires user authentication the following properties need also be specified in \/usr\/local\/unravel\/etc\/unravel.properties Property Description Default Value yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required) yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required) Open \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh If you are using Spark1 and Spark2 you must com.unraveldata.spark.eventlog.location For example, com.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/ Repoint Unravel application logs directory. com.unraveldata.job.collector.done.log.base=\/mr-history\/done \ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ \ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/ Add Hive Metastore database information for Unravel Hive configuration javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/UNRAVEL_HOST_IP:3306\/database_name javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName=HiveMetastoreUserName javax.jdo.option.ConnectionPassword=HiveMetastorePassword Log into Ambari Web UI (AWU) to verify the above properties have been set correctly in unravel.properties On the left-hand side of AWU's dashboard, click MapReduce2 Configs Advanced Advanced mapred-site Verify com.unraveldata.job.collector.done.log.base mapreduce.jobhistory.done-dir. On the left-hand side of AWU's dashboard, click YARN Configs Advanced Node Manager Verify com.unraveldata.job.collector.done.log. aggregation.base yarn.nodemanager.remote-app-log-dir On the left-hand side of AWU's dashboard, click Hive Configs Advanced Hive Metastore Verify that javax.jdo.option.ConnectionURL jdbc:mysql:\/\/ DatabaseHost DatabaseURL Verify that javax.jdo.option.ConnectionDriverName JDBC Driver Class Verify that javax.jdo.option.ConnectionUserName DatabaseUsername Start Unravel Server Unravel must be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start Install Unravel's Hive Hook and Spark Sensor Onto HDP Servers Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows (substitute the correct fully qualified host name or IP address for UNRAVEL_HOST_IP Login as root wget yum install -y wget From Unravel Server (in other words, the edge node), run the following commands on each server that will use instrumentation: Substitute valid values for UNRAVEL_HOST_IP SPARK_VERSION_ X.Y.Z HIVE_VERSION has the following possible values: SPARK_VERSION_ X.Y.Z for Spark 1.3.x spark-1.3.0 for Spark 1.5.x spark-1.5.0 for Spark 1.6.x spark-1.6.0 for Spark 2.0.x spark-2.0.0 for Spark 2.1.x spark-2.1.0 for Spark 2.2.x spark-2.2.0 for Spark 2.3.x spark-2.3.0 must be a Hive version that Unravel Supports: HIVE_VERSION for Hive 1.2.0 or 1.2.1 1.2.0 for Hive 0.13.0 0.13.0 cd \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/\nsudo python2 unravel_hdp_setup.py --sensor-only --unravel-server {Unravel Host}:3000 --spark-version {SPARK_VERSION} --hive-version {HIVE_VERSION} --ambari-server {Ambari Host} Files are installed locally on the edge node under: (Hive hook jar) \/usr\/local\/unravel_client (Resource metrics sensor jars) \/usr\/local\/unravel-agent\/jars\/ these two directories on all nodes in the cluster (worker \/ edge \/ master). In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install) Manually copy For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar oozie.libpath Add Unravel's Hive Hook Settings to All HDP Servers in the Cluster Add Unravel's hive-site settings to all of HDP's Servers in the cluster using AWU Completion of this step requires a restart of all affected Hive services in Ambari UI. In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR com.unraveldata.hive.hook.tcp=true com.unraveldata.host={add unravel gateway internal IP hostname} \n \/\/ Find below properties as it may already exists, concatenate it with a comma & no spaces hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook If LLAP is enabled copy the above settings in Custom hive-interactive-site Manual edit hive-site.xml (no AWU) file is located at hive-site.xml \/etc\/hive\/conf\/ Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar If LLAP is enabled copy above line of code into Advanced hive-interactive-env You can manually edit hive-env.sh without using AWU. The hive-env.sh \/etc\/hive\/conf\/ Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env export HADOOP_CLASSPATH export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar (Optional for Tez) Enable Unravel's Tez Instrumentation Enable Unravel's Tez instrumentation on all HDP services in the cluster. Completion of this step requires a restart of all affected Hive services in Ambari UI. Confirm that hive-execution.engine tez set hive.execution.engine=tez; Using the Ambari Web UI (AWU), configure the Btrace agent for Tez: Append the below java options to tez.am.launch.cmd-opts tez.task.launch.cmd-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment you need to modify tez.am.view-acls (Optional for Spark on YARN) Enable Unravel Spark Instrumentation on All HDP Servers in the Cluster Completion of this step requires a restart of all affected Spark services in Ambari UI. Substitute valid values for UNRAVEL_HOST_IP SPARK_VERSION_ X.Y has the following possible values: SPARK_VERSION_ X.Y for Spark 1.3.x spark-1.3 for Spark 1.5.x spark-1.5 for Spark 1.6.x spark-1.6 for Spark 2.0.x spark-2.0 for Spark 2.2.x spark-2.2 for Spark 2.3.x spark-2.3 Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults You can manually edit spark-defaults.conf The default location for spark-defaults.conf \/usr\/hdp\/current\/SPARK_VERSION_ X.Y The cluster only has one spark 1.X version: \/usr\/hdp\/current\/spark-client\/conf For spark 2.X version: \/usr\/hdp\/current\/spark2-client\/conf Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport=UNRAVEL_HOST_IP \nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs=SPARK_VERSION_X.Yspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs=SPARK_VERSION_X.Yspark.eventLog.enabled=true Enable Spark Streaming. The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark- X.Y Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited.\\ (Optional for YARN) Add Unravel to yarn.admin.acl If yarn.acl.enable=true yarn.acl.enable=false (set to yarn.admin.acl=userName * Unravel only requires yarn.admin.acl Confirm that Unravel UI Shows Tez Data Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel UI for Tez data. For instructions on using Unravel UI, see the User Guide Unravel UI may take a few seconds to load Tez data. " }, 
{ "title" : "Amazon Elastic MapReduce (EMR)", 
"url" : "current/install/install-emr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "current/install/install-emr/install-emr-intro.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Introduction", 
"snippet" : "Unravel offers full-stack performance management for modern data apps and systems. Use Unravel to monitor, manage, and optimize any modern data stack in the cloud (AWS or Azure) or on-premises (with Cloudera, HDP, or MapR). With full-stack visibility and AI-powered guidance, Unravel helps customers ...", 
"body" : "Unravel offers full-stack performance management for modern data apps and systems. Use Unravel to monitor, manage, and optimize any modern data stack in the cloud (AWS or Azure) or on-premises (with Cloudera, HDP, or MapR). With full-stack visibility and AI-powered guidance, Unravel helps customers to: Run apps more reliably, improve performance (improving SLA) Run apps more efficiently (lowering costs) Detect, troubleshoot and fix issues quickly (lowering MTTR) Dashboards and Reports (usage, chargeback\/show back, resource usage etc.) For a detailed overview, see Overview This guide is focussed on the Unravel for AWS EMR Unravel deployment involves creating a new EC2 instance and setting up RDS (optionally), installing Unravel Server on the new EC2 instance, configuring Unravel Server, and connecting it to the EMR cluster you want to monitor. You have three options for creating a new EC2 instance: Provision an Unravel EC2 instance using our CloudFormation Template Provision an Unravel EC2 instance using our Amazon Machine Image (AMI) Provision and configure an Unravel EC2 instance manually (RPM based deployment) These options are explained later in this guide. A typical deployment and configuration of Unravel Server takes less than an hour. However, in some cases, it could take a bit longer depending on the complexity of the setup in terms of security\/VPC settings, various permissions setup, and so on. " }, 
{ "title" : "Prerequisites and Requirements", 
"url" : "current/install/install-emr/install-emr-reqs.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Prerequisites and Requirements", 
"snippet" : "Unravel can monitor, manage, and optimize modern data applications running on Amazon EMR. These instructions for deploying Unravel assume you have sufficient knowledge to be able to: Provision EC2 instances and RDS instances Create and configure the required IAM roles, security groups, etc. Understa...", 
"body" : "Unravel can monitor, manage, and optimize modern data applications running on Amazon EMR. These instructions for deploying Unravel assume you have sufficient knowledge to be able to: Provision EC2 instances and RDS instances Create and configure the required IAM roles, security groups, etc. Understand AWS networking concepts such as virtual private clouds (VPCs), subnets, etc. Run Ansible scripts, basic Unix commands, and AWS CLI commands To deploy Unravel, you don't need to create any scripts or be familiar with any specific programming\/scripting language. These instructions are self-contained, and require only basic knowledge of AWS. Expert-level knowledge of AWS is not required. Since this solution is for monitoring, managing, and optimizing modern data applications running on Amazon EMR, the requirement for an AWS account to be in place will implicitly be met. There are no specific requirements for a client operating system to work on the Unravel deployment process. You must be able to connect to AWS for the deployment process. AWS Permissions and Access You need to have permission to: Create EC2 instances Connect to EC2 instances Install software on EC2 instances Create security groups and IAM roles Update IAM roles for the EMR cluster and the corresponding S3 storage If you want to deploy Unravel for a new EMR cluster, you also need AWS permissions to create an EMR cluster and necessary S3 buckets, create and configure VPCs, etc. Licensing Unravel comes with a 30-day trial license. This means you can deploy and use Unravel for 30 days without any additional licenses. To obtain a license after the trial, see Planning Guidance#Costs " }, 
{ "title" : "Architecture", 
"url" : "current/install/install-emr/install-emr-arch.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Architecture", 
"snippet" : "In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of...", 
"body" : " In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of this data is pushed to Unravel, and some is pulled by the daemons in Unravel Server. In order for this to work, you must allow both inbound and outbound traffic between Unravel Server (on the EC2 instance) and the EMR cluster. For details, see Backing up Unravel amounts to RDS backup (if that is the chosen database) and backing up the state of the Unravel Server. For more information, see " }, 
{ "title" : "Planning Guidance", 
"url" : "current/install/install-emr/install-emr-planning.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Planning Guidance", 
"snippet" : "This section explains what settings you need to specify or give access to, and why. For actual deployment steps, see Settings Related to IAM Roles and Security Groups In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel needs data from the clust...", 
"body" : "This section explains what settings you need to specify or give access to, and why. For actual deployment steps, see Settings Related to IAM Roles and Security Groups In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel needs data from the cluster as well as from apps running on the cluster. This data includes metrics, configuration information, and logs. Parts of this data is pushed to Unravel, and part of it is pulled by the daemons running on Unravel Server. In order for all data to be accessible, there must be both inbound and outbound access between Unravel Server (on the EC2 instance) and the EMR cluster. The Unravel Server must be in the same region as the target EMR cluster(s) it will be monitoring. There are two possible scenarios: Both the EMR cluster and the Unravel Server are created on the same VPC, same subnet; and the security group allows all traffic from the same subnet. The EMR cluster is located on a different VPC than Unravel Server. In this case you must configure VPC peering, route table creation, and update the security policy; for instructions, see The Unravel Server must have port 3000 open to serve the user interface. The Unravel Server needs a TCP and UDP connection to the EMR master node. To implement this, do either of the following: Create a security group that allows port 3000 and port 4043 from the EMR cluster node's IP address. Configure the security group on Unravel Server to allow TCP traffic on ports 3000 for EMR cluster nodes. Put the member of security group used on the EMR cluster in this rule. The Unravel Server and EMR cluster(s) must allow all outbound traffic. EMR cluster nodes must allow all traffic from Unravel Server. If it is not possible to for Unravel Server to access all traffic, you must minimally allow the Unravel Server to access cluster nodes' TCP port 8020, 50010, and 50020. Create a S3 ReadAccess only IAM role and assign it to Unravel Server to READ the archive logs on the S3 bucket configured for the EMR cluster. In other words, create an IAM role that contains the policy that can only READ the specific S3 bucket used on the EMR cluster; then, create an EC2 instance profile and add the IAM role to it. All of the points above are discussed in the context of the appropriate setup configuration Security Aspects Related to the Unravel Application Itself Unravel UI and API are Unravel's user-facing components. For instructions on enabling TLS (SSL) for the Unravel UI, see here Unravel (Application) Users - Authentication and Authorization Users access Unravel through the Unravel UI. The installation process creates one administrator account. To add more administrators, see To add LDAP authentication for Unravel UI, see To add SAML authentication, see To use role-based access control (RBAC) to restrict the views and applications users can see when they log into Unravel UI, see Risk Audit Mechanism Unravel logs its actions on the same EC2 instance that hosts Unravel Server, in \/usr\/local\/unravel\/logs\/unravel_ngui.log Costs There are two components of the cost to the user in using Unravel: Cost of AWS components: Cost of the running Amazon EC2 instance that has the Unravel Server deployed on it + Cost of Amazon EBS storage + Cost of RDS (if applicable) References: Amazon EC2 Pricing Amazon EBS Pricing Amazon RDS Pricing Cost of Unravel license: Unravel comes with an initial trial period of 30 days. To use Unravel beyond the initial trial period, contact us AWS cost allocation tags can be used to track your AWS costs on a detailed level. It is recommended to enable and set both types of cost allocation tags: AWS generated tags user-defined tags https:\/\/docs.aws.amazon.com\/awsaccountbilling\/latest\/aboutv2\/cost-alloc-tags.html Sizing EC2 Instance Specifications: Minimum: r4.2xlarge (61 GiB RAM) Maximum: r4.8xlarge (244 GiB RAM) Recommended: r4.4xlarge (122 GiB RAM) Virtualization type: HVM EBS Volume Specifications: Provision 200 GiB or above with Volume Type = Provisioned IOPS SSD ( io1 The Baseline IOPS (3 IOPS per GiB with a minimum of 100 IOPS, burstable to 3000 IOPS) is sufficient for Unravel (Optional) RDS Specifications: DB instance class: db.r3.xlarge (4 vCPU, 30.5 GiB RAM) Storage type: Provisioned IOPS (SSD) Allocated storage: 200 GiB or above Provisioned IOPS: 1000 While Unravel Server does not require a high level of resources, it's best to check your AWS Service Limits Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Virtual Private Cloud (Amazon VPC) Limits " }, 
{ "title" : "Step 1: Provision and Configure an Unravel EC2 Instance", 
"url" : "current/install/install-emr/install-emr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance", 
"snippet" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR cluster....", 
"body" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR cluster. " }, 
{ "title" : "Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"url" : "current/install/install-emr/install-emr-part1/install-emr-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Ma...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Allowed port for inbound access to EMR master node: Port 8020 (Name node access for spark event log) Allowed port for inbound access to EMR core node: Port 50010 (Data node access for spark event log) Port 50020 AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our CloudFormation Template Download our CloudFormation template from the following public S3 bucket into your local \/tmp folder: https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_aws_marketspace_01.json Use AWS CLI or the AWS Marketplace console to create the Unravel EC2 instance: From AWS CLI, run the aws cloudformation Syntax: aws cloudformation create-stack --stack-name UnravelEC2 \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue= AWS_ZONE_NAME AWS_INSTANCE_TYPE my_ssh_key AWS_VPC_BLOCK AWS_SSH_BLOCK Example: aws cloudformation create-stack --stack-name UnravelEC2number \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue=us-east-1d \\ \nParameterKey=InstanceType,ParameterValue=r4.2xlarge \\ \nParameterKey=KeyName,ParameterValue=topcat \\ \nParameterKey=UnravelVPCBlock,ParameterValue=10.12.0.0\/16 \\ \nParameterKey=TrustedSshIPBlock, ParameterValue=0.0.0.0\/0 \\ \n--capabilities CAPABILITY_IAM | tee UnravelEC2-stack.json From AWS Marketplace ( https:\/\/aws.amazon.com\/marketplace Type unravel In the search results, select Unravel APM for EMR Continue to Subscribe Click Continue to Configuration On the Configure this software : Unravel EC2 instance Fulfillment Option : 4.5.0.5 Software Version : Select the region that matches your existing Amazon EMR. Region Click Continue to Launch On the Launch this software Usage Instructions In the Choose Action Launch CloudFormation Click Launch You are now on the CloudFormation portal ( https:\/\/console.aws.amazon.com\/cloudformation\/home In the Choose a template Specify an Amazon S3 template URL Next You are now on the Specify Details On the Specify Details settings for this EC2 instance Next On the Options Next On the Review Create The deployed application together with its virtual resources is called a CloudFormation \"stack\" and is named UnravelEC2number If you don't see your new stack in the list, wait 30-60 seconds and refresh the page. When the status of the stack changes to CREATE_COMPLETE UnravelEC2number Outputs : The S3 bucket used by the Unravel EC2 node for backups UnravelBackupS3 : The Unravel EC2 node's instance ID UnravelAutoScalingGroupId : The virtual public cloud (VPC) created for this Unravel EC2 node VPC : The security group created for this Unravel EC2 node SecurityGroup For example: Get the public IP address and private IP address of this EC2 instance. You can do this through aws CLI or through the EC2 console, ( https:\/\/console.aws.amazon.com\/ec2\/ To use aws CLI, run the ec2 describe-instances UnravelAutoScalingGroupId Outputs AUTOSCALING_GROUP=UnravelAutoScalingGroupId_value_from_Outputs_tab \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PublicIpAddress |head -1 \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PrivateIpAddress |head -1 To use the EC2 console, highlight this EC2 instance and look at its IP addresses in its Description For example: Log into Unravel UI Using a web browser, navigate to http:\/\/ EC2_Public_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR cluster " }, 
{ "title" : "Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"url" : "current/install/install-emr/install-emr-part1/install-emr-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM)...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our Amazon Machine Image (AMI) Navigate to the EC2 console ( https:\/\/console.aws.amazon.com\/ec2\/ From the menu on the left, select IMAGES AMIs In search box pull-down menu, select Public images Search for ami-08d8b2a645bdc7482 Select this AMI and click Launch On the Choose an Instance Type R4.2xlarge Next: Configure Instance Details (Optional) Modify the configuration of the EC2 instance: On the Configure Instance Details only : Your selected VPC Network : Your selected subnet Subnet : The name of the IAM user you created for S3 access IAM role Click Next: Add Storage (Optional) Increase the storage capacity of the EC2 instance to a maximum of 500GiB, depending on the number of clusters, the number of jobs running on those clusters, and whether you plan to enable debug logging. Click Next: Add Tags (Optional) Add tags. Click Next: Configure Security Group On the Configure Security Group Rule Type Protocol Port(s) Source Inbound: SSH TCP 22 Your trusted CIDR for SSH access Inbound: Custom TCP TCP 3000 0.0.0.0\/0 or EMR security group ID or EMR subnet IP block Inbound: Custom TCP TCP 4043 EMR security group ID or EMR subnet IP block Outbound: All traffic All All 0.0.0.0\/0 Security policy required for all nodes in the EMR cluster (master, core, and task nodes) All All Unravel EC2 node security group ID or Unravel EC2 node private IP address Security reminder: Don't make Unravel UI accessible on the public Internet. Click Review and Launch Review your settings and click Launch Enter the name of your key pair file and click Launch Instances Verify that you see the following notice: Click the instance. Find the public IP address of the instance in its Description Log into Unravel UI Using a web browser, navigate to http:\/\/ EC2_Public_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR clusters " }, 
{ "title" : "Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"url" : "current/install/install-emr/install-emr-part1/install-emr-part1-option3.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"snippet" : "This topic explains how to manually deploy Unravel Server on an AWS EC2 instance. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI. For example, ami-02e98f78 Instance typ...", 
"body" : "This topic explains how to manually deploy Unravel Server on an AWS EC2 instance. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI. For example, ami-02e98f78 Instance type: Minimum: R4.2xlarge (60 GB Ram) Virtualization type: HVM Virtualization type: HVM Root device type: EBS Root disk space: Minimum: 100GB. Virtualization type: HVM Virtualization type: HVM In a PoC or evaluation, the minimal root disk size 100GB should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend to set minimal root disk from 300 - 500GB \"Provisioned IOPS\" EBS volume with 3000 IOPS. For production unravel use case, 200GB root disk Provisioned IOPS EBS and RDS are recommended. Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Create an EC2 Instance Base OS: See the table above. EC2 instance's type and size: See the table above. EC2 instance's security group \/ IAM role: See the table above. For instance, create an IAM role that contains the policy that only reads Creating IAM Role Create Instance Profile CLI Base OS: See the table above. Ports: See the table above. Networking: The EC2 instance must Security reminder: Don't make Unravel UI accessible on the public Internet. Security group or policy required for the Unravel EC2 instance: Create an S3 ReadAccess only IAM role and assign it to Unravel EC2 node to read the archive logs on the S3 bucket configured for the EMR cluster. Unravel EC2 node works with multiple EMR clusters, including existing and newly created clusters. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node. You must Create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address Put the member of security group used on EMR cluster in this rule. A sample security group used for Unravel EC2 node. Inbound Rule Type Protocol Port Range Source All traffic All All SG ID of this group or Subnet IP block (e.g. 10.10.0.0\/16) SSH TCP 22 0.0.0.0\/0 or trusted public IP for ssh access Custom TCP Rule TCP 3000 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Custom TCP Rule TCP 4043 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Outbound Rule Type Protocol Port Range Source All traffic All All 0.0.0.0\/0 The Unravel EC2 node should have all TCP access to the EMR cluster (master or slave) nodes. You can grant access by inserting a security policy into both SG (security group) of EMR master and slave with (All TCP, All port range) and the source is the SG ID of the unravel VM. (see screen capture below) If it's not possible to Unravel EC2 access All traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Configure the EC2 Instance at First Login Disable selinux # sudo setenforce Permissive Edit \/etc\/selinux\/config SELINUX=permissive # vi \/etc\/selinux\/config Install libaio.x86_64 lzop.x86_64 # sudo yum install -y libaio.x86_64\n# sudo yum install -y lzop.x86_64 Start ntpd # sudo service ntpd start\n# sudo ntpq -p Create a new user named hadoop # sudo useradd hadoop Install the Unravel RPM on the EC2 Instance Get the Unravel Server RPM; for download location, see Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. # sudo rpm -U unravel-4.5.0.*-EMR-latest.rpm Run the await_fixups.sh In a routine upgrade, it is okay to start all Unravel daemons, but do not stop or restart them until the await_fixups.sh Done # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n# \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Append the following line to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem=false For monitoring EMR Spark service, add the following properties to unravel.properties com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates the following items: Virtualization type: HVM User unravel Initial internal database and other durable states in \/srv\/unravel\/ Virtualization type: HVM scripts for controlling services, and \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh Log into Unravel UI Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Create an ssh # ssh -i ssh_key.pem centos@ UNRAVEL_HOST_IP Using a web browser, navigate to http:\/\/127.0.0.1:3000 Log into the Unravel UI with username admin unraveldata Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR cluster " }, 
{ "title" : "Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"url" : "current/install/install-emr/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"snippet" : "Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traff...", 
"body" : "Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes allow all traffic from the Unravel node . Both EMR cluster and Unravel node are created in same VPC, same subnet; and the security group allows all traffic from the same subnet. For existing EMR cluster connection located on a different VPC, you must configure VPC peering, route table creation, and security policy update. For details, see Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional) Network ACL on VPC allows all traffic. Connect the Unravel EC2 Instance to a New EMR Cluster To connect the Unravel EC2 instance to a new EMR cluster, follow the steps below to run the Unravel EMR bootstrap script on all nodes in the cluster. The bootstrap script does the following: On the master node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf Updates \/etc\/hadoop\/conf\/mapred-site.xml Updates \/etc\/hadoop\/conf\/yarn-site.xml If TEZ is installed, it updates \/etc\/tez\/conf\/tez-site.xml Installs and starts the unravel_es daemon \/usr\/local\/unravel_es Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs the Hive hook sensor in \/usr\/lib\/hive\/lib\/ On all other nodes: Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs Hive sensors in \/usr\/lib\/hive\/lib Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the Unravel EMR bootstrap script unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce In the AWS console, select the EMR service and click Create cluster In the Create Cluster - Quick Options Go to advanced options Select Step 1: Software and Steps emr-5.14.0 Release Next. Select Step 2: Hardware Set Network EC2 Subnet If you created the Unravel EC2 node from our CloudFormation template, then a new VPC was generated, named Unravel_VPC. This VPC comes with one configured subnet, and by default has a CIDR \/ network address block of 10.10.0.0\/16 (but you might have changed this during stack creation). If you created the Unravel EC2 node from our Amazon Machine Image (AMI), you must create the EMR cluster on the same VPC and same subnet as the Unravel EC2 node. Modify the instance type and enter the desired instance count for core (slave) node(s). Click Next. Select Step 3: General Cluster Settings Cluster name S3 folder Add bootstrap action Custom action Configure and add For details on how to set up your EMR cluster, see https:\/\/docs.aws.amazon.com\/emr\/latest\/ManagementGuide\/emr-gs-launch-sample-cluster.html In the Add Bootstrap Action Script location step 2 Optional arguments Add Sample script location s3:\/\/aws-logs-account_number-region\/elasticmapreduce Optional arguments (mandatory here) --unravel-server UNRAVEL_EC2_IP --bootstrap In the Bootstrap Actions Nex Advanced Options Select Step 4: Security Choose the EC2 key pair Select the EC2 security groups In this example, the security group picked for both Maste Core & Task You must choose the security group that includes the Unravel EC2 instance, otherwise bootstrap will fail. Click Create cluster If everything was entered correctly, your new EMR cluster should finish the bootstrap process and be in the Waiting Once your new EMR cluster is up and running, you can run some jobs and log into the Unravel EC2 node's web UI to see the metrics collected by the Unravel node. Connect the Unravel EC2 Instance to an Existing EMR Cluster To connect the Unravel EC2 instance to an existing EMR cluster, follow the steps below to run the Unravel EMR Ansible playbook either or Substitute your local values for variable Whenever you upgrade Unravel Server, repeat the steps below to upgrade Unravel Sensors as well. Prerequisites Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 <key> Option 1: Run Our Ansible Playbook on the EMR Master Node Download unravel-emr-ansible.zip curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip --output unravel-emr-ansible.zip\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 11708 100 11708 0 0 66541 0 --:--:-- --:--:-- --:--:-- 66902 Unzip unravel-emr-ansible.zip unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: : The full pathname of the SSH private key --ssh-key path : The Unravel EC2 host's internal IP address. --unravel-host hostname : The EMR cluster name as you want it to display in Unravel UI. --cluster-name displayname python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible on the EMR master node: sudo pip install ansible (Optional) Determine what directory Ansible was installed in, and add that directory to the $PATH variable in ~\/.bashrc which ansible\n\/usr\/local\/bin\/ansible In ~\/.bashrc export PATH=\/usr\/local\/bin\/:$PATH Run the Unravel Ansible playbook: cd unravel-emr-ansible\nANSIBLE_HOST_KEY_CHECKING=false \nansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Option 2: Run Our Ansible Playbook on Your Personal Workstation (Mac or Linux Only) Set up AWS CLI. For instructions, see https:\/\/aws.amazon.com\/cli\/ Make sure AWS CLI has permission to list EMR clusters: aws emr list-instances --cluster-id cluster id Download unravel-emr-ansible.zip wget https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip Unzip unravel-emr-ansible.zip unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: : Cluster ID. Optional on EMR master, required if run outside of EMR cluster. --cluster-id string : AWS region. Optional on EMR master, required if run outside of EMR master node --region string : Directory containing the Ansible inventory file, emr_ansible_inventory. Default is same directory as the playbook. --inventory path : The full pathname of the SSH private key --ssh-key path : SSH username with sudo privilege; default is --ssh-user string hadoop : The Unravel EC2 host's internal IP address. --unravel-host hostname : The EMR cluster name as you want it to display in Unravel UI. --cluster-name displayname : Use public IP address instead of private IP address in ansible inventory. Include this option if you need to connect to the EMR cluster solely through its public IP address. --use-public python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible: sudo pip install ansible Run the Unravel Ansible playbook: cd unravel-emr-ansible\nANSIBLE_HOST_KEY_CHECKING=false \nansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Sanity Check After you connect the Unravel EC2 instance to your EMR cluster, run some jobs on the EMR cluster and monitor the information displayed in Unravel UI ( http:\/\/unravel_ec2_node_public_IP:3000 Troubleshooting Check Ansible playbook logs in \/tmp\/unravel\/unravel_sensor_ansible.log If the EMR cluster is created in a different VPC, see Configure VPC Peering " }, 
{ "title" : "Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"url" : "current/install/install-emr/install-emr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"snippet" : "Unravel's default installation uses a bundled database for part of its storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Un...", 
"body" : "Unravel's default installation uses a bundled database for part of its storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group. Create new DB subnet group Create new DB parameter group Set Up MySQL RDS in AWS Create Unravel RDS instance In AWS portal ? RDS, click Create database Choose MySQL Next Choose Production - MySQL Change the following properties, leave all others as the default. : generic-public-license License model : 5.5.46 DB engine version : db.r3.xlarge (vCPU, 30.5 GiB RAM) DB instance class : Create replica in different zone Multi-AZ deployment : Provisioned IOPS (SSD) Storage type : 500GB (or more depending on number of jobs and clusters the unravel node will monitor) Allocated storage : 1000 Provisioned IOPS Create a new DB instance and Master user and password. Click Next : unravelmysqlprod DB Instance identifier : unravel Master username : Change_Password Master password In the Advanced Settings Network & Security Settings : choose the VPC that contains minimally two subsets and on the same region that you plan to deploy unravel and EMR cluster Virtual Private Cloud : create a db subnet group \"unravel\" by using two subnets in the same VPC Subnet group : No Public accessibility : No Preference Availability zone : Create new VPC security group VPC security group Create a new DB subnet group in advance. It is required for Multi-AZ deployment. The VPC should at least contains two subnets in at least two Availability zones in a given region. For further information please check AWS documentation. Screenshot for DB subnet group. Create a new DB parameter group in advance, and this group is based on mysql 5.5. Alter the parameters base on the custom db parameters. Screenshot for DB parameter group. Custom db parameters key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 net-read-timeout = 300 net-write-timeout = 600 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Database Options Settings Database name: unravel_mysql_prod Port: 3306 DB parameter group: unravel For other RDS options such as Encryption Backup Monitoring Maintenance Create database Connecting Unravel Node to the Unravel RDS Instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. Locate the MySQL database endpoint in the RDS dashboard. Look for the security group used for unravel RDS instance from RDS dashboard. Edit the inbound rule of the security group. Add a new rule to allow connections from either Unravel node's Security Group Subnet IP block which unravel node located The SG or IP block works provided the RDS instance is located on the same region as the VPC. Verify the MySQL connection from the Unravel Node. \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Click here to see a sample screenshot. Verify that the database unravel_mysql_prod # CREATE DATABASE IF NOT EXISTS unravel_mysql_prod; Create Unravel db Schema in RDS Unravel Database Stop Unravel Server. sudo \/etc\/init.d\/unravel_all.sh stop Configure the following properties in unravel.properties vi \/usr\/local\/unravel\/etc\/unravel.properties Locate and modify the properties below so that they reflect your particular values. If the property isn't found, add it. Use the actual values you set in the above steps, here here Encrypting Passwords in Unravel Properties and Settings unravel.jdbc.username=unravel\nunravel.jdbc.password={unraveldata}\nunravel.jdbc.url=jdbc:mysql:\/\/unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod Ensure the schema is up to date using the schema upgrade utility provided by Unravel server. The script step connects to the database and applies schema deltas in-order until the schema is up to date. The success or failure of the update is noted. sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user admin \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Start Unravel Daemon Disable the bundled db on Unravel Server. Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command produces an error that can be ignored. sudo chkconfig unravel_db off\nsudo chkconfig unravel_pg off Start Unravel Server. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"url" : "current/install/install-emr/install-emr-part4.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"snippet" : "Follow these steps only This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assumptions The V...", 
"body" : " Follow these steps only This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assumptions The VPC where Unravel EC2 located is in the same region where the EMR cluster located (e.g., us-east-1). The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in EMR cluster. Network ACL on both VPC for Unravel EC2 and EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. In the following steps, we have both Unravel EC2 node and EMR cluster located in us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. Resources Internal IP Address Subnet ID Subnet IP Block VPC ID (Name) IP block in VPC Security Group ID (Name) Unravel EC2 node 10.10.0.7 subnet-03b82c56b2c26dbd1 10.10.0.0\/24 vpc-0b0e17b01c4a3b54a (Unravel_VPC) 10.10.0.0\/16 sg-0e0a03084398287c9 (Unravel-EC2_SG) EMR Cluster Master node 10.11.0.53 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) EMR Cluster Core nodes 10.11.0.76 10.11.0.130 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) Create VPC Peering in VPC Dashboard From the AWS console VPC services Peering Connections Create Peering Connection Enter the name tag. For example, EMR_to_Unravel In the VPC (Requester) In the VPC (Accepter) Click Create Peering Connection A success message should appear in the screen. Click OK Accept the VPC Peering Request In the VPC Dashboard Pending Acceptance Select this connection, click Action Accept Request Click Yes Accept Close Create Routes Between Peered VPC To create the routes between peered VPCs (Unravel Server on Unravel_VPC and the EMR cluster on Test_EMR_VPC): Go to VPC Dashboard Route Tables After locating each route table, click Edit Add another route Find the Unravel_VPC route table. In the Destination For example, 10.11.0.0\/16 In the Target For example, pcx-0a57a978ef9a525e2 Click Save Find the Test_EMR_VPC route table. Set the Destination For example, 10.10.0.0\/16 In the Target For example, pcx-0a57a978ef9a525e2 Click Save In the Target For example, pcx-0a57a978ef9a525e2 Click Save Update Security Groups Go to VPC Dashboard Security Group After locating each security group: Click Add another rule Set Type ALL traffic Protocol ALL Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block, e.g., 10.11.0.0\/16 in the Source Save Locate the security group used on EMR cluster node and enter the Unravel VPC IP block. For example, 10.10.0.0\/16 Click Save Verify Connection Between Unravel EC2 Node and EMR Master Node Open SSH sessions to both Unravel EC2 nodeand EMR master node. Since the above example allows all traffic On Unravel EC2 node, telnet On the EMR master node, telnet If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network ACL, security groups, and route tables used on both VPCs. For more information, see unsupported VPC peering configurations " }, 
{ "title" : "Testing and Troubleshooting", 
"url" : "current/install/install-emr/install-emr-test-troubleshoot.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Testing and Troubleshooting", 
"snippet" : "This is an excerpt of the User Guide Testing the Deployment Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 instance. For example: ssh -i ssh_key.pem centos@ Unravel_node_public_IP Start your browser from your workstation and navigate to http:\/\/127.0...", 
"body" : "This is an excerpt of the User Guide Testing the Deployment Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 instance. For example: ssh -i ssh_key.pem centos@ Unravel_node_public_IP Start your browser from your workstation and navigate to http:\/\/127.0.0.1:3000 Log in with username admin unraveldata The OPERATIONS Trial versions include a message in the top menu bar about the trial license and the number of days remaining until it expires. To extend your trial period or inquiry about our license policy, contact us Run sample jobs from the EMR master node. The EMR master node has sample MapReduce and Spark jobs on it. Run these jobs to verify that the Unravel EC2 node is collecting data from the EMR cluster. Your usage may vary depending on what applications you installed on your cluster. Sample MapReduce Job Connect to the EMR master node via SSH: ssh -i ssh_key.pem ec2-user@ EMR_master_public_IP Run this MapReduce \"Pi\" job: sudo -u hdfs hadoop jar \/usr\/lib\/hadoop-mapreduce\/hadoop-mapreduce-examples.jar pi 100 100 When the MapReduce job finishes, check Unravel UI. You should see one successful application labeled MR To see details about the MapReduce job, click the APPLICATIONS The job's details are displayed. Click the orange bar that notifies you that Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job's details. Sample Spark Job Connect to the EMR master node via SSH: ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this Spark \"Pi\" job: sudo -u hdfs spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/lib\/spark\/examples\/jars\/spark-examples.jar 1000 When the Spark job finishes, check Unravel UI: You should see one successful application labeled SPARK To see details about the Spark job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. Sample Tez Job Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel UI for Tez data. For instructions, see Tez Application Manager. FIXLINK Sending Diagnostics to Unravel Support In the upper right corner of Unravel UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through Unravel UI: On the Unravel host, bundle the diagnostic information. \/usr\/local\/unravel\/install_bin\/diag_dump.sh Log into and upload the bundle. Reconnecting to Your EMR Cluster If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of your EC2 instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup saved in an S3 bucket. In this case, your existing EMR clusters just need to be reconnected to the newly created Unravel EC2 instance as described in Deleting the Unravel EC2 Instance If you're done with the Unravel EC2 instance, you can delete it as follows. From your EC2 console ( https:\/\/console.aws.amazon.com\/cloudformation\/ In the Actions Delete Stack Click Yes, Delete Monitor the Status Diagnosing and Using Oozie with Unravel You may see this common error: org.apache.oozie.action.ActionExecutorException: JA010: Property [fs.default.name] not allowed in action [job-xml] configuration This might indicate that you have an older version of a configuration file which contains some deprecated properties. The straightforward solution is to comment-out the <job-xml> workflow.xml Insufficient Historical Data in Unravel UI Settings are needed to adjust the time horizon. In this example, it is set to 2 years with recent data showing the max amount minus 2 weeks: In Unravel UI, navigate to Manage Core Retention Under TIME SERIES RETENTION DAYS com.unraveldata.retention.max.days \/usr\/local\/unravel\/etc\/unravel.properties In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.history.maxSize.weeks=104 Restart the servers sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Operational Guidance", 
"url" : "current/install/install-emr/install-emr-ops-guidance.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Operational Guidance", 
"snippet" : "Health Check You can monitor Unravel Server through CloudWatch by creating alerts for the specific EC2 instance that hosts Unravel Server. Unravel also has a monitoring service - a lightweight daemon which allows you to monitor various Unravel components. For details, see If you've set up Unravel to...", 
"body" : " Health Check You can monitor Unravel Server through CloudWatch by creating alerts for the specific EC2 instance that hosts Unravel Server. Unravel also has a monitoring service - a lightweight daemon which allows you to monitor various Unravel components. For details, see If you've set up Unravel to use RDS as its datavase, and you want to monitor RDS storage capacity, see (It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region disruption does not apply for Unravel. ) Backup and Recovery The best course of action to take in situations like instance failures is to have followed the process of backing up and disaster recovery as described in It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region recovery does not apply for Unravel. Unravel is designed to maintain business continuity and does not support complete\/true High Availability (HA). Routine Maintenance There are multiple means by which Unravel announces and documents details of availability of new versions: In the Unravel Solution Engineers and Account Management Team members engage with customers directly to tell them about the availability of upgrades and patches published on the Unravel website Blogs Through the Unravel Newsletter (sign up on the Unravel website Emergency Maintenance In the event of fault conditions, such as a transient failure of an AWS Service such that the availability of EC2 in a particular availability zone (AZ) is degraded, or a more permanent failure of an AWS service such that EC2 instance has faulted, or an EC2 Scheduled Maintenance Event is received, the best course of action to take in such situations is to have followed the process of backing-up and disaster recovery as described in adv-backingup-disaster-recovery-reverting adv-backingup-disaster-recovery-reverting Support To contact Unravel Support, visit Support Costs Currently, there are no additional costs for obtaining Unravel Support. " }, 
{ "title" : "Amazon Athena", 
"url" : "current/install/install-athena.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena", 
"snippet" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/...", 
"body" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/athena\/pricing\/?nc=sn&loc=3 You send Unravel information about your Athena queries through an AWS Lambda function which monitors your AWS CloudTrail trail for Athena events. Use Cases Amazon Athena is well suited to structured data such as logs. " }, 
{ "title" : "Integrating Athena with Unravel", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-IntegratingAthenawithUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Integrating Athena with Unravel", 
"snippet" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS...", 
"body" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS role to allow AWS Lambda functions to call AWS services on your behalf. Create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. In Unravel UI, look at Apps Athena " }, 
{ "title" : "Create a Trail in AWS CloudTrail", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaTrailinAWSCloudTrail", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Create a Trail in AWS CloudTrail", 
"snippet" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into you...", 
"body" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select CloudTrail On the CloudTrail Trails Create trail In the Trail name Unravel In the Apply trail to all regions Yes In the Management events section, next to Read\/Write events All In the Data events In the Storage location You can create a new S3 bucket or use an existing S3 bucket. If you create a new bucket: Set the S3 bucket name to unravel-cloudtrail Expand the Advanced Leave the Log file prefix For Encrypt log files with SSE-KMS No For Enable log file validation Yes For Send SNS notification for every log file delivery No Click Create Configure CloudWatch permissions on unravel-cloudtrail Click your newly created trail, unravel-cloudtrail CloudWatch Logs Click Configure In the New or existing log group CloudTrail\/UnravelLogGroup Click Continue On the next page, expand View Details : IAM Role Create a new IAM Role : Role Name unravel-cloudtrail-role Click Allow The configuration summary for this trail appears, and in the upper right corner the logging status is displayed. " }, 
{ "title" : "Create a Role for Unravel's AWS Lambda Function", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaRoleforUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Create a Role for Unravel's AWS Lambda Function", 
"snippet" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS L...", 
"body" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS Lambda with AWS CloudTrail Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select IAM On the IAM Roles Click Create role In the Select type of trusted entity AWS service In the Choose the service that will use this role Lambda Click Next: Permissions On the Attach permissions policies AmazonS3ReadOnlyAccess AWSLambdaVPCAccessExecutionRole Click Next: Tags (Optional) If you want to add tags to this role, add them here. Click Next: Review On the Review page, set Role name unravel-athena-lambda-role Click Create role The AWS console displays a message indicating that it created the role. Select the role in the list of roles. On the role summary page, select the Trust relationships " }, 
{ "title" : "Create Unravel's AWS Lambda Function", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Create Unravel's AWS Lambda Function", 
"snippet" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/conso...", 
"body" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select Lambda On the Lambda Create function On the Create function : Function name UnravelAthenaLambda : Runtime Python 3.7 : Execution role Use an existing role : Existing role unravel-athena-lambda-role Click Create function AWS displays a banner indicating success, and displays your new Lambda function’s page. Add a Trigger to the Lambda Function On your new Lambda function’s page, select Amazon S3 From the list of triggers on the right, select S3 In the Configure triggers : Bucket unravel-cloudtrail : Event type All object create events Select the Enable trigger At the top of the page, click Test Click Add AWS shows the new S3 trigger at the bottom of the page. At the top of the page, click Save Add Code to Unravel’s AWS Lambda Function Select the new Lambda function: AWS displays configurable settings for this function. In the Function code : Code entry type Upload a file Amazon S3 Amazon S3 link URL: s3:\/\/unraveldatarepo\/share\/lambda\/UnravelAthenaLambda.zip : Runtime Python 3.7 In the Environment variables Key: unravel_lr_url Value: : Private-IP-of-Unravel-Node Port Where: Private-IP-of-Unravel-Node Port In the Execution role Select Use an existing role Existing role: unravel-athena-lambda-role In the Network Don’t select No VPC Select your VPC. Select at least two subnets from the pull-down list (hold CTRL to select multiple subnets). Select your private security group (SG). Review the inbound and outbound rules. At the top of the page, click Test At the top of the page, click Save AWS displays a banner indicating success. " }, 
{ "title" : "View Athena Queries in Unravel UI", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-ViewAthenaQueriesinUnravelUI", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ View Athena Queries in Unravel UI", 
"snippet" : "In Unravel UI, look at Apps Athena User Guide...", 
"body" : "In Unravel UI, look at Apps Athena User Guide " }, 
{ "title" : "Resources", 
"url" : "current/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-Resources", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Resources", 
"snippet" : "Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs...", 
"body" : " Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs " }, 
{ "title" : "Creating Private Subnets for Unravel's Lambda Function", 
"url" : "current/install/install-athena/install-athena-lambda-vpc.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Amazon Athena \/ Creating Private Subnets for Unravel's Lambda Function", 
"snippet" : "To ensure that Unravel's Lambda Function can access Unravel Node port 4043 and a specific S3 bucket, you might need to create private subnets by following these steps. Step 1: Create Private Subnets In the AWS VPC Dashboard, click Create subnet Create two subnets within the same VPC Unravel Server i...", 
"body" : "To ensure that Unravel's Lambda Function can access Unravel Node port 4043 and a specific S3 bucket, you might need to create private subnets by following these steps. Step 1: Create Private Subnets In the AWS VPC Dashboard, click Create subnet Create two subnets within the same VPC Unravel Server is located. is the block of IP addresses that you're assigning to this subnet. This value can be different based on your environment. For example, IPv4 CIDR block 172.31.64.0\/24 172.31.64.0 Create a Route Table Create a route table for the two private subnets. In VPC Associate the two subnets with this route table. Create a NAT Gateway On the AWS VPC Dashboard, click Create NAT Gateway Attach the NAT gateway to a public subnet with an elastic IP: is the public subnet within Unravel VPC. Subnet is the elastic IP (EIP). If there is no available EIP, click Elastic IP ID Create New EIP Add the NAT Gateway to the Route Tables Select Unravel Lambda Route Table Select the Routes Click Edit routes Add the route gateway. Click Save routes References " }, 
{ "title" : "MapR", 
"url" : "current/install/install-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MapR", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on MapR....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on MapR. " }, 
{ "title" : "MapR Pre-Installation Check", 
"url" : "current/install/install-mapr/install-mapr-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MapR \/ MapR Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architectur...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network The following ports must be open on Unravel's host(s): Port(s) Direction Description 3000 or 4020 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on MapR", 
"url" : "current/install/install-mapr/install-mapr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR", 
"snippet" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you don't already have a gateway\/edge\/client host pro...", 
"body" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you don't already have a gateway\/edge\/client host provisioned for Unravel server, follow these steps which are needed to enable the hadoop fs For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands on Unravel Server as root NAME CLDB_LIST HISTORY_SERVER sudo yum install mapr-client.x86_64\nsudo \/opt\/mapr\/server\/configure.sh -N NAME CLDB_LIST HISTORY_SERVER Configure the host before installing the RPM: Run the following commands on Unravel Server as root sudo useradd -g mapr unravel\nhadoop fs -mkdir \/user\/unravel\nhadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr MAPR_TICKETFILE_LOCATION \/srv\/unravel\/unravel_ctl Check available RAM to ensure availability: free -g Adjust RAM if needed. Only change this setting on the Unravel gateway\/client machine. For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 For example, edit \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 Restart MapR-FS (mfs). Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. For download location, see Download Unravel Software Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv and symlink after installing). Do it before the first install if there is insufficient space on the target paths \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM. sudo rpm -U unravel-4.*.x86_64.rpm*\n\/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm .rpm -U Run the specified await_fizup.sh await_fizup.sh DONE The installation creates the following items: which contains the executables, scripts, the master configuration file ( \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ are scripts for controlling services. You can use \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh User unravel Initial internal database and other durable states in \/srv\/unravel\/ The initial installation includes a bundled database; you can switch to AWS RDS for production. For details, see externally managed MySQL During initial installation, a bundled database is used. This can be switched to use an for production. Do host-specific post-installation actions. Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh Configure Unravel Server (Basic\/Core Options) Depending on your workload volume or kind of activity, you can enable additional daemons at this point. For instructions, see Creating Multiple Workers for High Volume Data Modify unravel.properties Edit \/usr\/local\/unravel\/etc\/unravel.properties # General Property\/Description Set By User Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting purposes. Optional - com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Required string - com.unraveldata.tmpdir Location where Unravel's temp file resides. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer - com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer - Property\/Descripton Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Unravel UI admin. Set during installation. string admin com.unraveldata.s3.batch.monitoring.interval.sec Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. sec 300 com.unraveldata.spark.eventlog.location maprfs:\/\/\/apps\/spark [empty] 500 yarn.resourcemanager.webapp.address YARN resource manager web address URL directory String (path) yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required) string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required) string - https.protocol Enable https access to Resource Manager com.unraveldata.metastore.databasePattern Opt string dname* oozie.server.url The Oozie server URL to be monitored by Unravel Required path - If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties Substitute the correct filename and principal: com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab Find and verify the principal the keytab by running this command: klist -kt KEYTAB_FILE Set the Linux file permissions of the keytab file to 500 ( chmod 500 unravel Run Unravel Daemons with Custom User If Sentry is Enabled, Add These Permissions: Define your own alt principal with narrow privileges. The alt principal can be admin unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory mapr or alt read+execute Spark event log hdfs:\/\/usr\/history\/done mapr or alt read+execute MapReduce logs hdfs:\/\/tmp\/logs mapr or alt read+execute YARN aggregation folder hdfs:\/\/user\/hive\/warehouse mapr or alt read+execute Obtain table partition sizes Hive Metastore access hive read+execute Hive table information Switch User Depending on your cluster security configuration, you will need to run the switch_to_user sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where x y switch_to_user Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60 Run the echo echo \"http:\/\/( UNRAVEL_HOST_IP This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Hive Metastore Access", 
"url" : "current/install/install-mapr/install-mapr-part1.html#UUID-158a6015-68d9-d965-8f65-c308f915cbcb_UUID-b89885d6-6c2e-2aa1-aecf-28fee748aa84", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR \/  \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver My...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: com.mysql.jdbc.Driver Req string - javax.jdo.option.ConnectionPassword Password used to access the data store. Req string - javax.jdo.option.ConnectionUserName Username used to access the data store. Req string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT Example: postgresql: jdbc:postgresql:\/\/congo.unraveldata.com:7432\/hive Req url " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "current/install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP can have the following values: spark-version for Spark 1.3.x spark-1.3 for Spark 1.5.x spark-1.5 for Spark 1.6.x s...", 
"body" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP can have the following values: spark-version for Spark 1.3.x spark-1.3 for Spark 1.5.x spark-1.5 for Spark 1.6.x spark-1.6 for Spark 2.0.x spark-2.0 for Spark 2.2.x spark-2.2 for Spark 2.3.x spark-2.3 can have the following values: hive-version for Hive 1.2.0 1.2.0 can have the following values: tez-version for Tez 0.8 0.8 Enable Additional Instrumentation on Unravel Server's Host sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only For Sensor Upgrade Only sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only For Dry Runs (Test\/Check Instrumentation) This does not change any configuration file. sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --dry-run Hive hook JAR is installed under: \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2\/MapR 6.0) are made to the following files, (< SPARK_VERSION X.Y.Z> \/opt\/mapr\/spark\/spark-<SPARK VERSION X.Y.Z>\/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-site.xml \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-env.sh \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/yarn-site.xml \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/mapred-site.xml \/usr\/local\/unravel\/etc\/unravel.properties Copy of original configuration is saved in same directory named *.preunravel \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml.preunravel Once the files are present on edge host where Unravel rpm is installed, you can tar tar For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR in \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-agent\/ oozie.libpath Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server. This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel UI. sudo -u someUser Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property> \n<name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name> <value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property>> Enable Additional Instrumentation on Other Hosts in the Cluster Run the shell script unravel_mapr_setup.sh Copy the newly edited yarn-site.xml Do a rolling-restart of HiveServer2 To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml Enable Instrumentation Manually Enable instrumentation manually by updating the following files: hive-site.xml hive-env.sh spark-defaults.conf hadoop-env.sh mapred-site.xml tez-site.xml Once the files are updated on the edge host where the Unravel RPM is installed, you can use the scp Update hive-site.xml Copy the content in \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-site.xml <\/configuration> <property>\n<name>com.unraveldata.host<\/name>\n<value>{UNRAVEL_HOST_IP}<\/value>\n<description>Unravel hive-hook processing host<\/description>\n<\/property>\n<property>\n<name>com.unraveldata.hive.hook.tcp<\/name>\n<value>true<\/value>\n<\/property>\n<property>\n<name>com.unraveldata.hive.hdfs.dir<\/name>\n<value>\/user\/unravel\/HOOK_RESULT_DIR<\/value>\n<description>destination for hive-hook, Unravel log processing<\/description>\n<\/property>\n<property>\n<name>hive.exec.driver.run.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.pre.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.post.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.failure.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<\/configuration> Update hive-env.sh In \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-env.sh export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION_ X.Y.Z Update spark-defaults.conf In \/opt\/mapr\/spark\/spark-SPARK_VERSION_X.Y.Z\/conf\/spark-defaults.conf spark.unravel.server.hostport UNRAVEL_HOST_IP:4043 spark.eventLog.dir maprfs:\/\/\/apps\/spark \n\/\/ the following is one line\nspark.history.fs.logDirectory maprfs:\/\/\/apps\/spark spark.driver.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_ X.Y.Z X.Y.Z Update hadoop-env.sh In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION_ X.Y.Z Update mapred-site.xml In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/mapred-site.xml <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.params<\/name>\n<value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host-ip unravel-host-ip Make sure the original value of yarn.app.mapreduce.am.command-opts Update tez-site.xml In \/opt\/mapr\/tez\/ tez-version <property>\n <name>tez.task.launch.cmd-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043<\/value>\n <description \/>\n<\/property>\n\n<property>\n <name>tez.am.launch.cmd-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043<\/value>\n <description \/>\n<\/property> " }, 
{ "title" : "Azure HDInsight", 
"url" : "current/install/install-hdi.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight", 
"snippet" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters....", 
"body" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters. " }, 
{ "title" : "Unravel VM", 
"url" : "current/install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelVM", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel VM", 
"snippet" : "enables you to monitor multiple HDInsight clusters on the same virtual network. Installing Unravel as a separate Azure VM...", 
"body" : " enables you to monitor multiple HDInsight clusters on the same virtual network. Installing Unravel as a separate Azure VM " }, 
{ "title" : "Unravel App", 
"url" : "current/install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelApp", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel App", 
"snippet" : "during or after creation of a single HDInsight cluster is the best way to quickly try out Unravel for your on-demand HDInsight cluster. The Unravel app resides on the edge node of the HDInsight cluster, and monitors only that one cluster. Installing Unravel as an Azure Marketplace app...", 
"body" : " during or after creation of a single HDInsight cluster is the best way to quickly try out Unravel for your on-demand HDInsight cluster. The Unravel app resides on the edge node of the HDInsight cluster, and monitors only that one cluster. Installing Unravel as an Azure Marketplace app " }, 
{ "title" : "Option 1: Install Unravel on a Separate Azure VM", 
"url" : "current/install/install-hdi/install-hdi-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM", 
"snippet" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel...", 
"body" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Create Azure Storage", 
"url" : "current/install/install-hdi/install-hdi-part1-option1/install-hdi-azure-storage.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Create Azure Storage", 
"snippet" : "This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: (\"Azure Storage\") Windows Azure Storage Blob B...", 
"body" : "This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: (\"Azure Storage\") Windows Azure Storage Blob By default HDInsight 3.6 uses Blob storage, which is a general-purpose storage type for Big Data. Blob storage is a key-value store with a flat namespace. It has full support for: Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets Low-cost, tiered storage High availability\/disaster recovery Unravel doesn't support encryption (SSL) with Blob storage (WASB). (ADLS v1) Azure Data Lake Storage generation 1 The other major option for Hadoop clusters is ADLS v1. ADLS is a hierarchical file system. It has full support for Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets File system semantics File-level security Scalability (ADLS v2) ( Azure Data Lake Storage generation 2 Preview mode ADLS generation 2 combines the features of Blob storage andADLS generation 1. Unravel has not been tested with ADLS v2 since it is still in preview mode. For an in-depth comparison, see https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-comparison-with-blob-storage The rest of this document refers to these storage types as Blob ADLS Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network for your resource group. This virtual network will be shared by your Hadoop cluster and the Unravel VM. Steps Log into https:\/\/portal.azure.com Click Storage accounts + Add On the Basics : Select the subscription type. Subscription : Select the resource group to associate with this storage instance. Resource Group : Enter a name, using lowercase letters and numbers. Storage Account Name : Select a data center region. Location : Select Performance Standard Premium storage uses magnetic disks and is cheaper. Standard Premium : Select your storage type: Account kind Blob Storage OR StorageV2 StorageV2 (ADLS v2) is still in preview mode and is not currently supported by Unravel. : Select your desired replication to either be local, or always available in the same zone, region, or replicated geographically. See more choices in the Advanced section. Replication : Only handles failures within the data-center. Durability guarantee is 11 9's. Locally redundant storage (LRS) : Handles failures in the data-center and zone, but not region. Durability guarantee is 12 9's. Only supported on ADLS v2. Zone-redundant storage (ZRS) : Handles failures in the data-center, zone, and region, but does not allow read-access in another region in a failure scenario. Durability guarantee is 16 9's. Geo-redundant storage (GRS) Read-access geo-redundant storage (RA-GRS) FIXLINK: Handles failures in the data-center, zone, region, and allows read-access in another region. Durability guarantee is 16 9's. : Only available for Access Tier Blob ADLS v2 hot Click the Advanced Set Secure transfer required Disabled Enabled Unravel doesn't support encryption (SSL) with Blob storage (WASB). For Virtual Networks, Click Review + create If your settings are correct, click Create Previous Related Resources . Finding Unravel Properties' Values in Microsoft Azure Azure - creating a storage account Difference between Replication types " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"url" : "current/install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"snippet" : "This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and s...", 
"body" : "This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network and network security group set up for your resource group. Your virtual network and subnet(s) must be big enough to be shared by the Unravel VM and the target HDInsight cluster(s). You must have root privilege You must already have created a storage system. For instructions, see Create Azure Storage You must have an SSH key pair. Your VM host must meet the requirements below. Support Chart and VM Requirements Azure HDI cluster compatibility HDInsight 3.6 Storage type: Blob (WASB) or ADLS v1 Limitations: Unravel currently only works with Blob (WASB) or ADLS v1. It does not support multiple Azure Data Lake Storage accounts or ADLS v2 (preview). HDP 2.6.5 Spark 1.6.3, 2.1.0, 2.2.0, 2.3.0 Limitations: Spark relies on the yarn-site configuration property yarn.log-aggregation.file-formats <property>\n <name>yarn.log-aggregation.file-formats<\/name>\n <value>TFile<\/value>\n<\/property> Hive 2.1 Kafka 0.10.0, Kafka 1.0, Kafka 1.1 (preview) Image (underlying operating system for the VM) RHEL 7 or CentOS 7.2 - 7.6 Note that the actual HDInsight Kafka\/Spark cluster can run another OS. CPU and RAM minimum requirements Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 Cores: 8 min RAM: 64 GB min Disk requirement min 100GB for \/srv \/srv Network requirement Unravel VM should be located in the same VNET and VSNET as the HDInsight cluster Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server HDFS ports open from Hadoop cluster to Unravel Server Hive MetaStore DB port open to Unravel Server for partition reporting For Oozie, port 11000 open to Unravel Server Security requirement Allows inbound ssh to the unravel VM Allows outbound Internet access and all traffic within the subnet (VSNET). Allows TCP port 3000 and 4043 to Unravel VM from HDInsight cluster must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Provision an Azure VM for Unravel Server Log into https:\/\/portal.azure.com Select Virtual machines + Add On the Basics : Select the subscription type. Subscription : Select the resource group to associate with this VM. The VM inherits configurations for lifecycle, permissions, and policies from this group. Resource Group : Enter a name, using only alphanumeric characters, \"-\", and \"_\". This value becomes the VM's hostname. Virtual machine name : Select a data center region for this VM. Note that not all VM types are available in all regions. Region : Select your redundancy (durability) settings. Availability options : Select the underlying operating system for the VM. As noted above, Unravel supports RHEL 7 or CentOS 7.2 - 7.6 only. Image (required): Select a Size standard, memory optimized E8s_v3 Select your VM's Authentication type Best practice is to authenticate using an SSH public key, which you can generate using ssh-keygen Set Inbound Port Rules: If you plan on allowing external access to Unravel UI, then select Allow selected ports HTTPS SSH Click Next: Disks On the Disks : For better performance in production, we recommend a OS disk type Premium SSD Standard SSD : We recommend using Advanced managed disks : Data disks If you don't have a disk ready, click Create and attach new disk Otherwise, click Attach an existing disk Click Next: Networking On the Networking It is imperative that the VM, the Azure storage, and the cluster(s) you plan to monitor are all on the same virtual network and subnet(s). (required): Select the appropriate virtual network for your cluster(s). Virtual network (required): Select a subnet with the appropriate address range based on the number of IPs you plan to have in your network. For more information, see Subnet https:\/\/www.aelius.com\/njh\/subnet_sheet.html Set this to NIC network security group: Basic Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the \" head node Add an inbound security policy to allow SSH access and 443 access to the Unravel node. The default security policy should allow all access within the VNET. Default rules start with a priority of 65000. Click Review + create Click Create It takes about 2 minutes to create your VM. When Azure completes the creation of your VM,click Go to resource Copy the VM's public IP address. Open an SSH session to your VM's public IP address and verify that your IP address is as expected: # ssh -i {ssh_key} {user}@{IP} Verify that eth0 on the new VM is bound to the private IP address shown in the Azure portal. # ifconfig\neth0 Link encap:Ethernet HWaddr 00:0d:3a:1b:c2:48\n inet addr:10.10.1.96 Configure the VM at First Login Install ntpd ntpd https:\/\/wiki.archlinux.org\/index.php\/Network_Time_Protocol_daemon # sudo su -\n# yum install ntp\n# ntpd -u ntp:ntp Disable Security Enhanced Linux (SELinux) permanently. This is important because HDFS maintains replication in different nodes\/racks, so setting firewall rules in SELinux will lead to performance degradation. # sudo setenforce Permissive In \/etc\/selinux\/config SELINUX=permissive SELINUX=permissive Install libaio.x86_64 Libaio has a huge performance benefit over the standard POSIX asynchronous I\/O facility because the operations are performed in the Linux kernel instead of as a separate user process. # sudo yum -y install libaio.x86_64 Install lzop.x86_64 Hadoop requires LZO compression libraries. # sudo yum install lzop.x86_64 Disable the firewall and check your iptable rules. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L Prepare the second disk (for example, \/dev\/sdc fdisk -l # sudo su -\n\n\nList all disks and partitions\nYou should see one called \"sdc\" if you attached a 500-1000 GB disk.\n# fdisk -l\n# fdisk \/dev\/sdc\np (list current partitions)\nn (new partition)\np (primary)\nKeep accepting rest of default configs.\nw (save)\n\nFormat the disk\n# \/usr\/sbin\/mkfs -t ext4 \/dev\/sdc\n\n\n# mkdir -p \/srv\n\n# DISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\n# echo $DISKUUID\n\nMount the disk on \/srv\n# echo \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n# mount \/dev\/sdc1 \/srv\n\nVerify the disk space\n# df -hT \/srv\n\nFilesystem Type Size Used Avail Use% Mounted on\n\/dev\/sdc1 ext4 197G 61M 187G 1% \/srv\n\n\nSet permissions for Unravel and symlink Unravel's directories to the \/srv mount\n# mkdir -p \/srv\/local\/unravel\n# chmod -R 755 \/srv\/local\n# ln -s \/srv\/local\/unravel \/usr\/local\/unravel\n# chmod 755 \/usr\/local\/unravel Create the hdfs hadoop # sudo useradd hdfs\n# sudo groupadd hadoop\n# sudo usermod -a -G hadoop hdfs Install the Unravel Server RPM on the VM Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM. For instructions, see Download Unravel Software # cd \/tmp\n# Note that the same RPM is used for both EMR and HDInsight.\n# curl -u USERNAME PASSWORD VERSION VERSION VERSION Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. The rpm .rpm -U # sudo rpm -U unravel-4.4.3.0-EMR-latest.rpm Run the await_fixups.sh If you're doing a routine upgrade, you can start all Unravel daemons, but don't stop or restart them until await_fixups.sh DONE # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\nDONE This installation creates the following directories, databases, and users: : The installation creates Directories \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties contains scripts for controlling the Unravel services \/etc\/init.d\/unravel_* can be used to manually stop, start, restart, and get the status of all daemons in the proper order. \/etc\/init.d\/unravel_all.sh Subsequent RPM upgrades don't change \/usr\/local\/unravel\/etc\/unravel.properties : User Users unravel : The initial bundled Postgres database and other durable state are put in DB \/srv\/unravel\/ : The master configuration file is in Config \/usr\/local\/unravel\/etc\/unravel.properties : All logs are in Logs \/usr\/local\/unravel\/logs\/ Grant access to Unravel Server Security Reminder Do not make Unravel Server UI TCP port 3000 accessible on the public Internet because doing so would violate your licensing terms. By default, a Public IP should be assigned to the Unravel VM . Create a security policy that allows SSH access to Unravel VM from your trusted network. For the Azure HDInsight cluster(s), it is required to allow port 443 (HTTPS) from Azure networks (or simply allow TCP port 443 from the outside). It is recommended that you use an SSH key to access the Unravel node. Modify Properties and Start Unravel Daemons Open an SSH session to the Unravel VM. # ssh -i ssh_private_key ssh_user UNRAVEL_HOST_IP Set correct permissions on the Unravel configuration directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh To find your cluster's HDInsight version, see # Find the version of HDP that is installed by checking the HDP symlink. Take the first 2 digits, such as 2.6\n# You can also check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions \n# hdp-select status | grep hadoop\nhadoop-client - 2.6.5.3005-27\n\n# Append this classpath based on the version you found\n# echo \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Run the \"switch user\" script. # \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem This is optional at this time but is required later. echo \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties Modify other properties using the guidelines in the table below: General Property\/Description Set By User Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting purposes. Optional - com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Required string - com.unraveldata.tmpdir Location where Unravel's temp file resides. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer - com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer - Kafka - Cluster Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - . com.unraveldata.ext.kafka. cluster bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. For each cluster in com.unraveldata.ext.kafka.clusters e.g., com.unraveldata.ext.kafka.East.bootstrap_servers=localhost:9092,localhost:9093 Required CSL - . com.unraveldata.ext.kafka. cluster jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. You must assign aliases the cluster nodes. Use a comma separated list for multiple nodes. e.g., unraveldata.ext.kafka.East.jmx_servers=kNode-1, kNode-2 Required CSL - . com.unraveldata.ext.kafka. cluster jmx. kNode-1 host The host for a node in the cluster. You must define a host for each node in each cluster. e.g., com.unraveldata.ext.kafka.East.kNode1=localhost com.unraveldata.ext.kafka.East.kNode2=localhost Required - . com.unraveldata.ext.kafka. cluster jmx kNode-1 port For each node in each cluster you must assign a port. e.g., com.unraveldata.ext.kafka.East.jmx.kNode1.port=5005 Required number - To locate Kafka and JMX ports: . Navigate to: Clusters → Kafka → Configuration → Ports and Addresses. Cloudera Manager Alternatively, you may lookup up the information in the broker nodes of Zookeeper CLI. : For Protocol and broker port navigate to: Kafka → Configs → Kafka Broker HDP JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template Update the following properties for an HDInsight cluster, depending on whether you're using Blob storage or ADLS Set these properties with values you obtain from Azure. For help in locating the right values, see Finding Unravel Properties' Values in Microsoft Azure For Blob storage, update: com.unraveldata.hdinsight.storage-account-name-1\ncom.unraveldata.hdinsight.primary-access-key\ncom.unraveldata.hdinsight.storage-account-name-2\ncom.unraveldata.hdinsight.secondary-access-key For ADLS, update: com.unraveldata.adl.accountFQDN\ncom.unraveldata.adl.clientld\ncom.unraveldata.adl.clientKey\ncom.unraveldata.adl.accessTokenEndpoint\ncom.unraveldata.adl.clientRootPath Restart Unravel Server Whenever you modify com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties The echo If you are using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: # sudo \/etc\/init.d\/unravel_all.sh restart\n# sleep 60 Log into Unravel UI Use the echo If you are using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: # echo \"http:\/\/( UNRAVEL_HOST_IP Create an SSH tunnel to access the Azure VM for Unravel's TCP port 3000. # ssh -i ssh_private_key ssh_user UNRAVEL_HOST_IP Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "HDInsight", 
"url" : "current/install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html#UUID-7ce82313-53cd-ea56-872c-45f568a03e1a_UUID-8345e996-7df6-39f2-3628-f6d009c6c1ee", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster \/  \/ HDInsight", 
"snippet" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 O...", 
"body" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster string - com.unraveldata.hdinsight.primary-access-key Primary storage account key string - com.unraveldata.hdinsight.storage-account-name-2 Optional for Spark when HDInsight is using blob storage Storage account name for the HDInsight cluster (same as account-name-1 string - com.unraveldata.hdinsight.secondary-access-key Secondary storage account key string - " }, 
{ "title" : "Azure Data Lake", 
"url" : "current/install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html#UUID-7ce82313-53cd-ea56-872c-45f568a03e1a_UUID-36f9d3f6-5e23-ebae-29b1-ae90c990d80c", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster \/  \/ Azure Data Lake", 
"snippet" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application reg...", 
"body" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application registration has to be created in the Azure Active Directory string - com.unraveldata.adl.clientKey An application access key which can be created after registering an application string - com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal string - com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. URL - " }, 
{ "title" : "Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"url" : "current/install/install-hdi/install-hdi-part1-option1/install-hdi-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"snippet" : "This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure Script action There are two kinds of Un...", 
"body" : "This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure Script action There are two kinds of Unravel \"script actions\" depending on the type of cluster. Note: For HDInsight cluster without Internet access, you can download these scripts and store them in your Azure blob storage and use the blob storage URI on the script action's Bash script URI Cluster Type Download path Supported HDI cluster(s) Apply to cluster node type(s) Hadoop, Hive, or Spark unravel_hdi_spark_bootstrap_3.0.sh Hadoop 2.7.3 Hive 2.1 Spark 2.0, 2.1, 2.3 Head Node, Worker Node, Edge node Kafka unravel_hdi_kafka_bootstrap.sh Kafka 0.10.0, Kafka 1.0.0, Kafka 1.1.0 Head Node Checks before running script action Read the latest documentation on the ports required by HDInsight: https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-hadoop-port-settings-for-services Ensure Unravel service is running on Unravel VM and ports 3000 and 4043 are reachable from the Azure HDInsight cluster master node before running the the Unravel \"script action\" script. For example, # ssh -i ssh_key ssh_user UNRAVEL_HOST_IP UNRAVEL_HOST_IP Depending on the type of cluster you are deploying, follow one of these options: Option A: Deploy a new cluster Option B: Configure an existing Hadoop, Hive, or Spark cluster Option C: Configure an existing Kafka cluster Prerequisites You must already have an Unravel VM on Azure HDInsight running and the Unravel UI available on port 3000. For instructions, see Step 1: Install Unravel Server for Azure HDInsight Cluster If you plan to create a cluster, you must have the following information ready: Virtual Network and subnet of the Unravel VM Your Azure Storage details. For storage setup, see Create Azure Storage Option A: Deploy a New Cluster Log into the Azure portal ( https:\/\/portal.azure.com Select HDInsight cluster In the dialog box, enter the details for your desired cluster type, topology, OS, and so on. In the Security + networking In the Storage In the Cluster size : In the Optional Script action In the Summary - Confirm configurations Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Log into the Azure portal ( https:\/\/portal.azure.com Select HDInsight cluster Select the Hadoop, Hive, or Spark cluster that you want to apply the Unravel \"script action\" script to. Click Script actions Submit new Enter the following information. Script type Custom Name (or any name to identify this script action run) unravel-script-01 Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Node type(s) Head, Worker, Edge (only if you have deployed edge node) Parameters --unravel-server unravel_server_private_ip spark_version For example, --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 To undo the changes use the --uninstall For example, --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 --uninstall Persist this script action Checked. Note that persistence only applies on new Head and Worker nodes Click Create Option C: Configure an Existing Kafka Cluster Log into the Azure portal ( https:\/\/portal.azure.com Select HDInsight cluster Select the Kafka cluster that you want to apply the Unravel \"script action\" script to. If the Kafka cluster has no Internet access, download the HDInsightUtilities-v01.sh For example, wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click Script actions Submit new Script type Custom Name (or any name to identify this script action run) unravel-script-01 Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Node type(s) Head Parameters --unravel-server unravel_server_private_ip For example, --unravel-server 10.10.1.10:3000 Persist this script action Checked. Note that persistence only applies on new Head nodes Click Create After the Kafka script action script completed successfully, ssh to the Kafka cluster's \"head node\" and append the content of \/tmp\/unravel\/unravel.ext.properties \/usr\/local\/unravel\/etc\/unravel.properties unravel.ext.properties # Adding Kafka properties\ncom.unraveldata.ext.kafka.clusters=<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.bootstrap_servers=wn0-<cluster_name>:9092,wn1-<cluster_name>:9092\ncom.unraveldata.ext.kafka.<cluster_name>.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.host=wn0-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.host=wn1-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.port=9999 Unravel VM must have access to the Kafka worker nodes' broker port 9092 and Kafka JMX port 9999 After updating the Kafka properties, restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart Troubleshooting Tips From the Azure portal, you can check if a script action finished successfully by checking the SCRIPT ACTION HISTORY If script action process fails, you can check the error messages from the HDInsight cluster's Ambari dashboard, which has a balloon next to the cluster name on the top menu bar with the recent operations. Click Ops run_customscriptaction run_customscriptaction The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new \"script action\" script with a different name. " }, 
{ "title" : "Option 2: Install Unravel's Azure Marketplace App", 
"url" : "current/install/install-hdi/install-hdi-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Option 2: Install Unravel's Azure Marketplace App", 
"snippet" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure por...", 
"body" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure portal. Select the HDInsight service. Create a new cluster: For cluster type, select Spark For version 2.1 Enter the access credentials for the Spark cluster. Click Next. Set Up a Storage Account for the Cluster Create a new storage account or use existing one. Fill in the storage account information for the Spark cluster. Find the Unravel App on Azure Marketplace Enter UNRAVEL Available applications Click OK Click Create Accept the terms of use and privacy policy. Click Next Launch the Cluster and Unravel App Review the summary. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. Click Create Find the URL of the Unravel App After the Unravel app and the Spark2 cluster have launched successfully, go to the Azure portal and select the HDInsight service. Select the Spark2 cluster. Click Applications Click unravel-edgenode The Unravel HDInsight app's webpage URL is displayed. In most cases, the URL looks like https:\/\/clusterName-unr.apps.azurehdinsight.net\/ Log into Unravel UI In your web browser, navigate to the Unravel app's URL, https:\/\/clusterName-apps.azurehdinsight.net. The Unravel login screen appears. Log in with username admin The dashboard appears. (Optional) Start\/Stop Unravel Daemons Use ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Enter Your License By default Unravel app doesn't contains any license keys, and runs without any issue during the initial 30 days trial period. To continue using Unravel app and technical support, contact our sales team. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Get Started with Unravel UI See the Unravel User Guide Get Started with Unravel API Unravel provides REST API for some operations. To try the API, click the API An API page with available command options are displayed and explained. You can try the API by clicking \"Try it out\" ? Execute buttons as shown below. From the Unravel user interface, trying out the API results in an error \"TypeError: Failed to fetch\" because the generated curl Copy the generated curl Modify it to include default user credentials. For example: ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n Re-send it using the HTTPS protocol. The response body is in JSON format. The date field is in epoch time. {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "Upgrading the Unravel VM or App", 
"url" : "current/install/install-hdi/adv-unravel-servers-sensors-upgrading-unravel-vm-or-app.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Upgrading the Unravel VM or App", 
"snippet" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll a...", 
"body" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this requires you to re-submit the Unravel action scripts to head, worker, and edge nodes. " }, 
{ "title" : "Finding Unravel Properties in Azure", 
"url" : "current/install/install-hdi/install-hdi-mapping-properties-fix.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Finding Unravel Properties in Azure", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Using Azure HDInsight APIs", 
"url" : "current/install/install-hdi/install-hdi-azure-api-fix.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ Azure HDInsight \/ Using Azure HDInsight APIs", 
"snippet" : "Submit a Script Action: : On a host\/docker container, Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run t...", 
"body" : "Submit a Script Action: : On a host\/docker container, Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run the script action. First, refer to the script you want to run and ensure you have its proper parameters. You may need to remove any \"--\" from the parameters. azure hdinsight script-action create $CLUSTER -g $RESOURCEGROUP -n $SCRIPTNAME -u $SHELLSCRIPT -p 'unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH' -t \"headnode;workernode;edgenode\" -g = Resource Group name -n = Name of this script action task -u = script path -p = paramaters -t = node types For example, azure hdinsight script-action create DEVCLUSTER -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Create an Edge Node: An Edge Node 1. Determine which ARM template and parameter file to download to the workstation that contains Azure CLI. A. Edge node that also runs the Unravel script ( recommended ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json Parameter file https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json OR B. Edge node that only simple a emptynode-setup.sh script ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json Parameter file https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json 2. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. curl <file> -o <name.json> 3. Modify the VM type, parameters, Kafka\/Spark version, etc. For example, In the ARM Template file, edit these fields as appropriate. \"vmSize\": \"Standard_D3_v2\" \"parameters\": \"unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH\" \"applicationName1\": \"$NEW_EDGE_NODE_HOSTNAME\" In the Parameter file, modify the cluster name. E.g., \"clusterName\": {\n \"value\": \"$MY_CLUSTER_NAME\"\n} 4. Validate template before deployment. $ az group deployment validate --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json { \"error\": null, ... \"provisioningState\": \"Succeeded\", ... } 5. Create the edge node. This should take 10-15 minutes to run since it has to provision a VM and install the Hadoop binaries. $ az group deployment create --name deploymentname --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json 6. Verify it was added to Ambari. Auto-Scaling: HDInsight allows you to resize your cluster up\/down to meet your current demands. 1. From the Azure portal, navigate to HDInsight Clusters <Your Cluster> Cluster Size 2. Enter your desired number of workers and validate that you have enough resources for your resource group and region (based on any quotas). Click the Save button, and HDInsight will take the appropriate action. Down-size: Will run the \" Decommission Up-size: Will provision new VM, install the Hadoop bits, and add the worker components (DataNode, NodeManager, and potentially HBase RegionServer). Notice that if the Unravel script action was also \"persisted\" to run on \"worker nodes\", then new VMs will automatically run a custom command for the Unravel bootstrap script. " }, 
{ "title" : "MySQL", 
"url" : "current/install/install-mysql.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Install and Configure MySQL for Unravel", 
"url" : "current/install/install-mysql/install-mysql-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ Install and Configure MySQL for Unravel", 
"snippet" : "To use MySQL with Unravel, follow these steps before and after installing the Unravel RPM. Do Pre-RPM Installation Steps Do the following steps before installing the Unravel RPM. Install MySQL server 5.7 and database. On CentOS 6: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch...", 
"body" : "To use MySQL with Unravel, follow these steps before and after installing the Unravel RPM. Do Pre-RPM Installation Steps Do the following steps before installing the Unravel RPM. Install MySQL server 5.7 and database. On CentOS 6: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm\nsudo yum install yum-utils\nsudo rpm -ivh mysql80-community-release-el6-1.noarch.rpm\nsudo yum-config-manager --disable mysql80-community\nsudo yum-config-manager --enable mysql57-community\nsudo yum install mysql-community-server On CentOS 7: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el7-1.noarch.rpm\nsudo rpm -ivh mysql80-community-release-el7-1.noarch.rpm\nsudo yum-config-manager --disable mysql80-community\nsudo yum-config-manager --enable mysql57-community\nsudo yum install mysql-community-server On SELinux: If you are installing MySQL on an SELinux host and are not using the default datadir Installing MySql in Enforcing Mode Configure and Start MySQL Server Stop MySQL server if it is running. sudo service mysqld stop Backup old InnoDB log files to a directory of your choosing, { Backup_Path \/var\/lib\/mysql\/ib_logfile mv \/var\/lib\/mysql\/ib_logfile* {Backup_Path}\n\n\/\/ OR\n\n# rm -rf \/var\/lib\/mysql\/ib_logfile* Append the following properties at the end of [mysqld] \/etc\/my.cnf datadir \/srv\/unravel\/db_data key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M max_connections = 500 max_connect_errors = 2000000000 open_files_limit = 10000 port-open-timeout = 121 expire-logs-days = 1 character_set_server = utf8 collation_server = utf8_unicode_ci innodb_open_files = 2000 innodb_file_per_table = 1 innodb_data_file_path = ibdata1:100M:autoextend # The innodb_buffer_pool_size depends on load and cluster size. # On a dedicated machine, it can be 50% of the RAM size. # Using 1G is the absolute minimum. For a large cluster, we use 48G. innodb_buffer_pool_size = 4G innodb_flush_method = O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency = 20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format = mixed # if SSD disk is used uncomment the line below #innodb_io_capacity = 4000 Ensure MySQL server starts at boot. CentOS 6 sudo chkconfig mysqld on CentOS 7 sudo systemctl enable mysqld Start MySQL server. CentOS 6 sudo service mysqld start CentOS 7 sudo systemctl start mysqld Check disk space used by MySQL's datadir from the MySql configuration file (eg., ( \/etc\/my.cnf sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs du -sh Check available file system disk space for MySQL's datadir from the MySql configuration file (eg., \/etc\/my.cnf) sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs df -h Install MySQL JDBC Driver Download MySQL JDBC driver to \/tmp wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz to cd \/tmp cd \/tmp\ntar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz Do Post-RPM Installation Steps After installing the Unravel RPM, complete the following steps. Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Configure Unravel to connect to the MySQL server. Run mysql mysql\nmysql> CREATE DATABASE unravel_mysql_prod;\nmysql> CREATE USER 'unravel'@'localhost' IDENTIFIED BY '<password>';\nmysql> GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'localhost'; Update the following properties in \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.username=unravel\nunravel.jdbc.password= password If MySQL JDBC driver is installed, replace jdbc:mariadb jdbc:mysql unravel.jdbc.url=jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod Create schema for Unravel tables. sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create default admin for Unravel UI. \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Once you have successfully configured Unravel to use the MySQL database, ensure that the unravel_pg service unravel_pg stop; chkconfig unravel_pg off " }, 
{ "title" : "Move MySQL to a Custom Location", 
"url" : "current/install/install-mysql/install-mysql-move.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ Move MySQL to a Custom Location", 
"snippet" : "Depending on your deployment, follow the steps in one of the sections below. If you're using Unravel bundled MySQL, see Move a Bundled MySQL If you're using an external MySQL, see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another ho...", 
"body" : "Depending on your deployment, follow the steps in one of the sections below. If you're using Unravel bundled MySQL, see Move a Bundled MySQL If you're using an external MySQL, see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another host, you need to install MySQL first on the new host. For instructions, see here " }, 
{ "title" : "Move a Bundled MySQL", 
"url" : "current/install/install-mysql/install-mysql-move.html#UUID-820e9d12-82ab-0bfe-837e-281353506526_MoveaBundledMySQL", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ Move MySQL to a Custom Location \/ Move a Bundled MySQL", 
"snippet" : "The daemon user must Perform a slow shutdown ( Get DB root password from \/root\/unravel.install.include \/root\/unravel.install.include.prev The example below uses *.include grep 'DB_ROOT_PASSWORD' { \/root\/unravel.install.include | \/root\/unravel.install.include.prev } Run the following commands to set ...", 
"body" : " The daemon user must Perform a slow shutdown ( Get DB root password from \/root\/unravel.install.include \/root\/unravel.install.include.prev The example below uses *.include grep 'DB_ROOT_PASSWORD' { \/root\/unravel.install.include | \/root\/unravel.install.include.prev } Run the following commands to set MySQL clean shutdown. \/usr\/local\/unravel\/mysql\/bin\/mysql -uroot --port=3316 --host=127.0.0.1 -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop unravel_db \/etc\/init.d\/unravel_db stop Back up MySQL database folder \/srv\/unravel\/db_data cd \/srv\/unravel # tar cvf unravel_db_data.tar db_data\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION cp unravel_db_data.tar NEW_DB_LOCATION NEW_DB_LOCATION Update unravel.install.include .cnf In \/root\/unravel.install.include NEW_DB_LOCATION In \/usr\/local\/unravel\/mysql\/unravel_mysql.cnf innodb_data_home_dir innodb_log_group_home_dir NEW_DB_LOCATION " }, 
{ "title" : "Move an External MySQL", 
"url" : "current/install/install-mysql/install-mysql-move.html#UUID-820e9d12-82ab-0bfe-837e-281353506526_MoveanExternalMySQL", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ Move MySQL to a Custom Location \/ Move an External MySQL", 
"snippet" : "The MySQL user must Perform a slow shutdown ( Run the following commands to set MySQL clean shutdown. mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon service mysqld stop Backup MySQL database folder \/var\/lib\/mysql cd \/var\/lib # tar cvf unravel_db_data.tar mysql\/ Restore M...", 
"body" : " The MySQL user must Perform a slow shutdown ( Run the following commands to set MySQL clean shutdown. mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon service mysqld stop Backup MySQL database folder \/var\/lib\/mysql cd \/var\/lib # tar cvf unravel_db_data.tar mysql\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION cp unravel_db_data.tar NEW_DB_LOCATION Update MySQL cnf file In \/etc\/my.cnf NEW_DB_LOCATION " }, 
{ "title" : "MySQL Partitioning and Data Migration", 
"url" : "current/install/install-mysql/install-mysql-partition.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ MySQL Partitioning and Data Migration", 
"snippet" : "MySQL is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySQL configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upg...", 
"body" : "MySQL is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySQL configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upgrade Unravel Server. Stop all daemons. \/etc\/init.d\/unravel_all.sh stop Make sure MySQL is running. Perform the data migration. For external MySQL servers you might need to change the chunk size to 100 or 1000. \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh migration:migration_partitioning During migration process the status of the tables is written to stdout Start migration: migration_partitioning\nStart table rows calculation...\nTable blackboards_old has 0 rows total\nTable impala_queries_old has 0 rows total\nTable jobs_old has 205437 rows total\nTable event_instances_old has 0 rows total\nTable oozie_workflow_jobs_old has 0 rows total\nTable hive_queries_old has 0 rows total\nTable rows calculation finished.\nMigrating records for table: blackboards, chunk size: 10000, chunk count: 10...\nMigrated records for table: blackboards, rows total: 0, chunk size: 10000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: impala_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: impala_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 5000, chunk size: 500, chunk count: 10, finished: false, time: 32 seconds, progress 2,43 %\nMigrating records for table: event_instances, chunk size: 50000, chunk count: 10...\nMigrated records for table: event_instances, rows total: 0, chunk size: 50000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: oozie_workflow_jobs, chunk size: 1500, chunk count: 10...\nMigrated records for table: oozie_workflow_jobs, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: hive_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: hive_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 10000, chunk size: 500, chunk count: 10, finished: false, time: 33 seconds, progress 4,87 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10... When all the tables have been successfully migrated you see: Migration: migration_partitioning is finished During the process a temporary migration_partitioning The chunks 1 \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql> select * from migration_partitioning;\n+---------------------+--------------------+--------+----------+\n| migr_table_name | lowest_id_migrated | chunks | finished |\n+---------------------+--------------------+--------+----------+\n| blackboards | 11018258 | 119 | 0 |\n| event_instances | 0 | 1 | 1 |\n| hive_queries | 0 | 1 | 1 |\n| impala_queries | 0 | 1 | 1 |\n| jobs | 240884 | 100 | 0 |\n| oozie_workflow_jobs | 0 | 1 | 1 |\n+---------------------+--------------------+--------+----------+\n\n To see the oldest migrated records per each table you can use the following MySQL query. \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql>\nselect \"blackboards\" as \"table_name\", min(created_at) as \"oldest_migrated_record\" from blackboards\nunion select \"event_instances\", min(created_at) from event_instances\nunion select \"hive_queries\", min(created_at) from hive_queries\nunion select \"impala_queries\", min(created_at) from impala_queries\nunion select \"jobs\", min(created_at) from jobs\nunion select \"oozie_workflow_jobs\", min(created_at) from oozie_workflow_jobs;\n+---------------------+------------------------+\n| table_name | oldest_migrated_record |\n+---------------------+------------------------+\n| blackboards | 2018-08-30 00:57:45 |\n| event_instances | 2018-08-30 00:57:55 |\n| hive_queries | 2018-08-30 00:59:10 |\n| impala_queries | NULL |\n| jobs | 2018-08-30 00:57:50 |\n| oozie_workflow_jobs | 2018-09-07 10:25:22 |\n+---------------------+------------------------+ If the migration process is interrupted or killed, you can run shell script again. However, if the process has failed you must truncate and reset ID's During migration the original files were renamed to _old TableName migration_partitioning \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nDROP TABLE blackboards_old;\nDROP TABLE event_instances_old;\nDROP TABLE hive_queries_old;\nDROP TABLE impala_queries_old;\nDROP TABLE jobs_old;\nDROP TABLE oozie_workflow_jobs_old;\nDROP TABLE migration_partitioning;\nquit Restart daemons. \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Restarting a Failed Migration", 
"url" : "current/install/install-mysql/install-mysql-partition.html#UUID-aa105656-ef76-f548-7147-bcde7f4abe0e_RestartHowtorestartafailedmigration", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ MySQL \/ MySQL Partitioning and Data Migration \/ Restarting a Failed Migration", 
"snippet" : "Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning \/etc\/init.d\/unravel_all.sh stop \/usr\/local\/unravel\/install_bin\/db_access.sh truncate blackboards; truncate event_instances; truncate hive_queries; truncate impala_queries; truncate jobs; truncate oozie_wor...", 
"body" : " Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning \/etc\/init.d\/unravel_all.sh stop\n\n\/usr\/local\/unravel\/install_bin\/db_access.sh\n\ntruncate blackboards;\ntruncate event_instances;\ntruncate hive_queries;\ntruncate impala_queries;\ntruncate jobs;\ntruncate oozie_workflow_jobs;\n\ntruncate migration_partitioning;\n\ncall resetAutoIncrementId('blackboards_old', 'blackboards');\ncall resetAutoIncrementId('event_instances_old', 'event_instances');\ncall resetAutoIncrementId('hive_queries_old', 'hive_queries');\ncall resetAutoIncrementId('impala_queries_old', 'impala_queries');\ncall resetAutoIncrementId('jobs_old', 'jobs');\ncall resetAutoIncrementId('oozie_workflow_jobs_old', 'oozie_workflow_jobs'); Return to Step 3 and complete the remaining steps. " }, 
{ "title" : "OnDemand", 
"url" : "current/install/install-ondemand.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ OnDemand", 
"snippet" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on Unravel Server's host: | Operational Insights Queue Analysis | Operational Insights Cluster Optimization Cloud Reports | Data Insights Forecasting | Data Insights Small Files | Data Insights File Reports |...", 
"body" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on Unravel Server's host: | Operational Insights Queue Analysis | Operational Insights Cluster Optimization Cloud Reports | Data Insights Forecasting | Data Insights Small Files | Data Insights File Reports | Data Insights Top X " }, 
{ "title" : "Installation or Upgrade of OnDemand", 
"url" : "current/install/install-ondemand/install-ondemand-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ OnDemand \/ Installation or Upgrade of OnDemand", 
"snippet" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before insta...", 
"body" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before installing OnDemand. (Optional) 1. Install External MySQL on the Unravel Host You can skip these steps if you plan to use Postgres with OnDemand. Install MySQL. See MySQL Verify MySql is installed correctly by running netstats netstat -tunlp | grep :3306\ntcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld For fresh installations report_instances cat \/usr\/local\/unravel\/sql\/mysql\/20180517031500.sql | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh Run db_access.sh ondemand_tasks ondemand_sessions report_instances sudo \/usr\/local\/unravel\/install_bin\/db_access.sh\nmysql> show tables; Install the OnDemand Service on the Unravel Host Download the OnDemand package from https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/ \/tmp Contact Unravel Support for USERNAME PASSWORD For example, if your host operating system is Red Hat Enterprise Linux 7 (RHEL 7): cd \/tmp\ncurl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel7.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel7.tar.gz -u USERNAME PASSWORD For example, if your host operating system is Red Hat Enterprise Linux 6 (RHEL 6): cd \/tmp\ncurl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel6.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel6.tar.gz -u USERNAME PASSWORD Navigate to the the \/tmp ondemand cd \/tmp\nsudo rm -rf \/usr\/local\/unravel\/ondemand (Optional) If you want Unravel to generate Small Files reports and you've customized hive_properties.hive hive_properties.hive ondemand cp \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive \\\nondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive Extract the contents of the tarball. tar xvf Ondemand-4.5.0.0-GA-rhel<6|7>.tar.gz Run the installation script. sudo mv ondemand\/ \/usr\/local\/unravel\/\ncd \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\nchmod +rwx install\/ondemand_quick_install.sh\nsudo .\/install\/ondemand_quick_install.sh\nsudo \/etc\/init.d\/unravel_all.sh restart In unravel.properties Restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart If your host operating system is SELinux, you might get alerts like these after restarting Unravel Server, depending on your environment. You can ignore them: # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Howver, if you encounter problems, contact Unravel Support. Start the OnDemand Daemon Execute the following four commands in the order shown, replacing run_as_user run_as_group switch_to_user For RHEL 7.x: sudo systemctl stop unravel_ondemand.service\nsudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\nsudo service unravel_all.sh restart\nsudo systemctl start unravel_ondemand.service For RHEL 6.x: sudo service unravel_ondemand stop\nsudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\nsudo service unravel_all.sh restart\nsudo service unravel_ondemand start Confirm that the OnDemand service is running: You should see output similar to the example below. Process IDs will vary dynamically based on the number of processors in Unravel Server, number of current tasks, and so on. For RHEL 7.x: sudo systemctl status unravel_ondemand.service For RHEL 6.x: sudo service unravel_ondemand status You can also use ps ps -ef | grep ondemand | grep -v grep root 11159 1 0 Sep21 ? 00:00:00 su - -c bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11163 11159 0 Sep21 ? 00:00:00 -bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11450 11176 0 Sep21 ? 00:00:42 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/flask run hdfs 11452 11176 0 Sep21 ? 00:27:19 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11670 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11671 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11672 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 Enable Various OnDemand-based Reports or Features Cluster Optimization Reports - is enabled by default. There are no specific properties. Cloud Reports and Forecasting Reports - Requires extensive configuration. See cloud and forecasting Queue Analysis - is enabled by default. See queue analysis Small Files Reports and File Reports - are enabled by default. See enabling small files and file reports Top X Report- is enabled by default. See Top X Sessions - is enabled by default. See sessions " }, 
{ "title" : "General OnDemand Properties", 
"url" : "current/install/install-ondemand/install-ondemand-details.html#UUID-4f527dde-dafc-66d7-1857-11574dfc81e7_section-5c9fae168e908-idm46265902776336", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ OnDemand \/ Installation or Upgrade of OnDemand \/ General OnDemand Properties", 
"snippet" : "See...", 
"body" : "See " }, 
{ "title" : "Library Versions and Licensing for OnDemand", 
"url" : "current/install/install-ondemand/install-ondemand-libraries-licensing.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Installation Guides \/ OnDemand \/ Library Versions and Licensing for OnDemand", 
"snippet" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency ...", 
"body" : " Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency of FBprophet) fbprophet>=0.1.1 BSD scipy>=0.19.1 BSD seasonal>=0.3.1 MIT statsmodels>=0.8.0 BSD gatspy>=0.3 BSD 3-Clause numpy>=1.13.1 BSD PyAstronomy>=0.12.0 MIT python_dateutil>=2.6.1 Simplified BSD fastdtw>=0.3.2 MIT requests==2.20.1 Apache 2.0 seaborn>=0.8.1 BSD 3-Clause sqlalchemy==1.2.7 MIT License pymysql>=0.8.0 MIT psycopg2==2.7.6.1 LGPL, Version 3.0 cython 0.27.3 Apache License, Version 2.0 kombu 4.1.0 BSD 3-Clause \"New\" Thrift-sasl 0.3.0 Apache License, Version 2.0 " }, 
{ "title" : "Post-Installation Steps", 
"url" : "current/install-post.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Post-Installation Steps", 
"snippet" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If you want to use LDAP, see If you want to use SAML, see enable SAML authentication Configure the Set com.unraveldata.customer.organization . Configure Hive Metastore Permissions Set up ...", 
"body" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If you want to use LDAP, see If you want to use SAML, see enable SAML authentication Configure the Set com.unraveldata.customer.organization . Configure Hive Metastore Permissions Set up HBASE Configuration Configure Configure Configure the following yarn.ats.webapp.username yarn.ats.webapp.password yarn.timeline-service.webapp.address yarn.timeline-service.port " }, 
{ "title" : "Other Configuration Options", 
"url" : "current/install-post.html#UUID-6d82f0d9-86d3-5fa2-de6c-9064960c0b6d_id_PostInstallationSteps-OtherConfigurationOptions", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Post-Installation Steps \/ Other Configuration Options", 
"snippet" : ". Add more admins to Unravel UI . Add read-only admins . Create multiple workers for high volume data . Enable live monitoring of Spark Streaming applications adv-conf-custom-enable-live-streaming-spark to allow Unravel to generate the following reports: Install the OnDemand service Sessions Cluster...", 
"body" : " . Add more admins to Unravel UI . Add read-only admins . Create multiple workers for high volume data . Enable live monitoring of Spark Streaming applications adv-conf-custom-enable-live-streaming-spark to allow Unravel to generate the following reports: Install the OnDemand service Sessions Cluster Optimization Cloud and Capacity Forecasting. See Small Files and File reports. See Queue Analysis Top X See Custom Configurations Security Configurations " }, 
{ "title" : "User Guide", 
"url" : "current/uguide.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations....", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. " }, 
{ "title" : "Getting Started", 
"url" : "current/uguide/uguide-getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "current/uguide/uguide-getting-started.html#UUID-b3fb06e1-2e21-f098-7dd2-071d84f1cae6_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "The Use Case videos below use Unravel 4.2 Big Data Application Performance Management Examines optimizing Spark and using with Workflows How to Run Spark Reliably and Optimize Spark Performance How to Search for Applications and Optimize\/Tune a Hive Application How to Debug Failed Applications How t...", 
"body" : "[video] The Use Case videos below use Unravel 4.2 Big Data Application Performance Management Examines optimizing Spark and using with Workflows How to Run Spark Reliably and Optimize Spark Performance How to Search for Applications and Optimize\/Tune a Hive Application How to Debug Failed Applications How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Common UI Features", 
"url" : "current/uguide/uguide-common-ui-features.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Common UI Features", 
"snippet" : "Every page has the Unravel Title Bar. The pages available to you are listed on the left and the one you are viewing is underlined and noted below in the black bar. To the right there is a search box, Docs If you are an end-user restricted by Role Based Access Control About Logout If you are a unrest...", 
"body" : " Every page has the Unravel Title Bar. The pages available to you are listed on the left and the one you are viewing is underlined and noted below in the black bar. To the right there is a search box, Docs If you are an end-user restricted by Role Based Access Control About Logout If you are a unrestricted end-user or an admin, all the pages are available with possible read\/write restrictions. The pull-down menu has Manage About Logout If your admin has disabled Support If you can configure the date range time period cluster(s) When there are multiple tabs, click on the tab to display its contents. When detailed or further information is available open section ( To expand a section to the full width of the tile click on the double arrows displayed ( Clicking on the application name\/id\/workflow usually bring ups information on the application, fragment etc, e.g., the Spark Application Manager, table information, etc. Hovering over an auto action alert ( Lists\/Tables Can be sorted by a column, i.e., start time, in ascending or descending order. The sort column highlights the arrow indicating the sort order ( Clicking on a column being used reverses the sort order. If you can chose which columns to display a plus ( When applicable, the application status is color coded: Clicking on the app name\/id\/workflow usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. When applicable, the Notifications When relevant there is an Auto Actions\/Events column ( When an application has a parent a link to it will appear in the GoTo A block glyph ( Graphs Hovering over a line in a graph causes the information to be displayed in a text box ( \" Applications running at mm\/dd\/yy hh:mm:ss\" Clicking Show More ( If graph can be displayed based upon Group By Tags Metric Click If you can zoom in\/out of a diagram\/execution graph the magnifiers ( " }, 
{ "title" : "The Operations Page", 
"url" : "current/uguide/uguide-the-operations-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page", 
"snippet" : "The Operations Page provides a synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Details By default it opens showing Operations Dashboard See Common UI Features See Resource Metrics...", 
"body" : "The Operations Page provides a synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Details By default it opens showing Operations Dashboard See Common UI Features See Resource Metrics " }, 
{ "title" : "Dashboard", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-Dashboard", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applicat...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applications Tile The line graphs display the successful, failed, and killed jobs for the time period time increment cluster(s) Clicking on Open Section Applications Applications Finding Applications Running YARN Application Tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Clicking on Open Section Operations Usage Details Jobs here Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications Applications Finding Applications Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. A separate entry appears for each time a particular Auto Action was triggered. In the image below, the same auto action triggered at 23:56 and 2:58. Clicking an event\/alert brings up a Cluster Resource view ( Operations Usage Detail Infrastructure Add a New Auto Action Clear " }, 
{ "title" : "Resources Tile", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ResourcesTile", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Dashboard \/ Resources Tile", 
"snippet" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below...", 
"body" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below " }, 
{ "title" : "Usage Details", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ChartsUsageDetails", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details", 
"snippet" : "Usage Detail has six tabs: Infrastructure Jobs Nodes Impala Usage Kafka HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details For example, Unravel can pinpoi...", 
"body" : " Usage Detail has six tabs: Infrastructure Jobs Nodes Impala Usage Kafka HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details For example, Unravel can pinpoint the applications causing a sudden a spike in the total VCores or memory MB usage. This allows you to easily you drill down into these applications to understand their behavior. Whenever possible, Unravel provides recommendations and insights All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, e.g., change the date range, click on a point within in a graph. When disabled a Refresh Refresh By default the Usage Details tab opens showing the Infrastructure tab. For all charts, click on the menu bars ( Show more Reset Graph Infrastructure " }, 
{ "title" : "Infrastructure", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_N1553155703543", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Infrastructure", 
"snippet" : "This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, e.g., Application Type User Queue Clicking within a chart (1) displays the applications running for that point in t...", 
"body" : "This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, e.g., Application Type User Queue Clicking within a chart (1) displays the applications running for that point in time. You can chose how to display the bottom two graphs by clicking on the View By Showing View Showing x Infrastructure Application Type show more To View by use the Business Tags Showing " }, 
{ "title" : "Jobs", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_N1553155650893", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Jobs", 
"snippet" : "Graphs the running and accepted jobs as applicable. You can Group by...", 
"body" : "Graphs the running and accepted jobs as applicable. You can Group by " }, 
{ "title" : "Nodes", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_OperationPage--Nodes", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Nodes", 
"snippet" : "This chart graphs the Total = Total Active Unhealthy Where: : currently running and healthy nodes, and Active : currently running and unhealthy nodes. Unhealthy You can toggle the display of an item by clicking on its name....", 
"body" : "This chart graphs the Total = Total Active Unhealthy Where: : currently running and healthy nodes, and Active : currently running and unhealthy nodes. Unhealthy You can toggle the display of an item by clicking on its name. " }, 
{ "title" : "Impala Usage", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_N1553155840446", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Impala Usage", 
"snippet" : "Graphs memory MB consumption and Query Number. The # Queries Tags Group By...", 
"body" : "Graphs memory MB consumption and Query Number. The # Queries Tags Group By " }, 
{ "title" : "Kafka", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_N1553155826925", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Kafka", 
"snippet" : "Lists all the configured Kafka clusters. See Kafka Application Manager Clicking the cluster name brings detailed information about the Kafka Cluster...", 
"body" : "Lists all the configured Kafka clusters. See Kafka Application Manager Clicking the cluster name brings detailed information about the Kafka Cluster " }, 
{ "title" : "HBase", 
"url" : "current/uguide/uguide-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_N1553155816069", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ HBase", 
"snippet" : "Please see HBase Configuration Clusters View Clusters page lists all the available HBase clusters Click on a cluster name to bring up the cluster's information the HBase Cluster view. Cluster View This view is divided into four (4) sections. When an component's health is noted, hovering over it's he...", 
"body" : " Please see HBase Configuration Clusters View Clusters page lists all the available HBase clusters Click on a cluster name to bring up the cluster's information the HBase Cluster view. Cluster View This view is divided into four (4) sections. When an component's health is noted, hovering over it's health glyph brings up details, Cluster Information A bar at shows what cluster you are displaying with a pull-down which allows you to switch between clusters. Listed immediately below are the cluster metrics. You can choose to tab between clusters by choosing all cluster Region Servers Lists the the cluster regional services, with their KPI's and health. You can search on the region server by name. Click on the server's name to bring up its details. Region Servers KPI Graphs the regional server metrics, the graphs are linked with the table list below them. Click within a graph to see up the tables associated servers that point in time. Hover over a point to bring up a popup displaying the information for that point in time. Click Show More Tables List all the tables associated with the cluster, their KPIs and the table's health. Click on the table name to bring up its information. You can search for a table by name; any table with a name matching or containing the string is displayed. Region Server View Server, Operational, and OS Metrics are displayed. Hover over the metric for its description. For more information on the metrics see here Table View Table has two tabs, Table Region Table regionCount readRequestCount writeRequestCount Table Regions Lists all the regions with their KPIs and health. " }, 
{ "title" : "The Applications Page", 
"url" : "current/uguide/uguide-the-applications-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page", 
"snippet" : "The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron...", 
"body" : " The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron Unravel currently supports the the following application frameworks: Cascading\/Pig Hive (on Map-Reduce) Hive (on Tez) Impala Kafka MapReduce Tez Spark Native SparkSQL Spark Streaming Athena (preview) Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, etc. It takes significant expertise and effort to get to the root cause(s) of an application's problems. Unravel's Intelligence Engine provides insights into your application's run to help resolve its problems\/inefficiencies. These insights are called events Events & Insights The Applications Page has three tabs: Applications Workflows Sessions See Common UI Features See Resource Metrics " }, 
{ "title" : "Applications Tab", 
"url" : "current/uguide/uguide-the-applications-page/uguide-the-applications-page-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page \/ Applications Tab", 
"snippet" : "Note Click here By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for ...", 
"body" : " Note Click here By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application(s) in a variety of ways: The left sidebar allows you to filter you App Name App type Status Tags Queue User Cluster Duration Number of Events. By time period, If the job is part of a Hive query, Pig script, or a Workflow, a link to it is noted in the job's Go To Athena jobs are serverless and retrieved upon completion. Therefore, its Write Cluster ID Queue Go To " }, 
{ "title" : "Workflow Tab", 
"url" : "current/uguide/uguide-the-applications-page/uguide-the-applications-page-workflow.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page \/ Workflow Tab", 
"snippet" : "The layout of this window mirrors the Applications Workflow Manager Application Manager...", 
"body" : "The layout of this window mirrors the Applications Workflow Manager Application Manager " }, 
{ "title" : "Sessions Tab", 
"url" : "current/uguide/uguide-the-applications-page/uguide-the-applications-page-sessions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page \/ Sessions Tab", 
"snippet" : "Note Click here This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first prio...", 
"body" : " Note Click here This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliability: in attempting to reduce resources Unravelprioritizes memory allocation to ensure the application doesn't fail due to \"out of memory\" exceptions. Why use sessions when Unravel already offers insights and recommendations on an application's run? You direct the tuning goal. You can provide multiple runs of an application providing a larger data pool for Unravel to analyze. You can have Unravel apply the recommendations for you and run the newly configured application. You can see the effects, both positive and negative, the tuning has on an applications run. You can compare runs configurations. You can repeatedly tune the application until Unravel has no more recommendations. Your session is saved and can be run again, e.g., new runs added, cluster configuration changed. You can tune: Spark Hive on MapReduce Sessions can serve simply as a tool to compare two runs of the same application. The Sessions tabs opens displaying all current sessions sorted on Sessions Name Start Time Number of Apps The four (4) KPI's Duration IO vCore Seconds Memory Seconds Duration Cluster ID You can search for a session by name. Enter the string in the search box; any session name matching or containing the string will be displayed. " }, 
{ "title" : "Creating a Session", 
"url" : "current/uguide/uguide-the-applications-page/uguide-the-applications-page-sessions.html#UUID-e87bbec7-885d-a3f3-0bf4-8cd69a722ec8_id_TheApplicationsPage-CreatingaSession", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page \/ Sessions Tab \/ Creating a Session", 
"snippet" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Sessi...", 
"body" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Session. Application Type Tuning Goal App IDs +Add another App ID Add If you are tuning a Spark App you must supply the JAR path and Class Name. If you do not intend to use the \"Apply\" feature for sessions, you can enter \"none\" (without quotes) for JAR path and Class Name. Auto Tune Session You have the additional option to specify the maximum number of runs. If not specified, iterates continues until no recommendations are available. When specified, the iteration stops at the maximum number or lack of recommendations, whichever comes first. Via the Events Panel If an applications has events " }, 
{ "title" : "Session", 
"url" : "current/uguide/uguide-the-applications-page/uguide-the-applications-page-sessions.html#UUID-e87bbec7-885d-a3f3-0bf4-8cd69a722ec8_id_TheApplicationsPage-Session", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Applications Page \/ Sessions Tab \/ Session", 
"snippet" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applicat...", 
"body" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applications Right Tabs - Keeps a log of all the activity. See example above. Progress Tab - Expanded graphs of Duration, IO, Resources Trends - Allows you to compare two of the runs. Compare " }, 
{ "title" : "The Reports Page", 
"url" : "current/uguide/uguide-reports-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page", 
"snippet" : "Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. - provides the ability generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Operational Insights - provides data level insights including a snapshot of tables and...", 
"body" : " Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. - provides the ability generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Operational Insights - provides data level insights including a snapshot of tables and partitions over the last 24 hours and reports on disk capacity forecasting and small files. Data Insights - helps you plan your migration to the cloud by analyzing your on-prem cluster, three migration methods and the costs associated each method. Cloud Reports - lists all reports and attempts to generate a report, whether the reports were scheduled or run on ad hoc basis. Report Archives - lists all scheduled reports. Scheduled Reports - All user generated reports can be run on a scheduled or adhoc basis. Scheduling Reports The Reports page opens displaying Operational Insights See Common UI Features See Resource Metrics " }, 
{ "title" : "Operational Insights", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights", 
"snippet" : "- generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performa...", 
"body" : " - generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performance and provide fine tuning insights\/recommendations. Cluster Optimization - Generates a report of active queues for time frame. The report analyzes queue activity by apps, vcores and memory. Queue Analysis - shows the aggregated workload for all clusters. Cluster Workload When you specify a date range, a pull down menu appears on the right hand side of the Operational Insights Chargeback Application Type " }, 
{ "title" : "Chargeback", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1553887358177", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Chargeback", 
"snippet" : "The Chargeback Yarn and Impala tabs are identical expect that the reports are limited to Yarn and Impala jobs respectively. You can generate ChargeBack Group By Application Type User Queue Tags Application Type Donut graphs showing the top results for the Group by Charge back report showing costs, s...", 
"body" : "The Chargeback Yarn and Impala tabs are identical expect that the reports are limited to Yarn and Impala jobs respectively. You can generate ChargeBack Group By Application Type User Queue Tags Application Type Donut graphs showing the top results for the Group by Charge back report showing costs, sorted by the Group By choice(s), and List of Yarn applications running. " }, 
{ "title" : "Generate Charge Back Report", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702378671", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Chargeback \/ Generate Charge Back Report", 
"snippet" : "You can set the date range and clusters to use for the report in the Operational Insights Group By Group By User dept User dept VCore\/Hour Memory MB\/Hour Update Report CSV A new charge back report is generated each time you change the Group By must Update Report Charge Back Yarn...", 
"body" : "You can set the date range and clusters to use for the report in the Operational Insights Group By Group By User dept User dept VCore\/Hour Memory MB\/Hour Update Report CSV A new charge back report is generated each time you change the Group By must Update Report Charge Back Yarn " }, 
{ "title" : "Cluster Summary", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702408318", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Summary", 
"snippet" : "The Cluster Summary Applications User Queue User Applications You can sort applications on vCore or memory seconds. User Queue...", 
"body" : "The Cluster Summary Applications User Queue User Applications You can sort applications on vCore or memory seconds. User Queue " }, 
{ "title" : "Cluster Compare", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702424001", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Compare", 
"snippet" : "This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use Group By User Queue Time Range Compare With Range Any deviation in metrics across the time ranges is highlighted (3). A green red Time Compare With Group By...", 
"body" : "This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use Group By User Queue Time Range Compare With Range Any deviation in metrics across the time ranges is highlighted (3). A green red Time Compare With Group By " }, 
{ "title" : "Cluster Optimization", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702438660", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Optimization", 
"snippet" : "The OnDemand This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize your cluster throughput, resources, and performance. Currently this feature only supports Hive on MapReduce. You can these reports you can: Fine tune yo...", 
"body" : " The OnDemand This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize your cluster throughput, resources, and performance. Currently this feature only supports Hive on MapReduce. You can these reports you can: Fine tune your cluster to maximize its performance and minimize your costs. Compare your cluster's performance between two (2) time periods. Report are generated on an ad hoc or scheduled basis. All reports are archived and can accessed via the Reports Archive Download or Generate a Report Click Download JSON Reports Archive Click Generate New Report Date Range Run Running Run Generate New Report Click Schedule schedule your report Optimization Report The Report has three (3) sections. Contains the basic report information author, time run, and dates used to generate the report. Header KPIs Number of Jobs: per day average Number of vCore Hours: per day average Number of MapReduce Containers Percent used for Map Percent used for Reduce Amount of Memory from of MapReduce Containers Percent from Map containers Percent from Reduce containers The KPIs are a per-day average for the number of days in the report. In this case we generated a report for a two (2) day period. All the insights\/recommendations are based upon the analysis of all jobs, in this case113. Insights\/Recommendations This section contains a tab for each area, with the relevant properties under consideration for tuning. These are cluster wide properties and are the defaults for all applications. Applications, however, can override these properties on an application by application basis. : MapReduce mapreduce.map.memory.mb mapreduce.reduce.memory.mb mapreduce.input.fileinputformat.split.maxsize mapreduce.job.reduce.slowstart.completedmaps : Hive hive.exec.reducers.bytes.per.reducer hive.exec.parallel You can expand the insight tile to the full width of the window. Each tile is entitled with what's being tuned. Further below we go into greater detail on two of the insights to explain the contents. Insight\/Recommendations Tile Details Tune the size of the map containers Immediately below the title is the property to tune, in this case mapreduce.map.memory.mb Next (1) is the tuning suggestion (1460), the projected impact (High) and the effect on the current jobs. In this case the recommendation affects 51% of the total jobs. The final section has the analysis information. The default (2) is the current property value (8192) and the percent of the applications that are currently using that value (51%). Click on the As expected 51% of the jobs (58) used the default, while 33% (37) used 512MB with the remaining jobs distributed across the remaining values. This graph shows Unravel's analysis of the property potential values. It shows each candidate % of memory saved for the input workload % of jobs from the workload that would still run with the candidate When there are tuning instructions it is noted above the graph ( 3 Tune the number of the map containers In this example, there is a recommendation but no tuning suggestions since there is only a low chance of an impact. Tune the number of reduce containers in Hive queries This analysis has additional information in addition a tuning suggestion and instruction. Click on it to see further information. In this case, the information was simply informative. There are cases where tuning suggestions for specific apps are offered. " }, 
{ "title" : "Queue Analysis", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702453421", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Queue Analysis", 
"snippet" : "The OnDemand You can generate a report of active queues for all your clusters or a particular cluster. The report analyzes queue activity by applications, vcores, memory., and disk.  As with all reports. it can be generated on an ad hoc or scheduled basis. The tab opens displaying the last report, i...", 
"body" : " The OnDemand You can generate a report of active queues for all your clusters or a particular cluster. The report analyzes queue activity by applications, vcores, memory., and disk.  As with all reports. it can be generated on an ad hoc or scheduled basis. The tab opens displaying the last report, if any, generated. Reports are archived and can accessed via the Reports Archive Generate a New Report Click New Report Date Range Cluster Run Running Run Queue Analysis Started New Report Click Schedule Reports Archive Report If the report was successfully generated a light green bar appears and a table listing all the queues in existence during the time range is displayed. The table lists each queue with its KPIs average ( Apps Running VCores Memory Disk Filter By Click Applications VCore Usage Memory Usage Disk Usage Operations | Usage Details | Infrastructure Click on the expand arrows ( " }, 
{ "title" : "Cluster Workload", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1554702467640", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Workload", 
"snippet" : "Displays your cluster(s) yarn applications' workload across a date range using the following views: - by date, e.g., 10 October. Month - by hour regardless of date, e.g., 10.00 - 11.00. Hour - by weekday regardless of date, e.g., Tuesday. Day - by hour for a given weekday, e.g.,10.00 -11.00 on Tuesd...", 
"body" : "Displays your cluster(s) yarn applications' workload across a date range using the following views: - by date, e.g., 10 October. Month - by hour regardless of date, e.g., 10.00 - 11.00. Hour - by weekday regardless of date, e.g., Tuesday. Day - by hour for a given weekday, e.g.,10.00 -11.00 on Tuesday. Hour\/Day You can filter each view by App Count Vcores Hour Memory Hour To measure the Vcores or Memory Hour usage is straightforward; at any given point the memory or vcore is being used or it's not. The App Count is not a count of unique app instances The App Count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore an app can be counted multiple times in a view. on multiple dates, e.g., on 11 & 12 October (2 days) in multiple hours, e.g., 10pm, 11pm & 12am hours on multiple days, Thursday & Friday, and in multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day App Count) Sum(App Counts for dates representing the day) App Count for Wednesdays (10, 17 & 24 October) = 2492, and App Count across Hour\/Day We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. By default the tab opens in the Month App Count Date Range App Count Vcores Hour Memory Hour View By Hour Day Hour\/Day Average Sum See Drilling Down Hour, Day and Hour\/Day Graphs These graphs do not link jobs to any specific date at the graph level. For instance, the Hour at Day on Hour\/Day at a date Month By default each view opens using the metric selected for the prior view. For instance, if Vcores Hour Month Day Vcores Hour When the Date Range - aggregated sum of job count, vcore or memory hour during the time range (default view), or Sum - Sum \/ (# of Days in Date Range) Average Hour Breaks out information by hour. The interval label indicates the start, i.e., 2AM is 2-3AM. Hover over an interval for its details. Click on the interval to drill down into it. Day Displays the jobs run on a specific weekday. Hover over an interval for its details. Click on the interval to drill down into it. Hour\/Day This views displays the intersection of Hour and Day graphs. The Hour Day " }, 
{ "title" : "Month", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1553887547910", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Workload \/ Month", 
"snippet" : "Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is Previous Next...", 
"body" : "Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is Previous Next " }, 
{ "title" : "Drilling Down in a Workload View", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-operational-insights.html#UUID-fa32b155-7aec-0a97-f37a-490618adea1c_N1553886893670", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Operational Insights \/ Cluster Workload \/ Drilling Down in a Workload View", 
"snippet" : "Click on an interval to bring up its information. In our example, we selected 11 October in the Month App Count Click User Queue User Queue App Type: MR, User: HDFS, or User: ROOT, We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Appli...", 
"body" : "Click on an interval to bring up its information. In our example, we selected 11 October in the Month App Count Click User Queue User Queue App Type: MR, User: HDFS, or User: ROOT, We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Applications | Applications App Count Vcores Hour Memory Hour " }, 
{ "title" : "Data Insights", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights", 
"snippet" : "The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. Overview -gives a quick view into the tables' and partitions' size Details - Details - drills down into the tables. See Hive Metastore Configuration Hot Wa...", 
"body" : " The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. Overview -gives a quick view into the tables' and partitions' size Details - Details - drills down into the tables. See Hive Metastore Configuration Hot Warm Cold Configuration The last four provide disk management insights and help you manage your disk usage both in terms of capacity and cluster performance. In order to use Forecasting, Small Files, File Reports and Top X you must have the OnDemand Forecasting - forecasts needed disk capacity based upon past performance Small Files - generates a list of small files based upon user criteria File Reports - similar to Small Files, except canned reports for large, medium, tiny, and empty files. Top X - top X Hive or Spark applications with respect to cluster usage, longest duration, and most data I\/O. Click here " }, 
{ "title" : "Overview", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553160202758", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Overview", 
"snippet" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. Table KPI Partition The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Par...", 
"body" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. Table KPI Partition The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Partition KPIs sections contain: : Number of Tables\/Partitions accessed, # Accessed : Number of Tables\/Partitions created, # Created : Size of Tables\/Partitions created, and Size Created : Total Number of Tables\/Partitions currently in the system. Total Number The Table KPI's also contains: : Total number of queries accessing the tables, and Accessed Queries : Total Read IO due to accessing the tables. Total Read IO Donut Chart These display the Current Label Distribution Configuration " }, 
{ "title" : "Details", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553160223109", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Details", 
"snippet" : "The details tab has two sections, a graph and a table list. By default the graph uses the Total Users Total Users Total Users, Total Apps, Total Size Graph In this example, the first through third table are selected and graphed. Use the Metric Total Users lTotal Apps Total Size Reset Graph Total Use...", 
"body" : "The details tab has two sections, a graph and a table list. By default the graph uses the Total Users Total Users Total Users, Total Apps, Total Size Graph In this example, the first through third table are selected and graphed. Use the Metric Total Users lTotal Apps Total Size Reset Graph Total Users Table List You can Search Show All Read IO More Info Table Detail Download CSV Table Detail This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. lists both Hive and Impala queries The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed: Users # Apps, Size There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Total Users Total Apps, Total Size Application Detail Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition Size MR jobs By default the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View " }, 
{ "title" : "Configuration", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553871184841", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Details \/ Configuration", 
"snippet" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access this modal pane from the Data Details From the pull down menus: chose Age (days) Last Access (days) c...", 
"body" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access this modal pane from the Data Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save " }, 
{ "title" : "Forecasting (Disk Capacity)", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553160311526", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Forecasting (Disk Capacity)", 
"snippet" : "The OnDemand package must installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapol...", 
"body" : " The OnDemand package must installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapolate capacity trends allowing you to more effectively plan for, and allocate your disk resources. The tab opens displaying the last forecasting report, if any, generated. The graph displays the trend from the historical range start date to the forecast range end date (x-axis). The trend line (in blue) shows the lower, middle, and upper bounds of Unravel's prediction. The y-axis is determined by your actual physical disk capacity. The report parameters are listed above the table headings. Click New Report History (Date Range) Forecasting (number of days) Run Schedule While Unravel prepares to generate the report Run Running New Report New Report You can download the report currently displayed by clicking the Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. " }, 
{ "title" : "Small Files", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553160335882", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Small Files", 
"snippet" : "The OnDemand package must be installed to be able to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here for properties which control this report. Each small file is accessed by a single ma...", 
"body" : " The OnDemand package must be installed to be able to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here for properties which control this report. Each small file is accessed by a single mapper, therefore a large number of small files can lead to a large number of mappers. In turn, mappers are costly to run so applications using a large number of small files drive up your costs. This report helps you to identify users who create\/use an excessive amount of small files, allowing you to take corrective action such as: combine multiple files into large files, or notify, limit, or block\/ users who create\/use an excessive amount. in order to: correct and prevent future performance degradation, and lower your costs to run applications. The small file window by default opens with the last report that was generated, if any. The report parameters are listed above the table headings (1), and you can search the path list by string. The list is sorted in descending order of the total number of small files in the directory. Click Download CSV Click New Report : the average small file size in a directory. Uncheck the box to its right to enter a small file size to use instead. Average File Size (bytes) : which are in the directory. Minimum # of Small Files : is the maximum number of directories to display. # of Directories to Show : Advanced Options : minimum depth to start at, root + x descendants, i.e., 0=root, 1=root's children (\/one), etc. Min parent directory depth : maximum depth to end at, root + x descendants, i.e., 1=root's children (\/one), 2=root's grandchildren, (\/one\/two), etc. Max parent directory depth : determines how\/where the files are listed. Yes (default): lists the file in all it's ancestor's list. No: list file in it's directory list only. Drill down sub-directories Click Run Schedule While Unravel prepares to generate the report Run Running New Report New Report Click Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. File Reports The OnDemand package must be installed to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here and here for properties which control this report. This report is the same as Small Files except they are automatically generated using the File Reports properties. By default these reports are updated every 24 hours and are not archived. The default size for the files are: large file is any file with more than 100GB size, medium file is any file with 5GB - 10GB size, and tiny file is any file with less than 100KB size. Click on the size buttons ( Large Medium Tiny Empty Download CSV " }, 
{ "title" : "Top X", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-data-insights.html#UUID-0d1ec61a-9569-4ff2-b03e-4aa42347f487_N1553160956179", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Data Insights \/ Top X", 
"snippet" : "TheOnDemand package must be installed to use this report. This report lists the top X Hive or Spark jobs which have consumed the most: duration, and data I\/O, and cluster usage. Click New Report History (Date Range) Top X Run Running Run New Report The report includes up to X apps (when available). ...", 
"body" : " TheOnDemand package must be installed to use this report. This report lists the top X Hive or Spark jobs which have consumed the most: duration, and data I\/O, and cluster usage. Click New Report History (Date Range) Top X Run Running Run New Report The report includes up to X apps (when available). If the report has been successfully completed a light green bar appears noting \"Top X Completed Successfully\". The report displays the Hive jobs by default and notes the reports parameters. Click the Filter By Download JSON The display is composed of three (3) tiles: Applications The Applications tile lists the app total along with successful and failed app count. Three tables list the top X apps by consumption type: Most Duration Most I\/O Most Cluster Usage Duration App Parent Query Snippet Note: The applications in each table are not necessarily the same. The top X apps with the most duration are not necessarily the top X apps using the most I\/O. Resources and Data These tiles display the cumulative totals for the queries. The Data Read Write I\/O Resources Map Reduce Total Spark Slot Time " }, 
{ "title" : "Cloud Reports", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Overview", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540149781", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Overview", 
"snippet" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -an...", 
"body" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -analyzes your on-prem cluster usage, workload patterns, and your hosts' hardware specs. Cluster Discovery -estimates the cost of moving to the cloud. Unravel analyzes three migration strategies by cloud provider (Azure, EC2, and EMR). Migration Analysis : one-to-one mapping of each existing host’s Lift and Shift capacity : one-to-one mapping of each existing host’s Cost Reduction actual usage : unlike the other methods this is not a one-to-one mapping. Unravel analyzes your workload for the time period and bases it recommendations on that workload. Unravel provides multiple recommendations based on the resources needed to meet X% of your workloads. It determines the optimal assignment of VM types to meet the requirements while minimizing cost. This method is typically the most cost effective method. Workload Fit Cloud Reports is generated base upon an analysis of your cluster workload over a specific date range. You can use it to: analyze on-prem clusters running Ambari running HDP 2.5 - 2.6, or Cloudera Manager running CDH 5 and examine the costs of running your clusters on three cloud providers: Azure HDInsight, Amazon EC2, and Amazon EMR. Limitations Cloud reports has not been thoroughly tested for clusters running Kerberos. Your cluster must have at least seven days of metrics for Unravel to generate useful reports. The pricing in reports use a static list of VM Instance type specs. Azure HDInsight: https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/hdinsight\/ Amazon EC2: https:\/\/aws.amazon.com\/ec2\/instance-types\/ Amazon EMR: https:\/\/aws.amazon.com\/emr\/pricing\/ Clusters running MapR Control System are not supported. In order to use Cloud Reports you must have OnDemand cluster manager See here - a dashboard containing information about your on-prem cluster Cluster Discovery - generates a report comparing costs for multiple strategies and aggregates the results by the VM type. Cloud Mapping per Instance - generates a report comparing costs for multiple strategies and shows the mapping for each individual host. Cloud Mapping per Host " }, 
{ "title" : "Cluster Discovery", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552539868963", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Cluster Discovery", 
"snippet" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report s...", 
"body" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report successfully generated, if any. Click New Report . History (Date Range) Run Running Run New Report If the report has been successfully generated NEW REPORT NEW REPORT DOWNLOAD JSON On-Prem Cluster Identity This tile contains information about your cluster. Click on Hosts Host Summary The table lists the hardware specs for each host and can be searched by name. Cluster overall usage of Applications grouped by App Type, User, and Queue. The donut graphs display the top ten (10) of each category. Cluster Resource Availability & Usage These graph your cluster's CPU and memory. The average usage is listed on the right hand side of the title bar. Hover over the the parenthetical text next to the resource to see Unravel's analysis of your usage. Below we see the allocated CPU resource is \"Very under-utilized and over-provisioned\". Cluster Heatmap The heatmap shows the actual CPU\/Memory usage and capacity by a weekday and hour, e.g., Monday between 5 and 6 pm.Each time slot in the heat map represents how relatively " }, 
{ "title" : "Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540242292", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"snippet" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type...", 
"body" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type picked. " }, 
{ "title" : "Cloud Mapping per Instance", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540343800", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Instance", 
"snippet" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction...", 
"body" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction " }, 
{ "title" : "Cloud Mapping per Host", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540200429", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Host", 
"snippet" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on aver...", 
"body" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on average utilization. Cost Reduction -- a mapping based upon meeting SLA percentiles. Workload Fit The tab opens displaying the Lift and Shift Cost Reduction t Workload Fi Below we examine the three migration reports mapping the cluster to EMR with no specific instance picked. The Total Hourly Cost Lift and Shift Workload Fit Lift and Shift and Cost Reduction The report layout are exactly the same. The basis for the mapping recommendation is listed under the selection tabs, the hourly cost to map all hosts is aggregated and displayed in the upper right above the report table. The table has four columns: - in this case the on-prem host, Instance - the host's resources which were actually utilized, Actual Usage - the resources the host has available, Capacity - the cloud instance Unravel recommends mapping the host to, and Recommendation of the instance. Cost\/Hour Lift and Shift Lift and Shift is matching capacity, and therefore not attempting to minimize costs. You can see below the on-prem host is very underutilized. Since Unravel is matching capacity instance the on-prem host is mapped to will also be underutilized. This report mapped 20 hosts, only the first is shown. The hourly cost per instance is $5.69 for a Total Hourly Cost Cost Reduction Cost Reduction is again mapping each host to an instance (1-1), this time based on actual usage not capacity. Again we are only showing the first mapping. The EMR instance Unravel recommends is c3.8xlarge instead of m5d.24xlarge recommended above. The cost savings is significant compared to Lift and Shift; the Total Hourly Cost Workload Fit In this report, Unravel is not looking at individual host capacity\/usage but finding the optimal allocations of VM to meet a percentage of the workloads, specifically 80 - 100% in 5 point increments. For specifics details click on SLA x% mapping scroll through the window on the left size or click directly on the percentage. Below we see to meet 100% SLA – Unravel is recommends the same cloud server instance as in Lift and Shift, but instead of mapping to 1-1, it recommends using just six (6). The bar graph shows the SLA\/hour costs at a glance. You quickly can see there are costs saving from 100% to 95% (5%) and 95% to 90% (11%) but further reducing the SLA percentile results little to no savings. When comparing the total hourly cost of 100% SLA Workload Fit to Lift and Shift and Cost Reduction shows savings of 75% and 25% respectively. " }, 
{ "title" : "Scheduling Reports", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-schedulingreports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Scheduling Reports", 
"snippet" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule...", 
"body" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule " }, 
{ "title" : "Reports Archive and Scheduled Reports", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-archive-scheduled.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Reports Archive and Scheduled Reports", 
"snippet" : "Note Click here...", 
"body" : " Note Click here " }, 
{ "title" : "Reports Archive", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-archive-scheduled.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537890247", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Reports Archive and Scheduled Reports \/ Reports Archive", 
"snippet" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to vie...", 
"body" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to view and Example of Capacity Forecasting .csv file. Date Max Capacity Observed Capacity Trend Lower Trend Upper Trend 08\/23\/18 02:00 AM 3.172 kB 528.000 B 528.098 B 527.120 B 528.970 B 08\/29\/18 02:00 AM 3.172 kB 527.000 B 528.171 B 527.248 B 529.097 B 08\/28\/18 01:00 PM 3.172 kB 527.000 B 526.385 B 525.443 B 527.316 B 08\/27\/18 06:00 PM 3.172 kB 528.000 B 528.332 B 527.420 B 529.231 B 08\/29\/18 07:00 PM 3.172 kB 532.000 B 530.328 B 529.377 B 531.289 B 08\/26\/18 11:00 PM 3.172 kB 528.000 B 527.933 B 527.033 B 528.801 B 08\/26\/18 04:00 AM 3.172 kB 528.000 B 528.101 B 527.199 B 529.004 B 08\/22\/18 11:00 AM 3.172 kB 528.000 B 527.806 B 526.847 B 528.694 B 08\/24\/18 12:00 PM 3.172 kB 528.000 B 527.651 B 526.691 B 528.616 B 08\/26\/18 05:00 AM 3.172 kB 528.000 B 527.818 B 526.921 B 528.706 B 08\/29\/18 12:00 PM 3.172 kB 527.000 B 528.742 B 527.871 B 529.649 B 08\/28\/18 07:00 AM 3.172 kB 527.000 B 525.778 B 524.834 B 526.672 B Small File Report Small Files is a .csv file. The report is sorted on the number of file in descending order. Directory Number of Files Avg File Size Total File Size Min File Size Max File Size \/apps 26554 219.7 kB 5.6 GB 0 B 192.7 MB \/apps\/hbase 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data 14063 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data\/default 14044 378.0 B 5.1 MB 0 B 2.5 MB \/apps\/hive 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB \/apps\/hive\/warehouse 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB " }, 
{ "title" : "Scheduled Reports", 
"url" : "current/uguide/uguide-reports-page/uguide-reports-page-archive-scheduled.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537990066", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Reports Page \/ Reports Archive and Scheduled Reports \/ Scheduled Reports", 
"snippet" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close...", 
"body" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close " }, 
{ "title" : "The Application Managers", 
"url" : "current/uguide/uguide-apms.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Typical Application Manager's Layout", 
"url" : "current/uguide/uguide-apms/uguide-apms-layout.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Typical Application Manager's Layout", 
"snippet" : "See Common UI Features See Resource Metrics A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc. and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., ...", 
"body" : " See Common UI Features See Resource Metrics A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc. and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the parent's type. Clicking on it brings up its APM. If it is a running yarn job (MR, Tez, or Spark) there is an action box ( Unravel's Intelligence Engine can provide insights into an application and may provide recommendations, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Example The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) : notes the number of events the job had. Event icon No Events Event Panel & Insights notes the job type and status. The box is colored code to indicate as the application's status. Job icon: Next to the job name will be an auto actions glyph ( Job Name: : job number, owner, queue, cluster and start\/stop time. Job Information these vary by job type. KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation " }, 
{ "title" : "Cascading and Pig Application Managers", 
"url" : "current/uguide/uguide-apms/uguide-apms-cascading-pig.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Cascading and Pig Application Managers", 
"snippet" : "The only Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps By defaul...", 
"body" : "The only Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Left Tabs : Exceptions, errors, and warnings associated with this application. See here for an example. Gantt Chart Reusing topic #UUID-30559ede-3b59-31c6-1b76-c72d6f36ecb2 Right Tabs : Displays Map and Reduce task attempts by success, failed, and killed status.The data displayed is for the entire HIVE job. To see the details for a specific MapReduce task click on the job in the Navigation tab. The Pig APM above shows the Task Attempts. Task Attempts : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time is noted in the upper left hand corner. The computer slot usage is noted below the graph. Attempts " }, 
{ "title" : "Hive Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-hive.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Hive Application Manager", 
"snippet" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. By default the Hive ...", 
"body" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. By default the Hive APM opens displaying the Navigation Query For Hive queries that do not run using a Tez, LLAP, MapReduce, or Spark app, the duration shown by Unravel may be inaccurate because Hive does not call the Hive pre and post hooks correctly for these queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O : The number of YARN apps making up the query. Number of YARN apps Left Tabs : List all the MapReduce jobs associate with the query. Click on the job name to bring up job in the Navigation MapReduce Application Manager : Shows detailed information about the MapReduce jobs and their relationship with one another. Execution Graph The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other along some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). Click on a table name to bring up its details. See | Reports Data Insights Close Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. : Shows job sequencing using a gantt chart. Gantt Chart Lsts all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted for each job. The top line list the number of all jobs\/task. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Errors: Keywords : Lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr Right Tabs : Shows the Hive Query. See the Hive Application Manager Query window Copy Query A list tables accessed by Application. Tables: : Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the Task Attempts entire : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time the job started is listed in the upper left hand corner. The total Map and Reduce slot duration time is noted below the graph. Attempts " }, 
{ "title" : "", 
"url" : "current/uguide/uguide-apms/uguide-apms-hive.html#UUID-bb39f412-f037-c9c3-4df6-388509350c1e_N1553794396025", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Hive Application Manager \/ ", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Impala Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Impala Application Manager", 
"snippet" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Left Tabs : Displays a table with information about each fragment associated with this query. Click on Fragments More L ess This window shows the Fragment and it's KPIs. It defaults to the table of the Fragm...", 
"body" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Left Tabs : Displays a table with information about each fragment associated with this query. Click on Fragments More L ess This window shows the Fragment and it's KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking on the operator brings up the operator window. (See Operators Query Plan lists each instances with it's KPI's. Instance View: : Displays a list of all operators for all fragments. You can search the operators name. Click on the operator to display its details. Operators Scan HDFS details Aggregate Details Exchange Details : Charts the fragments and the time spent on each operation. Hover over a section see the operation and it's KPI's. Gannt Chart : Shows the query plan in fragment or operator view. Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. Query Plan : Lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr Right Tabs : Shows the query plan code. Click on Query Query Copy window : Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. Mem Usage " }, 
{ "title" : "", 
"url" : "current/uguide/uguide-apms/uguide-apms-impala.html#UUID-eddf98b1-01bf-1c2e-a09a-a6a13a43728d_N1553795269294", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Impala Application Manager \/ ", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Key Performance Indicators", 
"url" : "current/uguide/uguide-apms/uguide-apms-impala.html#UUID-eddf98b1-01bf-1c2e-a09a-a6a13a43728d_N1553795187640", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Impala Application Manager \/ Key Performance Indicators", 
"snippet" : ": The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Opera...", 
"body" : " : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Operators " }, 
{ "title" : "Kafka APM", 
"url" : "current/uguide/uguide-apms/uguide-apms-kafka.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Kafka APM", 
"snippet" : "The Kafka Application Manager provides Multi-Cluster support for monitoring: Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring.   displays the list of Configured Kafka Clusters. See Operations | Charts | Kafka Kafka Insights lagging or stalled Key Performance Indicat...", 
"body" : "The Kafka Application Manager provides Multi-Cluster support for monitoring: Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring.   displays the list of Configured Kafka Clusters. See Operations | Charts | Kafka Kafka Insights lagging or stalled Key Performance Indicators Bytes in\/sec Bytes out\/sec Messages in\/sec Total Fetch Requests per \/sec Number of Active Controller Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs Scroll down to see all the graphs. kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Consumer Group View Key Performance Indicators Number of Topics Number of Partitions The Topic lists displays the KPIs; when details are available a more info [empty] You can chose both the Partition Metric th offset Partition Details' The Kafka View has two tabs, Topic Detail Partition Detail Consumer Details' Kafka Topic Detail By default the Kafka Topic Detail Topic Detail Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Unravel provides auto-detection of lagging\/stalled Consumer Groups. It allow you to drill down into your cluster and determine which consumers, topics, partitions are lagging or stalled. Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is : if the Consumer commit offset for the topic partition is not increasing and lag is greater than zero. Stalled : if the Consumer lag for the topic partition is increasing consistently, and an increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). Lagging The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: : the consumer is working and is current. OK the consumer is working, but falling behind, or Warning: : the consumer has stopped or stalled. Error " }, 
{ "title" : "MapReduce Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-mapreduce.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ MapReduce Application Manager", 
"snippet" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sect...", 
"body" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O Tabs By default the MapReduce APM opens in the Graphs | Attempts : Has four (4) sub tabs. Graphs : Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in on the Map\/Reduce slot duration is noted below the graph. Attempts , Containers Vcores, : Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. Timeline The Timeline tab is divided into two sections: a Distribution Map Reduce a bottom table which lists either the tasks by stages on servers or the list of tasks and their associated KPIs' The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map Reduce Timeline Selected : The metrics, their definitions and values. Metrics Lists the available logs by Map, Reduce and Application Master. Click on the tab to see the listing for that type (Map, Reduce, or Application Master). Click on an item to see the log. Logs: The defined parameters and their values. Configuration: : Resource Usage Initially all the executors are displayed using the Metric Metric nly Show All Lsts all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted for each job. The top line list the number of all jobs\/task. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Errors: Keywords : Lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr " }, 
{ "title" : "Spark Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager", 
"snippet" : "Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. : corresponds to a spark action, e.g., count, take, foreach. Job : is comprised of transformations which can be pipelined, e.g., parallelize → filter → map The Job Stage The Spark Application Manager (AP...", 
"body" : " Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. : corresponds to a spark action, e.g., count, take, foreach. Job : is comprised of transformations which can be pipelined, e.g., parallelize → filter → map The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources. View your application as a RDD execution graph Drill into the source code from the stage tile, spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an application's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications. Optimize resource allocation for Spark driver and executors. Detect and fix poor partitioning. Detect and fix inefficient and failed Spark apps,. Tune JVM settings for driver and executors. Unravel provides insights into Spark applications and potentially tuning recommendations. There are multiple Spark application types and the Spark APM's information can vary by the application type. Currently Unravel distinguishes between: Scala, Java, and PySpark SQL-Query Streaming Regardless of the application type and how they are are submitted (e.g., from Notebooks, spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark Application Manager's Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). On the right side there are actions Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions, or insights into how to improve the application's run. When there are insights, a bar appears immediately below the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see Events and Insights The next section contains the Key Performance Indicators (KPIs) and general application information. : notes the number of events the Spark application had. If there were none, Event icon No Events Event Panel : notes the application's status and the window type (S-Spark, SJ-Spark Application and so on). The box is colored coded based upon its status. Application icon : notes the job type and status. The box is colored coded to indicate as the application's status. Job icon : Next to the job name will be an auto actions glyph Job Name : job number, owner, queue, cluster and start\/stop time. Job Information : these vary by job type. KPIs The last section, divided in half, has specific information related to the application. The sections for a specific Spark Application (e.g., Streaming) go into more detail. If the application is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Stream Common Tabs Except for the Spark streaming Left Tabs - Lists all errors associated with the application. The errors are color coded (fatal Errors Keywords listed. - Lists the critical logs that were collected for this Spark application. It is noted when no logs are available. Logs Click on the log name to see it, below is an excerpt of the executor-20 log. - Lists the configuration parameters and their values. The parameters vary according to app\/task\/job. The tab opens listing all the properties. The number of properties displayed is noted above the list. Conf You can narrow the list by choosing the configuration type to display; to see the spark version select metadata. Metadata and driver are selected in the example below and the list narrowed from 1042 to two (2) properties. Some properties appear in multiple categories, e.g., spark.executor.extraJavaOptions is listed under Memory, Driver and Executor. You can search by name; searching on yarn displays every property containing the word yarn. Click Reset Right Tabs - Displays the program's source code if available. Both the Spark Stage and Execution graph link to tab and display code associated with each. Note: Unravel allows you to upload Spark programs, see Program Uploading Spark Programs - Graphically and textually notes the number of tasks and their status. The donut graphs show the percentage of successful (green), failed (orange), and killed (red) tasks. The legend on the right lists the number of each. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination. Hovering over the chart tells you the percentage of each. Task Attempts - There are three (3) types of graphs. Graphs use \"wall clock time\" as opposed to computer usage time. Graphs : The number of running containers. Running Containers : Allocated vcores. Vcores : Allocated memory. Memory - Graphs the resources the application\/job\/stage consumed. By default the graph displays all executors using the metric Resource availableMemory Get Data You chose one or more series to display using the Select X Hovering over resource highlights it, clicking on it toggles the display, i.e., if currently displayed, the resource is removed from the graph. Conversely if you click the Only Show All Below is an example of the JSON when clicking on Get Data Scala, Java, PySpark, and SQL-Query Navigation Tab - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Tasks Read Write Stages Start Time While the application is running, the Status Tasks Stages Once completed the total number of the Tasks Stages Clicking on a job row brings up its details in a Job Block. You can open up the job even if it's running. The job block lists the KPIs Duration # of Stages Stages Gannt Metadata ID Status Start Time Duration Tasks Shuffle Read Shuffle Write Input Output Start Time Spark Stage This example is for Job 1 above, which is only 41% complete. Here the # of Stages Common Tiles " }, 
{ "title" : "Actions", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1553170011068", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Actions", 
"snippet" : "Click on When a yarn application is running you can kill or move it. After the application has stopped, you can load logs and diagnostics. Clicking Load Diagnostics Load Logs Logs...", 
"body" : "Click on When a yarn application is running you can kill or move it. After the application has stopped, you can load logs and diagnostics. Clicking Load Diagnostics Load Logs Logs " }, 
{ "title" : "Spark Job", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1553168728356", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark Job", 
"snippet" : "A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators : The wall-clock time it took to complete the job. Duration : Number of stages to the job. # of Stages It has three ...", 
"body" : "A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators : The wall-clock time it took to complete the job. Duration : Number of stages to the job. # of Stages It has three tabs: his tab is the default view. It lists the stages with their KPIs and is initially sorted on Stages Start Time - This tab again shows all the stages, graphically displaying the time spent in each. The stages are initially displayed in order of execution, i.e., first, second, ..., nth. Gannt Chart - Lists all the attributes and values for the jobs. Metadata " }, 
{ "title" : "Spark Stage", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611722669", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark Stage", 
"snippet" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators : The computer time it took to complete the job. Duration : Total input and output associated with the stage. Date IO It has two tabs, by default...", 
"body" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators : The computer time it took to complete the job. Duration : Total input and output associated with the stage. Date IO It has two tabs, by default it opens in the graph Graph - Displays the number of total tasks for the stage and number of total attempts made to run these tasks. The number of tasks is noted on the left side of the bar with the number of attempts on the right. The donut chart graphically displays the successful ( Task Attempt - Displays the stage's program details. Unravel extracts Source file name and call line. Click on the source file or line number to see program code. The Description shows the stage's Stack trace. The first line is always a call to the Spark library. The source file name and line number are extracted from the second line. Program Details Time Line Tab The Time Line tab has two sections, , and Distribution Charts Three tabs below the chart: , Time Line and Timeline Breakdown . Selected Tasks Time Line This tab has two sections, , and Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown Selected Tasks Be default the Time Line Distribution Charts ShuffleMap Disk Bytes Spilled Memory Bytes Spilled Records The lower section opens displaying the Time Line Timeline Breakdown This is useful to identify bottlenecks. For each executor used in the current stage, multiple metrics are graphed: Scheduler Delay Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Using the ratio of Executor Computing Time performing actual work thrashing, or waiting for scheduling. - A list of tasks, if any, for the stage. Selected Tasks " }, 
{ "title" : "Spark - Scala, Java, and PySpark", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1553168833366", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark - Scala, Java, and PySpark", 
"snippet" : "Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the applicatio...", 
"body" : " Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Partitions\/Tasks Read Write # Stages - The execution graph shows a RDD DAG of the application. The application's Execution RDDs Operations Program If the program tab displays the code it is linked with the DAG. If you display the execution and program tab simultaneously, as shown below. Click on the vertex to highlight relevant code. Below we see the corresponding code for vertex 18. Below we expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call and finally the number of stages which were involved. Below RDD represented by vertices 17-16 involved two (2) stages, while 15-16 had five (5). Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. Gantt Chart Displays the stages using a Gantt Chart. The table is sorted on Start Time , Errors Log Conf Errors Logs Conf Four Right Tabs - When available it displays the program associated with the application. See Program above , Task Attempts Graphs Resources Task Attempts Graphs Resource " }, 
{ "title" : "Spark - SQL-Query", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1553168860873", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark - SQL-Query", 
"snippet" : "Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the applicatio...", 
"body" : " Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Partitions\/Tasks Read Write . # Stages above - A execution graph of the query. There are times when the DAG is too large to display and it will be noted. See Execution above - Displays the stages using a Gantt Chart. For more details see Gantt Chart above Tabs For an explanation of these tabs see Errors, Log and Conf Errors Logs Conf Four Right Tabs - This tab connects all the pieces of a SQL query. The table lists all queries with significant KPI's and the top five stages, i.e., the stages with the longest duration. The lower section contains two tabs, Program SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. Click on query , Task Attempts Graphs Resources Task Attempts Graphs Resource " }, 
{ "title" : "Spark - Streaming", 
"url" : "current/uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611588233", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark - Streaming", 
"snippet" : "Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events here : the total time to process all stages. (The wall clock time can be calculated from the start and end times a...", 
"body" : " Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events here : the total time to process all stages. (The wall clock time can be calculated from the start and end times are on the left hand side of the job bar.) Duration : Total data read and written by the query. Data I\/O : The number of jobs that make up the streaming. Number of Jobs : The number of stages that make up the streaming. Number of Stages Unlike other Spark Application Managers this has a Stream Stream Program Left Four Tabs - Displays the core of an Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Stream Scheduling Delay Metric Scheduling Delay Processing Time Total Delay It has two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven (7) pages. In the example below, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one (1) page, versus the seven (7) above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time Click on a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. The KPI's, Duration Processing Delay Scheduling Delay Total Delay Output Operation Input The example below is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two (2) jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click on the Job ID Spark Job The Input Tab , Errors Log Conf Errors Logs Conf Right Four Tabs - The program (if uploaded by the user) is shown in this tab. Program , Task Attempts Graphs Resources Task Attempts Graphs Resource " }, 
{ "title" : "Tez Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-tez.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Tez Application Manager", 
"snippet" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. Se...", 
"body" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O By default the Tez APM opens showing the Navigation and Program Tabs. Left Tabs : List the Dag jobs with KPIs, Duration and I\/O. See the Dag Detail information below. Navigation : List the configuration parameters and their values. Configuration : Lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr Right Tabs : Displays the query. Program : Has three (3) sub tabs. Graphs , Containers Vcores Memory : Graphs the resources consumed. By default the Resources Resource systemCpuLoad Select series Metric Get Data DAG Detail The DAG detail has six tabs: Displays the query. Query: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Graph: : Lists all the relevant counters for the Tez-DAG and their values. Counter Vertex Timeline Wall Clock Total Run All Vertices : List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task Id and Vertex name; Tasks containing the string will be displayed. All Task : List all attempts, their status (failed, success, etc.), vertex name and other relevant information. The task attempts are searchable by Attempt Id, Task Id and Vertex name; Task attempts containing the string will be displayed. All Task Attempts : Lists all relevant parameters and their value. Changed Configuration " }, 
{ "title" : "Workflow Application Manager", 
"url" : "current/uguide/uguide-apms/uguide-apms-workflow.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Workflow Application Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applica...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications Applications Key Performance Indicators : The number, if any, of Unravel insights for this query. See G>Events Event Panel & Insights : Total time taken by the query Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow Number of Yarn Apps The APM opens showing the Navigation Compare Left Tabs : Provides an easy way to understand the breakdown of the workflow the applications which comprise the Workflow, i.e., Hive, Spark, MapReduce, Oozie. Click on Navigation More Below the second Oozienode is shown, it is comprised of one MapReduce job and three Hive jobs. The hive jobs comprise one or more tasks, so that too can be expanded. In the example below, the second Oozienode has been expanded along with the first hive job within it. You can click on any job to see the application manager for it. In the example, below you can click on the expanded hive job to bring up the hive application manager. Similarly you can click on the mapreduce job within the hive job to go directly to it. Click on Less : Displays the execution graph of the workflow. Click Execution Lsts all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted for each job. The top line list the number of all jobs\/task. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Errors: Keywords : Tags : Lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr Right Tabs : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events above : Displays charts for Map Task, Reduce, and Spark Tasks, broken down by success, failed, and killed as appropriate. Task Attempts : Graphs the attempts over the time interval in Wall Clock time and list the Map and Reduce Slot Duration in total computing time below. Attempts " }, 
{ "title" : "", 
"url" : "current/uguide/uguide-apms/uguide-apms-workflow.html#UUID-3b073de2-8f42-cd89-0c65-2d3700bc3e3b_N1553795465667", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ The Application Managers \/ Workflow Application Manager \/ ", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Auto Actions", 
"url" : "current/uguide/uguide-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Auto Actions Overview", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Auto Actions Overview", 
"snippet" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can b...", 
"body" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can be set to automatically kill an application or move it to a different queue. The Unravel Server processes auto actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: : The conditions which cause a violation, e.g., the number of jobs running, memory used. Prerequisite conditions : Who\/what\/when can cause the violation, e.g., user, applications Defining conditions for Unravel Server to execute whenever it detects a rule violation. Actions Manage | Auto Actions The auto actions tab provides a quick way to view auto actions and quickly see their status, along with its defined actions and scope. The tab displays all defined auto actions separated into an Active and Inactive list. You enable\/disable by clicking the check box on the left. You can edit ( define new auto actions Hovering over the auto action's name gives you the description which was entered when defining the auto action. Hovering over action or scope glyph brings up its detail. For example, for the active auto action above: rule description: email action: an email is sent to only one (1) person, queue scope: is three queues: The Actions Scope quicktest must Expert Rule quicktest History of Runs By default all actions are off. Possible actions are: Send an Email ( Kill the App ( Move the app to another queue ( Send a Http post ( By default the various scopes apply to all, i.e., all applications and constantly on. The scopes are: User ( Queue ( Cluster ( Application ( Time ( Sustained Violation: This is not shown in the auto actions list. If you have not defined a particular action or scope, i.e., it's using the default, the glyph is grey ( Click on the History of Runs Click on the run's Link Operations Usage Details | Infrastructure Applications | Applications Notifications 'Snoozing' Auto Actions The snooze function prevents automatic actions from repeating during a specified period of time, if and only if it is the same violation context and the action adds no further information to the violation, i.e., the new violation is essentially noise. See Snooze Feature Property\/Definition Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are to be ignored for the violator, e.g., app, user. If the violation is still occurring when awakened the Auto Action executes the action(s) and the violator is once again snoozed. An auto action containing a kill or move action is never snoozed. 0: snooze is turned off &amp;gt; 0: snooze is on, there is no upper bound 3,600,000 (1 hour) When you change the snooze time period all applications currently snoozed are reset. Upon next violation the application is \"snoozed\" using new snooze value. To change the snooze time On Unravel Server open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Limitations", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-overview.html#UUID-ea3f812a-70bb-cd8f-b19c-ba0e1d73e2f5_N1552724699500", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Auto Actions Overview \/ Limitations", 
"snippet" : "Alerting on Running Apps Applications of the following types do not provide any means for real-time alerts, i.e. when in the running state. Once the application has finished the alerts are generated notifying users about policy violations that already occurred. Hive Impala Kill and Move to Queue not...", 
"body" : " Alerting on Running Apps Applications of the following types do not provide any means for real-time alerts, i.e. when in the running state. Once the application has finished the alerts are generated notifying users about policy violations that already occurred. Hive Impala Kill and Move to Queue not Supported The following applications types do not support Kill Move to Queue Hive Impala Workflow Running Duration versus Final Duration Inconsistency Unravel calculates and publishes internally the current duration for applications of the following types in real-time, i.e., when in the running state. Upon the application completion Unravel receives the actual end time and performs the final duration calculation. This can lead to an inconsistency where the duration aggregated and published during the running state is greater than the duration published upon the application's completion. Workflow Missing Auto Action Violation Badge The badge is not displayed for following application types. For example, in the cluster view above these applications do not display Hive Workflow \"Cloud\" Type Setups are not Supported Currently, auto actions are not triggered for applications on a “cloud” type setup (EMR, HDI, Qubole, etc.) of Unravel; therefore the Kill Move to Queue etl Auto Actions' Properties See auto action properties " }, 
{ "title" : "Auto Action Template", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-templates.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Auto Action Template", 
"snippet" : "See here Whether using Create from Template Build Rule Expert Rule [empty] Name and Description The name is mandatory and is used by the UI for all auto actions' displays; we recommend using a name which reflects the auto action's purpose. The description is optional, but we recommend completing it ...", 
"body" : " See here Whether using Create from Template Build Rule Expert Rule [empty] Name and Description The name is mandatory and is used by the UI for all auto actions' displays; we recommend using a name which reflects the auto action's purpose. The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name the description is displayed. This example is from Create from Template Ruleset At least one rule type ( User Queue Cluster Apps Expert Rule metric type state Expert Rule Build Rule defines a rule \"metric\" \"comparision operator\" \"value\". Metric See Supported Cluster Metrics metrics The comparison value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type mapreduce, yarn, tez, spark, impala, workflow and hive The State new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: , Or And Same and Or And Same Logical Operator Same Using \"Create From Template\" This template has the Ruleset metric comparison type state Metric Type State metric Same'd Same Logical Operator Using Build Rule The Ruleset User Queue, Cluster App Expert Rule Add Queue metric type state Expert Rule must In the example below, Metric Type Queue metric comparison operator type state , Apps , Same Or And Same Logical Operator Same Or And . Click Close Options Define the scope ( User Queue Cluster Application Name Time Sustained When you select an options its default is All, Time Sustained Violation When using Create from Template the box next to the option's name to select it. Check You can narrow the scope of User Queue Cluster Application Name Only Except Only rule Except all but those specified Transform Application Name Except Add Application except Create from Template All The Time start end click specifies a length of time violation must occur before the auto action is triggered. This allows time for specified by the option to \"self correct\" and lowers the number of false positives The default is zero, i.e., all auto actions are immediately triggered upon violation and the specified action is carried out. You can select Sustained violation minimum maximum sustained mode triggers the action(s) only if this violation was continuously detected for at least the specified period of time. This allows you to suppress triggering of violation actions for “on-offs” and metric spikes. These are normal in multi-tenant cluster environments can return back to normal operation on their own. If a violation stops before the minimum time period, the clock is reset for that application. For instance, if the minimum time is one (1) hour and the application violates the auto action for 58 minutes and then returns to normal – no action is taken and the time period for that application resets to 0. Minimum sustained mode triggers the action(s) only if this violation is continuously detected for less than the specified period of time. This suppresses the triggering of violations for long running applications and triggers on auto action rule scope on ad-hoc short lived user applications. Maximum Actions Defines the actions to take when the auto action is triggered. and Build Rule Create from Template Send an email Http Post Post to Slack Move App to Queue Kill App Move Kill App Build Rule Expert Action Expert Mode Auto Actions and Pagerduty can not You can chose one of more actions. Check the text box to chose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. For Send Email Add Recipient Include Owner For HTTP post Add URL uses the official Slack API available at Post to Slack https:\/\/slack.com\/api\/chat.postMessage public Slack channel, private channel, or direct message\/IM channel, It provides a better integration with Slack Service and allows you to send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: - generic user icon and \"bot\" username test tokens - generic bot icon, with generic \"bot\" username custom bot user token with Slack App user token chat:write:bot - inherits Slack App's icon, with generic \"bot\" username Slack App bot user token or Move app to queue Kill App. The Move App Kill App Kill App Move App Have directly caused the rule violation, and Have allocated resources, i.e. in allocated or running states. is a non-destructive action that should not affect the cluster performance and its availability to the user; however we suggest using it with caution. Move App is a destructive action. It can affect the cluster performance and its availability to the users. This option is primarily to kill rogue applications that are causing contention of a cluster resources. Kill App The Build Rule Expert Rules " }, 
{ "title" : "Creating Auto Actions", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-create.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Creating Auto Actions", 
"snippet" : "See here Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edi...", 
"body" : " See here Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edit ( To create a new auto action, click either Create from Template, Build Rule, Expert Rule - provides a partially completed template designed for the task. The only ruleset options available are relevant to the task being defined. Fields which you need to fill in are highlighted. Create from Template - provides an empty template, with all the options available. Build Rule - provides only a text box for defining your auto action using JSON. Expert Rule Create an auto action. See Auto Actions Templates Using Create From Template Click on Create a Template Resource contention - monitors overall memory or vcore usage on a cluster and number of pending\/running jobs in order to detect when the cluster is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in cluster - monitors overall memory or vcore usage in a queue and number of pending\/running jobs in order to detect a state when the queue is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in queue Rogue Identification - identifies so-called “rogue” users on a cluster who can potentially affect other users and the cluster as a whole, i.e., users who are submitting jobs that are using too much of the cluster resources (memory or vcores). Once the rogue user is detected the specified auto action(s) are taken. Rogue user - identifies so-called “rogue” applications on a cluster which can potentially affect other applications and the cluster as a whole, i.e., applications that are using too much of the cluster resources (memory or vcores). Once the rogue application is detected the specified auto action(s) are taken. Rogue application - identifies so-called “rogue” impala queries on a cluster which can potentially affect other applications and the cluster as a whole, i.e., queries that are using too much of the cluster resources (memory or vcores). Once the rogue application is detected the specified auto action(s) are taken. Rogue Impala query (HDFS Read\/Write) Long Running Jobs - monitors elapsed time of a running YARN application (i.e., MapReduce, Spark, Hive, etc) and executes the action(s) if the job runs longer than desired. Long running YARN application - tracks Hive jobs by monitoring the total elapsed time of a running Hive query and executes the action(s) if the job runs longer than desired. Long running Hive query - tracks workflow jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long running workflow - tracks impala jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long Impala query Complete the template. Note: Changing the predefined ruleset can result in the action not behaving as anticipated. Using Build Rule Click on Build Rule Using Expert Rule The Auto Actions engine is capable of much more than is available through the templates. Expert Rule is a very powerful mode giving you total access to all of the Auto Actions engine capabilities. You can create complex rulesets to accommodate almost any cluster monitoring requirements. JSON is used to define the rules and actions. Before using this mode, you should have clear understanding of auto actions concepts. See the Expert Rule Sample Auto Actions Click on Save Auto Action Your auto action is now listed in the Manage Auto " }, 
{ "title" : "Expert Rule", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-expert.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Expert Rule", 
"snippet" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clea...", 
"body" : " Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts and capabilities, along with JSON, which is used to define the auto action. This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule See here In this mode you must specify : boolean conditions that must be met for the Unravel Server to evaluate the Auto Action's defining conditions, e.g., Auto Action should run from 8.00 and 14.00. Prerequisite conditions : boolean conditions that must be met for the Unravel Server to execute the corresponding action, e.g., the application can't run more than 50 mapper tasks. Defining conditions : steps to be taken when the prerequisite and defining conditions evaluate to true, e.g., send an email to admin. Actions When using Expert Rule must rules actions {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} : basic auto action information, name, etc., including status (in\/active). Header : the rules for the scope. You must define at least one rule. Rules : who, what, where causes a violation and when. You must specify at least one. Policy\/Scope Options - : actions to perform when a violation triggers the auto action. If none are defined the UI still implements and tracks auto actions. Actions Defining Your Auto Action Header You must define a header. The only item not required is the Description. Attributes Name Definition Possible Value Default Value enabled Whether the auto action is active or not. True: active\/enabled. False: inactive\/disabled True | False - policy_name Value defined by Unravel. AutoActions2 AutoActions2 policy_id Value defined by Unravel. 10 10 instance_id Any unique value - name_by_user Any unique string. The name is used when the auto action is displayed in the UI. - description_by_user Description of the auto action. - created_by Value defined by Unravel. admin admin last_edited_by Value defined by Unravel. admin admin created_at Time created. Date and time is in the form of a Epoch\/Unix timestamp. - updated_at Time updated. Date and time is in the form of a Epoch\/Unix timestamp. - \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: Defining conditions Field Name Definition Possible Values Required\/ Required by Default Value scope The rule scope. app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container, containers, multi_ containers queue, queues, multi_queue, user, users, multi_user : apps==multi_app, users==multi-user, etc Note √ - target Application name any valid application name when scope is by_name - metric Metric used for comparison. see supported metrics per type - comparison Comparison operator >, >=, ==, <=, < metric - value Value for comparison. The value form varies by metric. number metric - state Scope state new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * - type Job type mapreduce, yarn, tez, spark, workflow, hive - Logical Operators for Evaluating Multiple Rules Operator Condition for a Violation OR At least one rule evaluates to true. AND All rules evaluate to true. SAME All the rules evaluate to true and See Same Logical Operator You must define at least one rule. A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n \/\/ at least one of the following\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \"state\":\"\",\n \"type\":\"\"\n }\n] Violation occurs when the application is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the application is a workflow with a duration > 10. Removing state \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the application has a duration > 10. Removing state type \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n Note is equivalent to the plural of Multi_X X. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n OR Example When they are ORed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] AND Example When ANDed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] SAME Example When SAMEd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the auto action is not triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However if My_App violates both rules (allocatedMB andallocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the auto action is triggered for My_App but not Your_App. Given the same ruleset, evaluation becomes more restrictive. : the auto action is triggered if one or more of conditions is true. OR : the auto action is triggered if all of conditions are true. AND : the auto action is triggered if all of the conditions are true SAME within Options - Policy\/Scope: Prerequiste conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. Field Name\/Definition Required\/ Required by Possible Values Default Value _mode X where X The mode defines how the rules are applied to type X 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type X 2 - the rules are evaluated for only those in X X 3 - the rules are evaluated for everything but the those in X X You must define at least one option\/policy. 0, 1, 2, 3 Default: 0 0 _list X A list of type X if mode is set to 2 (only) or 3 (except). Applicable Only if_mode is 2 or 3 and X empty, single item or comma separated list. - _transform X A list of regex used to generate a list of X if mode is set to 2 (only) or 3 (except). Applicable Only if X X empty, single regex or comma separated regex list - Time The daily time the Auto Action is trigger. any time period spanning less than 24 hours. - Sustained Violation Set a minimum or maximum time period for the auto action to be triggered. See here for more information any time period less than 24 hours. - Options - Policy\/Scope Rule where X \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , - does not apply to any clusters. Cluster \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", - applies all queues. Queue \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", - applies to all applications Application Name except \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action(s) to implement upon violation You do not have to define any actions, but it defeats to purpose not to. If no actions are defined, the UI keeps track of the auto action and when triggered, who triggers it. Both the prerequisite defining Field Name\/Definition Required\/ Required by Possible Value Default Value action The action to be taken. at least one send_email, http_post, post_in_slack, move_to_queue, kill_app - to Email recipients. send_mail if to_owner One or more recipients in a comma separated list. - to_owner Send email to owner. send_mail if to false: do not send email true: send email false urls URLs for Http post http_post One or more URLs in a comma separated list. - token Token generated by slack. post_in_slack Slack token - channels Slack channel. post_in_slack One or more channels in a comma separated list - queue Queue name. move_to_queue The name of a valid queue to move the app to. - Single Action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple Actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] Actions can be Ignored When in Conflict Below we specified two actions, move_to_queue kill_app kill_app move_to_queue actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n }\n] Action(s) Fail if the Required Information is Invalid or not Specified. Below are two actions with invalid information. In send_mail http_post Operations Dashboard , \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] Example Actions There are five (5) main actions: send_email http_post post_in_slack move_to_queue kill_app and send_email, http_post, post_in_slack Creating Auto Actions send_email http_post post_in_slack Note You must take care when entering information. A specified action fails if you enter the incorrect information, i.e., bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template Build Rule to_owner \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] http_post Just like send_email \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma separated list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] kill_app This is straight forward, but kill_app is a destructive action \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] An Expert Rule Example This auto action triggers on applications using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the applications, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the application is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} Auto Actions Examples See Sample Auto Actions Build Rule " }, 
{ "title" : "Same Logical Operator", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-same.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Same Logical Operator", 
"snippet" : ": logically SAME and Example - A Rule Designed to Alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[ “SAME”:[ { “scope”:”users”, “metric”:”appCount”, “operator”:”>”...", 
"body" : ": logically SAME and Example - A Rule Designed to Alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[\n “SAME”:[\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:10,\n state”:”running”\n },\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:5,\n “state”:”pending”\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and 3 pending apps user B has 7 running and 1 pending apps user C has 21 running and 11 pending apps First, the two simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has 7 → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has 3 → FALSE User B has 1 → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called “linking” of rules (see Ruleset.java). Cluster “User A”, link count = 1. User A > 10 running apps? → TRUE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Cluster “User C”, link count = 2. User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule reset for the group. Cluster “User A” has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE   User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster “User C”, link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User Auto Action, therefore User C triggers the Auto Action and the alert is sent and\/or the actions performed. Comparison to AND Both User A and User C would have triggered the above rule were AND SAME any AND any To achieve the same result as the above example using AND SAME each and every user ( Username AND Username " }, 
{ "title" : "Snooze Feature", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-snooze.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App ...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App An auto action specifying a Kill App or Move App action can not be snoozed. Snooze is set the first time the app violates the rule. The auto action itself continues to run uninterrupted whether zero (0) or all apps currently covered by the auto action are snoozed. The auto action takes action for any app not snoozing If an app is still violating upon awaking, snoozed Example : if app uses memory > 1 GB send email Rule\/Action : A & B Two apps : 30 minutes Snooze time at 20:00 A > 1GB → email is sent + snooze set (snoozed until 20:30). B < 1GB → application is not violating so nothing is done. at 20:10 A > 1GB → snoozing B > 1GB → email is sent + snooze set (snoozed until 20:40). at 20:20 A > 1GB → snoozing B > 1GB → snoozing at 20:30 A > 1GB → application wakes >B > 1GB → snoozing at 20:40 A > 1GB → snoozing, >B < 1GB → app wakes To change the snooze time On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms. com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Example Auto Actions", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-examples.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Example Auto Actions", 
"snippet" : "Limitations on auto actions can be found here here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes { \"scope\": \"multi_app\", \"user_metric\": \"duration\", \"type\": \"HIVE\", \"state\": \"RUNNING\",...", 
"body" : " Limitations on auto actions can be found here here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Tez Alert if a Tez query duration is running longer than expected. Check if a Tez query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Tez query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Tez query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Tez query has extensive I\/O, wich may affect HDFS and other apps. Check if a Tez query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Tez query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Tez queries, i.e. for example alert if a Tez query has not read lots of data but running for a longer time. Check if any Tez query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Workflow Alert if a workflow is taking longer than expected. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} USER UserAlert for Rogue User - Any user consuming a major portion of cluster resources. Check for any user where the allocated vcores aggregated over all their applications is > 1000. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Below we are alerting on vcores; to alert on memory just subsitute memory for vcores in the following rules. Alert for Rogue application If any application (not sla bound) is consuming more than certain vcores at midnight, move it to a quarantine queue. You can use the Rogue Application Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Set Time rule condition as: Set Move app rule as: Any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n }\n Set Queue rule conditions as: Set Move app action as: Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-demos.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". indicates whe...", 
"body" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". indicates where you must substitute your particular values for the text. Text is the complete path of the auto-actions-demos directory. demo-path Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos The directory should contain the following files. # ls auto-actions-demos\ndemos\/ setup\/ Go to demo-path setup Open .\/settings Execute the .\/setup-all # .\/setup-all The auto action rules that include time specification are automatically adjusted to the current time period, e.g., from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Manage Auto Actions You should see all the auto action demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag AA-Spark-1c Map-1b Executing the demos Go to demo-path demos Manage Auto Actions For example, in the UI the auto action named AA-Spark-1c demo-Spark-1c # cd demo-path Execute .\/demo-tag AA-tag AA-Spark-1c demo-Spark-1c Some auto action's demo scripts trigger multiple auto actions. This side effect can happen when running your own defined auto actions due to auto actions having overlapping definitions. Cleaning up demos Run .\/clean-all script. # cd demo-path This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos again simply follow the procedure starting with extracting the files ( step 3 Auto Actions demos list Application and Alert Type Use case Auto Action Triggering Script [empty] Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vcores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. MapReduce Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Will also trigger MR-1a as well. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. AA-MR-2b Demo-MR-2b Will also trigger MR-2a as well. MapReduce Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. Spark Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark application has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark application allocated vcores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark application is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Spark Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Hive Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Hive Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Hive Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Workflow Alert if a workflow is reading more data than expected. Related articles Page: Auto Actions Overview Page: Snooze Feature Page: Sample Auto Actions Page: Setting Up Email for Auto Actions and Collaboration " }, 
{ "title" : "Supported cluster metrics", 
"url" : "current/uguide/uguide-auto-actions/uguide-auto-actions-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Auto Actions \/ Supported cluster metrics", 
"snippet" : "[empty] Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defi...", 
"body" : "[empty] Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on most live running here Monitoring is only Hive\/Workflow Metrics Metric Definition duration total time taken by the application totalDfsBytesRead total hdfs bytes read totalDfsBytesWritten total hdfs bytes written MapReduce Application Master and Map Reduce Metrics Type Metric Definition elapsedAppTime time since the application was started Map mapsCompleted number of completed maps mapsPending number of maps still to be run mapsRunning number of running maps mapsTotal total number of maps Map Attempts failedMapAttempts number of failed map attempts killedMapAttempts number of killed map attempts newMapAttempts number of new map attempts runningMapAttempts number of running map attempts Reduce reducesCompleted number of completed reduces reducesPending number of reduces still to be run reducesRunning number of running reduces reducesTotal total number of reduces Reduce Attempts failedReduceAttempts number of failed reduce attempts killedReduceAttempts number of killed reduce attempts newReduceAttempts number of new reduce attempts runningReduceAttempts number of running reduce attempts successfulReduceAttempts number of successful reduce attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduce and File System Counters Metric Definitions fileBytesRead mount of data read from local file system fileBytesWritten amount of data written to local file system fileReadOps number of read operations from local file system fileLargeReadOps number of read operations of large files from local file system fileWriteOps number of write operations from local file system hdfsBytesRead amount of data read from HDFS hdfsBytesWritten amount of data written to HDFS hdfsReadOps number of read operations from HDFS hdfsLargeReadOps number of read operations of large files from HDFS hdfsWriteOps number of write operations to HDFS Job Counters Type Metric Definition Map dataLocalMaps number of map tasks which were launched on the nodes containing required data mbMillisMaps total megabyte-seconds taken by all map tasks millisMaps total time spent by all map tasks slotsMillisMaps total time spent by all executing maps in occupied slots vcoresMillisMaps total vcore-seconds taken by all map tasks Reduce mbMillisReduces total megabyte-seconds taken by all reduce tasks millisReduces total time spent by all reduce tasks slotsMillisReduces total time spent by all executing reduces in occupied slots totalLaunchedReduces total number of launched reduce tasks vcoresMillisReduces total vcore-seconds taken by all reduce tasks File Input\/Output Format Counters Metric Definition bytesRead amount of data read by every tasks for every filesystem bytesWritten amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Map-Reduce Framework Counters Type Metric Definition Map failedShuffle total number of mappers which failed to undergo through shuffle phase mapInputRecords total number of records processed by all of the mappers mapOutputBytes total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes amount of (compressed) data which was actually written to disk mapOutputRecords total number of records produced by by all of the mappers mergedMapOutputs total number of mapper output files undergone through shuffle phase shuffledMaps total number of mappers which undergone through shuffle phase Reduce reduceInputGroups total number of unique keys reduceInputRecords total number of records processed by all reducers reduceOutputRecords total number of records produced by all reducers reduceShuffleBytes amount of data processed in shuffle and reduce phase Records combineInputRecords total number of records processed by combiners combineOutputRecords total number of records produced by combiners spilledRecords total number of map and reduce records that were spilled to disk Time gcTimeMillis wall time spent in Java Garbage Collection cpuMilliseconds cumulative CPU time for all tasks Memory committedHeapBytes total amount of memory available for JVM physicalMemoryBytes total physical memory used by all tasks including spilled data splitRawBytes amount of data consumed for metadata representation during splits virtualMemoryBytes total virtual memory used by all tasks Shuffle Errors Metric Definition badId total number of errors related with the interpretations of IDs from shuffle headers connection total number of established network connections ioError total number of errors related with reading and writing intermediate data wrongLength total number of errors related to compression and decompression of intermediate data wrongMap total number of errors related to duplication of the mapper output data wrongReduce total number of errors related to the attempts of shuffling data for wrong reducer Spark Metrics In addition to the metric set supported by MapReduce applications, Spark applications can be polled on: Type Metric Definition Join joinInputRowCount the total input rows of the first join of the SQL query, aggregated for all the queries that are part of the application totalJoinInputRowCount total number of input rows count for all join operators of all SQL queries that are part of the application totalJoinOutputRowCount total number of output rows count for all join operators of all SQL queries that are part of the application joinOutputRowCount the total output rows of the first join of the SQL query, aggregated for all the queries that are part of the application Partitions inputPartitions total number of input partitions for all SQL queries that are part of the application outputPartitions total number of output partitions for all SQL queries that are part of the application Records inputRecords cumulative number of input records for all SQL queries that are part of the application (collected at stage level) outputRecords cumulative number of output records for all SQL queries that are part of the application (collected at stage level) outputToInputRecordsRatio outputRecords \/ inputRecords if inputRecords > 0, else 0 YARN Resource Manager metrics Metric Definition allocatedMB The sum of memory in MB allocated to the application’s running containers allocatedVCores The sum of virtual cores allocated to the application’s running containers appCount total number of applications elapsedTime The elapsed time since the application started (in ms) runningContainers The number of containers currently running for the application memorySeconds The amount of memory the application has allocated (megabyte-seconds) vcoreSeconds The amount of CPU resources the application has allocated (virtual core-seconds) For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API " }, 
{ "title" : "Use Cases", 
"url" : "current/uguide/uguide-usecase.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "current/uguide/uguide-usecase/uguide-usecase-id-rouge-apps.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations | Charts | Infrastructure In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp at the bottom of the page. Click on the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the event panel in the application's APM to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an Auto Action " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "current/uguide/uguide-usecase/uguide-usecase-res-content.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "current/uguide/uguide-usecase/uguide-usecase-opt-spark.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web...", 
"body" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching at PetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Kafka Insights", 
"url" : "current/uguide/uguide-usecase/uguide-usecase-kafka.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : " Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition. The states are: : commit offset is in pace with the log end offset. OK : the Consumer is lagging behind the Producer. Both the consumer's and log commit offsets are increasing but the Producer’s is increasing faster. When graphed over time, the Producer's slope than the consumer's slope. Lagging : the Consumer has essentially stopped while the Producer is still active. The consumer's offset is not increasing while the log commit offset is increasing. Graphically, the slope of the consumer is essentially zero. Stalled A Topic's status is set to the lowest status among its Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Example Go to Operation | Charts | Kafka Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 demo test-consumer-group Consumers with the same name are grouped together into one consumer group. Choosing all clusters The Topic Topic Detail Consumer Details test2 demo demo Click on the Partition Detail Consumer Details offset test-consumer-group demo Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group The CG view lists the Topics Click on the Partition Detail Partition Details status offset Use the pull down menus to change Metric Partition consumer lag. " }, 
{ "title" : "Event Panel & Insights", 
"url" : "current/uguide/uguide-events-panels-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Event Panel & Insights", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. For detail information Impala Insights see here uguide-events-impala Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job Example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. Recommendations Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG Example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. Recommendations Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. Efficiency 2: Tez DAG Resulted in one recommendation (#1). Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Resource Metrics", 
"url" : "current/uguide/uguide-resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory avail...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory available for launching new processes avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations committedHeap bytes Committed heap size committedNonHeap bytes Committed non-heap size committedVirtualMemory bytes The committed virtual memory in the operating system currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems edenPeakUsage bytes Maximum memory usage in the eden space freePhysicalMemory bytes The free physical memory in the operating system freeSwap bytes The free swap size fullGcCount COUNT Number of full GC runs fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms initHeap bytes Initial heap size initNonHeap bytes Initial non-heap size maxHeap bytes Maximum heap size maxNonHeap bytes Maximum non-heap size minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC oldPeakUsage bytes Maximum memory usage in the old space processCpuLoad PERCENT Average process CPU load for the last minute (all cores) snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started survivorPeakUsage bytes Maximum memory usage in the survivor space systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) totalPhysicalMemory bytes The total physical memory in the operating system totalSwap bytes The total swap size usedHeap bytes Used heap size usedNonHeap bytes Used non-heap size vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "current/uguide/uguide-resource-metrics/uguide-resource-metrics-keywords-error-messages.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ User Guide \/ Resource Metrics \/ Some Keywords and Error Messages", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Proces...", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application on a worker node Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset spark.default.parallelism Default number of partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.executor.memory Related to executor memory spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.sql.shuffle.partitions Number of SparkSQL partitions spark.yarn.executor.memoryOverhead YARN memory overhead SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SparkConf Spark configuration object SQLContext Main Spark SQL entry point StreamingContext Main Spark Streaming entry point Spark Error Messages Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. MapReduce\/Hive Keywords Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Advanced Topics", 
"url" : "current/adv.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster Wide Report", 
"url" : "current/adv/adv-cluster-wide-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "This topic explains how to set up and run Unravel's Cluster Wide Report. Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report Collects performance data of prior complet...", 
"body" : "This topic explains how to set up and run Unravel's Cluster Wide Report. Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report Collects performance data of prior completed jobs. Analyzes the jobs in relation to the cluster's current configuration. Generates recommended cluster parameter changes. Predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of these recommendations revolve around the parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "current/adv/adv-cluster-wide-report.html#UUID-3b31a398-1e68-ff7b-3ff4-39555bca00d8_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSet...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/usr\/local\/unravel\/install_bin\/ClusterReport cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz to cd dbin Input.txt # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date=2018-01-01\nend_date=2018-03-28\nmapreduce.map.memory.mb=2048\nmapreduce.reduce.memory.mb=2048\nhive.exec.reducers.bytes.per.reducer=268435456\nmapreduce.input.fileinputformat.split.maxsize=256000000 Run the report. # su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Custom Configurations", 
"url" : "current/adv/adv-conf-custom.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Configuring Permissions for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-permission-unravel-daemons-cdh-sentry-secured.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Configuring Permissions for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"snippet" : "Verify HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl HDFS folder path Principal Read + Execute Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal READ Spark event log hdfs:\/\/user\/history\/done Your alt principal READ MapReduce logs hdfs...", 
"body" : " Verify HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl HDFS folder path Principal Read + Execute Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal READ Spark event log hdfs:\/\/user\/history\/done Your alt principal READ MapReduce logs hdfs:\/\/tmp\/logs Your alt principal READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse Your alt principal READ Obtain table partition sizes with \"stat\" only. hdfs:\/\/user\/spark\/spark2applicationHistory Your alt principal READ Spark2 event log (only if Spark2 installed). Please see Alternate Kerberos Principal for Cluster Access on CDH Set HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, to locate the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. For TLS-secured CM # curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ ClusterName For non-TLS secured CM # curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ ClusterName SQL\/PostgresSQL Login, as admin or root user who has grant privilege, to database host that is hosting the hive metastore database , and create a new database user and grant select privilege on the hive metastore database. For MySQL, the following is the mysql commands to create a new user unravelka MySQL GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; PostgreSQL CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt command login as admin user psql cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE Metastore Access Information on Unravel UI See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-multiple-host-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 Each ...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 Each host is assigned unique roles identified by daemon names that start with unravel_ The internal DNS or IP address of a host is specific to your installation. Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc Port 4043 unravel_lr If you do not use an external database (db), unravel_db unravel_db is identical on all Unravel hosts in the ensemble. \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig Stop unravel server. On each Unravel host, run this command: # sudo \/etc\/init.d\/unravel_all.sh stop Modify unravel.properties Pick a machine to be host1, where the Unravel Web UI will run. If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties must be a fully qualified DNS or IP address. Replace UNRAVEL_HOST_IP 3316 unravel_mysql_prod To find your fully qualified hostname Replace all text # hostname unravel.jdbc.url=jdbc:mysql:\/\/ UNRAVEL_HOST_IP 3316 unravel_mysql_prod Copy host1's unravel.properties Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh \/etc\/unravel_ctl # scp \/usr\/local\/unravel\/etc\/unravel.properties host2 host2 host2 Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel \/etc\/unravel_ctl root:root. The scripts invoked below will make an identical change to the unravel.properties Assign roles. Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id.properties For a 2-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh host1 host2 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh host1 host2 For a 3-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh host1 host2 host3 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh host1 host2 host3 on host3 #sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh host1 host2 host3 Set-up Zookeeper and Kafka. Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host2 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host3 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host2 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host3 # sudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15 seconds for Zookeeper quorum to settle. # sleep 15 Start up Kafka ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-k on host2 # sudo \/etc\/init.d\/unravel_all.sh start-k on host3 # sudo \/etc\/init.d\/unravel_all.sh start-k Wait 10 seconds for Kafka coordination:. # sleep 10 Create the Kafka topics (only on one host): on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh Start the Unravel Server. Finish multi-host installation by starting up Unravel Server: on host1 # sudo \/etc\/init.d\/unravel_all.sh start \necho \"http:\/\/ UNRAVEL_HOST_IP on host2 # sudo \/etc\/init.d\/unravel_all.sh start Edit Hive-site Snippet for Hive-Hook. The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 Snapshot unravel.properties " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-create-multiple-workers-high-volume.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multi-host deployments, please contact . Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # sudo chkconfig --add unravel...", 
"body" : " These instructions apply to single host Unravel deployments only; for multi-host deployments, please contact . Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 \n# sudo chkconfig --add unravel_ma_2 20000-30000 jobs per day # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3\n# sudo chkconfig --add unravel_ma_3 Greater than than 30000 jobs per day # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4\n# sudo chkconfig --add unravel_ma_4 Start Unravel Server. Start the workers' daemons. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Creating Read-only Admins", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-creating-read-only-admins.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Creating Read-only Admins", 
"snippet" : "A read-only admin has access to all the UI's page including the Manage Open \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.login.admins.readonly com.unraveldata.login.admins.readonly= user1 user2 user3 If you are using LDAP or SAML, you must configure the read-only admins using...", 
"body" : "A read-only admin has access to all the UI's page including the Manage Open \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.login.admins.readonly com.unraveldata.login.admins.readonly= user1 user2 user3 If you are using LDAP or SAML, you must configure the read-only admins using com.unraveldata.login.admins.readonly. MODE groups MODE com.unraveldata.login.mode Configure LDAP or SAML RBAC Properties com.unraveldata.login.admins.readonly. MODE admin1 admin2 admin3 user2 user3 " }, 
{ "title" : "Configuring Hive Metastore Permissions", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-hive-metastore-permissions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Configuring Hive Metastore Permissions", 
"snippet" : "Set HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provide you the hive met...", 
"body" : " Set HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provide you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive ClusterName For TLS-secured CM # curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/ ClusterName For non-TLS secured CM # curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/ ClusterName SQL\/PostgresSQL Login to database host, as admin or root user who has grant privilege, that is hosting the hive metastore database , and create a new database user and grant select privilege on the hive metastore database. MySQL commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; PostgreSQL commands to create a new user unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE Metastore Access Information See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Defining a Custom Banner", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-custom-banner.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom Banner", 
"snippet" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner....", 
"body" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner. " }, 
{ "title" : "Custom UI Banner\/Notification", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-custom-banner.html#UUID-7f059e75-e067-3c06-72f5-6efca9d53442_UUID-7a680b81-a73b-ac1d-f073-2de13d2c803f", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom Banner \/ Custom UI Banner\/Notification", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display The text end.date Optional ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display The text end.date Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. The text end.date Format: YYYYMMDDTHHMMSSZ-000000 Optional string (date) - " }, 
{ "title" : "Defining a Custom Web UI Port", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-defining-custom-web-ui-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom Web UI Port", 
"snippet" : "These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT= 18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart...", 
"body" : " These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT= 18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Email Alerts", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-email-alerts.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Email Alerts", 
"snippet" : "Configure the following properties to set up email alerts. In addition to these properties you must also configure com.unraveldata.login.admins...", 
"body" : "Configure the following properties to set up email alerts. In addition to these properties you must also configure com.unraveldata.login.admins " }, 
{ "title" : "Email Properties", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-email-alerts.html#UUID-3ff6bdf7-c4cf-0b86-115d-7b415ab737fc_UUID-56fa4e97-970d-08f1-6fad-ed2c1db29de7", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Email Alerts \/ Email Properties", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. : enables alerts true : disables false boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - mail.smtp.from Use...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. : enables alerts true : disables false boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - mail.smtp.from Used for email \"from\" and \"reply-to\" headers Required string - mail.smtp2.from Used for email \"from\" and \"reply-to\" headers Optional string - mail.smtp.port Port integer 25 mail.smtp.auth Enable\/ SMTP authentication. : If true then mail.smtp.user and mail.smtp.pw must be set as they are used when connecting. Note boolean false mail.smtp.starttls.enable Use start-TLS. boolean false mail.smtp.ssl.enable Use SSL right from the start.string boolean false mail.smtp.user Username for SMTP authentication : If Note mail.smtp.auth Optional string - mail.smtp.pw Password for SMTP authentication : If Note mail.smtp.auth Optional string - mail.smtp.host Host for SMTP server string localhost mail.smtp.localhost A domain name for apparent sender; must have at least one dot (e.g. organization.com) string localhost.local mail.smtp.debug Enable debug mode. boolean false " }, 
{ "title" : "Configuring HBASE", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-hbase-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Configuring HBASE", 
"snippet" : "To access Rest API the user must have at least read-only access. Cloudera Manager and Ambari supports creating a local user via their administration page. See HBASE Configuration properties Edit \/usr\/local\/unravel\/etc\/unravel.properties If a property is not found then add it; be sure to substitute y...", 
"body" : "To access Rest API the user must have at least read-only access. Cloudera Manager and Ambari supports creating a local user via their administration page. See HBASE Configuration properties Edit \/usr\/local\/unravel\/etc\/unravel.properties If a property is not found then add it; be sure to substitute you local value for highlighted text On-Prem Clusters Managed by Ambari or Cloudera Manager You must \ncom.unraveldata.hbase.rest.url= Ambari_or_Cloudera_base_url hbase_rest_username hbase_rest_password comma_separated_cluster_names You can optionally com.unraveldata.hbase.service.name= comma_separated_clustername_and_servicename true_or_false Port# Port# Seconds Seconds Number Clusters Running Java Management Extension (JMX), Including Amazon EMR You must com.unraveldata.hbase.source.type=JMX\ncom.unraveldata.hbase.clusters= comma_separated_cluster_names cluster_name comma_separated_base_node_http_api You can optionally com.unraveldata.hbase.metric.poll.interval= Seconds Seconds Seconds Number Number Restart the HBase service unravel_us_1 # sudo service unravel_us_1 restart Verify unravel_us_1 # sudo service unravel_us_1 status unravel_us_1 is running Verify the metrics are collected in Elasticsearch. A separate index file is created for each week with an alias hb-search, e.g., hb-20180813_19. Troubleshooting If unravel_us_1 usr\/local\/unravel\/logs\/unravel_us_1.out \/usr\/local\/unravel\/logs\/unravel_us_1.log If hb-* index is not created or no data in Elasticsearch, verify the following daemons are running. (Step 1 above) unravel_us_1 (elastic service) unravel_s_1 (hitdocloader sevice) unravel_hl (kafka service) unravel_k_1 Resources : Service name unravel_us_1 : Service logs \/usr\/local\/unravel\/logs\/unravel_us_1.log \/usr\/local\/unravel\/logs\/unravel_us_1.out : Configuration file \/usr\/local\/unravel\/etc\/unravel.properties : Elasticsearch : Template File \/usr\/local\/unravel\/etc\/template_hbase_metrics.json : Index Name hb-* " }, 
{ "title" : "Running Unravel Daemons with Custom User", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-run-unravel-daemon-w-custom-user.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Running Unravel Daemons with Custom User", 
"snippet" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies. The Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs t...", 
"body" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies. The Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to Switch User Run the following command to switch running Unravel daemons to user USER GROUP # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh USER GROUP Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\". foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\". hdfs hadoop Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh defines env vars \/etc\/unravel_ctl RUN_AS USE_GROUP and HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL and \/usr\/local\/unravel\/ \/srv\/unravel\/* RUN_AS: directory is removed (no longer needed). \/srv\/unravel\/tmp_hdfs\/ Logs in \/srv\/unravel\/log_hdfs \/usr\/local\/unravel\/logs directory is removed (no longer needed). \/srv\/unravel\/log_hdfs The umask of the run-as The chmod bits of \/usr\/local\/unravel \/srv\/unravel " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-set-retention-time-in-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties : number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. com.unraveldata.retention.max.days : number of weeks retained for search results in Elastic Search. com.un...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties : number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. com.unraveldata.retention.max.days : number of weeks retained for search results in Elastic Search. com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space; you can store approximately 1000 jobs per 1GB of disk. Open \/usr\/local\/unravel\/etc\/unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Search for and set the following properties. If not found, add them. com.unraveldata.retention.max.days=30\ncom.unraveldata.history.maxSize.weeks=5 is the most significant factor in controlling disk space usage in the database used by Unravel. com.unraveldata.retention.max.days After changing any of the properties, restart Unravel for the change to take effect. # sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Spark Properties for Spark Worker daemon @ Unravel", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Spark", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-e22dd71e-072e-5a34-2459-fcc787d299b0", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ Spark", 
"snippet" : "Live Pipeline Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percenta...", 
"body" : " Live Pipeline Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percentage of jobs in the Spark APM. true : process after the application completes and the event log file has been processed false boolean true com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has (# jobs\/stages) maxStoredStages maxStoredStages This setting affects only the live pipeline integer 1000 com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor. yarn Event Log Processing Property\/Description Set By User Unit Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when string hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum size of the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize bytes 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. sec 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. : prefix search true : prefix + suffix search false Prefix + suffix search is faster as it avoids listFiles() API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes boolean false com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata.spark.hadoopFsMulti.useFilteredFiles : the empty suffix (,,) be part of this value for uncompressed event log files. NOTE CSL ,,.lz4,.snappy,.inprogress com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. integer 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry ms 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Events Related Property\/Description Set By User Unit Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Other properties Property\/Description Set By User Unit Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Timeline histogram generation is memory intensive. Note: False " }, 
{ "title" : "Executor Logs", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-cb25240d-0cbb-59ab-48bc-334f8c6db03c", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ Executor Logs", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application. byte 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application. byte 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds). ms 60000 (10 mins) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application ms 6000 (1 min) com.unraveldata.spark.eventlog.location maprfs:\/\/\/apps\/spark [empty] 500 com.unraveldata,spark.master Default master for spark applications. (Used to download executor log using correct APIs)Valid Options: yarn, mesos, standalone yarn " }, 
{ "title" : "Spark S3 Specific Properties", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-619cea5e-87cc-8640-3e6f-e040acfe854c", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ Spark S3 Specific Properties", 
"snippet" : "Property\/Description Set By user Unit Default com.unraveldata.spark.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties string - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format...", 
"body" : " Property\/Description Set By user Unit Default com.unraveldata.spark.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties string - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets Note Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_key aws_secret_access_key access_key\/secretKey CSL - " }, 
{ "title" : "Tagging", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-af8578c1-5487-10fd-6914-2111772cfcc2", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ Tagging", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled string (path) \/usr\/loc...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled string (path) \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.tagging.script.method.name The name of the method in the python script that generates the tagging dictionary. string generate_unravel_tags " }, 
{ "title" : "HDInsight", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-8345e996-7df6-39f2-3628-f6d009c6c1ee", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ HDInsight", 
"snippet" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 O...", 
"body" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster string - com.unraveldata.hdinsight.primary-access-key Primary storage account key string - com.unraveldata.hdinsight.storage-account-name-2 Optional for Spark when HDInsight is using blob storage Storage account name for the HDInsight cluster (same as account-name-1 string - com.unraveldata.hdinsight.secondary-access-key Secondary storage account key string - " }, 
{ "title" : "Azure Data Lake", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-spark-properties-spark-worker-daemon.html#UUID-49dd26bd-eff1-77ae-0e14-63a92c0016b0_UUID-36f9d3f6-5e23-ebae-29b1-ae90c990d80c", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel \/ Azure Data Lake", 
"snippet" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application reg...", 
"body" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application registration has to be created in the Azure Active Directory string - com.unraveldata.adl.clientKey An application access key which can be created after registering an application string - com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal string - com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. URL - " }, 
{ "title" : "Using the Impala Daemon (impalad) as a Data Source", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-using-impalad-as-data-source.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Using the Impala Daemon (impalad) as a Data Source", 
"snippet" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad Securing the Impala Web User Interfacel Impala Web User Interface for Debugging CM as the data source Before following these instructions, follow the st...", 
"body" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad Securing the Impala Web User Interfacel Impala Web User Interface for Debugging CM as the data source Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM On Unravel Server, set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties com.unraeldata.source impalad Property\/Description Set By User Unit Default com.unraveldata.source Can be cm or impalad Optional cm com.unraveldata.impalad.nodes Node list in the form of IP Address:Port Required CSL com.unraveldata.data.source=impalad \ncom.unraveldata.impalad.nodes= IP:port,IP:port,IP:port Change the Impala Lookback Window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us References Cloudera: Impala Installationl Cloudera: Securing the Impala Web User Interface Cloudera: Impala Web User Interface for Debugging " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "current/adv/adv-conf-custom/adv-conf-custom-using-private-certificate-authority.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Custom Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with \"sudo \/etc\/init.d\/unravel_all.sh restart\" after making the change. is the path for your local settings \/path\/to\/jks_keystore Externally Managed JKS Keystore The bundled JRE will use an external keystore ( jssecacerts cacerts # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts \/usr\/local\/unravel\/etc\/unravel.ext.sh bin\/java \/usr\/java\/jdkl1.8 For example: export JAVA_HOME \/usr\/java\/jdk1.8 Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts jssecacerts # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for mycompanyca something.cer # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias mycompanyca something.cer " }, 
{ "title" : "Security Configurations", 
"url" : "current/adv/adv-conf-security.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Configure JVM Sensor", 
"url" : "current/adv/adv-conf-security/adv-conf-security-jvm-sensor.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Configure JVM Sensor", 
"snippet" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)...", 
"body" : " CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) " }, 
{ "title" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"url" : "current/adv/adv-conf-security/adv-conf-security-jvm-sensor/adv-conf-security-cdh-clusterwide-mr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Configure JVM Sensor \/ CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"snippet" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_...", 
"body" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/ UNRAVEL_SENSOR UNRAVEL_HOST_IP Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property> \n\/\/ this is one line \n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> Save the changes. Deploy the client configuration by clicking the deploy glyph ( Actions Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services Use the Unravel UI to monitor the situation. When you view the MapReduce APM Resource Usage " }, 
{ "title" : "HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"url" : "current/adv/adv-conf-security/adv-conf-security-jvm-sensor/adv-conf-security-hdp-enable-jvm-sensor-cluster-wide-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Configure JVM Sensor \/ HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"snippet" : "must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/bt...", 
"body" : " must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP On the top notification banner, click Save In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site Inside Custom mapred-site Add Property On the top notification banner, click Save You can manually edit \/etc\/hadoop\/conf\/mapred-site.xml -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Propagate the Unravel resource metrics sensor JAR onto all the servers in the cluster. If you have already run the unravel_hdp_setup.sh Open a ssh With root sudo \/usr\/local\/unravel-agent # cd \/usr\/local\/unravel-agent\n# curl http:\/\/localhost:3000\/hh\/unravel-agent-pack-bin.zip -o unravel-agent-pack-bin.zip\n# unzip -d jars unravel-agent-pack-bin.zip Ensure you have already installed unzip curl Create a .tar \/usr\/local\/unravel-agent # cd \/usr\/local\/\n# tar -cvf unravel-agent.tar .\/unravel-agent Copy the unravel-agent.tar untar \/usr\/local untar unravel-agent Restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. " }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "current/adv/adv-conf-security/adv-conf-security-adding-admins.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui res...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui restart " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "current/adv/adv-conf-security/adv-conf-security-adding-ssl-tls-to-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Secure cookies are NOT supported when using this Apache2 reverse-proxy method. Follow the instructions in Enabling TLS to Unravel Web UI...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Secure cookies are NOT supported when using this Apache2 reverse-proxy method. Follow the instructions in Enabling TLS to Unravel Web UI Directly unravel_ngui These steps were tested with httpd 2.4 and support listening on port 443. Install needed packages. # sudo yum install httpd mod_ssl There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf <VirtualHost *:80> \n ServerName unravelhost_FQDN \n Redirect permanent \/ https:\/\/ unravelhost_FQDN Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_ngui # sudo service unravel_ngui restart Start the http # sudo service httpd start Visit https:\/\/unravelhost_FQDN (using value appropriate for your site) to test access. Troubleshooting To enable verbose logging in Apache2, add LogLevel LogLevel LogLevel debug Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ Add the following line after the ServerName httpd RequestHeader set X-FORWARDED-PROTO 'https' Restart apache2. " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "current/adv/adv-conf-security/adv-conf-security-alt-kerebros-principal-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in part 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in part 2 of the installation guide. Check HDFS Default umask. For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see sub-directories and read files. In Cloudera Manager check the value of dfs.umaskmode hdfs-site.xml fs.permissions.umask-mode Enable ACL Inheritance. In Cloudera Manager, HDFS Configuration, search for \"namenode advanced configuration snippet\", and set dfs.namenode.posix.acl.inheritance.enabled hdfs-site.xml Apache JIRA HDFS-6962 Restart cluster. When you are ready, restart the cluster to effect the change of dfs.namenode.posix.acl.inheritance.enabled Change ACL of Target HDFS directories. Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented): Set ACL for future directories. The following example applies to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse Please make sure you set the permissions at the \/user\/history \/user\/history \/user\/history\/done Set ACL for existing directories # hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target user ( unravel hdfs mapr KEYTAB_FILE PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt KEYTAB_FILE PRINCIPAL " }, 
{ "title" : "Changing the Admin Password", 
"url" : "current/adv/adv-conf-security/adv-conf-security-changing-admin-password.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Changing the Admin Password", 
"snippet" : "Retrieve the following configs from the \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.url=jdbc:$ENGINE:\/\/$HOST:$PORT\/$DATABASE unravel.jdbc.username=$USERNAME unravel.jdbc.password=$PASSWORD Connect to your database (either MySQL or Postgres) CLI using one of the following methods. We use M...", 
"body" : " Retrieve the following configs from the \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.url=jdbc:$ENGINE:\/\/$HOST:$PORT\/$DATABASE\nunravel.jdbc.username=$USERNAME \nunravel.jdbc.password=$PASSWORD\n Connect to your database (either MySQL or Postgres) CLI using one of the following methods. We use MySQL in this example. Run the db_access.sh script # \/usr\/local\/unravel\/install_bin\/db_access.sh\n Open the your database's (MySQL CLI or Postgres) command line. # mysql -u $USER -h $HOST $DATABASE -p  Query for the user whose password you want to change and keep track of its \"id\" field. mysql> use $DATABASE; \nmysql> select id, login, encrypted_password from users;\n+----+-------+-------------------------------------------------------+ \n| id | login | encrypted_password | \n+----+-------+-------------------------------------------------------+ \n| 1 | admin | ######################################################| Use the bcrypt Navigate to  https:\/\/www.dailycred.com\/article\/bcrypt-calculator Update the record in the database with the new hash. Replace your local value for {HASH}: hash value you generated, {USER} user whose password you are changing, and {ID}: the ID from the query in Step 3. mysql> UPDATE users SET encrypted_password = '{HASH}' WHERE id = {ID} AND login = '{LOGIN}' LIMIT 1; Navigate to the Unravel UI. Logout and then login with your new password. " }, 
{ "title" : "Disabling Browser Telemetry", 
"url" : "current/adv/adv-conf-security/adv-conf-security-disabling-browser-telemetry.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Disabling Browser Telemetry", 
"snippet" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/u...", 
"body" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Set com.unraveldata.do.not.track false If this property isn't in the file, add it and set it to false com.unraveldata.do.not.track=false Restart the Unravel UI. # sudo service unravel_ngui restart Re-Enable Mixpanel Follow the above steps but in step 2 set com.unraveldata.do.not.track true " }, 
{ "title" : "Disabling Support\/Comments Panel", 
"url" : "current/adv/adv-conf-security/adv-conf-security-disable-support-comments-panel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Disabling Support\/Comments Panel", 
"snippet" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel...", 
"body" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. # sudo service unravel_ngui restart Your title bar should be missing the support button like below. Show\/Re-enable Support Button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled true unravel.properties Pop-up Support Box " }, 
{ "title" : "Enabling LDAP Authentication for Unravel UI", 
"url" : "current/adv/adv-conf-security/adv-conf-security-enable-ldap-authentication.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Enabling LDAP Authentication for Unravel UI", 
"snippet" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3...", 
"body" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3.1.2, the property com.unraveldata.ldap.use_jndi true If you used com.unraveldata.ldap.search_bind_authentication If you set the bind_dn bind_pw Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties For MYDOMAIN QA.EXAMPLE.COM If SSL is in use: Use ldaps:\/\/ LDAP_HOST ldap:\/\/ LDAP_HOST Put a truststore unravel\/jre\/ unravel\/etc\/unravel.ext.sh You can append a port number, if needed; for example, ldap:\/\/ LDAP_HOST Knowing your precise DN is essential for some configuration examples below. If you are uncertain about what a normal DN is, use ldapsearch If you use AD, you can find the Windows domain using ldapsearch @ userPrincipalName To debug, use the property com.unraveldata.ldap.verbose null= Active Directory Domain Setup This is the simplest to implement and enables the widest access to Unravel Server. On the login page, if the user enters a login name with has no domain name appended --like thisUser @ MYDOMAIN @ MYDOMAIN The login name can appear in AD or be seen in an ldapsearch userPrincipalName sAMAccountName @ MYDOMAIN For MYDOMAIN com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldap:\/\/ LDAP_HOST MYDOMAIN You can combine the optional com.unraveldata.ldap.userFilter Example 1: Active Directory with Base DN Defined This example works by composing a DN from the entered user account name in the web UI. For a user login foo uid=foo,ou=myou,dc=domain,dc=com guidKey uid sAMAccountName com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldaps:\/\/ LDAP_HOST You can combine the optional com.unraveldata.ldap.userFilter Example 2: Active Directory with User Pattern Here we match users with one or more colon-separated patterns, combined with an optional inclusion filter ( userFilter name@MYDOMAIN %s Multiple userDNPatterns can be defined by using a colon ( : The userFilter userDNPattern userDNPattern LDAP_HOST CN=%s,CN=Users,DC=domain DC=com =%s uid com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldap:\/\/ LDAP_HOST Example 3: Active Directory with Group Pattern Here we match users with one or more patterns, and verify group membership in order to approve login. In this example, if the user enters name@MYDOMAIN %s Multiple groupDNPattern : groupDNPattern groupFilter :memberOf=CN= mygroup mygroup myou subdomain domain com Spaces (' ') are significant in the Unravel web login form. Use values relevant to your installation for LDAP_HOST %s myou subdomain com %s uid UNRAVEL_GROUP com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldap:\/\/ LDAP_HOST For Open LDAP LDAP Example 1 Use values relevant to your installation for LDAP_HOST myunit example com uid You can combine the optional com.unraveldata.ldap.userFilter com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldap:\/\/ LDAP_HOST LDAP Example 2 Below is a typical DN to be uid=%s,ou=myunit,dc=example,dc=com %s cn uid You can specify multiple userDNPattern : com.unraveldata.ldap.userFilter com.unraveldata.login.mode=ldap \ncom.unraveldata.ldap.url=ldap:\/\/ LDAP_HOST Optional Master Bind Account When user accounts cannot do an LDAP search, you can add the properties below to specify a search account. This can be combined with any of the examples above. The password can be encrypted com.unraveldata.ldap.bind_dn=CN=unravel,ou=myunit,dc=example,dc=com \ncom.unraveldata.ldap.bind_pw= bigsecret Web UI Login Syntax In most cases, people want to enter a simple login account name on the Unravel login page, and the instructions here reflect that. However, if @ name windowsDomain The name field of the login page allows spaces to be entered. That way a name like john smith Restart unravel_ngui # sudo \/etc\/init.d\/unravel_ngui restart Advanced Properties and Details These properties narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The Unravel login process is described next. Property names are shortened for readability, but full names should be used. The SIMPLE LDAP authentication mechanism is used. Authentication Process If a bind_dn bind_pw Authentication starts out by binding with the given account. If the bind works, the verbose log contains Connected using bindDN inetOrgPerson sAMAccountName guidKey guidKey guid userPassword pw is a match inetOrgPerson userDNPattern guidKey If Windows domain is set: Bind as username + at sign + Windows domain, using the given password. The verbose log contains Connecting Connected using principal unravel_ngui.log userDNPattern guidKey Connected using DN Authorization Process These extra authorization steps are optional to further narrow who can access Unravel Server. The searches will be done using a bindDN account, if specified, otherwise the user account bind is used. If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful. If there are no results from the custom query, then login fails. Each search result will be matched with the simple name entered on the login page. User or group filters are ignored if a custom query is specified. If a user filter is specified, it is matched with the simple name entered on the login page. No additional LDAP search is done in this case. This allows access to Unravel to be controlled with an explicit list in Unravel. If a group pattern or filter is specified, it is checked. A query is made to find the groups to which a user belongs. The user membership list is scanned and if one of the groups is in the specified list of allowed groups, then this authorization step succeeds. The verbose log contains the resulting list and the match arguments in effect under Checking group " }, 
{ "title" : "LDAP Properties", 
"url" : "current/adv/adv-conf-security/adv-conf-security-enable-ldap-authentication.html#UUID-72a62062-eac3-6ea3-87f0-04305a1e05ac_UUID-f4fd84ac-4f2c-035e-59b4-d2e5eb0c8864", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Enabling LDAP Authentication for Unravel UI \/ LDAP Properties", 
"snippet" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. s...", 
"body" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. string - com.unraveldata.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null result set, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. [empty] string - com.unraveldata.ldap.groupClassKey DAP attribute name on the group entry that is to be used in LDAP group searches. string  - com.unraveldata.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. [empty] string  - com.unraveldata.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) string - com.unraveldata.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. [empty] string - com.unraveldata.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. Default is \"uid\"; not used when custom query is specified. string - com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server uses to extract the ldap user's email address. If not configured, Unravel server uses the attribute name \"mail\". [empty] string - com.unraveldata.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s where the actual group name is to be substituted for. This is used like a list of baseDNs and baseDN is ignored if this is set. [empty] string - com.unraveldata.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). string - com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. Example: ldap:\/\/host ldaps:\/\/hostldap:\/\/host:9999 ldaps:\/\/host1:9999  ldaps:\/\/host2:9999 [empty] string - com.unraveldata.ldap.verbose Enables verbose logging. Grep for \"Ldap\" entries in the  unravel_ngui.log \/usr\/local\/unravel\/logs\/ : user names and group names can appear in this log, but raw passwords are not logged. true : logging turned off false boolean false " }, 
{ "title" : "Enabling SAML Authentication for Unravel Web UI", 
"url" : "current/adv/adv-conf-security/adv-conf-security-enabling-saml-auth-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Enabling SAML Authentication for Unravel Web UI", 
"snippet" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configu...", 
"body" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml \ncom.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configure LDAP or SAML RBAC Properties Edit saml.config.json file Property Description Req Example Values entryPoint Identity provider entrypoint, Ping IdP address (SSO URL). Note: Identity provider entrypoint is required to be spec-compliant when the request is signed. Yes \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SSOService.php\" issuer Name of app that will connect to the saml server. Issuer string to supply to identity provider (Environment name). Should match the name configured in Idp. Yes \"unravel-myHost” cert IDP's public cert to validate auth response signature. Note: You retrieve this from saml host. Yes Idp Cert String logoutUrl Base address to call with logout requests. Default: entryPoint No \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\" logoutEnabled If true logs you out from every app. No false unravel_mapping Mapping saml auth response attributes to Unravel attributes. Yes { \"username\":\"userid\", \"groups\":\"ds_groups\" } privateCert Unravel private cert string to sign Auth requests. No Unravel cert string Example saml.json {\n \"entryPoint\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\",\n \"issuer\":\"localhost\",\n \"logoutUrl\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\",\n \/\/ generate by saml host\n\"cert\":\"MIIDXTCCAkWgAwIBAgIJALmVVuDWu4NYMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEwHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdAcQf2CGAaVfwTTfSlzNLsF2lW\/ly7yapFzlYSJLGoVE+OHEu8g5SlNACUEfkXw+5Eghh+KzlIN7R6Q7r2ixWNFBC\/jWf7NKUfJyX8qIG5md1YUeT6GBW9Bm2\/1\/RiO24JTaYlfLdKK9TYb8sG5B+OLab2DImG99CJ25RkAcSobWNF5zD0O6lgOo3cEdB\/ksCq3hmtlC\/DlLZ\/D8CJ+7VuZnS1rR2naQ==\",\n\"privateCert\":\"-----BEGIN PRIVATE \/\/ generated by unravel node\n KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDEt4Ma2k4DUkoW\\nG9QDHUBnY7S\/iS\/+u2BjPZqUG2JktzYZl30J05zA6i642i2VDn8eUIPHqt2Hw249\\nZ3nHKL4YnBVqa3yTfEkdMB\/6GSAkoCbnufaD3IsGcFJnlW5raDiT\/GZMy+1WnDfJ\\npB0\/.......vD8kRkcmEi9t3KLmKVy3SO15\/YHAhLxP9oTnTFGkPnIqZLRM0Y55UfwbRSZDlgH\/\\ny9GGmsV5IaIwhepuALJMdkHp\\n-----END PRIVATE KEY-----\\n\",\n \"unravel_mapping\":\n {\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n }\n} For Ping, the IdP certificate can be obtained as follows: In the Server Configuration Certificate Management and Digital Signing & XML Decryption Keys & Certificates. Click Export Select Certificate Only Next Click Export Configure SAML Server Configure the following properties on the SAML server. Replace UNRAVEL_HOST Property Description Req PingFederate Specific configuration AssertionConsumerService \/ ACS Url http(s):\/\/ UNRAVEL_HOST Yes https:\/\/docs.pingidentity.com\/bundle\/p1_enterpriseEditAnApplication_cas\/page\/p1_t_EditASAMLApplication.html https:\/\/documentation.pingidentity.com\/pingfederate\/pf84\/#concept_settingAssertionConsumerServiceUrlsSaml.html%23concept_settingAssertionConsumerServiceUrlsSaml Entity Identifier unravel-Congo24 Yes Should be same as the issuer in saml.json Single Logout Endpoint http:\/\/ UNRAVEL_HOST Single Logout Response Endpoint http:\/\/ UNRAVEL_HOST No " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "current/adv/adv-conf-security/adv-conf-security-enable-tls-reach-un-ui-directly.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port ...", 
"body" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties OPTION 1 - Simple ssl config Update or add the following properties. For example, to enable ssl with minimal configuration #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true \n#PATH TO CERT FILE \ncom.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate \n#PATH TO KEY FILE \ncom.unraveldata.ngui.ssl.key.file=\/etc\/certs \/wildcard_unravelhost_RSA_private.key \n#OPTIONAL - COMMA SEPARATED LIST OF CA FILES \ncom.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt \n#OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE \ncom.unraveldata.ngui.ssl.passphrase=testp OPTION 2 - Advanced ssl config Update or add the following properties. For example, to enable SSL with advance configuration, update\/add these properties: #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true \n#PROVIDE SSL CONFIG THROUGH JS FILE FOR ADVANCE CONFIG\ncom.unraveldata.ngui.ssl.advance.config=\/usr\/local\/unrave\/etc\/advanced_unravel_ssl.js Content of advanced_unravel_ssl.js \/* advanced_unravel_ssl.js \n update below config variables \n SSL_KEY_FILE_PATH \n CA_CERT_FILE_PATH \n comment and uncomment the needed blocks \n *\/ \nconst fs = require('fs');\nconst constants = require('constants');\n\/* absolute path for ssl key file *\/\nconst SSL_KEY_FILE_PATH= '\/cert\/unravel_ssl.key'\n\/* absolute path for ssl cert file *\/\nconst SSL_CERT_FILE_PATH= '\/certunravel_ssl.crt'\n\/* absolute path for CA certs *\/\n\/* const CA_CERT_FILE_PATH=''*\/\nmodule.exports = {\nkey: fs.readFileSync(SSL_KEY_FILE_PATH),\npassphrase:'The password you gave when you created the key',\ncert: fs.readFileSync(SSL_CERT_FILE_PATH),\n\/\/ un comment below if using custom ca certs\n\/\/ ca : fs.readFileSync(CA_CERT_FILE_PATH),\n\/\/ uncomment below to enable disable TLS version.\n\/\/ secureOptions: constants.SSL_OP_NO_TLSv1 | constants.SSL_OP_NO_TLSv1_1,\n\/* LIST OF RECOMMENDED CIPHERS *\/\n\/* note OpenSSL-style format *\/\nciphers: ['TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA',\n'ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_RSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_RSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA'].join(':')\n} Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/unravel.example.com:3000 Restart Unravel web UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "current/adv/adv-conf-security/adv-conf-security-encrypting-passwords-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, incl...", 
"body" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diagnostic or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Kafka Security", 
"url" : "current/adv/adv-conf-security/adv-conf-security-kafka.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Kafka Security", 
"snippet" : "You can improve the Kafka cluster security by having kafka authenticate connections to brokers from client using either SSL or SASL. SSL + Kerberos for Kafka clients : kafka brokers are configured with SSL and kerberos. Please refer to your hadoop providers documentation for configuring SSL and Kerb...", 
"body" : "You can improve the Kafka cluster security by having kafka authenticate connections to brokers from client using either SSL or SASL. SSL + Kerberos for Kafka clients : kafka brokers are configured with SSL and kerberos. Please refer to your hadoop providers documentation for configuring SSL and Kerberos for kafka brokers. Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For Multiple Kafka clients Each cluster must have a separate consumerConfig.properties Open \/usr\/local\/unravel\/unravel.properties com.unraveldata.ext.kafka.clusters The property should be defined with a comma separated list. If there is only one cluster name see above com.unraveldata.ext.kafka.clusters= ClusterName1 ClusterName2 ClusterName3 Create a file named consumerConfig ClusterName .properties ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL \nsasl.kerberos.service.name = kafka \nssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 \nssl.truststore.password = changeit \nssl.truststore.type = JKS \nssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks \nssl.keystore.password = password ssl.keystore.type = JKS \nssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\n principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Copy\/move each file to \/usr\/local\/unravel\/etc Edit \/usr\/local\/unravel\/unravel.properties com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties Restart the kafka monitor daemon unravel_km. # service unravel_km restart Kafka Authorizations Unravel consumes message to topic __consumer_offsets UnravelOffsetConsumer Sentry Authorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read\nHOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write\nHOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe\nHOST=*->TOPIC=__consumer_offsets→action=read\nHOST=*->TOPIC=__consumer_offsets→action=write\nHOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization Kafka with Ranger Authorization The following privilege must be granted using Ranger for the topic __consumer_offsets Publish\nConsume\nDescribe For further details, see Security - Create a Kafka Policy References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "For Single Kafka clients", 
"url" : "current/adv/adv-conf-security/adv-conf-security-kafka.html#UUID-7310a682-608d-0e3d-eef3-bee430df26b3_N1553513015914", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Kafka Security \/ For Single Kafka clients", 
"snippet" : "Create a file named consumerConfig.properties \/usr\/local\/unravel\/etc ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ...", 
"body" : " Create a file named consumerConfig.properties \/usr\/local\/unravel\/etc ssl.protocol = TLSv1\nsasl.mechanism = GSSAPI security.protocol = SASL_SSL\nsasl.kerberos.service.name = kafka\nssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 \nssl.truststore.password = changeit ssl.truststore.type = JKS\nssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks \nssl.keystore.password = password\nssl.keystore.type = JKS\nssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1\nsasl.jaas.config = \\\ncom.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\\nkeyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\nprincipal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" sasl.mechanism = GSSAPI\nsecurity.protocol = SASL_PLAINTEXT \nsasl.kerberos.service.name = kafka\nsasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\\nkeyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\nprincipal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Copy\/move consumerConfig.properties \/usr\/local\/unravel\/etc Edit \/usr\/local\/unravel\/unravel.properties com.unraveldata.ext.kafka.clusters com.unraveldata.ext.kafka.clusters= ClusterName Add the following property using the ClusterName com.unraveldata.ext.kafka. ClusterName Restart the kafka monitor daemon unravel_km. # service unravel_km restart " }, 
{ "title" : "Restricting Direct Access to Unravel UI", 
"url" : "current/adv/adv-conf-security/adv-conf-security-restrict-direct-access-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Restricting Direct Access to Unravel UI", 
"snippet" : "On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. # SET NGUI_HOSTNAME export NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart...", 
"body" : " On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. # SET NGUI_HOSTNAME \nexport NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Deploying Unravel over SELinux", 
"url" : "current/adv/adv-conf-security/adv-conf-security-deploying-unravel-over-selinux.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Security Configurations \/ Deploying Unravel over SELinux", 
"snippet" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux Modes : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabl...", 
"body" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux Modes : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enforced whenever any violations or errors are detected and the violations\/errors are logged. Enforcing : The policies and rules of SELinux are applied over the subjects and objects but are not enforced. All violations and errors based on the SELinux policy are ignored and logged into the log files. If the SELinux policy prevents a specific service from accessing a specific folder, this mode allows access but logs a denial message. This mode provides enough debugging information to fine tune the SELinux Policy so it runs smoothly in enforcing mode. Permissive : No policies are enforced. Disabled SELinux Policy Unravel currently only supports the targeted Prerequisites Enable SELinux on Unravel Node running Linux. Open \/etc\/sysconfig\/selinux Set the SELinux mode. This is SELinux's default; whenever the system reboots it starts SELinux in this mode. See Switching Modesfor how to change the mode while running. SELINUX=enforcing Use the default policy, targeted SELINUXTYPE=targeted Reboot the system to effect the changes. # getenforce\nenforcing Verify the mode setting after reboot. Installing Unravel Core RPMs on a Node with SELinux Install Unravel in permissive mode or enforcing mode. You can install Unravel in either mode. However, installing Unravel in enforcing mode is highly discouraged since SELinux issues a warning regarding uncertainty of functionality. Installing in Permissive Mode (Recommended) Set mode to permissive # setenforce 0\n# getenforce\npermissive Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n SELinux may generate similar alerts during the installation process depending on the environment. But this should not hinder with the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Installing in Enforcing Mode (Highly Discouraged) When Unravel is installed in enforcing Execute getenforce enforcing # getenforce\nenforcing Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n The rpm installation sets SELINUX permissive -----RPM installation log\n+ setenforce Permissive\n+ echo\n+ tee_echo '[CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.'\n+ tee -a \/tmp\/rpm_upgrade.log\n++ date '+%Y-%m-%d %H:%M:%S'\n + echo '[2019-01-28 06:33:17] [CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.' \n+ echo\n+ FILE_CACHE_HEADROOM_MB=2000\n----- # getenforce\npermissive SELinux generates two alerts like the ones below. Similiar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Switch to user. There should be no alerts at this stage. Set SELINUX enforcing # setenforce 1\n# getenforce\nenforcing Run the script switch_to_user.sh X Y switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y Start Unravel services after RPM installation. Run the following command to make sure all services start up successfully. # sudo \/etc\/init.d\/unravel_all.sh start SELinux generates two alerts. Similar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Verify SELINUX is set to enforcing. # getenforce\nenforcing If getenforce permissive # sudo \/etc\/init.d\/unravel_all.sh stop\n# setenforce 0\n# sudo \/etc\/init.d\/unravel_all.sh start Configure Unravel Server and install sensors. Substitute your fully qualified domain name or your host's IP for UNRAVEL_HOST # python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_cdh_setup.py --spark-version 2.3.0 --unravel-server UNRAVEL_HOST Checking for Any Alerts, Denials, or Policy Violations Run these commands to check for any violations which might have after an installation or an operation\/job submission to see if any violations have occurred. To view any Unravel specific alerts: # sealert -a \/var\/log\/audit\/audit.log | grep unravel To view any system specific alerts: # sealert -a \/var\/log\/audit\/audit.log Installing and Using SELinux Tools # yum install setroubleshoot setools\n# yum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setroubleshoot setools setools-console These tools help you get more information about the policy and analyze the avc Use seinfo # seinfo\nStatistics for policy file: \/sys\/fs\/selinux\/policy\nPolicy Version & Type: v.28 (binary, mls)\n\nClasses: 94 Permissions: 262 \nSensitivities 1 Categories: 1024\nTypes: 4747 Attributes: 251\nUsers: 8 Roles: 14\nBooleans: 307 Cond. Expr.: 56\nAllow: 101746 Neverallow: 0\nAuditallow: 155 Dontaudit: 8846\nType_trans: 17759 Type_change: 74\nType_member: 35 Role allow: 39\nRole_trans: 416 Range_trans: 5697\nConstraints: 109 Validatetrans: 0\nInitial SIDs: 27 Fs_use: 29\nGenfscon: 105 Portcon: 602\nNetifcon: 0 Nodecon: 0\nPermissives: 6 Polcap: 2 Use semodule # semodule -DB Use sealert Enter the following command to see all the alerts generated by SELinux # sealert -a \/var\/log\/audit\/audit.log Enter the following command to see Unravel specific alerts # sealert -a \/var\/log\/audit\/audit.log | grep unravel For debugging (in other words, if you're testing in enforcement mode), run the following commands: Log all trivial violations logged by SELinux. # semodule -DB Set the audit log file to 0 so you get to know of access violations happening during the testing of enforcement mode. # > \/var\/log\/audit\/audit.log Working with Modes Checking Which Mode SELinux is Running Retrieve the current SELinux mode. Output is permissive enforcing # getenforce Switching Modes You can switch modes on the fly using the setenforce When Unravel is restarted SELinux returns to the default mode set in \/etc\/sysconfig\/selinux To set permissive mode # setenforce 0 To set enforcement mode # setenforce 1 Installing MySQL in Enforcing Mode Instructions for installing and configuring MySQL are available here If the datadir \/srv\/unravel\/db_data MySQL installation During Configure and Start MySQL Server The following alert is thrown when starting mysqld datadir=\/srv\/unravel\/db_data : If you believe that mysqld should be allowed read access on the plugin.frm file by default Alert During Configure Unravel to Connect My SQL Server The following alert is thrown while creating the database (Step 1). : If you believe that mysqld should be allowed create access on the ibdata1 file by default. Alert The following alert is thrown when creating the schema for Unravel (Step 3). : If you believe that mysqld should be allowed remove_name access on the Alert edge-4.lower-test Sample policy module my-mysqld 1.0;\nrequire {\ntype mysqld_safe_t;\ntype var_t;\ntype mysqld_t;\nclass process siginh;\nclass dir { add_name create remove_name write };\nclass file { create getattr lock open read rename unlink write };\n}\n#============= mysqld_safe_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_safe_t mysqld_t:process siginh;\n#============= mysqld_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:dir { add_name create remove_name write };\nallow mysqld_t var_t:file rename;\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:file { create getattr lock open read unlink write }; " }, 
{ "title" : "Configurations", 
"url" : "current/adv/adv-conf-general.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"url" : "current/adv/adv-conf-general/adv-conf-creating-aws-rds-cloudwatch-alarm-freestorage.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"snippet" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdent...", 
"body" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier Next In Alarm Threshold - for this Database Metrics (e.g. RDS_FreeStorageSpace_for_MySQL-A) Name - describe what the above database metrics name you entered (e.g. Disk space monitor of RDS MySQL-A) Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 Under Actions Send notifications to This SNS topic should already be set up before you add it. Click Create Alarm The UI displays the alarm you just created in Alarms INSUFFICIENT DATA ALARM Alarms Click Create Alarm " }, 
{ "title" : "Autoscaling HDInsight Spark Cluster using Unravel API", 
"url" : "current/adv/adv-conf-general/adv-conf-autoscaling-hdinsight-spark-cluster-with-un-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Autoscaling HDInsight Spark Cluster using Unravel API", 
"snippet" : "Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click here After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using ...", 
"body" : " Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click here After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using this command. # azure hdinsight cluster list Download the customizable script from https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/unravel-autoscaling\/unravel_HDInsight_autoscaling.py Open unravel_HDInsight_autoscaling.py Property Notes Example Value unravel_base_url http:\/\/localhost:3000\/ memory_threshold scale up\/down when memory_usage higher\/lower 80% 80 cpu_threshold scale up when cpu_usage higher\/lower 10% 10 min_nodes min worker nodes 4 max_nodes max worker nodes can scale up to 10 resource_group UNRAVEL01 cluster_name estspk2rh75` Run auto scaling script # python unravel_HDInsight_autoscaling.py Below is a screenshot ( Operations Dashboard " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "current/adv/adv-conf-general/adv-conf-create-active-dir-kerberos-principal-keytabs.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replac...", 
"body" : " Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel andhdfs: Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires Uncheck Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: Run these commands in a cmd powershell # setspn -A unravel\/ HOST HOST On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass utility in Active Directory: ktpass -princ unravel\/ HOST REALM REALM HOST REALM REALM Copy the two keytabs ( unravel.keytab hdfs.keytab \/etc\/keytabs\/ # sudo chmod 700 \/etc\/keytabs\/*\n# sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab\n# sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab : Assurances hdfs.keytab " }, 
{ "title" : "Configuring Ondemand", 
"url" : "current/adv/adv-conf-general/adv-conf-config-ondemand.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Configuring Ondemand", 
"snippet" : "Further properties See Celery HiveServer2...", 
"body" : " Further properties See Celery HiveServer2 " }, 
{ "title" : "", 
"url" : "current/adv/adv-conf-general/adv-conf-config-ondemand.html#UUID-86b97dd4-bc08-d058-8717-6b7b4ba1d4f5_UUID-09c7bc12-7617-7652-0017-11b098587ff3", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Configuring Ondemand \/  \/ ", 
"snippet" : "Property\/Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Req string - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the ondemand installation.) boolean true...", 
"body" : " Property\/Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Req string - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the ondemand installation.) boolean true " }, 
{ "title" : "Enable Various OnDemand-based Reports or Features", 
"url" : "current/adv/adv-conf-general/adv-conf-config-ondemand.html#UUID-86b97dd4-bc08-d058-8717-6b7b4ba1d4f5_section-5c9fb92698214-idm46265902754672", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Configuring Ondemand \/ Enable Various OnDemand-based Reports or Features", 
"snippet" : "Cluster Optimization Reports - is enabled by default. There are no specific properties. Cloud Reports and Forecasting Reports - Requires extensive configuration. See cloud and forecasting Queue Analysis - is enabled by default. See queue analysis Small Files Reports and File Reports - are enabled by...", 
"body" : " Cluster Optimization Reports - is enabled by default. There are no specific properties. Cloud Reports and Forecasting Reports - Requires extensive configuration. See cloud and forecasting Queue Analysis - is enabled by default. See queue analysis Small Files Reports and File Reports - are enabled by default. See enabling small files and file reports Top X Report- is enabled by default. See Top X Sessions - is enabled by default. See sessions " }, 
{ "title" : "Enabling or Disabling Small Files Report and Files Reports", 
"url" : "current/adv/adv-conf-general/adv-conf-enable-disable-small-and-file-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Small Files Report and Files Reports", 
"snippet" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property\/Description Set By User Unit Def...", 
"body" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property\/Description Set By User Unit Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. : disables the functionality in the Backend and UI. true : enables the functionality in the Backend and UI to generate the Small Files\/File Reports. false boolean false Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property\/Description Set By User Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance URL - unravel.hive.server2.port Port for the HiveServer2 instance number 10000 Security-Based Settings Settings for Kerberos-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property\/Description Set By User Unit Default unravel.hive.server2.authentication KERBEROS, LDAP, or CUSTOM When set to KERBEROS you must - - unravel.hive.server2.kerberos.service.name Set only This must hive string - Settings for Sentry-Secured CDH Clusters (With or Without Kerberos) If your CDH cluster is secured with Sentry, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative To grant this privilege, set\/update the HDFS configuration property dfs.cluster.administrators If you can't grant dfsadmin privilege to the Unravel user, follow the steps in the Troubleshooting Triggering an import of FSImage Remove the following line from \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive ADD JAR {UDF_JAR_LOC}\/unravel-udf-0.1.jar This line is no longer required because UDFs jars are stored locally on the HiveServer2 node in Sentry-secured CDH clusters. Allow the Unravel user to submit Hive queries to a YARN queue. If you can't allow this on your default YARN queue, you can grant this permission on a different YARN queue: Create a different YARN queue for the Unravel user. Give the Unravel user permission to submit Hive queries to the new YARN queue. In unravel.properties unravel.python.reporting.files.hive_mr_queue Create a new Sentry role, unravel_role beeline create role unravel_role Map the unravel unravel_role grant role unravel_role to group unravel Set HDFS access privileges for the Unravel user: The Unravel user needs to copy FSImage to \/tmp\/fsimage grant all on uri 'hdfs:\/\/\/tmp\/fsimage' to role unravel_role; Grant the Unravel user the following privileges on the Hive tables under the default Create\/drop\/truncate\/alter Hive tables Run\/select\/insert queries on Hive tables Alternatively, you can Use a different database for the Unravel user, such as unravel_db Give the Unravel user the above permissions on that database: grant all on database unravel_db to role unravel_role; In unravel.properties unravel.python.reporting.files.hive_database For example, unravel.python.reporting.files.hive_database=unravel_db Add a JAR and create temporary UDFs: As the Unravel user, copy the Unravel JAR, \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/jars\/small_files\/unravel-udf-0.1.jar The HiveServer2 aux JAR path is specified by the Hive Auxillary Jars directory (in a Hive Service wide configuration) OR by hive.reloadable.aux.jars.path Hive user and group should own this JAR. For example, if the HiveServer2 aux JAR path is \/tmp\/hive_jars hive chown -R hive:hive \/tmp\/hive_jars Grant the Unravel user access to this JAR: grant all on uri 'file:\/\/\/tmp\/hive_jars\/' to role unravel_role; In Cloudera Manager, restart HiveServer2. Use the show grant role unravel_role +--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| file:\/\/\/tmp\/hive_jars\/unravel-udf-0.1.jar | | | | unravel_role | ROLE | * | false | 1550829915318000 | -- |\n\n| unravel_db | | | | unravel_role | ROLE | * | false | 1550829820331000 | -- |\n\n| hdfs:\/\/\/tmp\/fsimage | | | | unravel_role | ROLE | * | false | 1550830532328000 | -- |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n Settings for Ranger-Secured HDP Clusters (With or Without Kerberos) If your HDP cluster is secured with Ranger, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative For example: Allow the Unravel user to connect to HiveServer2. Allow the Unravel user to CREATE, TRUNCATE, ALTER, DROP, INSERT, and SELECT Hive tables. For example: Allow the Unravel user to change or switch the Hive database. For example: Allow the Unravel user to submit Hive queries to a particular YARN queue. For example: Allow the Unravel user to use concurrent hive queries: Set hive.txn.manager=DbTxnManager hive.support.concurrency=true Allow the Unravel user to do the following actions dynamically: Set the following parameters: hive.auto.convert.join\nhive.support.concurrency\nhive.support.sql11.reserved.keywords\nhive.txn.manager\nhive.variable.substitute\nmapred.job.queue.name\nmapreduce.map.java.opts\nmapreduce.map.memory.mb Add JAR and create temporary functions from UDFs in this JAR. Settings for SSL-Enabled Systems Currently not supported. " }, 
{ "title" : "Triggering an import of FSImage", 
"url" : "current/adv/adv-conf-general/adv-conf-triggering-import-of-fsimage.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Configurations \/ Triggering an import of FSImage", 
"snippet" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage # curl -v http:\/\/lo#calhost:5000\/small-files-etl This script ens...", 
"body" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage # curl -v http:\/\/lo#calhost:5000\/small-files-etl This script ensures that the latest FSImage is incorporated in Unravel's Small Files\/File Reports. The etl_fsimage etl_fsimage FSImage Size Run Time etl_fsimage 19 GB 24 hours 9 GB 14 hours 4 GB 7 hours Troubleshooting If etl_fsimage dfsadmin [2018-09-10 23:11:57,357: WARNING\/ForkPoolWorker-1]* stderr: sudo: hdfs: command not found* In this case, do the following: Fetch the FSImage as a user with dfadmin # rm -rf unravel_node_fsimage_dir\/*\nhdfs dfsadmin -fetchImage unravel_node_fsimage_dir These commands delete all existing FSImage files and then copy the latest FSImage into the directory you specify ( unravel_node_fsimage_dir The directory unravel_node_fsimage_dir must \/srv\/unravel\/tmp\/reports\/fsimage and it should be readable by unravel user. Best practice is to run these commands in a cron job that completes before Unravel's etl_fsimage task is triggered every day at 00:00 UTC. Configure Unravel OnDemand to access FSImage from unravel_node_fsimage_dir unravel.properties unravel.python.reporting.files.skip_fetch_fsimage=true\nunravel.python.reporting.files.external_fsimage_dir=unravel_node_fsimage_dir For this to work, the OnDemand user must have read privileges for the directory specified by Note: unravel_node_fsimage_dir. Restart the Unravel OnDemand daemon. # rm -rf unravel_node_fsimage_dir\/*\n hdfs dfsadmin -fetchImage unravel_node_fsimage_dir Unravel OnDemand assumes the FSImage filename starts with fsimage not .txt " }, 
{ "title" : "Connectivity", 
"url" : "current/adv/adv-connectivity.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "current/adv/adv-connectivity/adv-connectivity-connect-config-kafka-stream.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Change the configuration of the Kafka cluster. Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JM...", 
"body" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Change the configuration of the Kafka cluster. Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= port_num The default JMX port for kafka in CDH is 9393. In HDP you would export this parameter under Advanced kafka-env kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS in kafka_run_class.sh -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Djava.rmi.server.hostname=127.0.0.1\n-Djava.net.preferIPv4Stack=true Not required for HDP. Verify the configuration changes on the Kafka cluster. Restart the Kafka broker. Add properties to monitor the Kafka cluster. The unravel daemon, unravel_km Kafka Monitoring Properties In \/usr\/local\/unravel\/etc\/unravel.properties ITALICS italics com.unraveldata.ext.kafka.clusters= cluster-list CLUSTER_ID boot-servers CLUSTER_ID jmx-servers CLUSTER_ID JMX_SERVER_ID jmx-hosts CLUSTER_ID JMX-SERVER_ID jmx-port Example: com.unraveldata.ext.kafka.clusters=c1,c2 \ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093 \ncom.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193 \ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Hive Metastore Access", 
"url" : "current/adv/adv-connectivity/adv-connectivity-hive-access.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access", 
"snippet" : "Enabling Hive Metastore access in Unravel requires the following steps: Gather Hive Metastore Details Configure Unravel to Access Hive Metastore...", 
"body" : "Enabling Hive Metastore access in Unravel requires the following steps: Gather Hive Metastore Details Configure Unravel to Access Hive Metastore " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "current/adv/adv-connectivity/adv-connectivity-hive-access/adv-connecting-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from th...", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API. From CDH version 5.5 onward, use the REST API http:\/\/ CMGR_HOSTNAME_IP Look at the response body, a JSON-like text format as in the image below. Search the response body for metastore Edit \/usr\/local\/unravel\/etc\/unravel.properties for information. See Hive Metastore Configuration Locate and edit the following properties. If necessary, add the properties. Substitute your local values for text in red javax.jdo.option.ConnectionURL=jdbc: drivertype hive_metastore_database_host driverclass hive_metastore_database_password hive_metastore_database_user For example, javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/congo.unraveldata.com\/hive\njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver \njavax.jdo.option.ConnectionUserName=hive \njavax.jdo.option.ConnectionPassword=hadoop Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in | Applications Applications For HDP and MapR Please contact your cluster administrator. " }, 
{ "title" : "Hive Metastore Configuration", 
"url" : "current/adv/adv-connectivity/adv-connectivity-hive-access/adv-connecting-hive-metastore-conf.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration", 
"snippet" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: MySQL Download Connector\/J \/usr\/local\/unravel\/dlib\/mybatis\/ You must confi...", 
"body" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: MySQL Download Connector\/J \/usr\/local\/unravel\/dlib\/mybatis\/ You must configure the following properties for the Data Insights tab to populate its information correctly. All properties are defined in \/usr\/local\/unravel\/etc\/unravel.properties Reference [1] c3p0 project page: http:\/\/www.mchange.com\/projects\/c3p0 " }, 
{ "title" : "Hive Metastore Access", 
"url" : "current/adv/adv-connectivity/adv-connectivity-hive-access/adv-connecting-hive-metastore-conf.html#UUID-087fa033-5fa4-7c8f-07bd-0c5a9ab021e6_UUID-b89885d6-6c2e-2aa1-aecf-28fee748aa84", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver My...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: com.mysql.jdbc.Driver Req string - javax.jdo.option.ConnectionPassword Password used to access the data store. Req string - javax.jdo.option.ConnectionUserName Username used to access the data store. Req string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT Example: postgresql: jdbc:postgresql:\/\/congo.unraveldata.com:7432\/hive Req url " }, 
{ "title" : "Hive Metastore JDBC", 
"url" : "current/adv/adv-connectivity/adv-connectivity-hive-access/adv-connecting-hive-metastore-conf.html#UUID-087fa033-5fa4-7c8f-07bd-0c5a9ab021e6_UUID-29c56966-d8f3-8e73-a16a-bd3b096a15c6", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration \/ Hive Metastore JDBC", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to ...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. count 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. ms 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false boolean false com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. count 5 com.unraveldata.metastore.db.c3p0.idleconnectiontestperiod Optional 0 com.unraveldata.metastore.databasePattern Opt string dname* com.unraveldata.metastore.print.metastore.stats Optional boolean false " }, 
{ "title" : "How to Write Jolokia JMX MBean", 
"url" : "current/adv/adv-how-to-write-jolokia-jmx-bean.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean", 
"snippet" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class MBean can be injected by google Guice and used by Unravel daemon (or by...", 
"body" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class MBean can be injected by google Guice and used by Unravel daemon (or by other java classes). MBean must be thread safe The Unravel daemon (typically for write access). The Jolokia agent (typically for read access). The Jolokia MBean is singleton The Jolokia MBean interface has one mandatory method: which must return an unique MBean ObjectName getName() Jolokia MBean can also contain operations. Operations are methods that can be called remotely on a MBean. They may: Trigger some action on an unravel daemon. Have any number of parameters. Return any supported type. Jolokia supports following MBean attribute types: Primitive types and their object equivalents. List, Set, and Map types. POJO type composed of types mentioned above and which can be nested. Objects of other types have to be converted to supported types, e.g., to string. Use the helper class com.unraveldata.jmx.Converter " }, 
{ "title" : "Example", 
"url" : "current/adv/adv-how-to-write-jolokia-jmx-bean.html#UUID-95179eee-0e3f-fc1a-9f38-47f5239772a5_id_HowtoWriteJolokiaJMXMBean-Example", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Example", 
"snippet" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean public interface DbStatusMXBean extends JolokiaMBean { boolean isConnectionOk(); } Class DbStatusMBean @JsonMBean @Singleton @AutoService(JolokiaMBean.class) public class DbStatusMBean implements DbS...", 
"body" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean\npublic interface DbStatusMXBean extends JolokiaMBean {\n\n boolean isConnectionOk();\n} Class DbStatusMBean @JsonMBean\n@Singleton\n@AutoService(JolokiaMBean.class)\npublic class DbStatusMBean implements DbStatusMXBean {\n private static final String JMX_NAME = \"com.unraveldatal:type=Monitoring,group=Database,name=DbStatus\";\n\n private boolean isConnectionOk;\n private MBeanStatus mBeanStatus;\n\n public synchronized void setConnectionOk(boolean connectionOk) {\n this.isConnectionOk = connectionOk;\n this.mBeanStatus = new MBeanStatus(LocalDateTime.now());\n }\n\n @Override\n public synchronized boolean isConnectionOk() {\n return isConnectionOk;\n }\n\n @Override\n public synchronized MBeanStatus getMBeanStatus() {\n return mBeanStatus;\n }\n\n @Override\n public String getName() {\n return JMX_NAME;\n }\n} Usage DbStatusMonitor @Singleton\npublic class Dl\njavbStatusMonitor {\n\n private final DbStatusMBean mBean;\n\n @Inject\n public DbStatusMonitor(DbStatusMBean mBean) {\n this.mBean = mBean;\n }\n\n void execute(Connection sqlConnection) {\n boolean isConnectionOk = testConnection();\n mBean.setConnectionOk(isConnectionOk);\n }\n} " }, 
{ "title" : "Deploying Unravel over SELinux", 
"url" : "current/adv/adv-how-to-write-jolokia-jmx-bean/adv-conf-security-deploying-unravel-over-selinux.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Deploying Unravel over SELinux", 
"snippet" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux Modes : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabl...", 
"body" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux Modes : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enforced whenever any violations or errors are detected and the violations\/errors are logged. Enforcing : The policies and rules of SELinux are applied over the subjects and objects but are not enforced. All violations and errors based on the SELinux policy are ignored and logged into the log files. If the SELinux policy prevents a specific service from accessing a specific folder, this mode allows access but logs a denial message. This mode provides enough debugging information to fine tune the SELinux Policy so it runs smoothly in enforcing mode. Permissive : No policies are enforced. Disabled SELinux Policy Unravel currently only supports the targeted Prerequisites Enable SELinux on Unravel Node running Linux. Open \/etc\/sysconfig\/selinux Set the SELinux mode. This is SELinux's default; whenever the system reboots it starts SELinux in this mode. See Switching Modesfor how to change the mode while running. SELINUX=enforcing Use the default policy, targeted SELINUXTYPE=targeted Reboot the system to effect the changes. # getenforce\nenforcing Verify the mode setting after reboot. Installing Unravel Core RPMs on a Node with SELinux Install Unravel in permissive mode or enforcing mode. You can install Unravel in either mode. However, installing Unravel in enforcing mode is highly discouraged since SELinux issues a warning regarding uncertainty of functionality. Installing in Permissive Mode (Recommended) Set mode to permissive # setenforce 0\n# getenforce\npermissive Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n SELinux may generate similar alerts during the installation process depending on the environment. But this should not hinder with the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Installing in Enforcing Mode (Highly Discouraged) When Unravel is installed in enforcing Execute getenforce enforcing # getenforce\nenforcing Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n The rpm installation sets SELINUX permissive -----RPM installation log\n+ setenforce Permissive\n+ echo\n+ tee_echo '[CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.'\n+ tee -a \/tmp\/rpm_upgrade.log\n++ date '+%Y-%m-%d %H:%M:%S'\n + echo '[2019-01-28 06:33:17] [CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.' \n+ echo\n+ FILE_CACHE_HEADROOM_MB=2000\n----- # getenforce\npermissive SELinux generates two alerts like the ones below. Similiar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Switch to user. There should be no alerts at this stage. Set SELINUX enforcing # setenforce 1\n# getenforce\nenforcing Run the script switch_to_user.sh X Y switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y Start Unravel services after RPM installation. Run the following command to make sure all services start up successfully. # sudo \/etc\/init.d\/unravel_all.sh start SELinux generates two alerts. Similar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Verify SELINUX is set to enforcing. # getenforce\nenforcing If getenforce permissive # sudo \/etc\/init.d\/unravel_all.sh stop\n# setenforce 0\n# sudo \/etc\/init.d\/unravel_all.sh start Configure Unravel Server and install sensors. Substitute your fully qualified domain name or your host's IP for UNRAVEL_HOST # python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_cdh_setup.py --spark-version 2.3.0 --unravel-server UNRAVEL_HOST Checking for Any Alerts, Denials, or Policy Violations Run these commands to check for any violations which might have after an installation or an operation\/job submission to see if any violations have occurred. To view any Unravel specific alerts: # sealert -a \/var\/log\/audit\/audit.log | grep unravel To view any system specific alerts: # sealert -a \/var\/log\/audit\/audit.log Installing and Using SELinux Tools # yum install setroubleshoot setools\n# yum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setroubleshoot setools setools-console These tools help you get more information about the policy and analyze the avc Use seinfo # seinfo\nStatistics for policy file: \/sys\/fs\/selinux\/policy\nPolicy Version & Type: v.28 (binary, mls)\n\nClasses: 94 Permissions: 262 \nSensitivities 1 Categories: 1024\nTypes: 4747 Attributes: 251\nUsers: 8 Roles: 14\nBooleans: 307 Cond. Expr.: 56\nAllow: 101746 Neverallow: 0\nAuditallow: 155 Dontaudit: 8846\nType_trans: 17759 Type_change: 74\nType_member: 35 Role allow: 39\nRole_trans: 416 Range_trans: 5697\nConstraints: 109 Validatetrans: 0\nInitial SIDs: 27 Fs_use: 29\nGenfscon: 105 Portcon: 602\nNetifcon: 0 Nodecon: 0\nPermissives: 6 Polcap: 2 Use semodule # semodule -DB Use sealert Enter the following command to see all the alerts generated by SELinux # sealert -a \/var\/log\/audit\/audit.log Enter the following command to see Unravel specific alerts # sealert -a \/var\/log\/audit\/audit.log | grep unravel For debugging (in other words, if you're testing in enforcement mode), run the following commands: Log all trivial violations logged by SELinux. # semodule -DB Set the audit log file to 0 so you get to know of access violations happening during the testing of enforcement mode. # > \/var\/log\/audit\/audit.log Working with Modes Checking Which Mode SELinux is Running Retrieve the current SELinux mode. Output is permissive enforcing # getenforce Switching Modes You can switch modes on the fly using the setenforce When Unravel is restarted SELinux returns to the default mode set in \/etc\/sysconfig\/selinux To set permissive mode # setenforce 0 To set enforcement mode # setenforce 1 Installing MySQL in Enforcing Mode Instructions for installing and configuring MySQL are available here If the datadir \/srv\/unravel\/db_data MySQL installation During Configure and Start MySQL Server The following alert is thrown when starting mysqld datadir=\/srv\/unravel\/db_data : If you believe that mysqld should be allowed read access on the plugin.frm file by default Alert During Configure Unravel to Connect My SQL Server The following alert is thrown while creating the database (Step 1). : If you believe that mysqld should be allowed create access on the ibdata1 file by default. Alert The following alert is thrown when creating the schema for Unravel (Step 3). : If you believe that mysqld should be allowed remove_name access on the Alert edge-4.lower-test Sample policy module my-mysqld 1.0;\nrequire {\ntype mysqld_safe_t;\ntype var_t;\ntype mysqld_t;\nclass process siginh;\nclass dir { add_name create remove_name write };\nclass file { create getattr lock open read rename unlink write };\n}\n#============= mysqld_safe_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_safe_t mysqld_t:process siginh;\n#============= mysqld_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:dir { add_name create remove_name write };\nallow mysqld_t var_t:file rename;\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:file { create getattr lock open read unlink write }; " }, 
{ "title" : "Monitoring Workflows", 
"url" : "current/adv/adv-workflows-monitoring.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Monitoring Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "current/adv/adv-workflows-monitoring/adv-workflows-monitoring-oozie.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url= Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to | Applications Workflows...", 
"body" : " Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url= Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to | Applications Workflows " }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "current/adv/adv-workflows-monitoring/adv-workflows-monitoring-ariflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This topic describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that run...", 
"body" : "This topic describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runs the unravel_jcs2 Highlighted text Airflow Web UIAccess Open \/usr\/local\/unravel\/etc\/unravel.properties Http For Airflow Web UI Access Set the following three (3) properties. Replace AIRFLOW_WEB_URL http:\/\/ com.unraveldata.airflow.protocol=http\ncom.unraveldata.airflow.server.url= AIRFLOW_WEB_URL Https For Airflow Web UI Access Set the following four (4) properties. Replace AIRFLOW_WEB_URL https:\/\/ AIRFLOW_WEB_UI_USERNAME AIRFLOW_WEB_UI_PASSWORD com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL AIRFLOW_WEB_UI_USERNAME AIRFLOW_WEB_UI_PASSWORD Restart the unravel_jcs2daemon # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Range By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X Open \/usr\/local\/unravel\/etc\/unravel.properties airflow.look.back.num.days=-X Restart the unravel_jcs2daemon # sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Below is a sample script, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add hooks for Unravel instrumentation. This script below, example-hdp-client.sh spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application only spark-defaults.conf This script can be invoked to submit an Airflow Spark application via spark-submit =\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar PATH_TO_SPARK_EXAMPLE_JAR =10.20.30.40:4043 UNRAVEL_SERVER_IP_PORT =hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog SPARK_EVENT_LOG_DIR example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the workflow. Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call Configuration Properties See Airflow properties Due to an Airflow bug " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "current/adv/adv-rbac-overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Roles and Role Based Access Control", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-rbac.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Roles and Role Based Access Control", 
"snippet" : "Unravel supports three roles. – has complete access to UI with read\/write permissions. Admin – has complete access to the UI but cannot write. Read-Only Admin - has access to the UI but not the Manage page (e.g., can't see auto actions). End-User...", 
"body" : "Unravel supports three roles. – has complete access to UI with read\/write permissions. Admin – has complete access to the UI but cannot write. Read-Only Admin - has access to the UI but not the Manage page (e.g., can't see auto actions). End-User " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-rbac.html#UUID-39a2a71d-49e7-bc3c-8e86-eef3e7c96971_id_RolesandRoleBasedAccessControl-RoleBasedAccessControlRBAC", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Roles and Role Based Access Control \/ Role Based Access Control (RBAC)", 
"snippet" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected by RBAC, i.e., RBAC is irrelevant. If you are not familiar with the concept of tagging, see What is Tagging?...", 
"body" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected by RBAC, i.e., RBAC is irrelevant. If you are not familiar with the concept of tagging, see What is Tagging? " }, 
{ "title" : "RBAC Roles", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-roles.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ RBAC Roles", 
"snippet" : "RBAC allows admins to restrict the pages a specific end-user can view and how those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restric...", 
"body" : "RBAC allows admins to restrict the pages a specific end-user can view and how those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restricted Based Upon Three Factors Tags for the End-user You can create tags for - See Applications Tagging Applications - See Workflows Tagging Workflows End-users are then associated with the tags via LDAP or SAML. See com.unraveldata.login.mode When RBAC is turned on an end-user's view is filtered based upon their tags. For instance, if a user only has the defined tag dept:marketing dept:marketing. Unravel Default Tag is always used to filter the end-user's view. It is set to com.unraveldata.rbac.default Username Mode determines which pages the end-user can access. It is set to com.unraveldata.ngui.user.mode extended In extended Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage Reports | Operational Insights | Chargeback In restricted Application | Applications What the End-user Sees When RBAC is Turned On The available pages (as defined by com.unraveldata.ngui.user.mode (filtered end-user tags) ∪ com.unraveldata.rbac.default If com.unraveldata.rbac.default and How to Exempt an End-User from RBAC To exempt an end-user from RBAC, you must make them a read-only admin " }, 
{ "title" : "Configuring RBAC General Properties", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-configure.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Configuring RBAC General Properties", 
"snippet" : "Open \/usr\/local\/unravel\/etc\/unravel.properties RBAC properties com.unraveldata.login.mode com.unraveldata.login.mode= mode ath_of_tag_file If you are upgrading from 4.3 you must replace\/redefine the following properties. Below we are assuming login.mode ldap login.mode saml saml ldap 4.3 property Re...", 
"body" : " Open \/usr\/local\/unravel\/etc\/unravel.properties RBAC properties com.unraveldata.login.mode com.unraveldata.login.mode= mode ath_of_tag_file If you are upgrading from 4.3 you must replace\/redefine the following properties. Below we are assuming login.mode ldap login.mode saml saml ldap 4.3 property Replacement =ldap com.unraveldata.rbac.mode =ldap com.unraveldata.login.mode =dept- com.unraveldata.rbac.prefix =dept-(.*) com.unraveldata.rbac.ldap.tag.dept.regex.find =dept com.unraveldata.rbac.tag =dept com.unraveldata.rbac.ldap.tags =true com.unraveldata.rbac.user.operations.enabled =(extended | restricted) com.unraveldata.ngui.user.mode You can exempt specific end-users from RBAC effects by adding them to the read-only admin group. Modify the following property, based upon com.unraveldata.login.mode Open com.unraveldata.login.admins.readonly=user1,user2,user3 LDAP com.unraveldata.login.admins.ldap.readonly=user1,user2,user3 SAML com.unraveldata.login.admins.saml.readonly=user1,user2,user3 " }, 
{ "title" : "Configuring LDAP or SAML RBAC Properties", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-configure-ldap-saml.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Configuring LDAP or SAML RBAC Properties", 
"snippet" : "RBAC uses tags, if you are not familiar with tagging please see What is Tagging Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode LDAP SAML LDAP \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 com.unraveldata.rbac...", 
"body" : "RBAC uses tags, if you are not familiar with tagging please see What is Tagging Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode LDAP SAML LDAP \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.ldap.tags.find=proj,dept \ncom.unraveldata.rbac.ldap.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.ldap.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.ldap.groups=RO-admin4,RO-admin5,RO=admin6 SAML \/\/ Required com.unraveldata.login.admins.saml.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.saml.tags.find=proj,dept \ncom.unraveldata.rbac.saml.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.saml.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.saml.groups=RO-admin4,RO-admin5,RO=admin6 Example When a user logs on, their LDAP or SAML group User LDAP Groups Tags Key Value user1 [\"dept-hr,\"dept-sale\",\"dept-finance\"] {\"dept\":[\"hr\",\"sale\",\"finance\"]} dept hr, sales, finance user2 [\"proj-group1\",\"proj-group2\", \"proj-group3\"] {\"proj\":[\"group1\",\"group2\", \"group3\"]} proj group1, group2, group3 user3 [\"proj-group1\",\"proj-group2\", \"proj-group3\", \"dept-hr,\"dept-sale\",\"dept-finance\"] {\"proj\":[\"group1\",\"group2\", \"group3\"]} proj group01, group02, group03 user4 [\"div-div1\",\"div-div2\", \"div-div3\"] n\/a n\/a n\/a and user1 user2 LDAP groups has two valid key, but Unravel stops parsing the when it finds a match. In this case the key user3 proj LDAP groups has one key, user4 div " }, 
{ "title" : "Example RBAC Configurations", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-example-config.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Example RBAC Configurations", 
"snippet" : "Admins and read-only admins are always exempt from RBAC restrictions. To use RBAC, set these properties: =true com.unraveldata.rbac.enabled =[extended | restricted] com.unraveldata.ngui.user.mode [.readyonly] are irrelevant if mode is LDAP or SAML. com.unraveldata.login.admins [empty] In the example...", 
"body" : "Admins and read-only admins are always exempt from RBAC restrictions. To use RBAC, set these properties: =true com.unraveldata.rbac.enabled =[extended | restricted] com.unraveldata.ngui.user.mode [.readyonly] are irrelevant if mode is LDAP or SAML. com.unraveldata.login.admins [empty] In the examples below use LDAP, for SAML just subsitute saml for ldap. Replace your local values for text Set Admin Access To set admin and not read-only admin access, set and comment out: com.unraveldata.login.admins=L772417,K228680\n#com.unraveldata.login.admins.readonly= For LDAP or SAML, set and comment out: com.unraveldata.login.mode=LDAP\ncom.unraveldata.login.admins.ldap.groups=LDAP_Users,,,,\n#com.unraveldata.login.admins.readonly.ldap.groups=LDAP_Users,,,, Set Only Read-Only Admin Access To set only read-only admin access, set and comment out: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680\n#com.unraveldata.login.admins=L772417,K22868 For LDAP or SAML, set set and comment out: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Set Admin and Read-Only Admin Access To set admin and read-only admin access, set: com.unraveldata.login.admins=L772417,K228680\ncom.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Exempt Select End-Users from RBAC To exempt an end-users from RBAC add them to the read-only admin property: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML add them to: com.unraveldata.login.admins.ldap.groups= LDAP_Users " }, 
{ "title" : "RBAC UI", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ RBAC UI", 
"snippet" : "If com.unraveldata.login.mode The tags used for RBAC end-users must also be loaded as Application Workflows com.unraveldata.rbac.default RBAC Configuration Go to Manage | Role Manager The RBAC default is set via com.unraveldata.rbac.enabled If you are not using LDAP\/SAML login mode, you can add filt...", 
"body" : " If com.unraveldata.login.mode The tags used for RBAC end-users must also be loaded as Application Workflows com.unraveldata.rbac.default RBAC Configuration Go to Manage | Role Manager The RBAC default is set via com.unraveldata.rbac.enabled If you are not using LDAP\/SAML login mode, you can add filters for specific end-users. Any end-user roles you have previously set are displayed. If the Unravel daemon was restarted after you added end-user roles the entries are lost. You can add end-users one at at time via Add New Role csv Adding Roles You limit end-user access through tags. In the example below only two tags are available, project tenant department csv Clicking on Add New Role Adding one or more roles via a role file Click on Select role file csv csv : first row is a header row defining the columns tags user, tagKey[,tagKey]* : is a valid tag key, i.e., department, tenant. tagKey : one or more rows defining user and tag values user, tagValue[,tagValue]* : is empty, a valid tag value for tagValue tagKey tagString : is a series of tagString tagValue : means zero or more * Note: The file must tagKey tagValue user After you add your last tagValue .csv s must be ordered as defined in the header row. tagValue No special characters or spaces are allowed in file. The . csv user,project,tenant \nuser72,\"group1,group2\",mm \nuser25,,\"3n,3m\" userNew,groupNew \nuser33,\"group3,group2\",\"3m,mm\"\n Editing or Deleting Roles To edit a role, click the edit glyph ( To delete a role, click the delete glyph. Effect of Role Access Control End-user's Access with RBAC turned off The user has access to all the Unravel UI features and all applications. End-user's Access with RBAC turned on. The user only has access to their applications or those matching their tags. " }, 
{ "title" : "Manage Page", 
"url" : "current/adv/adv-rbac-overview/adv-rbac-manage-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Role Based Access Control (RBAC) \/ Manage Page", 
"snippet" : "This page is only available to Admins and read-only Admins....", 
"body" : "This page is only available to Admins and read-only Admins. " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "current/adv/adv-running-verification-scripts-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "current/adv/adv-running-verification-scripts-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "current/adv/adv-running-verification-scripts-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. Replaced highlighted content UNRAVEL_HOST_IP_ADDRESS CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HO...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. Replaced highlighted content UNRAVEL_HOST_IP_ADDRESS CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS $ \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server UNRAVEL_HOST_IP_ADDRESS Note: You can run this script before configuring the Gateway Automatic Deployment of Spark Instrumentation After you configure the Gateway Automatic Deployment of Spark Instrumentation $ \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark $ \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark $ \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "current/adv/adv-running-verification-scripts-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where LOCATION FNAME $ curl LOCATION FNAME Once downloaded, run md5sum FNAME $ md5sum FNAME Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz 71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz $ tar -zxvf FNAME After unpacking, cd demo_dir $ cd demo_dir The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. $ ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ : contains compiled jar of all examples. lib : contains all the scripts needed to run the example. There are scripts two scripts .\/example $ .\/example $ : contains the source files for the driver program. src: : contains the queries of TPC-H benchmark tpch-query-instances to the data folder which contains the datasets used by the examples. cd $ cd data\n$ ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) $ hdfs dfs -put tpch10g\/ \/tmp\/\n$ hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where $ $ .\/example $ After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example $ The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations $ .\/example $ After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. $ sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. $ sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x Example\/Description Demonstrates example1 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the number of partitions and container sizes for best performance, e.g., increasing the number of partitions and reducing per-container memory resources. example2 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. example3 A Scala program containing a SparkSQL How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, e.g., increasing the number of executors. example4 A Scala-based application. This application generates its input and applies multiple transformations to the generated data. How Unravel helps to root-cause a failed example5 A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group : Add the property Pre-requirement com.unraveldata.spark.events.enableCaching unravel.properties This property is disabled only Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x Example Demonstrates example1 see example1 in Benchmarks for Spark 1.6.x example2 A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, e.g., increasing the number of partitions. example3 - example5 see example3 - example5 in Benchmarks for Spark 1.6.x example6 A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance of a Spark application, e.g., reducing the memory requirements per executor. " }, 
{ "title" : "Tagging", 
"url" : "current/adv/adv-tagging.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging", 
"snippet" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control...", 
"body" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control " }, 
{ "title" : "What is Tagging?", 
"url" : "current/adv/adv-tagging/adv-tagging-what-is-tagging.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ What is Tagging?", 
"snippet" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. tags provide ways to filter applications. Application tags logically group a set of ap...", 
"body" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. tags provide ways to filter applications. Application tags logically group a set of applications. Workflow Application Tagging By the time your application reaches the cluster much of its context is lost, e.g., the dept which submitted it, what project it belongs to, etc. Available Metadata by Default All applications have the following metadata. (See how the metadata fields are populated App type App Id App Name Username Cluster (empty for server-less applications) Queue (empty for server-less applications) Configuration (accessible for some applications) As an admin you need to be able to organize\/view the applications running on the cluster with more granularity than this limited set allows. You will want to “slice and dice\" applications a myriad of ways for a variety of reasons. Unravel can not “magically” deduce which applications belong to what tenants, projects, teams, etc.; to gain such granularity you must provide Unravel with this information. Tagging an app is how you tell Unravel about its context. What is a tag At the basic level a tag is simply a < key value> A simple way to conceptualize tagging is to think of a spreadsheet. The key value App Type App ID App Name User Cluster Queue Tenant Project MR j_134 distc john def root.UM-proj1.print marketing proj1 Spark s_345 rate.spark.sim jane def root.UM-proj2.print sales proj2 Spark s_456 over.spark.sim jane def root.UM-proj3.print sales proj3 See below Effective Use of Tags You can use tags, among other things, to Generate chargeback reports based upon specific criteria, e.g., project, dept, team, etc. Decide which apps a specific user can see, e.g., the marketing head can see all marketing apps while user2 can only see specific marketing project apps. Group applications together. (See Tagging Workflows In order to effectively use tags you need to understand your requirements for displaying or grouping your apps. You might want to consider, among other things Do you need chargebacks reports for each tenant in a multi-tenant cluster? Apps must be tagged with the tenant it belongs to. Do applications need to be billed back to departments and teams? Apps need department and team tags. Do you want to allow some users to see all projects and others just a subset (see Role Based Access Control A specific tag can be used for different purposes. You can use the project tag to generate chargeback reports and to filter views specific users. Assigning Tags You generate a tagging dictionary But like Unravel, the only information you have about the application running on the cluster is its metadata. So how can you develop a script to tag specific applications; how can you determine and generate your < key value Methods to generate\/create tags for an app Naming Conventions By creating naming conventions for app, queues, and cluster names you can embed information to use for your tags, e.g., placing all apps belonging to project-1 root.UM-proj1.prin Using the Metadata You can use the app’s metadata to create the tag values, some examples directly, e.g., <team, username parse it to extract information, e.g., <project, {extracted from queue name concatenate metadata with other metadata or strings, e.g., <dept, { username app name External Mapping Information A tagging script can access files that contain further mapping information. e.g., maps projects to tenants. You can download example tagging scripts Unravel Support Workflow Tagging Workflow tags are much simpler than application tags. You use pre-existing Unravel tags to create workflows, specifically unravel.workflow.name unravel.workflow.utctimestamp Tagging Workflows " }, 
{ "title" : "Example", 
"url" : "current/adv/adv-tagging/adv-tagging-what-is-tagging.html#UUID-5b63b87e-09c3-35dd-d847-8e27ce745fac_N1554596243922", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ What is Tagging? \/ Example", 
"snippet" : "The preceding table is fairly simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. Below we explain how the tags were developed Determining the Tags The cluster is multi-tenant; we created the tags: <tenant, marketing> <tenant, sa...", 
"body" : "The preceding table is fairly simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. Below we explain how the tags were developed Determining the Tags The cluster is multi-tenant; we created the tags: <tenant, marketing> <tenant, sales> There are three projects; we created the tags: <project, proj1> <project, proj2> <project, proj3> Finally, we have a file that maps the projects to the tenants: proj1 to marketing proj2 and proj3 to sales Assigning the Tags Then we told Unravel about the tags and how to assign them, i.e., developed the tagging dictionary. First, an app’s queue is parsed to extract the project it belongs to. The project name is encoded between \"UM-\" and \" . value value key The script resulted in the three applications being assigned tenant and project tags. " }, 
{ "title" : "Tagging Workflows", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows", 
"snippet" : "About Unravel Workflow Tags You can add two Unravel tags (<key, value> pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.w...", 
"body" : " About Unravel Workflow Tags You can add two Unravel tags (<key, value> pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: [Incorrect syntax] SET unravel.workflow.name=\"ETL-Workflow; [Correct syntax] SET unravel.workflow.name=ETL-Workflow; Different runs of the same the same unravel.workflow.name values for different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name Hive Query Example This is a Hive query that was marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\nSELECT foo FROM table WHERE … Your Hive Query text goes here Easy Recipes for Tagging Workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Here, we use bash date export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Follow the instructions for your job type. Hive on MR query Hive on Tez query Sqoop job Direct MapReduce job Spark job Pig job Impala Job Examples by Job Type Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications | Workflows " }, 
{ "title" : "Hive on MR Query Using SET Commands in Hive", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683383875", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Hive on MR Query Using SET Commands in Hive", 
"snippet" : "# hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; SET unravel.workflow.utctimestamp=20160201T000000Z; SELECT foo FROM table WHERE … Your Hive Query text goes here...", 
"body" : "# hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; \nSET unravel.workflow.utctimestamp=20160201T000000Z;\nSELECT foo FROM table WHERE … Your Hive Query text goes here " }, 
{ "title" : "Sqoop Job Using –D Command Line Parameters", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683433104", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Sqoop Job Using –D Command Line Parameters", 
"snippet" : "# sqoop export \\ -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\ --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\ --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar Sqoop has bugs related to quotes...", 
"body" : "# sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes " }, 
{ "title" : "Direct MapReduce Job Using –D Command Line Parameters", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683447404", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Direct MapReduce Job Using –D Command Line Parameters", 
"snippet" : "Substitute your file name for \/tmp\/data\/small \/tmp\/outsmoke # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\ -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\ -p \/wordcount.properties -input \/tmp\/data\/small \/tmp\/outsmoke...", 
"body" : "Substitute your file name for \/tmp\/data\/small \/tmp\/outsmoke # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input \/tmp\/data\/small \/tmp\/outsmoke " }, 
{ "title" : "Spark Job Using --conf Command Line Parameters", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683460679", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Spark Job Using --conf Command Line Parameters", 
"snippet" : "For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name # spark-submit \\ --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" --conf \"spark.eventLog.enabled=true\" \\ --class org.apache...", 
"body" : " For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name # spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster " }, 
{ "title" : "Pig Job Using –param and SET Commands", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683473560", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Pig Job Using –param and SET Commands", 
"snippet" : "# pig \\ -param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\ -x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME; SET unravel.workflow.utctimestamp $UTC_TIME_STAMP; lines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); ...", 
"body" : "# pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME; \nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP; \nlines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); \nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word; \ngrouped = GROUP words BY word; \nwordcount = FOREACH grouped GENERATE group, COUNT(words); DUMP wordcount; " }, 
{ "title" : "Impala Job Using SET Commands", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows.html#UUID-29da8a7b-6201-788e-b7bc-de2566278059_N1554683484495", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Impala Job Using SET Commands", 
"snippet" : "# impala-shell -i <impald_host:port> \\ -f simpleImpala.sql \\ --var=workflowname='ourImpalaWorkflow' \\ --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql SET DEBUG_ACTION=\"::::unravel.workflow.name::${var:workflowname}::::unravel.workflow.utctimestamp::${var:utctimestamp}::::\"; se...", 
"body" : "# impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql SET \n DEBUG_ACTION=\"::::unravel.workflow.name::${var:workflowname}::::unravel.workflow.utctimestamp::${var:utctimestamp}::::\"; \n select * from usstates;; " }, 
{ "title" : "Tagging a Hive on Tez Query", 
"url" : "current/adv/adv-tagging/adv-tagging-workflows/adv-tagging-workflows-hive-on-tez.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Tagging a Hive on Tez Query", 
"snippet" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path= \/usr\/local\/unravel\/etc\/tag_app.py get_tags You can create tagged workflows for tez appl...", 
"body" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path= \/usr\/local\/unravel\/etc\/tag_app.py get_tags You can create tagged workflows for tez applications in four (4) ways. Use --hiveconf via hive command. Enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql set hive.execution.engine=tez;\nselect count(*) from my_test_table; Use the global python script for application tagging. Assuming the global script is \/tmp\/tag_app.py Use--hiveconf via beeline command. Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/ host2.unraveldata.com Use the tez.sql You must define these the two (2) workflow tags in tez.sql set hive.execution.engine=tez;\nset unravel.workflow.name=my_tez_workflow;\nset unravel.workflow.utctimestamp=20180801T000001Z;\nselect count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/ host2.congo6.unraveldata.com " }, 
{ "title" : "Tagging Applications", 
"url" : "current/adv/adv-tagging/adv-tagging-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications", 
"snippet" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script as creating a database comprised of a list keys values <key, value> For example, You have three depar...", 
"body" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script as creating a database comprised of a list keys values <key, value> For example, You have three departments: finance, hr, and marketing. You would create the key give it three values You would then associate applications with one of more of <key, value> one hive query might be associated with dept:marketing dept:market dept:finance See What is Tagging? Your Python script must be idempotent, i.e., it must produce the same result over multiple invocations with different input (metadata) for the same application. Application tags are immutable and once created they cannot be changed. To Use Python Script See Writing a Python Script example script Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path= python_script method_name Restart the following daemons. You must # \/etc\/init.d\/unravel_all.sh stop-etl\n# \/etc\/init.d\/unravel_all.sh start Running Scripts The tags computed in the Python script feed into Unravel core ETL pipeline. The Python script is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. The first time an application is invoked and running it is not listed when applications are filtered by tags. Debug and print statements are logged multiple times as the script is invoked multiple times over a run. References You can download example tagging scripts Unravel Support " }, 
{ "title" : "Writing a Python Script", 
"url" : "current/adv/adv-tagging/adv-tagging-applications.html#UUID-f4df4d7a-cda3-2055-d9e5-eaff424e42c5_N1552802203691", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications \/ Writing a Python Script", 
"snippet" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags Format In the Python script, you set a ta...", 
"body" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags Format In the Python script, you set a tag_key tag_value Your tag_value tag[\"auth\"]=\"admin\" tag[\"scope\"]=app_obj.getAppQueue() tags[\"dept\"]=app_obj.getAppName() + \"_\" + app_obj.getQueue() Field\/Description Where generated Method app_id Application ID Hadoop app_obj.getAppId() app_type Application type: mr, spark, hive, impala, or tez Unravel app_obj.getAppType() app_name Application name Hadoop app_obj.getAppName() username Application's owner Hadoop app_obj.getUsername() queue The queue the app is running in. Hadoop app_obj.getQueue() cluster_id The cluster the the app is running on. : This is not fully supported. When Unravel cannot obtain the cluster_id it's set to Note default Hadoop app_obj.getClusterId() For Hive on MapReduce applications the following property is available. (This method is currently unavailable for Spark or Tez.) getAppConf(\" parameter\" Any field which exists within the MR configuration object. e.g., app_obj.getAppConf (“hive.query.id”) app_obj.getAppConf(\" parameter_name " }, 
{ "title" : "Example Python Script", 
"url" : "current/adv/adv-tagging/adv-tagging-applications.html#UUID-f4df4d7a-cda3-2055-d9e5-eaff424e42c5_N1552802253349", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications \/ Example Python Script", 
"snippet" : "The below script creates seven (7) tag_keys hive_query_id dept team auth scope , and unravel.workflow.name unravel.workflow.utctimestamp tagged workflows The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path-=\/usr\/scripts\/Tagging.py com.unraveldat...", 
"body" : "The below script creates seven (7) tag_keys hive_query_id dept team auth scope , and unravel.workflow.name unravel.workflow.utctimestamp tagged workflows The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path-=\/usr\/scripts\/Tagging.py\ncom.unraveldata.app.tagging.script.method.name-=get_tags # filename: \/usr\/scripts\/Tagging.py\n\nfrom datetime import datetime\n\n# get_tags is the method so com.unraveldata.app.tagging.script.method.name=get_tags \ndef get_tags(app_obj):\n\n tags = {}\n\n# MR apps get the hive_query_id tag\n if app_obj.getAppType() == \"mr\":\n tags[\"hive_query_id\"] = app_obj.getAppConf(\"hive.query.id\")\n\n# every app gets a dept and team tag\n tags[\"dept\"] = app_obj.getAppName() + \"_\" + app_obj.getQueue())\n tags[\"team\"] = app_obj.getUsername()\n\n# Only apps with username=admin get this tag\n if app_obj.getUsername() == \"admin\": \n tags[\"auth\"] = \"admin\"\n\n# Every app gets a scope tag based upon queue they are in\n if app_obj.getQueue() == \"engr\":\n # All apps in the \"engr\" queue get this tag\n tags[\"scope\"] = \"engineering-application\"\n elif app_obj.getQueue() == \"qa\":\n # All apps in the \"qa\" queue get this tag\n tags[\"scope\"] = \"qa-application\"\n else:\n # All apps not in the\"engr\" or \"qa\" queues get this tag\n tags[\"scope\"] = \"daily-application\"\n\n# creates the workflow tags, these are Unravel tags and you should contact support@unraveldata.com before using them\n tags[\"unravel.workflow.name\"] = \"Workflow-\" + tags[\"team\"] \n tags[\"unravel.workflow.utctimestamp\"] = app_obj.getAppType() + \"-\" + str(datetime.utcnow())\n\n\n return tags " }, 
{ "title" : "Unravel APIs", 
"url" : "current/adv/adv-unravel-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "REST API", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel's REST API", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-general.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Unravel's REST API", 
"snippet" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI ...", 
"body" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI with a curl command using the same credentials (see Sign In role All requests\/responses are in JSON Format. You can test APIs via the Manage API Applications\/Apps Like the Applications app APIs application APIs Operations Provides detailed data on various ongoing activities\/resource usage of your cluster. These include memory and vcore usage by cluster, queue, application, and user. You can also obtain information about job status and recent alerts. See here Operations Reports Returns key cluster KPIs and the small files report. See here Generates chargeback reports (by user, queue, app), and Cluster workload reports. See here s Reports | Operational Insight Monitoring Provides lightweight daemon which allows you to monitor various Unravel components via the REST API, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. See here Available REST Resources This API supports a Representational State Transfer (REST) model for accessing a set of resources through a fixed set of operations. API Http Method Description Sign In POST \/signIn Apps POST GET GET GET GET GET GET GET \/apps\/search \/apps\/events\/inefficient_apps\/ \/apps\/status\/running\/ \/apps\/status\/finished\/ \/apps\/resources\/cpu\/allocated \/apps\/resources\/memory\/allocated \/yarn_rm\/kill_app \/yarn_rm\/move_app Applications GET GET GET GET GET \/common\/app\/{app_id}\/recommendation \/common\/app\/{app_id}\/summary \/common\/app\/{app_id}\/status \/common\/app\/{app_id}\/errors \/common\/app\/{app_id}\/logs apps\/{app_id}\/resource_usage Data Insights GET GET \/reports\/data\/kpis \/reports\/data\/small_file_report_details Auto Actions GET GET GET \/autoactions \/autoactions\/recent_violations \/autoactions\/metrics Jobs GET \/jobs\/count Operations GET GET GET GET GET GET GET \/clusters\/resources\/cpu\/allocated \/clusters\/resources\/cpu\/total \/clusters\/resources\/memory\/allocated \/clusters\/resources\/memory\/total \/clusters\/nodes \/clusters\/resources\/tagged\/cpu \/clusters\/resources\/tagged\/memory Reports GET GET GET GET GET \/search\/cb\/appt \/search\/cb\/appt\/user \/search\/cb\/appt\/queue \/reports\/operational\/clusterstats \/reports\/operational\/clusterworkload Workflow GET GET \/workflows \/workflows\/missing_sla HTTP\/HTTPS Methods The API resources listed below follow standard Create-Read-Update-Delete (CRUD) semantics. The HTTP request path defines the Unravel Server and the method the action to perform. Method Operation POST Create entries GET Read entries PUT Update or edit entries DELETE Delete entries Error Codes Upon failure one of the following errors are returned Error Code Description 400 Invalid request parameters; Malformed requests 401 Authentication failure 403 Authorization failure 404 Object not found 500 Internal API error 503 Response temporarily unavailable; the caller should retry later " }, 
{ "title" : "Sign In", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-signin.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Sign In", 
"snippet" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you en...", 
"body" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you enter in the CLI. Click Authorize Post \/signIn Authorize Command Line to your cluster. Be sure to replace ssh ClusterName # ssh@root ClusterName Once you have successfully logged in, enter the curl command from step 2 above. You should see output as shown below. # curl -X POST \"http:\/\/UNRAVEL_HOST:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=user1&password=Password1\" Example Response {\n \"message\": \"ok\",\n \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTUzNzc3NDMwOSwiZXhwIjoxNTM3NzgxNTA5fQ.iD6NXDRj1UqRYr58H4xYlNRcdrWFcU9l3p8NmbpN30k\",\n \"role\": \"admin\",\n \"readOnly\": false,\n \"tags\": \"\",\n \"id\": \"admin\",\n \"username\": \"admin\"\n} The token is needed for each REST API curl command executed. Each time you log in a new token is generated to be used in commands for that session. " }, 
{ "title" : "Apps APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-apps.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Apps APIs", 
"snippet" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | s...", 
"body" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | spark | pig | cascading | impala | tez appTypes : S(ucess) | F(ailed) | K(illed) | R(unning) | W | P(ending) | U(nkown) appStatus : valid users for the Unravel_host users : valid queues for the Cluster queues : \"key\":[\"value,value\"], e.g., \"dept\":[finance, mktg] taglist For Hive and MR you must specify at least one Status type. Code: 200\n{\n \"metadata\": {},\n \"results\": [\n { }\n ]\n} curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" All Failed, Killed and Unknown Hive and Tez Apps Apps All apps with the failed, killed or unknown status regardless of user, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"hive\",\"tez\"],\"appStatus\":[\"\"F\",\"K\",\"U\"],\"end_time\":\"2018-10-09T05:18:42.000Z\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App User All apps owned by a user regardless of status, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"users\":[\"root\"]}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search Sample Output {\n \"metadata\": {\n \"duration\": {\n \"max\": 21228577,\n \"min\": 0\n },\n \"resource\": {\n \"max\": 1,\n \"min\": 0\n },\n \"events\": {\n \"max\": 4,\n \"min\": 0\n },\n \"appTypes\": {\n \"mr\": 115,\n \"hive\": 67\n },\n \"appStatus\": {\n \"S\": 168,\n \"F\": 9,\n \"K\": 4,\n \"R\": 1\n },\n \"users\": {\n \"root\": 106,\n \"hdfs\": 68,\n \"user11\": 8\n },\n \"queues\": {\n \"root.users.root\": 83,\n \"root.users.hdfs\": 62,\n \"root.users.user11\": 8,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjhfsjhfhfsjkhfjhfdjhfj\": 6,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjh\": 4,\n \"root.abcdefghijklmnopqrstuvwxyz\": 1,\n \"root.pooja.pooja\": 1\n },\n \"clusters\": {\n \"default\": 157\n },\n \"totalRecords\": 182\n },\n \"results\": [\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1201\",\n \"nick\": \"1201\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:48:29\",\n \"start_time_long\": \"2018-08-30T11:48:29.315Z\",\n \"duration_long\": 88000,\n \"predDuration_long\": 0,\n \"io_long\": 327736439,\n \"read_long\": 293122770,\n \"write_long\": 34613669,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 5,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 5,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 67916,\n \"totalReduceSlotDuration\": 38679,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n }\n ]\n} Returns list of events Inefficient apps \/apps\/events\/inefficient_apps\/ Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to : Type of applications. 0=> MR, 1 => HIVE , 2 => SPARK, 16 =>IMPALA entity_type Response Schema '200':\nschema:\ntype: array\nitems:\nstring curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"F\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\"}' http:\/\/myserver.com:3000\/api\/v1\/apps\/search Returns list of running apps \/apps\/status\/running\/ No Parameters Response Schema '200':\n{\n\"date\":\n\"appsRunning\":\n\"appsPendiing\"::}\n} curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output {\n \"date\":1551947819180,\n \"appsRunning\":3,\n \"appsPending\":0}\n} Returns list of finished apps \/apps\/status\/finished\/ No Parameters Response Schema '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for Allocated CPU by app type \/apps\/resources\/cpu\/allocated Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Response Schema '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for allocated memory by app type \/apps\/resources\/memory\/allocated Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Response Schema '200':\nschema:\ntype: array\nitems:\nstring curl -X GET \"http:\/\/myserver.co:3000\/api\/v1\/apps\/resources\/memory\/allocated?from=2019-03-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" {\n \"1551787200000\":\"2\"\n}} \/yarn_rm\/kill_app - Returns the status code only Parameters: : cluster cluserid : app id appid Response Schema '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/kill_app?clusterid=ignite1&appid=application_1550764666755_0668\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" \/yarn_rm\/move_app - Returns the status code only Required Parameters: : cluster cluserid : app id appid :queue name queue Response Schema '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/move_app?clusterid=ignite1&appid=application_1550764666755_0668&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" " }, 
{ "title" : "Applications APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Applications APIs", 
"snippet" : "All commands require: : Unravel server Unravel_Host : port # Port : assigned application id app_id Unravel's Recommendations for the Application \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v1\/common\/app\/job_1543784013107_1631\/recommendation\" -H \"accept: application\/js...", 
"body" : "All commands require: : Unravel server Unravel_Host : port # Port : assigned application id app_id Unravel's Recommendations for the Application \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v1\/common\/app\/job_1543784013107_1631\/recommendation\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTQwNzI5MiwiZXhwIjoxNTUxNDE0NDkyfQ.Kf8bIhSoWFVY2uaR0WHrPw_nq6k0iVH9piSnqbO6vLg\" Sample Output Summary [\n {\n \"parameter\": \"mapreduce.map.memory.mb\",\n \"current_value\": \"7596\",\n \"recommended_value\": \"3896\"\n },\n {\n \"parameter\": \"mapreduce.map.java.opts\",\n \"current_value\": \"-Xmx8192m\",\n \"recommended_value\": \"-Xmx3117m\"\n }\n] Application's Summary \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/summary\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Summary {\n \"@class\": \"com.unraveldata.annotation.HiveQueryAnnotation\",\n \"vcoreSeconds\": 0,\n \"memorySeconds\": 0,\n \"cents\": 0,\n \"version\": 1,\n \"source\": \"post-db\",\n \"kind\": \"hive\",\n \"id\": \"parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\",\n \"nick\": \"Hive Query\",\n \"status\": \"S\",\n \"user\": \"parwinder\",\n \"mrJobIds\": [\n \"job_1550559654567_8334\"\n ],\n \"duration\": 82120,\n \"startTime\": 1551405626487,\n \"numMRJobs\": 0,\n \"totalMRJobs\": 1,\n \"totalMapTasks\": 0,\n \"sm\": 0,\n \"km\": 0,\n \"kmu\": 0,\n \"fm\": 0,\n \"fmu\": 0,\n \"totalReduceTasks\": 0,\n \"sr\": 0,\n \"kr\": 0,\n \"kru\": 0,\n \"fr\": 0,\n \"fru\": 0,\n \"totalMapSlotDuration\": 0,\n \"totalReduceSlotDuration\": 0,\n \"totalDfsBytesRead\": 0,\n \"totalDfsBytesWritten\": 0,\n \"queryString\": \"\\nINSERT OVERWRITE DIRECTORY '\\\/user\\\/benchmark-user\\\/benchmarks\\\/oozie\\\/workflows\\\/road_accident_db\\\/output\\\/'\\nROW FORMAT DELIMITED\\nFIELDS TERMINATED BY '\\\\t'\\nSTORED AS TEXTFILE\\nselect reflect(\\\"java.lang.Thread\\\", \\\"sleep\\\", bigint(60000))\",\n \"type\": \"DML\",\n \"numEvents\": 0\n} Application's Status \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/status\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN Sample Output {\n \"status\": \"Success\",\n \"message\": \"The app status of parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF is Success\"\n} Application's Errors \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/errors\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN Sample Output No errors found Application's Logs \/common\/app\/ app_id curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-u_SOyF\/logs\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN Sample Output No log view found for app hive_20190126160707_1d3f4e51-55eb-4cc7-8a8b-bca05d598920\n Application's Resource Usage \/apps\/ app_id Required Parameter metric_id: where 115: gcLoad 125: maxHeap 126: usedHeap 134: processCpuLoad 135: systemCpuLoad 137: availableMemory 138: vmRss curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/apps\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/logs\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN Sample Output [\n {\n \"appId\":\"job_1552417866244_0074\",\n \"unravel_metric_id\":138,\n \"system\":\"mr\",\n \"entity_id\":\"a_1\",\"entity_name\":\"a_1\",\n \"host_name\":\"congo28.unraveldata.com\",\n \"sampleCount\":2,\"sum\":772329472,\n \"min\":384274432,\n \"max\":388055040,\n \"avg\":386164736,\n \"ts\":1553500850000,\n \"endTs\":1553500860000,\n \"unit\":\"BYTES\"\n }\n] " }, 
{ "title" : "Auto Actions APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Auto Actions APIs", 
"snippet" : "All commands require: : Unravel server Unravel_Host : port # Port Output schema: schema: type: array items: type: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization:...", 
"body" : "All commands require: : Unravel server Unravel_Host : port # Port Output schema: schema:\ntype: array\nitems:\ntype: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization: JWT TOKEN\" [{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"5346557784540652874\",\n\"name_by_user\":\"Long running YARN application Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125124967,\"updated_at\":1551861204577,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"elapsedTime\",\"compare\":\">\",\"state\":\"*\",\"type\":\"mapreduce\",\"value\":10000}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false},{\"action\":\"kill_app\"}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user7\"],\"user_transform\":\"\",\"app_mode\":2,\"app_list\":[\"QuasiMonteCarlo\"],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":1,\"start_hour\":13,\"start_min\":59,\"end_hour\":13,\"end_min\":59,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"7944134729015777055\",\n\"name_by_user\":\"Long running Hive query Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125155192,\"updated_at\":1551787684684,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"duration\",\"compare\":\">=\",\"state\":\"*\",\"type\":\"hive\",\"value\":100}]}],\"actions\":[{\"action\":\"http_post\",\"urls\":[\"https:\/\/unraveldata.slack.com\/messages\/CA2RX1M35\/\"]}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":0,\"user_list\":[],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":5,\"end_hour\":12,\"end_min\":5,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"2556543518905386312\",\n\"name_by_user\":\"Resource contention in cluster allocated memory Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125186605,\"updated_at\":1551787687869,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedMB\",\"compare\":\">\",\"value\":1024},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user1\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"8761136669870544897\",\n\"name_by_user\":\"Resource contention in cluster allocated vcores Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125217281,\"updated_at\":1551787691083,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedVCores\",\"compare\":\">=\",\"value\":2},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">=\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user2\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"}] Returns list of violations \/autoactions\/violations Parameters: : starting date of date range. Format from YYYY-MM-DD : ending date of date range. Format to YYYY-MM-DD : maximum number of violations to return limit curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/recent_violations?from=2019-03-01&to=2019-03-06&limit=10\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Returns list of auto action metrics \/autoactions\/metrics No parameters curl -X GET \"http:\/\/devcdh513k.unraveldata.com:3000\/api\/v1\/autoactions\/metrics\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" { \"appCount\",\"elapsedTime\",\"allocatedMB\",\"allocatedVCores\",\"runningContainers\",\"memorySeconds\",\"vcoreSeconds\",\"jobCount\",\"elapsedAppTime\",\"mapsTotal\",\"mapsCompleted\",\"reducesTotal\",\"reducesCompleted\",\"mapsPending\",\"mapsRunning\",\"reducesPending\",\"reducesRunning\",\"newReduceAttempts\",\"runningReduceAttempts\",\"failedReduceAttempts\",\"killedReduceAttempts\",\"successfulReduceAttempts\",\"newMapAttempts\",\"runningMapAttempts\",\"failedMapAttempts\",\"killedMapAttempts\",\"successfulMapAttempts\",\"badId\",\"connection\",\"ioError\",\"wrongLength\",\"wrongMap\",\"wrongReduce\",\"fileBytesRead\",\"fileBytesWritten\",\"fileReadOps\",\"fileLargeReadOps\",\"fileWriteOps\",\"hdfsBytesRead\",\"hdfsBytesWritten\",\"hdfsReadOps\",\"hdfsLargeReadOps\",\"hdfsWriteOps\",\"mapInputRecords\",\"mapOutputRecords\",\"mapOutputBytes\",\"mapOutputMaterializedBytes\",\"splitRawBytes\",\"combineInputRecords\",\"combineOutputRecords\",\"reduceInputGroups\",\"reduceShuffleBytes\",\"reduceInputRecords\",\"reduceOutputRecords\",\"spilledRecords\",\"shuffledMaps\",\"failedShuffle\",\"mergedMapOutputs\",\"gcTimeMillis\",\"cpuMilliseconds\",\"physicalMemoryBytes\",\"virtualMemoryBytes\",\"committedHeapBytes\",\"totalLaunchedMaps\",\"totalLaunchedReduces\",\"dataLocalMaps\",\"slotsMillisMaps\",\"slotsMillisReduces\",\"millisMaps\",\"millisReduces\",\"vcoresMillisMaps\",\"vcoresMillisReduces\",\"mbMillisMaps\",\"mbMillisReduces\",\"bytesRead\",\"bytesWritten\",\"duration\",\"totalDfsBytesRead\",\"totalDfsBytesWritten\",\"inputRecords\",\"outputRecords\",\"outputToInputRecordsRatio\",\"totalJoinInputRowCount\",\"totalJoinOutputRowCount\",\"inputPartitions\",\"outputPartitions\",\"joinInputRowCount\",\"joinOutputRowCount\",\"joinOutputToInputRowRatio\"\n}] " }, 
{ "title" : "Data APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Data APIs", 
"snippet" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Parameters within the command are indicated by parameter UNRAVEL_HOST KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST NumDays token Required parameters are: : number of days numDays Schema [ { \"st\": start t...", 
"body" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Parameters within the command are indicated by parameter UNRAVEL_HOST KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST NumDays token Required parameters are: : number of days numDays Schema [\n {\n \"st\": start time EPOCH_timestamp,\n \"et\": end time EPOCH_timestamp,\n \"nlaTb\": # of tables accessed,\n \"nlaPr\": # of partitions accessed,\n \"nlaQr\": # of queries accessing the table,\n \"nlaRi\": Total Read I\/O due to accessing the Tables,\n \"nlcTb\": # of tables created,\n \"nlcPr\": # of partitions created,\n \"nlcTz\": size of tables created,\n \"nlcPz\": size of partitions created,\n \"ntoTb\": total number of tables in the system,\n \"ntoPr\": total number of partitions in the system,\n \"nhtTb\": ,\n \"nwaTb\": ,\n \"ncoTb\": ,\n \"nhtPr\": ,\n \"nwaPr\": ,\n \"ncoPr\": ,\n \"rp\": ,\n \"rs\": ,\n \"fs\": ,\n \"users\": [\n \/\/ comma separated list of users\n ]\n }\n] curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/kpis?numDays=1\" -H \"accept: application\/json\" -H \"Authorization: JWT token Sample Output [\n {\n \"st\": 1536795705,\n \"et\": 1536882105,\n \"nlaTb\": 15,\n \"nlaPr\": 0,\n \"nlaQr\": 57,\n \"nlaRi\": 101489411821,\n \"nlcTb\": 7,\n \"nlcPr\": 0,\n \"nlcTz\": 304768890,\n \"nlcPz\": 0,\n \"ntoTb\": 378,\n \"ntoPr\": 30061,\n \"nhtTb\": 19,\n \"nwaTb\": 0,\n \"ncoTb\": 359,\n \"nhtPr\": 1823,\n \"nwaPr\": 0,\n \"ncoPr\": 28238,\n \"rp\": 30061,\n \"rs\": 92544293666,\n \"fs\": 92544293666,\n \"users\": [\n \"root\",\n \"hdfs\"\n ]\n }\n] " }, 
{ "title" : "Monitors REST API", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-monitors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Monitors REST API", 
"snippet" : "Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response { \"request\": { \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\", \"attribute\": \"MBeanStatus\", \"type\": \"read\" }, \"value\": { \"lastUpdated\": \"2018-05-15T08:31:36\", \"lastUpdateSucces...", 
"body" : "Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response {\n \"request\": {\n \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\",\n \"attribute\": \"MBeanStatus\",\n \"type\": \"read\"\n },\n \"value\": {\n \"lastUpdated\": \"2018-05-15T08:31:36\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n },\n \"timestamp\": 1526373099,\n \"status\": 200\n} There is value Set MONITOR_HOST points to node where monitoring service is running MONITOR_HOST PartitionInfo Monitor MBean Status http:\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:30:17\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Partitions Info http:\/\/ MONITOR_HOST PartitionInfo - Output {\n \"\\\/home\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/home\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/tmp\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/tmp\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n }\n} DbStatus Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:31:49\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true DbPerformance Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:45:06\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true Last Query Duration http:\/\/ MONITOR_HOST LastQueryDuration - Output 14 Query Timed Out http:\/\/ MONITOR_HOST QueryTimedOut false Query Exception http:\/\/ MONITOR_HOST QueryException - Output RuntimeException: Cannot read configuration Zookeeper Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T11:33:32\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMax\": 0,\n \"leader\": false,\n \"follower\": false,\n \"created\": \"2018-05-14T11:33:52\",\n \"connectionsCount\": 0,\n \"mode\": null,\n \"latencyAvg\": 0,\n \"latencyMin\": 0,\n \"port\": 2000,\n \"outstandingCount\": 0,\n \"host\": \"localhost\",\n \"nodeCount\": 0,\n \"ok\": false\n }\n] Historical Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMinAvg\": 0,\n \"connectionsCountTrend\": 0,\n \"nodeCountTrend\": 0,\n \"latencyMinTrend\": 0,\n \"outstandingCountTrend\": 0,\n \"latencyAvg\": 0,\n \"latencyMaxTrend\": 0,\n \"port\": 2000,\n \"isOkCount\": 0,\n \"host\": \"localhost\",\n \"outstandingCountAvg\": 0,\n \"connectionsCountAvg\": 0,\n \"leaderCount\": 0,\n \"followerCount\": 0,\n \"latencyAvgTrend\": 0,\n \"latencyMaxAvg\": 0,\n \"nodeCountAvg\": 0,\n \"isNotOkCount\": 5\n }\n] Kafka Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T11:55:24\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST {\n \"consumerGroups\": [\n {\n \"groupName\": \"0_1790010567376612\",\n \"consumerTopicList\": [\n {\n \"consumerHost\": \"172.16.1.111\",\n \"clientId\": \"unravel_diag_meta\",\n \"lag\": 0,\n \"currentOffset\": 0,\n \"partitionId\": 0,\n \"consumerId\": \"unravel_diag_meta-1971faac-1d57-462b-b73e-c45e9c3cee52\",\n \"topicName\": \"meta\",\n \"logEndOffset\": 0\n }\n ]\n }\n ],\n \"created\": \"2018-05-14T12:06:38\",\n \"kafkaRunning\": true,\n \"topicList\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"meta\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ],\n \"topicsWithoutConsumer\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ]\n} Historical Data http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.kafka.history.size property Elastic Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T12:15:02\",\n \"lastUpdateSuccessful\": false,\n \"errorMessage\": \"Cannot retrieve ElasticSearch data: Cannot get ElasticSearch data. Address: sako1:4171\",\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST Recent Data {\n \"running\": true,\n \"nodes\": [\n {\n \"indices\": {\n \"search\": {\n \"fetchTimeInMillis\": 0,\n \"queryTimeInMillis\": 0,\n \"scrollTimeInMillis\": 0\n },\n \"docs\": {\n \"deleted\": 0,\n \"count\": 0\n },\n \"indexing\": {\n \"noopUpdateTotal\": 0,\n \"indexTimeInMillis\": 0,\n \"throttleTimeInMillis\": 0,\n \"indexCurrent\": 0,\n \"deleteTimeInMillis\": 0,\n \"deleteCurrent\": 0,\n \"indexTotal\": 0,\n \"indexFailed\": 0,\n \"deleteTotal\": 0,\n \"throttled\": false\n },\n \"get\": {\n \"missingTimeInMillis\": 0,\n \"existsTimeInMillis\": 0,\n \"timeInMillis\": 0\n },\n \"store\": {\n \"sizeInBytes\": 0,\n \"throttleTimeInMillis\": 0\n }\n },\n \"roles\": [\n \"master\",\n \"data\",\n \"ingest\"\n ],\n \"name\": \"unravel_s_1\",\n \"timestamp\": 1526327791715\n }\n ],\n \"port\": 4171,\n \"created\": \"2018-05-14T19:56:33\",\n \"host\": \"sako1\",\n \"clusterHealth\": {\n \"activeShardsPercentAsNumber\": 100,\n \"numberOfPendingTasks\": 0,\n \"numberOfInFlightFetch\": 0,\n \"timedOut\": false,\n \"activePrimaryShards\": 0,\n \"unassignedShards\": 0,\n \"numberOfFailedNodes\": 0,\n \"numberOfNodes\": 1,\n \"taskMaxWaitingInQueueMillis\": 0,\n \"initializingShards\": 0,\n \"numberOfDataNodes\": 1,\n \"relocatingShards\": 0,\n \"clusterName\": \"unravel18679\",\n \"activeShards\": 0,\n \"delayedUnassignedShards\": 0,\n \"numberOfSuccessfulNodes\": 1,\n \"status\": \"green\"\n }\n} Historical Data http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.elastic.history.size " }, 
{ "title" : "Operational APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-operational.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Operational APIs", 
"snippet" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: All commands require the following: Unravel server Unravel_Host: port # Port: start timestamp Start: end timestamp End: see token: sign in All...", 
"body" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: All commands require the following: Unravel server Unravel_Host: port # Port: start timestamp Start: end timestamp End: see token: sign in All timestamps are in EPOCH. You must substitute your local or relevant information for fields indicated by RED CLUSTER Cluster Vcore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port resourceType Query End Interval Start token Required Parameters: : cpu | memory Resoure Type cpu: vcores memory: returns memory in bytes : allocated | total Query Type allocated: allocated total: used : 1m | 5m | 10m | 30m |1hk Polling Interval Schema {\n EPOCH_timestamp : count\n} Allocated Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?to=1538666160&interval=30m&from=1536273311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output {\n \"1535360400000\": \"12\", \n \"1535364000000\": \"3.125\", \n \"1535367600000\": \"5.0303030303\"\n} Total Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/memory\/total?to=1536273841&interval=1h&from=1536187441\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output {\n \"1535364000000\": 47460,\n \"1535367600000\": 45087,\n \"1535371200000\": 37968\n} Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End Interval Start token Additional required parameters: : 1m | 5m | 10m | 30m |1h Polling Interval Schema : total = active + unhealthly Note { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n \"total\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"active\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"lost\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"unhealthy\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"decommissioned\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"rebooted\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }\n}\n Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/nodes?to=1536339111&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output {\n \"date\": [\n 1535688000000,\n 1535691600000,\n 1535695200000,\n 1535698800000\n ],\n \"total\": {\n \"1535688000000\": 3,\n \"1535691600000\": 3,\n \"1535695200000\": 3,\n \"1535698800000\": 3\n },\n \"active\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"lost\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n },\n \"unhealthy\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"decommissioned\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"rebooted\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n }\n} APPLICATION VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End AppType Interval Start token Required parameters: : Query Type cpu: returns vcores as count memory: returns memory in bytes : cascading | impala | hive | mr | pig | spark | tez Application Type : 1m | 5m | 10m | 30m |1h Polling Interval Schema {\n EPOCH_timestamp : count\n} Mapreduce Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22MAPREDUCE%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Spark Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22SPARK%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" User VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End UserName Interval Start token Required parameters: : Query Type cpu: returns vcores as count memory: returns memory in bytes : username, specify multiple users with a comma separated list User : 1m | 5m | 10m | 30m |1h Polling Interval Schema {\n EPOCH_timestamp : result\n} Output Format Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" QUEUE VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End QueueName Interval Start token Additional required parameters: : Query Type cpu: returns vcores as count memory: returns memory in bytes : queue, specify multiple queues with a comma separated list Queue : 1m | 5m | 10m | 30m |1h Polling Interval Schema {\n EPOCH_timestamp : result\n} Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" JOBS Returns average number of jobs by Groupby type. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End GroupBy Interval Start token Required parameters state | applicationType | user | queue Groupby: : 1m | 5m | 10m | 30m |1h |1d | 1w Polling Interval State Example Schema - State {\n \"date\": [\n EPOCH_timestamp\n ],\n \"RUNNING\": {\n EPOCH_timestamp: average\n },\n \"ACCEPTED\": {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536275822&groupBy=state&interval=1h&from=1535671022\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output {\n \"date\": [\n 1535626800000\n ],\n \"RUNNING\": {\n \"1535626800000\": \"1.3333333333\"\n },\n \"ACCEPTED\": {\n \"1535695200000\": \"1\"\n }\n} Application Type Schema - Application {\n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n\n APP_TYPE : { \n \/\/ APP_TYPE mr | hive | spark | pig | cascading | impala | tez \n \/\/ for each polling interval where the app type is running\n EPOCH_timestamp: count\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536280789&groupBy=applicationType&interval=1h&from=1535675989\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - Application {\n \"date\": [\n 1535644800000,\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000\n ],\n \"MAPREDUCE\": {\n \"1535644800000\": \"1\",\n \"1535695200000\": \"1\",\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1.5\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1.1538461539\"\n },\n \"SPARK\": {\n \"1535698800000\": \"1\"\n }\n} User Example Schema - User {\n \"date\": [\n EPOCH_timestamp\n ],\n userName: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536281730&groupBy=user&interval=1h&from=1535676930\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - User {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000\n ],\n \"root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\"\n },\n \"hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\"\n }\n} Queue Example Schema - Queue {\n \"date\": [\n EPOCH_timestamp\n ],\n queue Name: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536339111&groupBy=queue&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample - Queue {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000,\n 1536044400000,\n 1536048000000,\n 1536051600000,\n 1536055200000,\n 1536058800000,\n 1536076800000\n ],\n \"root.users.root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536076800000\": \"1\"\n },\n \"root.users.hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535961600000\": \"2\",\n \"1536040800000\": \"1.1304347826\",\n \"1536044400000\": \"1\",\n \"1536055200000\": \"1.3333333333\",\n \"1536058800000\": \"1\"\n },\n \"root.users.user11\": {\n \"1535698800000\": \"1\"\n },\n \"root.abcdefghijklmnopqrstuvwxyz\": {\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536044400000\": \"1\"\n }\n} " }, 
{ "title" : "Reports APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Reports APIs", 
"snippet" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel server Unravel_Host: port # Port: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. c...", 
"body" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel server Unravel_Host: port # Port: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: : starting date of date range. Format YYYY-MM-DD gte : ending date of date range. Format YYYY-MM-DD lte Response Schema {\n \"cb\": [\n \/\/ array organizing by application type\n {\n \"ms\": memory usage seconds,\n \"count\": application count,\n \"v1\": \"application type (mr | spark)\",\n \"vs\": vcore usage in second\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt?from=1536670860<to=1536757260\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample Output {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Chargeback by user \/search\/cb\/appt\/user Required parameters: : starting date of date range. Format YYYY-MM-DD gte : ending date of date range. Format YYYY-MM-DD lte Response Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback by queue \/search\/cb\/appt\/queue Required parameters: : starting date of date range. Format YYYY-MM-DD gte : ending date of date range. Format YYYY-MM-DD lte Response Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback app type Return report by application type for a specific queue. Required parameters: : mr | spark | tez AppType : name of queue Queue Name : EPOCH timestamp Start : EPOCH timestamp End curl -X GET \"http:\/\/ UNRAVEL_HOST AppType End Start QueueName Response Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"ms\": memory usage in milliseconds,\n \"count\": application count,\n \"v2\": userName | queueName,\n \"vs\": vcore usage in seconds\n }\n ]\n}\n Example App Type = mr # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/user?appt=mr>e=1536677580<e=1536763980&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Clusters Cluster Summary curl -X GET \"http:\/\/ UNRAVAcrEL_HOST Required parameters : user | app | queue Mode :EPOCH timestamp Start : EPOCH timestamp End Response Schema for user or queue {\n \"userStats\": [\n\/\/ array of users | queues currently on cluster\n {\n \"root\": {\n \"running\": {\n \"min\": minimum applications running,,\n \"max\": minimum applications running,\n \"mean\": average appication number running,\n \"stddev\": standard deviation\n },\n \"memory\": { \/\/ see running above },\n \"pending\": { \/\/ see running above },\n \"vcores\": { \/\/ see running above }\n }\n }\n ]\n} Example mode = user # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=user&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\" Sample output mode = user {\n \"userStats\": [\n {\n \"root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Example mode = queue # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=queue&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\" Sample output mode = queue {\n \"queueStats\": [\n {\n \"root.users.root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Output for mode = app {\n [\n appID : application id,\n type : memorySeconds\/vcoreSeconds\n vcore : vcore value\n memory : memory value\n ]\n} Cluster Workload curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: : EPOCH timestamp Start : EPOCH timestamp End : month | hour | month | hourday reportby Response Schema \/\/ work load by month one of more months with the application count for month\n\/\/ minimum of one month\n { timestamp: appcount[,timestamp: appcount] }\n\n\/\/ work load by hour array of 25 hours\n [\n { timestamp: appcount }, ... { timestamp: appcount }\n ]\n\n\/\/ work load by day - array for each days contained with the time period (Mon - Sun)\n\/\/ minimum of one day\n [\n { timestamp: appcount }\n ]\n\n\/\/ work load by hour\/day:array for each day (Mon - Sun) by hour\n\/\/ minimum of 24 hours for one day\n [\n { timestamp: appcount }\n ]\n\n curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterworkload?gte=1536777000Z<=1536863400Z&reportBy=month\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample output {\"1536777000\":96,\"1536863400\":4} \n[\n{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}\n[\n{\"1536777000000\":96},{\"1536863400000\":7}\n]\n[{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}]\n " }, 
{ "title" : "Small Files", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-small-files.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Small Files", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Workflow APIs", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-rest/adv-unravel-apis-rest-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Workflow APIs", 
"snippet" : "\/workflows All commands require: Unravel_Hos : port # Port : application id app_id List of Workflows \/workflows No Parameters : Response Schema Code: 200 type: array items: type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN...", 
"body" : "\/workflows All commands require: Unravel_Hos : port # Port : application id app_id List of Workflows \/workflows No Parameters : Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with only One Missing SLA: {\n \"Benchmark: Road_Accident_2005-2016\": {\n \"key\": \"Benchmark: Road_Accident_2005-2016\",\n \"doc_count\": 565,\n \"duration_stats\": {\n \"count\": 565,\n \"min\": 21000,\n \"max\": 772000,\n \"avg\": 174877.8761061947,\n \"sum\": 98806000,\n \"sum_of_squares\": 17972790000000,\n \"variance\": 1227976236.197041,\n \"std_deviation\": 35042.49186626204,\n \"std_deviation_bounds\": {\n \"upper\": 244962.85983871878,\n \"lower\": 104792.8923736706\n }\n }\n }\n} List of Workflows which are missing SLA \/workflows\/missing_sla Required Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to : Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\/missing_sla?from=2019-02-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with no Missing SLAs. [ ] " }, 
{ "title" : "Use Case - Auto Actions and Pagerduty", 
"url" : "current/adv/adv-unravel-apis/adv-unravel-apis-use-case-autoactions-and-pagerduty.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel APIs \/ Use Case - Auto Actions and Pagerduty", 
"snippet" : "The auto action's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a third party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be d...", 
"body" : "The auto action's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a third party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's auto actions API Currently, this action is initiated outside of Unravel's Server. For the integration you need to complete two (2) setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerduty information. The python script must pagerduty Using pagerduty for notifications Set up a pagerduty service. Login to your Pagerduty Account. Select the Configuration Services Pager Duty Select Add New Service Name the new service. Select Use our API directly Events API v2 Use the defaults for the remaining options and click Add Service Copy the Integration Key Integrations Run the Unravel API python script on your local computer. Download the Unravel demo program # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 to cd unravel-api-demo-v1 # cd unravel-api-demo-v1\n# ls\nREADME.md Run the python script to bring up the Unravel API Demo window Enter your Unravel Server's address in Unravel Autoactions API Address The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions UNRAVEL_HOST_IP https:\/\/playground.unraveldata.com. Enter the pagerduty integration key in the Pagerduty API key Click Start To scroll within list, click within the Autoaction Name Open your browser and login to the Unravel Server you entered above. Navigate to the Auto Actions page ( Manage Auto Actions Creating Auto Actions Both the Unravel API window and the Unravel UI shows the exact same list of Autoactions and their status. Toggle one of the autoactions to see the effect. In our example there were 2 active autoactions, Kill job hogging the cluster Rogue App AA #1. . Kill job hogging the cluster In the UI and Unravel API window, you will see the change. was moved to the inactive status\/list in both the Unravel UI and Unravel Autoactions API window. Kill job hogging the cluster Updated Unravel UI - In both screenshots below, Updated Unravel API Kill job hogging the cluster Pagerduty, upon receiving the status change information, will transmit it to the parties specified in the service detail. Sample sms message Sample email The python script must pagerduty " }, 
{ "title" : "Unravel Monitoring Service", 
"url" : "current/adv/adv-unravel-monitoring-service.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Monitoring Service", 
"snippet" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: which keep data about monitored entities (disk, database, kafka, etc). Please see JMX MBeans How to Write Jolokia JM...", 
"body" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: which keep data about monitored entities (disk, database, kafka, etc). Please see JMX MBeans How to Write Jolokia JMX MBean which keeps JMX Beans up to date. (See also Monitors Monitors REST API To configure the monitoring service see the following: Email Alerts General Monitoring Configuration Disk Monitoring " }, 
{ "title" : "JMX Client", 
"url" : "current/adv/adv-unravel-monitoring-service/adv-unravel-monitoring-service-jmx-client.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ JMX Client", 
"snippet" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unrave monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client It is enough to start index.html config.js index.html JMX client dis...", 
"body" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unrave monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client It is enough to start index.html config.js index.html JMX client displays monitoring beans. In the right panel below JSON response data is associated REST API call by which given data can be retrieved. " }, 
{ "title" : "Unravel Properties", 
"url" : "current/adv/adv-un-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties", 
"snippet" : "Properties, unless otherwise, are located in \/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact The Set By User : when you must supply a value Required : there is no default, but you need to specify a property Optional is blank if there is a defa...", 
"body" : "Properties, unless otherwise, are located in \/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact The Set By User : when you must supply a value Required : there is no default, but you need to specify a property Optional is blank if there is a default value Unit Abbreviations : comma separated list CSL : milliseconds ms : minutes min : nanoseconds ns : seconds sec : fully qualified directory path, e.g., \/tmp\/dir\/lower path : percentage, e.g., 1.2, .5 percent " }, 
{ "title" : "Unravel Basic Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-basic.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Unravel Basic Properties", 
"snippet" : "JBDC Property\/Description Set By User Unit Default unravel.jdbc.username Unravel database user string - unravel.jdbc.password password for unravel.jdbc.username string - unravel.jdbc.url MySQL mariadb string (path) jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_...", 
"body" : " JBDC Property\/Description Set By User Unit Default unravel.jdbc.username Unravel database user string - unravel.jdbc.password password for unravel.jdbc.username string - unravel.jdbc.url MySQL mariadb string (path) jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod General Property\/Description Set By User Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting purposes. Optional - com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Required string - com.unraveldata.tmpdir Location where Unravel's temp file resides. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer - com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer - HDFS, ElasticSearch, Zookeepepr Property\/Description Set By User Unit Default com.unraveldata.hive.hdfs.dir \/user\/unravel\/HOOK_RESULT_DIR Required - com.unraveldata.es.cluster Unravel elastic search cluster name, e.g., unravel21650 com.unraveldata.zk.quorum comma separated list Kerberos Property\/Description Set By User Unit Default com.unraveldata.kerberos.principal Spark event logs directory String (path) com.unraveldata.kerberos.keytab.path An HDFS path that helps locate MR job logs to process String (path) " }, 
{ "title" : "Login", 
"url" : "current/adv/adv-un-properties/adv-un-properties-basic.html#UUID-377acc39-0f2a-301c-92e3-186e876328c1_UUID-786c6895-187c-e36b-517a-e18870e30684", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Unravel Basic Properties \/ Login", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.login.admins Unravel UI admin. Set during installation. string admin com.unraveldata.login.admins.readonly Optional CSL - com.unraveldata.login.mode mode to use for login : uses ldap ldap : uses saml saml : users logs directly into Unrave...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.login.admins Unravel UI admin. Set during installation. string admin com.unraveldata.login.admins.readonly Optional CSL - com.unraveldata.login.mode mode to use for login : uses ldap ldap : uses saml saml : users logs directly into Unravel UI open Required string - " }, 
{ "title" : "Executor Logs", 
"url" : "current/adv/adv-un-properties/adv-un-properties-basic-executor-logs.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Executor Logs", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application. byte 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application. byte 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds). ms 60000 (10 mins) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application ms 6000 (1 min) com.unraveldata.spark.eventlog.location maprfs:\/\/\/apps\/spark [empty] 500 com.unraveldata,spark.master Default master for spark applications. (Used to download executor log using correct APIs)Valid Options: yarn, mesos, standalone yarn " }, 
{ "title" : "Airflow", 
"url" : "current/adv/adv-un-properties/adv-un-properties-airflow.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Airflow", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.airflow.available Notes if airflow is currently available : not available false : available true Required boolean - com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Required url - com.unrav...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.airflow.available Notes if airflow is currently available : not available false : available true Required boolean - com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Required url - com.unraveldata.airflow.protocol Type of connection, e.g., https or http https com.unraveldata.airflow.login.name Airflow login username. Required string - com.unraveldata.airflow.login.password Password for login username. Required string - com.unraveldata.airflow.status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 com.unraveldata.airflow.http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow web UI. Default unlimited. bytes 0 airflow.look.back.num.days Date range for workflows, specified in days to look back. The value must start with a minus, -5 is past 5 days, -5 " }, 
{ "title" : "Auto Actions", 
"url" : "current/adv/adv-un-properties/adv-un-properties-autoactions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Auto Actions", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts boolean true com.unraveldata.auto.action.default.snooze.period.ms The time repeated ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts boolean true com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are to be ignored for the violator, e.g., app, user. If the violation is still occurring when awakened the Auto Action executes the action(s) and the violator is once again snoozed. An auto action containing a kill or move action is never snoozed. 0: snooze is turned off &amp;gt; 0: snooze is on, there is no upper bound ms 3,600,000 (1 hour) " }, 
{ "title" : "Azure Data Lake", 
"url" : "current/adv/adv-un-properties/adv-un-properties-azure-lake.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Azure Data Lake", 
"snippet" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application reg...", 
"body" : "These properties are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application registration has to be created in the Azure Active Directory string - com.unraveldata.adl.clientKey An application access key which can be created after registering an application string - com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal string - com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. URL - " }, 
{ "title" : "Celery Configurations", 
"url" : "current/adv/adv-un-properties/adv-un-properties-celery.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Celery Configurations", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unra...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unravel_mysql_prod sqla+mysql Optional - unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unravel_mysql_prod db+mysql+pymysql Optional - " }, 
{ "title" : "Cloud and Forecasting Reports", 
"url" : "current/adv/adv-un-properties/adv-un-properties-cloud-forecasting.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Cloud and Forecasting Reports", 
"snippet" : "See Cluster Cluster Manager Property\/Description Set by User Unit Default unravel.python.reporting.cloudreport.enable Controls the generation Forecasting and Cloud Reports : Reports can be created. true : Reports cannot be created. false boolean true...", 
"body" : "See Cluster Cluster Manager Property\/Description Set by User Unit Default unravel.python.reporting.cloudreport.enable Controls the generation Forecasting and Cloud Reports : Reports can be created. true : Reports cannot be created. false boolean true " }, 
{ "title" : "Cluster Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Cluster Properties", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Unravel first attempts to match on the cluster ID, and then falls back to matching on the display name. For Ambari, the cluster ID ...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Unravel first attempts to match on the cluster ID, and then falls back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:8080\/api\/v1\/clusters\/ For Cloudera Manager, the cluster id is the \"name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:7180\/api\/v17\/clusters\/ com.unraveldata.cluster.type Possible values are HDP, CDH, or MAPR " }, 
{ "title" : "Cluster Manager Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-cluster-manager.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Cluster Manager Properties", 
"snippet" : "These properties need to be set for Cloud Reports Forecasting Report Impala This content can not be edited in the markdown editor. The following properties are defined by Cluster manager tool. Be sure to only set the properties for your cluster manager tool, eiither Ambari or Cloudera.. {ManagerName...", 
"body" : " These properties need to be set for Cloud Reports Forecasting Report Impala This content can not be edited in the markdown editor. The following properties are defined by Cluster manager tool. Be sure to only set the properties for your cluster manager tool, eiither Ambari or Cloudera.. {ManagerName}: ambari or cloudera Ambari Cluster Manager Name\/Description Set By User Unit Default com.unraveldata.ambari.manager.url URL of Ambari Manager, e.g.,https:\/\/$ambariserver:8083. It must start with http:\/\/ or https:\/\/ Required string - com.unraveldata.ambari.manager.username Ambari username Required string - com.unraveldata.ambari.manager.password Ambari password Required string - Cloudera Manager Properties Name\/Description Set By User Unit Default com.unraveldata.cloudera.manager.url URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8083 If the Cloudera Manager URL does not contain a port you must define port below Required string - com.unraveldata.cloudera.manager.username CM username Required string - com.unraveldata.cloudera.manager.password This is required only for Cloudera when port is missing from the CM password Required string - com.unraveldata.cloudera.cloudera.manager.port You only need to specify this if your Cloudera Manager is not on port 7180. Optional integer - com.unraveldata.cloudera.cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Optional integer - " }, 
{ "title" : "Custom UI Banner\/Notification", 
"url" : "current/adv/adv-un-properties/adv-un-properties-custom-banner.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Custom UI Banner\/Notification", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display The text end.date Optional ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display The text end.date Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. The text end.date Format: YYYYMMDDTHHMMSSZ-000000 Optional string (date) - " }, 
{ "title" : "Email Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-email.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Email Properties", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. : enables alerts true : disables false boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - mail.smtp.from Use...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. : enables alerts true : disables false boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - mail.smtp.from Used for email \"from\" and \"reply-to\" headers Required string - mail.smtp2.from Used for email \"from\" and \"reply-to\" headers Optional string - mail.smtp.port Port integer 25 mail.smtp.auth Enable\/ SMTP authentication. : If true then mail.smtp.user and mail.smtp.pw must be set as they are used when connecting. Note boolean false mail.smtp.starttls.enable Use start-TLS. boolean false mail.smtp.ssl.enable Use SSL right from the start.string boolean false mail.smtp.user Username for SMTP authentication : If Note mail.smtp.auth Optional string - mail.smtp.pw Password for SMTP authentication : If Note mail.smtp.auth Optional string - mail.smtp.host Host for SMTP server string localhost mail.smtp.localhost A domain name for apparent sender; must have at least one dot (e.g. organization.com) string localhost.local mail.smtp.debug Enable debug mode. boolean false " }, 
{ "title" : "File Reports", 
"url" : "current/adv/adv-un-properties/adv-un-properties-file-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ File Reports", 
"snippet" : "Name\/Description Set By User Unit Default unravel.python.reporting.files.huge_files_threshold_size file size >= threshold_size bytes 100GB unravel.python.reporting.files.huge_files_min_files number of files in directory >= min_files integer 1 unravel.python.reporting.files.huge_files_top_n_dirs numb...", 
"body" : " Name\/Description Set By User Unit Default unravel.python.reporting.files.huge_files_threshold_size file size >= threshold_size bytes 100GB unravel.python.reporting.files.huge_files_min_files number of files in directory >= min_files integer 1 unravel.python.reporting.files.huge_files_top_n_dirs number of files in directory >= min_files integer 10 unravel.python.reporting.files.medium_files_max_threshold_size file size <= max_threshold_size bytes 10GB unravel.python.reporting.files.medium_files_min_threshold_size file size >= min_threshold_size integer 5GB unravel.python.reporting.files.medium_files_min_files number of files in directory >= min_files integer 5 unravel.python.reporting.files.medium_files_top_n_dirs maximum number of directories to display integer 20 unravel.python.reporting.files.tiny_files_threshold_size file size <= threshold_size bytes 100KB unravel.python.reporting.files.tiny_files_min_files maximum number of directories to display integer 10 unravel.python.reporting.files.tiny_files_top_n_dirs maximum number of directories to display integer 30 unravel.python.reporting.files.empty_files_min_files number of files in directory >= min_files integer 10 unravel.python.reporting.files.empty_files_top_n_dirs maximum number of directories to display integer 3 The following four properties are defined per file size type; Size Name\/Description Set By User Unit Default Size _files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean false Size _files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two HDFS_root\/one\/two integer 0 Size _files_max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven five integer 10 Size _files_drill_down_subdirs_flag When true a file is accounted (listed) for all its ancestors.   : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with its ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two boolean false " }, 
{ "title" : "HBase", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hbase.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ HBase", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values: AMBARI, CDH, and JMX. Required - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,c...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values: AMBARI, CDH, and JMX. Required - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,clustername2,... Required CSL - com.unraveldata.hbase.metric.poll.interval Polling interval in minutes integer 5 com.unraveldata.hbase.http.read.timeout Polling read timeout in seconds integer 5 com.unraveldata.hbase.http.poll.parallelism Polling parallelism, no. of cores integer 10 com.unraveldata.hbasealert.average.threshold Threshold factor above average value for alerts alerts integer 2 com.unraveldata.hbase. 1.2 (120%) HBase - source.type=JMX Name\/Description Set By User Unit Default com.unraveldata.hbase. clustername .node.http.apis HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... Example: http:\/\/your.master.server:16010, http:\/\/your.region.server:16030\/ Required CSL - HBase - source.type=AMBARI | CDH Name\/Description Set By User Unit Default com.unraveldata.hbase.rest.url Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443) Format: http[s]:\/\/your.ambari.server[:port]\" Example: http:\/\/your.ambari.server http:\/\/your.ambari.server:88 Required integer - com.unraveldata.hbase.rest.user Username for rest api Required string com.unraveldata.hbase.rest.pwd Password for rest api Required string - com.unraveldata.hbase.rest.ssl.enabled hbase.ssl.enabled property value boolean true com.unraveldata.hbase.master.port hbase.master.info.port property value For AMBARI: 16010 For CDH: 60010 Optional integer - com.unraveldata.hbase.regionserver.port hbase.master.info.port property value For AMBARI: 16030 For CDH: 60030 Optional integer com.unraveldata.hbase.service.name HBase service name if not the default - “HBASE” Format: clustername1=servicename1,clustername2=servicename2,... Optional CSL {clustername}=hbase " }, 
{ "title" : "Hive Metastore Access", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hive-metastore-javax.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver My...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: com.mysql.jdbc.Driver Req string - javax.jdo.option.ConnectionPassword Password used to access the data store. Req string - javax.jdo.option.ConnectionUserName Username used to access the data store. Req string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT Example: postgresql: jdbc:postgresql:\/\/congo.unraveldata.com:7432\/hive Req url " }, 
{ "title" : "Hive Metastore JDBC", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hive-metastore-jdbc.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Metastore JDBC", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to ...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. count 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. ms 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false boolean false com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. count 5 com.unraveldata.metastore.db.c3p0.idleconnectiontestperiod Optional 0 com.unraveldata.metastore.databasePattern Opt string dname* com.unraveldata.metastore.print.metastore.stats Optional boolean false " }, 
{ "title" : "Hive Hook SSL", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hivehook.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Hook SSL", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.live.logreceiver.port.https HTTPS server port(negative value means HTTPS server is disabled) number -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Required string - com.unraveldata.server.ssl.cer...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.live.logreceiver.port.https HTTPS server port(negative value means HTTPS server is disabled) number -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Required string - com.unraveldata.server.ssl.cert_password KeyStore password Required string - Set these properties in hive-site-xml or in Hive CLI (using --hiveconf Name\/Description Set by User Unit Default com.unraveldata.hive.hook.insecure.ssl : SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store false : certificate is not validated, trust store not needed true boolean false com.unraveldata.hive.hook.use.ssl Enables SSL boolean false com.unraveldata.hive.hook.ssl.trust_store Trust store string - com.unraveldata.hive.hook.ssl.trust_store_password Trust store password as plain text string - com.unraveldata.hive.hook.ssl.trust_store_password_file Path to file of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence string - com.unraveldata.host hostname com.unraveldata.port port number integer 4043 " }, 
{ "title" : "HiveServer2", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hiveserver2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ HiveServer2", 
"snippet" : "Property\/Description Set By User Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance unravel.hive.server2.port Port for the HiveServer2 instance number 10000 unravel.hive.server2.authentication KERBEROS, LDAP, or CUSTOM When set to KERBEROS you must - - unravel.hive...", 
"body" : " Property\/Description Set By User Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance unravel.hive.server2.port Port for the HiveServer2 instance number 10000 unravel.hive.server2.authentication KERBEROS, LDAP, or CUSTOM When set to KERBEROS you must - - unravel.hive.server2.kerberos.service.name Set only This must hive string - unravel.hive.server2.password Use only when unravel.hive.server2.authentication=LDAP or CUSTOM string - unravel.hive.server2.thrift.transport for \"TTransportBase\" for custom advanced usage - com.unraveldata.kerberos.principal Required when unravel.hive.server2.authentication=KERBEROS only, e.g., user1@xyz.com string - com.unraveldata.kerberos.keytab.path This keytab file will be used to init and renew Kerberos Tickets path - " }, 
{ "title" : "Impala", 
"url" : "current/adv/adv-un-properties/adv-un-properties-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Impala", 
"snippet" : "Property\/Definition Set By User Unit Default com.unraveldata.source Can be cm or impalad Optional cm com.unraveldata.impalad.nodes Node list in the form of IP Address:Port Required CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Propertie...", 
"body" : " Property\/Definition Set By User Unit Default com.unraveldata.source Can be cm or impalad Optional cm com.unraveldata.impalad.nodes Node list in the form of IP Address:Port Required CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Properties cloudera.manager.impalanum.queries.limit Maximum number of queries that will be returned by a poll to the Cloudera Manager API. integer 1000 cloudera.manager.impalapoll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 cloudera.manager.impalalook.back.minutes Number of minutes to look back when polling the Cloudera Manager API. min -5 cloudera.manager.impalaskip.duration.millis Queries with duration shorter than this threshold will get captured but not analyzed. ms 1000 com.unraveldata.cloudera.manager.read.timeout.millis HTTP Read timeout for Cloudera Manager connections. ms 5000 com.unraveldata.cloudera.manager.connect.timeout.millis HTTP Connect timeout for Cloudera Manager connections. ms 30000 The following properties defaults should be fine and shouldn't need to be changed. hitdoc.impala.operator.info.length 20480 impala.events.stalestats.threshold.bytes bytes 1000 impala.events.stalestats.ratio percent 0.2 impala.events.longop.time.millis ms 2000 impala.events.longop.ratio percent 0.2 impala.events.cost.diff.bytes bytes 500000000 impala.events.skew.time.millis ms 500000000 impala.events.skew percent 1.5 " }, 
{ "title" : "HDInsight", 
"url" : "current/adv/adv-un-properties/adv-un-properties-hdinsight.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ HDInsight", 
"snippet" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 O...", 
"body" : "Block storage specific properties (for HDInsight). These properties are required for blob storage. For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster string - com.unraveldata.hdinsight.primary-access-key Primary storage account key string - com.unraveldata.hdinsight.storage-account-name-2 Optional for Spark when HDInsight is using blob storage Storage account name for the HDInsight cluster (same as account-name-1 string - com.unraveldata.hdinsight.secondary-access-key Secondary storage account key string - " }, 
{ "title" : "Impala", 
"url" : "current/adv/adv-un-properties/adv-un-properties-impala-62105.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Impala", 
"snippet" : "Property\/Definition Set By User Unit Default com.unraveldata.source Can be cm or impalad Optional cm com.unraveldata.impalad.nodes Node list in the form of IP Address:Port Required CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Propertie...", 
"body" : " Property\/Definition Set By User Unit Default com.unraveldata.source Can be cm or impalad Optional cm com.unraveldata.impalad.nodes Node list in the form of IP Address:Port Required CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Properties cloudera.manager.impalanum.queries.limit Maximum number of queries that will be returned by a poll to the Cloudera Manager API. integer 1000 cloudera.manager.impalapoll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 cloudera.manager.impalalook.back.minutes Number of minutes to look back when polling the Cloudera Manager API. min -5 cloudera.manager.impalaskip.duration.millis Queries with duration shorter than this threshold will get captured but not analyzed. ms 1000 com.unraveldata.cloudera.manager.read.timeout.millis HTTP Read timeout for Cloudera Manager connections. ms 5000 com.unraveldata.cloudera.manager.connect.timeout.millis HTTP Connect timeout for Cloudera Manager connections. ms 30000 The following properties defaults should be fine and shouldn't need to be changed. hitdoc.impala.operator.info.length 20480 impala.events.stalestats.threshold.bytes bytes 1000 impala.events.stalestats.ratio percent 0.2 impala.events.longop.time.millis ms 2000 impala.events.longop.ratio percent 0.2 impala.events.cost.diff.bytes bytes 500000000 impala.events.skew.time.millis ms 500000000 impala.events.skew percent 1.5 " }, 
{ "title" : "Kafka", 
"url" : "current/adv/adv-un-properties/adv-un-properties-kafka.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Kafka", 
"snippet" : "Kafka - Cluster Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - . com.unraveldata.ext.kafka. cl...", 
"body" : " Kafka - Cluster Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - . com.unraveldata.ext.kafka. cluster bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. For each cluster in com.unraveldata.ext.kafka.clusters e.g., com.unraveldata.ext.kafka.East.bootstrap_servers=localhost:9092,localhost:9093 Required CSL - . com.unraveldata.ext.kafka. cluster jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. You must assign aliases the cluster nodes. Use a comma separated list for multiple nodes. e.g., unraveldata.ext.kafka.East.jmx_servers=kNode-1, kNode-2 Required CSL - . com.unraveldata.ext.kafka. cluster jmx. kNode-1 host The host for a node in the cluster. You must define a host for each node in each cluster. e.g., com.unraveldata.ext.kafka.East.kNode1=localhost com.unraveldata.ext.kafka.East.kNode2=localhost Required - . com.unraveldata.ext.kafka. cluster jmx kNode-1 port For each node in each cluster you must assign a port. e.g., com.unraveldata.ext.kafka.East.jmx.kNode1.port=5005 Required number - To locate Kafka and JMX ports: . Navigate to: Clusters → Kafka → Configuration → Ports and Addresses. Cloudera Manager Alternatively, you may lookup up the information in the broker nodes of Zookeeper CLI. : For Protocol and broker port navigate to: Kafka → Configs → Kafka Broker HDP JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template Kafka - General Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.servers Use a omma separated list for multiple servers. Required CSL - com.unraveldata.ext.kafka.insight.interval_min min 15 com.unraveldata.ext.kafka.insight.sw_size integer 30 com.unraveldata.ext.kafka.insight.num_ignored_intervals integer 2 com.unraveldata.ext.kafka.insight.lag_threshold integer 100 com.unraveldata.monitoring.kafka.check.interval sec 30 com.unraveldata.monitoring.kafka.ignore.topics Topics to ignore. Use a comma separated list for multiple topics. Optional CSL - com.unraveldata.monitoring.kafka.history.size integer 5 " }, 
{ "title" : "LDAP Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-ldap.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ LDAP Properties", 
"snippet" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. s...", 
"body" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. string - com.unraveldata.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null result set, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. [empty] string - com.unraveldata.ldap.groupClassKey DAP attribute name on the group entry that is to be used in LDAP group searches. string  - com.unraveldata.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. [empty] string  - com.unraveldata.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) string - com.unraveldata.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. [empty] string - com.unraveldata.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. Default is \"uid\"; not used when custom query is specified. string - com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server uses to extract the ldap user's email address. If not configured, Unravel server uses the attribute name \"mail\". [empty] string - com.unraveldata.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s where the actual group name is to be substituted for. This is used like a list of baseDNs and baseDN is ignored if this is set. [empty] string - com.unraveldata.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). string - com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. Example: ldap:\/\/host ldaps:\/\/hostldap:\/\/host:9999 ldaps:\/\/host1:9999  ldaps:\/\/host2:9999 [empty] string - com.unraveldata.ldap.verbose Enables verbose logging. Grep for \"Ldap\" entries in the  unravel_ngui.log \/usr\/local\/unravel\/logs\/ : user names and group names can appear in this log, but raw passwords are not logged. true : logging turned off false boolean false " }, 
{ "title" : "Oozie", 
"url" : "current/adv/adv-un-properties/adv-un-properties-oozie.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Oozie", 
"snippet" : "Property\/Description Set By User Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel Required path - oozie.server.username Optional string - oozie.server.password Optional string - oozie.log.length The maximum number of characters in Oozie workflow log that Unravel fetches....", 
"body" : " Property\/Description Set By User Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel Required path - oozie.server.username Optional string - oozie.server.password Optional string - oozie.log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x will be trimmed from the beginning and only last x characters are kept. count 1000000 com.unraveldata.oozie.disable Whether to disable bringing in Oozie workflows into Unravel. The underlying jobs will not be affected. bool false com.unraveldata.oozie.fetch.num Number of workflows to pull in each API call. count 100 com.unraveldata.oozie.fetch.interval.sec sec 120 com.unraveldata.oozie.retry.sec sec 600 " }, 
{ "title" : "Unravel Monitoring General Configuration", 
"url" : "current/adv/adv-un-properties/adv-un-properties-unravel-monitoring-general.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Unravel Monitoring General Configuration", 
"snippet" : "JavaScript Rules Property\/Description Set By User Unit Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaluation. sec 60 com.unraveldata.monitoring.js_rules.cool.off.period Alert action cool-off period. s...", 
"body" : " JavaScript Rules Property\/Description Set By User Unit Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaluation. sec 60 com.unraveldata.monitoring.js_rules.cool.off.period Alert action cool-off period. sec 1800 File System Monitoring File system related rules (disk usage) are evaluated separately. Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. Sets the timing interval for partition (volume) and folder (directory) monitoring seconds 90 DB Performance Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.db.performance.check.interval Frequency, in seconds, that the database performance metrics are gathered. 0: disables evaluation. sec 30 com.unraveldata.monitoring.db.performance.query Default query; select count(*) from (select 1 from blackboards limit 1000) b see note Kafka Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.kafka.check.interval Frequency, in seconds, Kafka metrics should be queried. 0: disables monitoring. sec 30 com.unraveldata.monitoring.kafka.ignore.topics Zookeeper topics to be ignored during Kafka monitoring. Topics can be ignored from different reasons, e.g., all internal topics are ignored. Use a comma separated list to specify more than one. CSL __consumer_offsets, connect-configs, connect-offsets, connect-status com.unraveldata.monitoring.kafka.history.size Number of data sets stored in the memory and used as historical data. integer 5 ElasticSearch Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.elastic.check.interval Frequency, in seconds, ElasticSearch metrics should be queried. 0: disables monitoring. sec 30 com.unraveldata.monitoring.elastic.history.size Number of data sets stored in the memory and used as historical data. integer 5 " }, 
{ "title" : "Disk Monitoring", 
"url" : "current/adv/adv-un-properties/adv-un-properties-unravel-monitoring-disk.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Disk Monitoring", 
"snippet" : "File System Monitoring File system related rules (disk usage) are evaluated separately. Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. Sets the timing interval for partition (volume) and folder (directory) monit...", 
"body" : " File System Monitoring File system related rules (disk usage) are evaluated separately. Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. Sets the timing interval for partition (volume) and folder (directory) monitoring seconds 90 Partition (Volumes) Monitoring Given list of partitions are monitored. Once partition usage exceeds given threshold (configured as high watermark limit) email alert is sent. Another email is not sent until disk usage goes below low watermark limit and above high watermark limit again. Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs.partitions.check.interval Frequency to check monitored partitions. seconds 60 com.unraveldata.monitoring.fs.partitions.csv Comma separated list of monitored partitions. Symbolic links are supported. string \/srv\/unravel,\/usr\/local\/unravel com.unraveldata.monitoring.fs.partitions.high.watermark Trigger alert if disk usage is over this limit. Do not trigger next alert until disk usage is below low watermark limit. percentage 85 com.unraveldata.monitoring.fs.partitions.low.watermark If disk usage goes below this limit then disk alert can be triggered again. percentage 70 Deprecated Partition Properties The following properties have been deprecated and replaced with the above partition properties. We strongly suggest you remove these deprecated properties from unravel.properties If you have both the new and deprecated properties defined, the value of the deprecated property is used. For instance, if you have the defined both: new: com.unraveldata.monitoring.fs.partitions.high.watermark=80 deprecated: com.unraveldata.kafka.monitor.disk.high.watermark=60 Unravel uses the deprecated property value, so triggering occurs when disk usage is > 60% not 80% Property Replaced by com.unraveldata.filesystem.volumes.csv com.unraveldata.monitoring.fs.partitions.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.monitoring.fs.partitions.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark com.unraveldata.monitoring.fs.partitions.low.watermark Folders Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs.folders.check.interval Frequency to check monitored folders (directories). seconds 0 com.unraveldata.monitoring.fs.folder_limit.pairs.csv Comma separated list of monitored folders and their size limits, <foldername>:<folderlimit>[KB|MB|GB], e.g., \/srv\/unravel:100GB,\/usr\/local\/unravel:200GB. The folder must be a fully qualified and may be a symbolic link. If no size unit is specified, size is evaluated as bytes. string \/srv\/unravel:100GB com.unraveldata.monitoring.fs.folders.low.watermark Percentage of folder limit above. Once an alert is triggered, a new alert is only triggered if the size first drops below this limit and then rises above the folder limit. The purpose is to prevent false and repeated alerts. Example: \/srv\/unravel:100GB with a low water mark of 80% The first time (\/srv\/unravel size > 100GB) the alert is triggered. No new alert is triggered unless (\/srv\/unravel size drops < 80GB) and then percentage 80 " }, 
{ "title" : "Queue Analysis Reports", 
"url" : "current/adv/adv-un-properties/adv-un-properties-q-analysis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Queue Analysis Reports", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 com.unraveldata.report.queue.http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. number 1000 " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "current/adv/adv-un-properties/adv-un-properties-rbac.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Role Based Access Control (RBAC)", 
"snippet" : "For non-RBAC specific properties that affect RBAC see RBAC Configuration Property\/Description Set By User Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off boolean true com.unraveldata.rbac.default Determines how the End-user's vi...", 
"body" : "For non-RBAC specific properties that affect RBAC see RBAC Configuration Property\/Description Set By User Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off boolean true com.unraveldata.rbac.default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName com.unraveldata.rbac.tagcmd string (ldap | saml) is the value of Mode com.unraveldata.login.mode General Basic Properties . com.unraveldata.login.admins Mode groups Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a comma separated list of groups. CLS - . com.unraveldata.login.admins.readonly Mode groups Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a comma separated list of groups. CLS - . com.unraveldata.rbac Mode tags.find A comma separated list of the prefix of LDAP\/saml group to be used as the PROJECT { Mode - CLS - . com.unraveldata.rbac Mode Tag regex.find Defines regular expression used to parse LDAP\/saml groups for generating the TENANTs PROJECT. Value = Tag -REGEX PROJECT com.unraveldata.rbac Mode tags If you have defined more than one group in your LDAP\/saml group definition only the first prefix will be processed with the remaining ignored. The best practice is to define each Note: PROJECT - CLS - " }, 
{ "title" : "SAML Configuration Properties", 
"url" : "current/adv/adv-un-properties/adv-un-properties-saml.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ SAML Configuration Properties", 
"snippet" : "This property is required when com.unraveldata.login.mode \/usr\/local\/unravel\/etc\/unravel.properties Name\/Description Set By User Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file Optional string (path) - com.unraveldata.login.admins.ldap.groups Grants rea...", 
"body" : " This property is required when com.unraveldata.login.mode \/usr\/local\/unravel\/etc\/unravel.properties Name\/Description Set By User Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file Optional string (path) - com.unraveldata.login.admins.ldap.groups Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a comma separated list of groups. CLS - com.unraveldata.login.admins.readonly.ldap.groups Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a comma separated list of groups. CLS - These properties are sent in the login.saml.config com.unraveldata.login.saml.config Name\/Description Set By User Unit Default entryPoint Identity provider entry point, It must be specified in order to be spec-compliant when the request is signed. e.g., \"http:\/\/c24.unravel.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\" Optional - issuer Issuer string to supply to identity provider (Environment name). Should match the name configured in ldp e.g., “Congo24”, “Localhost” , Optional - cert IDP's public signing certificate. e.g., Idp Cert String Optional - cert IDP's public signing certificate. e.g., Idp Cert String Optional - unravel_mapping Mapping SAML attributes to Unravel attributes. Specific to unravel Integration. e.g., \n{\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n}\n - " }, 
{ "title" : "Sensors", 
"url" : "current/adv/adv-un-properties/adv-un-properties-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Sensors", 
"snippet" : "Most Sensor properties are set via JVM arguments; the file name is noted when the properties can be set in a file. All Sensors Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set By User Unit Default com.unraveldat...", 
"body" : "Most Sensor properties are set via JVM arguments; the file name is noted when the properties can be set in a file. All Sensors Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set By User Unit Default com.unraveldata.client.rest.queue The queue length for outgoing REST HTTP requests [empty] 20000 com.unraveldata.client.rest.retryfail Cool-down period after unsuccessful attempt to make REST HTTP request in nanoseconds ns 30 seconds com.unraveldata.client.rest.conn.timeout.ms REST HTTP request timeout in milliseconds ms 100 com.unraveldata.client.rest.shutdown.ms Maximum time to wait for orderly shutdown of the REST client (if exceeded some messages still in the queue will be lost) ms 10 com.unraveldata.client.rest.dns.ttl The period to refresh the DNS info in milliseconds - IP is pre-resolved and kept until the next refresh if no failures are observed ms 6 hours com.unraveldata.client.rest.priority.retries Certain critical messages have priority flag and their transmission will be reattempted this many times 5 unravel.server.hostport Unravel server host:port information - Resource Usage Sensor Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set By User Unit Default com.unraveldata.agent.metrics.enabled_keys Comma separated list of metric type names which are enabled for collection CLS availableMemory,cpuUtilization, processCpuLoad,systemCpuLoad, maxHeap, usedHeap, vmRss,gcLoad unravel.metrics.factor Sampling period scale down factor 1 Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= Agent Argument Set By User Unit Default metricsCaptureFilter Format allow specifying single ordinals for component IDs as well as ranges and enumerations - e.g., metricsCaptureFilter=1,2,5-10,turns on metrics collection for components 1, 2, 5 to 10 0-1500 Spark Sensor Specified by adding -D<propertyName>=<propertyValue>; to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set By User Unit Default com.unraveldata.spark.sensor.enableLiveUpdates Enable live updates for Spark apps [empty] boolean False com.unraveldata.spark.sensor.enableCachingInfo Enable tracking caching info for Spark apps boolean False com.unraveldata.spark.sensor.enableSampling Enable data sampling between operators for Spark apps boolean False Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= Agent Argument\/Definition Set By User Unit Default com.unraveldata.spark.sensor.clusterID The cluster ID - currently only used in Spark " }, 
{ "title" : "Sessions", 
"url" : "current/adv/adv-un-properties/adv-un-properties-session.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Sessions", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.session.enabled Enables Sessions features tab in the UI. true: Sessions enabled false: Sessions disabled You must restart the ngui and ondemand demons, when changing the value. boolean true com.unraveldata.session.max.autotune.runs Maximum nu...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.session.enabled Enables Sessions features tab in the UI. true: Sessions enabled false: Sessions disabled You must restart the ngui and ondemand demons, when changing the value. boolean true com.unraveldata.session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 " }, 
{ "title" : "Small Files and File Reports", 
"url" : "current/adv/adv-un-properties/adv-un-properties-smallnfiles-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Small Files and File Reports", 
"snippet" : "You must restart the unravel_ondemand unravel_ngui Name\/Description Set By User Unit Default com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true com.unravel...", 
"body" : " You must restart the unravel_ondemand unravel_ngui Name\/Description Set By User Unit Default com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true com.unraveldata.ngui.sfhivetable.schedule.enabled Controls the frequency with which Unravel fetches fsimage from the cluster. For 4.5.0.0, it is recommended to use the default setting. Using this setting, Unravel triggers fetch fsimage at 00:00 UTC every day. day 1 You must restart the unravel_ondemand Name\/Description Set By User Unit Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. : disables the functionality in the Backend and UI. true : enables the functionality in the Backend and UI to generate the Small Files\/File Reports. false boolean false unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, then setting this to true allows an externally fetched fsimage for use by Unravel's Ondemand process. : Ondemand etl_fsimage process does not fetch fsimage from name node. Instead, the fsimage is expected to be available in directory specified by true unravel.python.reporting.files.external_fsimage_dir boolean false unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. string - unravel.python.reporting.files.hive_database Hive Database where Ondemand creates 5 hive tables (4 temporary, 1 permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the hive queries used for this feature run against default MR queue. It must point to a valid Hive database. string default database unravel.python.reporting.files.hive_mr_queue The hive queries ran by Ondemand process run against this MR queue. It must point to a valid MR queue. string default Small Files and File Reports have equivalent \"local\" properties which take precedence if they are set. Should you unset\/delete any of the below properties or their equivalent properties for sidebar Unravel has hard-coded values to ensure your reports are generated. Name\/Description Set By User Unit Default unravel.python.reporting.files.files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean - unravel.python.reporting.files.min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two HDFS_root\/one\/two count - unravel.python.reporting.files.max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven five count - unravel.python.reporting.drill_down_subdirs_flag When true a file is accounted (listed) for all its ancestors.   : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with its ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two boolean - " }, 
{ "title" : "Small Files Report", 
"url" : "current/adv/adv-un-properties/adv-un-properties-small-files.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Small Files Report", 
"snippet" : "You must restart the unravel_ondemand Property\/Description Set By User Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria....", 
"body" : " You must restart the unravel_ondemand Property\/Description Set By User Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false true unravel.python.reporting.files.small_files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two HDFS_root\/one\/two 0 unravel.python.reporting.files.small_files_max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven five 10 unravel.python.reporting.files.small_files_drill_down_subdirs_flag When true a file is accounted (listed) for all its ancestors.   : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with its ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two boolean - " }, 
{ "title" : "Spark", 
"url" : "current/adv/adv-un-properties/adv-un-properties-spark.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Spark", 
"snippet" : "Live Pipeline Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percenta...", 
"body" : " Live Pipeline Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percentage of jobs in the Spark APM. true : process after the application completes and the event log file has been processed false boolean true com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has (# jobs\/stages) maxStoredStages maxStoredStages This setting affects only the live pipeline integer 1000 com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor. yarn Event Log Processing Property\/Description Set By User Unit Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when string hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum size of the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize bytes 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. sec 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. : prefix search true : prefix + suffix search false Prefix + suffix search is faster as it avoids listFiles() API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes boolean false com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata.spark.hadoopFsMulti.useFilteredFiles : the empty suffix (,,) be part of this value for uncompressed event log files. NOTE CSL ,,.lz4,.snappy,.inprogress com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. integer 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry ms 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Events Related Property\/Description Set By User Unit Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Other properties Property\/Description Set By User Unit Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Timeline histogram generation is memory intensive. Note: False " }, 
{ "title" : "Tagging", 
"url" : "current/adv/adv-un-properties/adv-un-properties-tagging.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Tagging", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled string (path) \/usr\/loc...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled string (path) \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.tagging.script.method.name The name of the method in the python script that generates the tagging dictionary. string generate_unravel_tags " }, 
{ "title" : "Tez", 
"url" : "current/adv/adv-un-properties/adv-un-properties-tez.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Tez", 
"snippet" : "Property\/Descripton Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Optional string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Optional string com.unraveldata.yarn....", 
"body" : " Property\/Descripton Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Optional string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Optional string com.unraveldata.yarn.timeline-service.webapp.address Hostname of Application Timeline Server, e.g., http:\/\/$atshostname Required string (url) com.unraveldata.yarn.timeline-service.port HTTP port of Application Timeline Server integer 8188 com.unraveldata.tez.app.ats.connect.timeout.millis HTTP Connect timeout for ATS connections ms 30000 com.unraveldata.tez.app.ats.read.timeout.millis HTTP Read timeout for ATS connections ms 5000 com.unraveldata.tez.app.ats.poll.timeout.millis Controls the timeout after which we will stop trying to poll ATS if the polling is failing ms 120000 com.unraveldata.tez.ats.poll.interval.millis Interval between consecutive polls of ATS if the polling fails. ms 10000 com.unraveldata.tez.ats.poll.max.retries Maximum number of retries if the polling of ATS fails integer 30 The following properties defaults shouldn't need to be changed. com.unraveldata.tez.events.low.tasks integer 25 com.unraveldata.tez.events.low.tasks integer 50 com.unraveldata.tez.events.min.task.millis ms 2000 com.unraveldata.tez.events.max.task.millis ms 50000 com.unraveldata.tez.events.task.percentage percent 0.2 " }, 
{ "title" : "Top X Report", 
"url" : "current/adv/adv-un-properties/adv-un-properties-topx.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Properties \/ Top X Report", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.ngui.topx.enabled Controls the generation of Top X reports. : Top X reports enabled. true : Top X reports are disabled. false boolean true...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.ngui.topx.enabled Controls the generation of Top X reports. : Top X reports enabled. true : Top X reports are disabled. false boolean true " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "current/adv/adv-unravel-server-sensor.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Installing Sensors", 
"url" : "current/adv/adv-unravel-server-sensor/adv-unravel-server-sensor-installing-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors", 
"snippet" : "Individual Applications Submitted Through spark-submit Individual Hive Queries...", 
"body" : " Individual Applications Submitted Through spark-submit Individual Hive Queries " }, 
{ "title" : "Individual Applications Submitted Through spark-submit", 
"url" : "current/adv/adv-unravel-server-sensor/adv-unravel-server-sensor-installing-sensors/adv-unravel-server-sensor-indiv-spark-submit.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Highlighted text must be a fully qualified DNS or ...", 
"body" : " Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Highlighted text must be a fully qualified DNS or IP address. UNRAVEL_HOST_IP Obtain the Sensor. The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST_IP http:\/\/ UNRAVEL_HOST_IP To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Locate the sensor file: unravel-agent-pack-bin.zip Run the Sensor to Intercept Spark Apps. Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit by first creating a destination directory that is readable by all users. We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit # mkdir UNRAVEL_SENSOR_PATH UNRAVEL_SENSOR_PATH UNRAVEL_HOST_IP If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp # mkdir UNRAVEL_SENSOR_PATH UNRAVEL_SENSOR_PATH UNRAVEL_HOST_IP UNRAVEL_SENSOR_PATH Define spark.driver.extraJavaOptions and spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: : Parent directory of the Unravel Sensor .zip file, UNRAVEL_SENSOR_PATH unravel-agent-pack-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the name node IP address and port. SPARK_EVENT_LOG_DIR : PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit : Spark version to be instrumented. Valid options are SPARK_VERSION 1.3 1.5 1.6 2.0 export UNRAVEL_SENSOR_PATH= UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR SPARK_VERSION Option B: If You Run Spark Apps in yarn-client Mode. To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST If you use multiple hosts as clients, on each client. # mkdir UNZIPPED_ARCHIVE_DEST UNZIPPED_ARCHIVE_DEST UNRAVEL_HOST_IP Define spark.executor.extraJavaOptions To use the example below, substitute your local values for: : directory of the unzipped Unravel Sensor files. UNZIPPED_ARCHIVE_DEST : IP address and port of the UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. SPARK_EVENT_LOG_DIR : PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit : Spark version to be instrumented. Valid options are SPARK_VERSION 1.3 1.5 1.6 2.0 export UNZIPPED_ARCHIVE_DEST= UNZIPPED_ARCHIVE_DEST UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR SPARK_VERSION " }, 
{ "title" : "Individual Hive Queries", 
"url" : "current/adv/adv-unravel-server-sensor/adv-unravel-server-sensor-installing-sensors/adv-unravel-server-sensor-indiv-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profi...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true;\n Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5;\n Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=- javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=- javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor and Configure Impala Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true;\n Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; \nset mapreduce.task.profile.reduces=0-5;\n Enable the JVM agent for map and reduce tasks: :4043; set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: :4043; set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP " }, 
{ "title" : "Upgrading the Unravel Server and Sensors", 
"url" : "current/adv/adv-unravel-server-sensor/install-hdi-upgrade-vm.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Upgrading the Unravel Server and Sensors", 
"snippet" : "See Unravel Software Versions Contact Unravel Support at Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a sin...", 
"body" : "See Unravel Software Versions Contact Unravel Support at Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Lines beginning with '\/\/' are comments. Copy the new RPM to each Unravel host. Stop each host simultaneously. \/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously. \/\/ host1\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host2\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host3\n# sudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties. You must enter add license key to unravel.properties After all the RPM upgrades finish, restart Unravel Server on each host simultaneously. \/\/ host1\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host2\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host23\n# sudo \/etc\/init.d\/unravel_all.sh start Complete any deployment-specific upgrade steps. Unravel Sensor text indicates where you must substitute your particular values. HIGHLIGHTED must be a fully qualified path or IP address. UNRAVEL_HOST_IP - target Spark version (eg. 1.6.0, 2.0.1, 2.2.0, etc.) SPARK_VERSION_X.Y.Z - target Hive version (e.g. 1.2.0) HIVE_VERSION_X.Y.Z Upgrade sensors on CDH cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Check you current sensor version. Log in to the Cloudera Manager and click the parcel () in the top menu bar. Click the Check for New Parcels button and look for UNRAVEL_SENSOR entries. If newer sensors are available, it will be shown as another entry like below. Click the Download and then Distribute button, then activate for the newer version of the sensors. When activating the new sensors, you will be notified that Hive and Spark services must be restarted. Once new sensor activation is completed, the old version is automatically disabled. Upgrade sensors on HDP cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION_X.Y.Z Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client folders. Tar these two folders and scp to all cluster nodes. # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar $ CLUSTER_NODE On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar Upgrade sensors on MAPR cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark- SPARK_VERSION_X.Y.Z Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION_X.Y.Z Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar $ CLUSTER_NODE On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Uploading Spark Programs to Unravel", 
"url" : "current/adv/adv-unravel-server-sensor/adv-unravel-server-sensor-uploading-spark-prog-to-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading Spark Programs to Unravel", 
"snippet" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: Substitute your particular values for the highlighted text Option 1: Upload Individual Source Fi...", 
"body" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: Substitute your particular values for the highlighted text Option 1: Upload Individual Source Files Upload Spark source files and specify their location on the spark-submit Example: In yarn-client mode local on the --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" spark-submit export PROGRAM_DIR= FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY FULLY_QUALIFIED_JAR_PATH} The default value of spark.unravel.program.dir Example: In yarn-cluster mode on the --files comma-separated-list-of-source-files spark-submit export PROGRAM_DIR= FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY FULLY_QUALIFIED_JAR_PATH Option 2: Upload a Zip Archive Package all relevant source files into a zip archive. It's advisable to keep the archive small by including only the relevant driver source files. Example: In yarn-client mode local on the --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" spark-submit export PROGRAM_DIR= FULLY_QUALIFIED_ZIP_PATH SRC_ZIP_NAME FULLY_QUALIFIED_JAR_PATH Example: In yarn-cluster mode and its filename with --files $SRC_ZIP_PATH --conf \"spark.unravel.program.zip= SRC_ZIP_NAME spark-submit export PROGRAM_DIR= FULLY_QUALIFIED_ZIP_PATH SRC_ZIP_NAME FULLY_QUALIFIED_JAR_PATH SRC_ZIP_NAME Unravel searches for source files in this order: (Option 1) spark.unravel.program.dir Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Spark Application Manger Program tab " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "current/adv/adv-unravel-server-sensor/adv-unravel-server-sensor-uninstalling-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel # sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel\n# sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Appendices", 
"url" : "current/appx.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Server Daemon Reference", 
"url" : "current/appx/appx-server-daemon.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference", 
"snippet" : "This reference covers both on-premises and cloud deployments. Some daemons and properties only apply to on-premises deployments....", 
"body" : "This reference covers both on-premises and cloud deployments. Some daemons and properties only apply to on-premises deployments. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "current/appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew _N Event Worker unr...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew _N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_host _N Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2 _N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma _N Metrics Analyzer unravel_ngui aNGular web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s _N Elasticsearch unravel_sw _N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us _N Universal sensor \\ Impala unravel_zk _N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "current/appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/Property Name\/Description Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/Property Name\/Description Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. or \/srv\/unravel\/tmp UNRAVEL_DATA_DIR \/tmp HDFS com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) 30 JDBC unravel.jdbc.username MySQL (embedded or external) username for db unravel unravel.jdbc.password MySQL (embedded or external) password for db random generated for bundled MySQL unravel.jdbc.url This is JDBC URL without username and password jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prodc Kafka com.unraveldata.kafka.broker_list embedded 127.0.0.1:4091 mapreduce com.unraveldata.longest.job.duration.days Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) 2 Oozie com.unraveldata.oozie.fetch.interval.sec seconds between intervals for fetching Oozie workflow status 120 com.unraveldata.oozie.fetch.num Max number of jobs to fetch during an interval 100 oozie.server.url URL for accessing Oozie to track workflows http:\/\/localhost:11000\/oozie Zookeeper com.unraveldata.zk.quorum embedded Zookeeper ensemble in form host1:port1,host2:port2, 127.0.0.1:4181 " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "current/appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Environment Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance argum...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Environment Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to jvm to override other settings unset HADOOP_CONF_DIR The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml as discovered by running hadoop fs -ls UNRAVEL_DATA_DIR A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data, zk_data tmp_data com.unraveldata.tmpdir \/srv\/unravel UNRAVEL_LISTEN_PORT The web UI port on the primary or standalone Unravel installation ( service unravel_ngui 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable Root Environment Settings", 
"url" : "current/appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Root Environment Settings", 
"snippet" : "The optional file \/etc\/unravel_ctl Environment Variable Description Default if not Set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel...", 
"body" : "The optional file \/etc\/unravel_ctl Environment Variable Description Default if not Set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "current/appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl Control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh Convenience script for Unravel start, restart, status, and ...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl Control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh Convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/etc\/unravel.ext.sh An optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties Site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties Version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~3.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel Server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB; depending on activity level, retention This directory or its subdirectories can be a symlink(s) to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel Server must be EBS optimized. " }, 
{ "title" : "HBASE Alerts and Metrics", 
"url" : "current/appx/appx-hbase.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ HBASE Alerts and Metrics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Alerts", 
"url" : "current/appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-Alerts", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Alerts", 
"snippet" : "Alerts generated and stored along with metrics. Unravel UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Reg...", 
"body" : "Alerts generated and stored along with metrics. Unravel UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region in transition beyond threshold period. If a region server is dead, this is common. If not run \"hbase hbck\" to see if your HBase cluster has corruptions. Server availability Dead region servers Check region server logs for more information Performance Region servers with reads > 20% of avg Region server hotspotting. Split regions or randomize the keys. Region servers with writes > 20% of avg Region server hotspotting. Split regions or randomize the keys. Regions within a table with reads > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a table with writes > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a regionserver with reads > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Regions within a regionserver with writes > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Load, osload > 20% of avg Check for compactions, regions in transition and server logs Balancer not running Enable Blancer # of compactions and length of compaction Disable periodic automatic major compactions by setting - hbase.hregion.majorcompaction to 0 Storage Regionservers with storage (storefilesie sum) > 20% of avg Split or randomize the keys Regions within a table with storage (storefilesie sum) > 20% of avg for that table Split or randomize the keys Temporal e.g. requests > 20% higher for the last 1 hour as compared to the prior 3 hours (just an example) Check master and region server alerts or environment issues which could be slowing down the read\/write " }, 
{ "title" : "Metrics", 
"url" : "current/appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-HBaseClusterMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Metrics", 
"snippet" : "Master\/Cluster & JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in millisec...", 
"body" : " Master\/Cluster & JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseconds numDeadRegionServers Number of dead Region Servers count numRegionServers Number of live Region Servers count ritCount The number of regions in transition count ritCountOverThreshold The number of regions that have been in transition longer than a threshold time seconds ritOldestAge The age of the longest region in transition, in milliseconds millliseconds OS Metrics (Ambari Only) OS Metrics Description Unit jvm_* jvm metrics number rpc_* rpc metrics number " }, 
{ "title" : "Region Server Metrics", 
"url" : "current/appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-RegionServerMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Region Server Metrics", 
"snippet" : "JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local Data...", 
"body" : " JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local DataNode, 0-100 percentage readRequestCount The number of read requests received count regionCount The number of regions hosted by the regionserver count slowOPCount The number of operations we thought were slow. OP: delete, get, put, increment, append count storeFileSize Aggregate size of the store files on disk bytes writeRequestCount The number of write requests received count OS Metrics (Ambari Only) OS Metrics Description Unit cpu_user cpu percentage disk.disk_free Amount of free disk space bytes disk.write_bps Number of bytes written per second to disk. bytes per second disk.read_bps Number of bytes read per second to disk. bytes per second load.load_one load number memory.mem_free Percentage of free memory. percentage network.bytes_in Total number incoming bytes to network. bytes network.bytes_out Total number outgoing bytes to network. bytes " }, 
{ "title" : "Table\/Region Metrics", 
"url" : "current/appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-TableRegionMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Table\/Region Metrics", 
"snippet" : "Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served byt...", 
"body" : " Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served bytes readRequestCount Number of read requests this region server has answered count writeRequestCount Number of mutation requests this region server has answered count " }, 
{ "title" : "Troubleshooting", 
"url" : "current/trouble.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Troubleshooting", 
"snippet" : "If you can't reach Unravel Server, ping your LAN DNS....", 
"body" : "If you can't reach Unravel Server, ping your LAN DNS. " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "current/trouble/trouble-diag.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unra...", 
"body" : " In the upper right corner of Unravel UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the diagnostic bundle through the UI, create the bundle on the Unravel host with the following script, and send the bundle to Unravel Support \/usr\/local\/unravel\/install_bin\/diag_dump.sh " }, 
{ "title" : "Release Notes", 
"url" : "current/release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "v4.5.0.6 Release Notes", 
"url" : "current/release-notes/release-notes-4506.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes", 
"snippet" : "Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties Improvements and Bug Fixes Improvements Application Page Filters - added ability to select multiple users and multiple queues.  (CUSTOMER-491) Restrict TLS protocols for Log Receiver communication.  (CUSTOMER-669) H...", 
"body" : " Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties Improvements and Bug Fixes Improvements Application Page Filters - added ability to select multiple users and multiple queues.  (CUSTOMER-491) Restrict TLS protocols for Log Receiver communication.  (CUSTOMER-669) HDP 3.0 now supports the Small Files Report. (REPORT-315) Bug Fixes Session can now be enabled\/disabled via com.unraveldata.session.enabled. (CUSTOMER-425) Spark application now displays job status in navigation tab for RBAC controlled non-admin users. (CUSTOMER-616) Auto action is not working as per the condition. (CUSTOMER-632) Add OID query filter to avoid timeouts in oid based group lookup. (CUSTOMER-643) Symbolic link to  \/usr\/local\/unravel\/logs is retained after upgrade. (CUSTOMER-647) HBASE OS Metrics supports CDH. (CUSTOMER-653) Impala queries now showing up on the Unravel UI. (CUSTOMER-654) Cluster optimization tab populates correctly. (CUSTOMER-656) Restrict TLS protocols for Log Receiver communication.  (CUSTOMER-669) Min and Max Filters have been added for for queue analysis page. (CUSTOMER-685) Unravel no longer loses connectivity with RM for long period of time. (CUSTOMER-708) Airflow Dags are parsed correctly. (CUSTOMER-725) AA processing leading to JdkHttpClient: HTTP Transfer will be suspended. (CUSTOMER-736) Schedule notification mail modification. (CUSTOMER-751) Cloud discovery functionality in cloud reports no longer fails when using Ambari Metrics System without sufficient historic data. (CLOUD-220) Add es_checker in es migration. (INSTALL-144) Removed \/bin\/hadoop dependency in rpm installation. (INSTALL-147) Moved Airflow to a different thread. (PLATFORM-1294) Queue analysis data is correct when queues are created or deleted in a middle of the report range. (REPORT-316) Cluster name containing spaces are shown properly in UI. (UIX-1647) Spark Program display now links to the source code line correctly. (UIX-1677) Clicking an application in application tab no longer leads to spinner and no server response on API query. Detail is now displayed for application. (UIX-1690) " }, 
{ "title" : "Software Version", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Software Version", 
"snippet" : "Release Date: 03\/11\/2019 For download location, see...", 
"body" : "Release Date: 03\/11\/2019 For download location, see " }, 
{ "title" : "Software Upgrade Support", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Software Upgrade Support", 
"snippet" : "See...", 
"body" : " See " }, 
{ "title" : "Certified Platforms", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "Supported Browsers", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Supported Browsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.6 Needed? 4.5.0.6 CDH 5.15\/CDH 6.0 (Parcel version: 1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.4 CDH 5.15\/CDH 6.0 (Parcel version: 1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.3 CDH 5.1...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.6 Needed? 4.5.0.6 CDH 5.15\/CDH 6.0 (Parcel version: 1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.4 CDH 5.15\/CDH 6.0 (Parcel version: 1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Known Issues", 
"url" : "current/release-notes/release-notes-4506.html#UUID-1a18df34-c8fe-44e6-c39e-c63aac345846_id_v4504ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.6 Release Notes \/ Known Issues", 
"snippet" : "On table details page, table names are shown without name space. (UIX-1736) HBASE table pagination is incorrect (HBASE-95) In Usage details tab, filtering by tag does not show the corresponding apps. (UIX-1738) Data IO does not capture the Read IO. (UIX-1737) PostgreSQL DB is now usable across all f...", 
"body" : " On table details page, table names are shown without name space. (UIX-1736) HBASE table pagination is incorrect (HBASE-95) In Usage details tab, filtering by tag does not show the corresponding apps. (UIX-1738) Data IO does not capture the Read IO. (UIX-1737) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Ondemand is not compatible with RHEL 7.1. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) get_db_env.sh need to be updated with the full path of mysq. (CUSTOMER-648) Add support for custom SSL cert for Ambari and Cloudera Manager in Ondemand reports. (CUSTOMER-649) Forecasting tab throwing error potentially due to SSL or bad configs. (CUSTOMER-655) appstatus.AppInfoAccessor: Unable to update ES document. (CUSTOMER-658) Unravel_tw is not able to connect to hive metastore hosted on mariadb. (CUSTOMER-716) Auto Action Inconsistencies: Hive Long-Running Query (CUSTOMER-724) Data Insights page does not show data. (CUSTOMER-754) MR: Actions: Move to Queue does not work. (PLATFORM-1195) Cloud Reports: You must select an instance type  (>30 types) when choosing EMR as the Cloud Provider. MR app does not have cluster ID when it has Data I\/O. (CLOUD-263) Upgrade from 4.5.0.3 to 4.5.0.6 on CDH-5.16 cluster 17 services stop working. The services start only after executing switch to user hdfs. (INSTALL-145) Kerberos: kerberos.principal in unravel properties does not take effect. (INSTALL-148) fs image download will not trigger after install. (REPORT-262) Ondemand feature (small files) is not working if sentry is enabled in the cluster. (REPORT-323) Vertex name is null. (TEZLLAP-227) Top X Report: Hive-on-Tez apps will show vcoreseconds and memory seconds values as zero. (TEZLLAP-249) Repeated Events are showing up in the Event box for the failed Tez apps. (TEZLLAP-259) The group-by pie charts in Cluster Discovery may be empty if don't have enough historic data. (CLOUD-221) Queue Analysis graph zooming and resetting not working properly in Edge. (UIX-1589) Data Insights Page: Does not show any data\/graph for Hot,Warm and Cold data fields. (UIX-1707) Sessions button is not enabled by default in the UI. (UIX-1725) Last update for SQL query plan is not reaching to spark worker. (USPARK-162) For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.0.4 Release Notes", 
"url" : "current/release-notes/release-notes-4504.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes", 
"snippet" : "Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in...", 
"body" : " Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in running state from application page. (CUSTOMER-337) Bug Fixes RBAC tagcmd does not always work in all environments. (CUSTOMER-592) LDAP AUTH failing due to null pointer exception. (CUSTOMER-600) Tagged Spark jobs not showing in workflow view. (CUSTOMER-608) Diagnostics files are either empty or not complete. (CUSTOMER-621) OnDemand reports runs \"successfully\" but not showing in UI. (CUSTOMER-635) Queue analysis report generation fails with a \"x not in list\" error. (REPORT-277) Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) " }, 
{ "title" : "Software Version", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Version", 
"snippet" : "Release Date: 02\/11\/2019 For download location, see...", 
"body" : "Release Date: 02\/11\/2019 For download location, see " }, 
{ "title" : "Software Upgrade Support", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Upgrade Support", 
"snippet" : "See...", 
"body" : " See " }, 
{ "title" : "Certified Platforms", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7 (recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7 (recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "Supported Browsers", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Supported Browsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4 Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4 Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ New Features", 
"snippet" : "Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363)...", 
"body" : " Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363) " }, 
{ "title" : "Known Issues", 
"url" : "current/release-notes/release-notes-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Known Issues", 
"snippet" : "HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characte...", 
"body" : " HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge. (UIX-1589) Cloud Reports: You must select an instance type (< 30 types) when choosing EMR as the Cloud Provider. Queue analysis data can be missing or skewed if queues were created or deleted in a middle of the report range. (REPORT-316) Top X Report: Hive-on-Tez apps will show vcoreseconds and memory seconds values as zero. (TEZLLAP-249) The group-by pie charts in Cluster Discovery may be empty if don't have enough historic data (CLOUD-221) Cloud discovery functionality in cloud reports may fail with \"UDObject\" object has no attribute \"warn\" Add support for custom SSL cert for Ambari and Cloudera Manager in ondemand reports (CUSTOMER-649) For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.0.3 Release Notes", 
"url" : "current/release-notes/release-notes-4503.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes", 
"snippet" : "Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties...", 
"body" : " Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/28\/2019 For download location, see...", 
"body" : "Release Date: 01\/28\/2019 For download location, see " }, 
{ "title" : "Software Upgrade Support", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Upgrade Support", 
"snippet" : "See...", 
"body" : " See " }, 
{ "title" : "Certified Platforms", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3 Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (P...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3 Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ New Features", 
"snippet" : "NONE...", 
"body" : " NONE " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node...", 
"body" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node. (CUSTOMER-419) Bug Fixes Impala Chargeback shows \"User\" field data in report. (CUSTOMER-605) Unravel Spark sensor supports clusterId with spaces. (CUSTOMER-595) Airflow connectivity fixed. (CUSTOMER-575) Airflow Workflow are now displayed on Workflow page. (CUSTOMER-458) unravel_ondemand daemon is now included in unravel_all.sh. (CUSTOMER-434, CUSTOMER-375) Queue Analysis collects metrics for nested queue on HDP when configured to use Fair Scheduler. (REPORT-294) Queue metric sensor works with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled, (PLATFORM-927, PLATFORM-1052) " }, 
{ "title" : "Known Issues", 
"url" : "current/release-notes/release-notes-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger a...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge.(UIX-1589) Cloud Reports: You must select an instance type (< 30 types) when choosing EMR as the Cloud Provider. Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.0.2 Release Notes", 
"url" : "current/release-notes/release-notes-4502.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes", 
"snippet" : "Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties...", 
"body" : " Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2019 For download location, see...", 
"body" : "Release Date: 01\/21\/2019 For download location, see " }, 
{ "title" : "Software Upgrade Support", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Upgrade Support", 
"snippet" : "See...", 
"body" : " See " }, 
{ "title" : "Certified Platforms", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7 (recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7 (recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) starts. (PLATFORM-1073) Unravel_km no longer clear indexes. (PLATFORM-1070) es_clear.sh...", 
"body" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) starts. (PLATFORM-1073) Unravel_km no longer clear indexes. (PLATFORM-1070) es_clear.sh " }, 
{ "title" : "Known Issues", 
"url" : "current/release-notes/release-notes-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supp...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) Unravel Spark sensor doesn't support clusterId with spaces. (CUSTOMER-59) fs image download will not trigger after install. (REPORT-262) Queue Analysis Graph zooming and resetting not working properly in Edge. (UIX-1589) May fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP. (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled. (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type (< 30 types) when choosing EMR as the Cloud Provider. For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.0 Release Notes", 
"url" : "current/release-notes/release-notes-4500.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes", 
"snippet" : "Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties...", 
"body" : " Updates to Unravel's Configuration Properties v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01 \/14\/2019 For download location, see...", 
"body" : "Release Date: 01 \/14\/2019 For download location, see " }, 
{ "title" : "Software Upgrade Support", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Upgrade Support", 
"snippet" : "See...", 
"body" : " See " }, 
{ "title" : "Certified Platforms", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 (Postgres is bundled) Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "Supported Browsers", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported Browsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ New Features", 
"snippet" : "Applications Ability to interactively 'Kill' and 'Move' YARN applications via UI. (CUSTOMER-110) Live view for Spark Applications\/Jobs Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark...", 
"body" : " Applications Ability to interactively 'Kill' and 'Move' YARN applications via UI. (CUSTOMER-110) Live view for Spark Applications\/Jobs Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark jobs show completed stage, running stage and overall progress. (CUSTOMER-111) Insights New Data, operator and SQL-level insights for Impala. New efficiency and failure events for Hive\/Tez. Cloud Reports enable you in planning → migrating → managing workloads in cloud environments (Preview*) Added several new API end points. Retrieve app recommendations. (CUSTOMER-396) Get finished and total number of jobs that ran on a particular date range. (CUSTOMER-367) Get status, error, logs, summary of individual Apps. (CUSTOMER-253) * Preview features are in beta and subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend you deploy Preview features in a production environment. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions on mouse hover over the AA2 badge for an application. (CUSTOMER-85, CUSTOMER-275, UIX-1249) Link to the offending application is contained in email notification. (...", 
"body" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions on mouse hover over the AA2 badge for an application. (CUSTOMER-85, CUSTOMER-275, UIX-1249) Link to the offending application is contained in email notification. (CUSTOMER-251) Violation email is now sent to the owner using LDAP email address field. (CUSTOMER-350) Rules for catching named workflows that have missed their SLAs are applied are no longer indiscriminately to all running workflows. (CUSTOMER-400) Operations Chargeback Reports now support Impala queries. (IMPALA-118) Top-X Report now supports Spark and Hive-On-Tez. (REPORT-241) Sessions Improved Spark's session recommendations for bigger containers, effective broadcast size & shuffle partition size. (SESS-128, SESS-129, SESS-130) Spark defines default spark master mode when not available from Sensor. (USPARK-170) com.unraveldata.spark.master prevents event log from loading if application duration is more than configured threshold. (USPARK-168) com.unraveldata.spark.eventlog.appDuration Reporting OnDemand is now supported via PostgreSQL DB. (REPORT-229) Added support to disable Small Files. (REPORT-204) Small File Report generation is faster. (REPORT-181) Small Files\/ File Reports can run even without dfsadmin privileges. (REPORT-143) Queue Analysis now is interactive, supports MapR disk metrics, and has UI presentation and performance improvements. (REPORT-105) Tez Improvements in map tasks recommendations for Hive\/Tez apps. (TEZLLAP-47) Removed recommendations to increase the number of tasks for ORDER BY reducer vertices. (TEZLLAP-35) Improvements in hive.tez.auto.reducer.parallelism Improvements in failure events for Hive\/Tez apps. (TEZLLAP-80) Added support for Hive\/Tez on MapR 5.2 and 6.0. (TEZLLAP-103) A failure event is now triggered when a Tez DAG fails. (TEZLLAP-145) Hive queries executed as Tez apps are now connected with their Tez parent app in the UI. (TEZLLAP-199) Hive query metrics are now published to the Auto-Actions framework. (TEZLLAP-206) Impala Input tables used by Impala queries are now displayed in the Data Page. (IMPALA-121) DML statements are now captured. ( IMPALA-166) Cluster name is now available when using impalad as the data source. (IMPALA-78) UIX UI shows list of triggered auto actions on mouse hover over the AA2 badge. (UIX-1249) Queue Analysis Graphs are now interactive and clicking on any of the graphs will open up the cluster view for that point in time with a list of all apps that were running at that time. (UIX-1443) Average values for each queue metric is now displayed next to its value when selecting a point on a graph. (UIX-1493) Average run apps, used memory, used vcores, and used disk (MaprR only) now are included as columns in the list of analyzed queues and can be sorted by these values. (UIX-1535) All queue metric labels now have descriptions, when hovering over the metric label a pop-up window will be shown giving a complete description. (UIX-1444) Raw queue metrics now have a retention policy to preventing possibility of the database running out of space, default is 90 days. (PLATFORM-945) MapR disk metrics are supported, the 4th graph \"Disk Usage\" will be displayed alongside with Apps, Memory, VCore usage when monitoring MapR clusters. (REPORT-220) Rendering performance of graphs was greatly improved in 4.5.0.0 and users should notice much less lag when opening queue metric graphs. (UIX-1515) Queue metric collector sensor and queue analysis reports now fully support PostgreSQL as backend storage database alongside with MySQL. (REPORT-274) HiveHook Sensors HiveHook Support for Hive 2.2.0 & Hive 2.3.0 Support for Single HiveHook class Bug Fixes Customer Fixes Provide 'path' filter options in Small Files Report. (CUSTOMER-371) Support for com.unraveldata.rbac.tagcmd For inbound HTTPS connections to Unravel UI, provide support to restrict TLS protocols. (CUSTOMER-515) =bigsecret needs to support encrypted passwords. (CUSTOMER-473) com.unraveldata.ldap.bind_pw Unravel using global resources to make connections to internet. (CUSTOMER-449) Support for RHEL6 with OnDemand Framework. (CUSTOMER-435) Spark Spark application\/job progress (CUSTOMER-475) Spark “Load Logs” and “Application Diagnostics” (CUSTOMER-508) Spark-shell is generating unwanted warnings (CUSTOMER-407) Spark execution graph display issue (CUSTOMER-283) Execution tab needs to show unique Stage IDs (CUSTOMER-236) Kafka Kafka Consumer Groups not displaying (CUSTOMER-366) Kafka page not showing all partitions in UI (CUSTOMER-544) Application Page Filters - added ability to select multiple users and multiple queues. (CUSTOMER-491) Sorting doesn't take into account MB vs GB vs TB .(CUSTOMER-383) Airflow Monitoring does not work consistently. (CUSTOMER-304) Workflow compare panel only showing 24 hours worth of data. (CUSTOMER-291) Queue analysis reports is empty and sometimes hits 1000 field limit in ES. (CUSTOMER-471) Sessions Recommendations generated by Application are now consistent Inside and Outside Sessions. (SESS-138) CMP API issue picks up data ~4 days later than the earliest data point viewable. ( SESS-60) Auto Actions Auto actions are not triggering for long running Hive apps with Tez as Execution Engine. (PLATFORM-643) Reporting TOP-X UI Stability Fixes (REPORT-263) Error handling Improvements for Small Files \/ File Reports (REPORT-235) Small Files Data Inconsistencies Improvements (REPORT-209) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) Tez Tez apps are now visible in the UI regardless of missing or incorrect ATS configuration. (TEZLLAP-50) A Hive\/Tez query is no longer associated with multiple DAGs in the Unravel UI. (TEZLLAP-198) Fixed NullPointerException raised in unravel_ew_1.log Fixed NumberFormatException raised when tez.am.grouping.max-size tez.grouping.max-size Fixed NullPointerException raised when dagMap is not populated. (TEZLLAP-219) Fixed NumberFormatException raised in MapR 6.0 when tez.grouping.min-size Tez apps are no longer stuck in running state in MapR 6.0. (TEZLLAP-231) Impala HTTP connection to CM will no longer wait indefinitely. (IMPALA-139) Short-running Impala queries are no longer missed. (IMPALA-165) Start and end query times use the correct timezone when using impalad as the data source. (IMPALA-164) UIX Queue Analysis graphs are taking long time to render. (UIX-1515) Added Report Archives Support for PostgresSQL. (REPORT-257) Added appsRun metric on queue analysis graph. (UIX-1476) Global search no longer returns multiple results for single Hive Query ID. (UIX-1224) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) Spark Major performance improvement in compressed Event log parsing. (USPARK-164 ) Queue Analysis MapR queue analysis report is missing Disk metrics. (REPORT-220) Queue metric sensor is not collecting any data if PostgreSQL is used. (REPORT-274) ES reports index limit was increased to 10000. (PLATFORM-821) Queue Analysis report stuck at putting data to ES. (REPORT-291) " }, 
{ "title" : "Known Issues", 
"url" : "current/release-notes/release-notes-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis Graph zooming and resetting ...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis Graph zooming and resetting not working properly in Edge. ( UIX-1589 ) May fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP. (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or TLS with authentication is enabled. (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type (< 30 types) when choosing EMR as the Cloud Provider. For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.x - Upgrade Instructions", 
"url" : "current/release-notes/release-notes-upgrade.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions", 
"snippet" : "PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports which only work with MySQL. Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL eit...", 
"body" : " PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports which only work with MySQL. Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL either as an internal or external database 4.4.x you might be using with PostgreSQL which was bundled " }, 
{ "title" : "The following upgrade options arenotsupported:", 
"url" : "current/release-notes/release-notes-upgrade.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-Thefollowingupgradeoptionsarenotsupported", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ The following upgrade options arenotsupported:", 
"snippet" : "Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL...", 
"body" : " Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL " }, 
{ "title" : "Supported Installs\/Upgrade", 
"url" : "current/release-notes/release-notes-upgrade.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-SupportedInstallsUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ Supported Installs\/Upgrade", 
"snippet" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQ Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel ...", 
"body" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQ Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel for MySQL Upgrade from 4.4.x If your database is MySQL If the MySQL data has not been migrated to the new partitioned tables, then follow all the steps inUpgrade from 4.3.1.Xsection If MySQL data is already migrated to partitions, then follow all the steps inUpgrade from 4.3.1.X section but skip the \"MySql Partitioning and Data Migration\" step To check if the tables are already partitioned, execute the following command: echo \"show table status like '%blackboards%'\" | \/usr\/local\/unravel\/install_bin\/db_access.sh If your database is PostgreSQL Install RPM If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.3.x Embedded\/external MySQL to 4.5 using external MySQL Unravel 4.5 uses partitioned MySQL tables to manage the disk space and you must prepare for the upgrade. Prepare for upgrade Check amount of disk space used by MySQL (Space_Used) via the CLI with the command Embedded MySQL du -sh \/srv\/unravel\/db_data External MySQL grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs du -sh Check amount of disk space available in the database partition (Space_Available) via the CLI with the command Embedded MySQL df -h \/srv\/unravel\/db_data External MySQL grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs df -h If Space_Used > Space_Available then follow instructions to move MySQL Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/usr\/local\/unravel\/dlib\/mybatis\/mysql-connector*.jar \/usr\/local\/unravel\/share\/java Upgrade Download the RPM Install RPM Complete the instructions at MySql Partitioning and Data Migration RBAC properties have been changed. For properties you must replace or add in order for RBAC to work, see here If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.2.x 4.2.x (embedded MySQL) -> 4.5 (external MySQL) 4.2.x (external MySQL) -> 4.5 (external MySQL) Upgrade to 4.3.1.7 then follow the instructions above. For support issues, visit Unravel Support " }, 
{ "title" : "v4.5.x - Updates to Unravel Properties", 
"url" : "current/release-notes/release-notes-upgrade-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties", 
"snippet" : "This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties...", 
"body" : " This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties " }, 
{ "title" : "New", 
"url" : "current/release-notes/release-notes-upgrade-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-New", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ New", 
"snippet" : "Property\/Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replace...", 
"body" : " Property\/Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replaced hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.groupFilter replaced hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupDNPattern replaced hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.guidKey replaced hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.userDNPattern replaced hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userFilter replaced hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.groupMembershipKey replaced hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupClassKey replaced hive.server2.authentication.ldap.groupClassKey Custom Banner com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays true text end.date : no change to UI false com.unraveldata.custom.banner.text Text to display when display - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the custom banner Format: YYYYMMDD T Z - HBASE (See here com.unraveldata.hbase. CLUSTER-NAME .node.http.apis HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... * Example: http:\/\/your.master.server:16010,http:\/\/your.region.server:16030 Hive-hook SSL (See here com.unraveldata.live.logreceiver.port.https HTTPS server port (negative value means disabled HTTPS server) -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks - com.unraveldata.server.ssl.cert_password KeyStore password - com.unraveldata.server.ssl.trust_store_path TrustStore file path - com.unraveldata.server.ssl.trust_store_password TrustStore password - Small Files and File Reports (See here unravel.python.files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true : absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false - unravel.python.min_parent_dir_depth Directory depth to start search at. - unravel.python.max_parent_dir_depth Directory depth to end search at. Maximum is 50. - unravel.python.drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false - Small Files (See here unravel.python.reports.files.small_files_use_avg_file_size_flag true unravel.python.reports.files.small_files_min_parent_dir_depth 0 unravel.python.reports.files.small_files_max_parent_dir_depth 10 reports.files.small_files_drill_down_subdirs_flag true File Reports (See here The following four properties are defined per file size Size unravel.python.reports.files. Size _files_use_avg_file_size_flag false unravel.python.reports.files. Size _file_min_parent_dir_depth 0 unravel.python.reports.files. Size _file_max_parent_dir_depth 10 unravel.python.reports.files. Size _file_drill_down_subdirs_flag false Forecasting & Cloud Reports (See here com.unraveldata.ambari.manager.url URL of cloud manger, e.g., http:\/\/$clouderaserver:7180, http:\/\/$ambariserver:8080 For Cloudera, if the URL does not contain a port you must define manager.port - com.unraveldata.ambari.manager.username Username to log into the manager - com.unraveldata.ambari.manager.password Password for the username. - Spark com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor com.unraveldata.spark.eventlog.appDuration If application duration is more than configured value load the event log is not loaded. 1440 mins Application Liveness com.unraveldata.appstatus.refresh.mins Time interval in minutes to scan for running applications and marking the stale ones. 5 mins com.unraveldata.appstatus.stale_limit.mins Maximum number of minutes since the latest app update from RM before it is marked as stale. 10 mins Experimental Experimental features (not to be used in production) [empty] com.unraveldata.cluster_access.host Cluster Access Service host (where the service will be bound) 0.0.0.0 com.unraveldata.cluster_access.port Cluster Access Service port 4020 com.unraveldata.sregistry.hostport Service Registry host:port ${com.unraveldata.zk.quorum} com.unraveldata.sensor.polling.secs The base polling period of Unravel reactive sensors in seconds. 30s com.unraveldata.appevents.emitters.exclude.list Comma separated list of the application event emitter IDs which will be disabled\/excluded. - com.unraveldata.multicluster.enabled Allow Unravel to operate in multi-cluster mode. In this mode a service registry will be used to discover and access all registered (local and remote) clusters. false " }, 
{ "title" : "Renamed\/Replaced Properties", 
"url" : "current/release-notes/release-notes-upgrade-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-RenamedReplacedProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ Renamed\/Replaced Properties", 
"snippet" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPa...", 
"body" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPattern hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.guidKey hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userDNPattern hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.userFilter hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupMembershipKey hive.server2.authentication.ldap.groupClassKey com.unraveldata.ldap.groupClassKey RBAC You must update these properties manually. Property Replaced with com.unraveldata.rbac.mode com.unraveldata.login.mode com.unraveldata.rbac.user.operations.enabled com.unraveldata.ngui.user.mode " }, 
{ "title" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"url" : "current/adv-conf-security-cdh-clusterwide-mr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5.0.5 \/ CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"snippet" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_...", 
"body" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/ UNRAVEL_SENSOR UNRAVEL_HOST_IP Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property> \n\/\/ this is one line \n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> Save the changes. Deploy the client configuration by clicking the deploy glyph ( Actions Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services Use the Unravel UI to monitor the situation. When you view the MapReduce APM Resource Usage " }, 
{ "title" : "", 
"url" : "downloads.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Downloads Unravel maintains symlinks to the latest versions of all its software. You can download them from : http:\/\/preview.unraveldata.com\/unravel\/RPM UV_X.Y.Z TYPE UV_X.Y.Z TYPE where: is the Unravel software version UV_X.Y.Z is TYPE CDH-HDP-MapR EMR Azure is the username for downloading Unravel ...", 
"body" : " Downloads Unravel maintains symlinks to the latest versions of all its software. You can download them from : http:\/\/preview.unraveldata.com\/unravel\/RPM UV_X.Y.Z TYPE UV_X.Y.Z TYPE where: is the Unravel software version UV_X.Y.Z is TYPE CDH-HDP-MapR EMR Azure is the username for downloading Unravel software. Contact USERNAME Unravel Support is the password for downloading Unravel software. Contact PASSWORD Unravel Support For example, # curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/ UV_X.Y.Z TYPE UV_X.Y.Z TYPE UV_X.Y.Z TYPE USERNAME PASSWORD " }, 
{ "title" : "Unravel 4.5.0.6", 
"url" : "downloads/download-4506.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.6", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4506.html#UUID-60f980d1-e3c9-da7f-b946-f912680c679e_id_Unravelv4504-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.6 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.6-GA-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.6-GA-CDH-HDP-MAPR-latest.rpm -u USERNAMEE PASSWORD MD5SUM: 2dbb9963edf9cd8cab47b4f67bc7f82f...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.6-GA-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.6-GA-CDH-HDP-MAPR-latest.rpm -u USERNAMEE PASSWORD MD5SUM: 2dbb9963edf9cd8cab47b4f67bc7f82f " }, 
{ "title" : "OnDemand - RHEL 6", 
"url" : "downloads/download-4506.html#UUID-60f980d1-e3c9-da7f-b946-f912680c679e_id_Unravelv4504-OnDemand-RHEL6", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.6 \/ OnDemand - RHEL 6", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.6-GA-rhel6.tar.gz -o Ondemand-4.5.0.6-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: dfae29908e13474b787b377c25998265...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.6-GA-rhel6.tar.gz -o Ondemand-4.5.0.6-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: dfae29908e13474b787b377c25998265 " }, 
{ "title" : "OnDemand - RHEL 7", 
"url" : "downloads/download-4506.html#UUID-60f980d1-e3c9-da7f-b946-f912680c679e_id_Unravelv4504-OnDemand-RHEL7", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.6 \/ OnDemand - RHEL 7", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.6-GA-rhel7.tar.gz -o Ondemand-4.5.0.6-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: 30d935fbec1e4710f6aad3430f099b65...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.6-GA-rhel7.tar.gz -o Ondemand-4.5.0.6-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: 30d935fbec1e4710f6aad3430f099b65 " }, 
{ "title" : "Unravel 4.5.0.4", 
"url" : "downloads/download-4504.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.4", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4504.html#UUID-a0f4e0ac-7923-7746-ae1b-09b5dcc122ef_id_Unravelv4504-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.4 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.4-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.4-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: ff3326cd81280179dce72806c138d72e...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.4-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.4-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: ff3326cd81280179dce72806c138d72e " }, 
{ "title" : "OnDemand - RHEL 6", 
"url" : "downloads/download-4504.html#UUID-a0f4e0ac-7923-7746-ae1b-09b5dcc122ef_id_Unravelv4504-OnDemand-RHEL6", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.4 \/ OnDemand - RHEL 6", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.4-GA-rhel6.tar.gz -o Ondemand-4.5.0.4-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: f09523b0cd5c6b68d028968c395bf157...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.4-GA-rhel6.tar.gz -o Ondemand-4.5.0.4-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: f09523b0cd5c6b68d028968c395bf157 " }, 
{ "title" : "OnDemand - RHEL 7", 
"url" : "downloads/download-4504.html#UUID-a0f4e0ac-7923-7746-ae1b-09b5dcc122ef_id_Unravelv4504-OnDemand-RHEL7", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.4 \/ OnDemand - RHEL 7", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.4-GA-rhel7.tar.gz -o Ondemand-4.5.0.4-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: a0e3829e11e8530f0860730afa6b665b...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.4-GA-rhel7.tar.gz -o Ondemand-4.5.0.4-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: a0e3829e11e8530f0860730afa6b665b " }, 
{ "title" : "Unravel 4.5.0.3", 
"url" : "downloads/download-4503.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.3", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4503.html#UUID-8ba10d1f-106d-f282-62de-7937e47109b2_id_Unravelv4503-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.3 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.3-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.3-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 1999a8ca1b5c485acff4549ab1069dde...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.3-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.3-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 1999a8ca1b5c485acff4549ab1069dde " }, 
{ "title" : "OnDemand - RHEL 6", 
"url" : "downloads/download-4503.html#UUID-8ba10d1f-106d-f282-62de-7937e47109b2_id_Unravelv4503-OnDemand-RHEL6", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.3 \/ OnDemand - RHEL 6", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.3-rhel6.tar.gz -o Ondemand-4.5.0.3-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: 629cadb170cef0fce02407a17f036bd2...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.3-rhel6.tar.gz -o Ondemand-4.5.0.3-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: 629cadb170cef0fce02407a17f036bd2 " }, 
{ "title" : "OnDemand - RHEL 7", 
"url" : "downloads/download-4503.html#UUID-8ba10d1f-106d-f282-62de-7937e47109b2_id_Unravelv4503-OnDemand-RHEL7", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.3 \/ OnDemand - RHEL 7", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.3-rhel7.tar.gz -o Ondemand-4.5.0.3-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: e421b4b5a20c70667d2a9994597333d2...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.3-rhel7.tar.gz -o Ondemand-4.5.0.3-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: e421b4b5a20c70667d2a9994597333d2 " }, 
{ "title" : "Unravel 4.5.0.2", 
"url" : "downloads/download-4502.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.2", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4502.html#UUID-f27e1ecc-eacf-3469-309a-1698579ce454_id_Unravelv4502-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.2 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.2-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.2-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 84fbded89fefa41c44d0dfba9eb87b12...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/unravel-4.5.0.2-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.2-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 84fbded89fefa41c44d0dfba9eb87b12 " }, 
{ "title" : "OnDemand - RHEL 6", 
"url" : "downloads/download-4502.html#UUID-f27e1ecc-eacf-3469-309a-1698579ce454_id_Unravelv4502-OnDemand-RHEL6", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.2 \/ OnDemand - RHEL 6", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.2-GA-rhel6.tar.gz -o Ondemand-4.5.0.2-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: 234c34983ed46e13c3627a3d4896c23e...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.2-GA-rhel6.tar.gz -o Ondemand-4.5.0.2-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: 234c34983ed46e13c3627a3d4896c23e " }, 
{ "title" : "OnDemand - RHEL 7", 
"url" : "downloads/download-4502.html#UUID-f27e1ecc-eacf-3469-309a-1698579ce454_id_Unravelv4502-OnDemand-RHEL7", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.2 \/ OnDemand - RHEL 7", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.2-GA-rhel7.tar.gz -o Ondemand-4.5.0.2-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: 055fc036ee69b278909ea42c3c2a2133...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/Ondemand-4.5.0.2-GA-rhel7.tar.gz -o Ondemand-4.5.0.2-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: 055fc036ee69b278909ea42c3c2a2133 " }, 
{ "title" : "Unravel 4.5.0.0", 
"url" : "downloads/download-4500.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.0", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4500.html#UUID-9e263146-2d48-7380-7cb1-8bc1ea7933dc_id_Unravelv4500-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.0 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/CDH-HDP-MapR\/unravel-4.5.0.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 60bd762830088ab709c3ce6dee1b1d6c...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/CDH-HDP-MapR\/unravel-4.5.0.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.5.0.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 60bd762830088ab709c3ce6dee1b1d6c " }, 
{ "title" : "OnDemand - RHEL 6", 
"url" : "downloads/download-4500.html#UUID-9e263146-2d48-7380-7cb1-8bc1ea7933dc_id_Unravelv4500-OnDemand-RHEL6", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.0 \/ OnDemand - RHEL 6", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel6.tar.gz -o Ondemand-4.5.0.0-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: fa91f6baca2d1417b8598002c4b2f85e...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel6.tar.gz -o Ondemand-4.5.0.0-GA-rhel6.tar.gz -u USERNAME PASSWORD MD5SUM: fa91f6baca2d1417b8598002c4b2f85e " }, 
{ "title" : "OnDemand - RHEL 7", 
"url" : "downloads/download-4500.html#UUID-9e263146-2d48-7380-7cb1-8bc1ea7933dc_id_Unravelv4500-OnDemand-RHEL7", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.5.0.0 \/ OnDemand - RHEL 7", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel7.tar.gz -o Ondemand-4.5.0.0-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: abbe875a4fb514a13ae712956e02a278...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel7.tar.gz -o Ondemand-4.5.0.0-GA-rhel7.tar.gz -u USERNAME PASSWORD MD5SUM: abbe875a4fb514a13ae712956e02a278 " }, 
{ "title" : "Unravel 4.4.2.1", 
"url" : "downloads/download-4421.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.2.1", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4421 CDH, HDP, and MapR # curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/CDH-HDP-MapR\/unravel-4.4.2.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.2.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: b37f39d5a7fd9d54c1e4445a3bedfe8...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4421 CDH, HDP, and MapR # curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/CDH-HDP-MapR\/unravel-4.4.2.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.2.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: b37f39d5a7fd9d54c1e4445a3bedfe84 " }, 
{ "title" : "Unravel 4.4.2.0", 
"url" : "downloads/download-4420.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.2.0", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4420 CDH, HDP, and MapR # curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/CDH-HDP-MapR\/unravel-4.4.2.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.2.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 8a91ad3d3e74dc3a29a8f3e086f564d...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4420 CDH, HDP, and MapR # curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/CDH-HDP-MapR\/unravel-4.4.2.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.2.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 8a91ad3d3e74dc3a29a8f3e086f564d4 " }, 
{ "title" : "Unravel 4.4.1.1", 
"url" : "downloads/download-4411.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.1.1", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4411...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4411 " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4411.html#UUID-36150ddc-397f-0274-a1c5-29c7e053de76_id_Unravelv4411-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.1.1 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/CDH-HDP-MapR\/unravel-4.4.1.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.1.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/CDH-HDP-MapR\/unravel-4.4.1.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.1.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD " }, 
{ "title" : "Unravel 4.4.1.0", 
"url" : "downloads/download-4410.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.1.0", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4410...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4410 " }, 
{ "title" : "CDH, HDP & MapR", 
"url" : "downloads/download-4410.html#UUID-acc39fb3-953d-cc10-59ec-dd1f5ad4a62b_id_Unravelv4410-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.1.0 \/ CDH, HDP & MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/CDH-HDP-MapR\/unravel-4.4.1.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.1.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: b73b7746826a8f85df5e9089e11176be...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/CDH-HDP-MapR\/unravel-4.4.1.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.1.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: b73b7746826a8f85df5e9089e11176be " }, 
{ "title" : "Unravel 4.4.0", 
"url" : "downloads/download-4400.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.0", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4400...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4400 " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4400.html#UUID-9aeab61e-877f-33cc-bda9-b531be4c8ad3_id_Unravelv440-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.4.0 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.0\/CDH-HDP-MapR\/unravel-4.4.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 4ae12f59f97918d392053ee4d13e0c79...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.0\/CDH-HDP-MapR\/unravel-4.4.0-CDH-HDP-MAPR-latest.rpm -o unravel-4.4.0-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 4ae12f59f97918d392053ee4d13e0c79 " }, 
{ "title" : "Unravel 4.3.1.9", 
"url" : "downloads/download-4319.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.9", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4319...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4319 " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4319.html#UUID-2448f382-92e3-5634-d796-f2b44aeaf4dd_id_Unravelv4319-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.9 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.9-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.9-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 0731b830fae3d70c302e1fd4150283b9...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.9-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.9-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 0731b830fae3d70c302e1fd4150283b9 " }, 
{ "title" : "Unravel 4.3.1.8 - Not Available", 
"url" : "downloads/download-4318.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.8 - Not Available", 
"snippet" : "This release was not made available to our customers....", 
"body" : "This release was not made available to our customers. " }, 
{ "title" : "Unravel 4.3.1.7", 
"url" : "downloads/download-4317.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.7", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4317...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4317 " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4317.html#UUID-8b7385b2-d430-5fce-2c17-02c7d9ec8c9a_id_Unravelv4317-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.7 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.7-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.7-CDH-HDP-MAPR-latest.rpm -u -u USERNAME PASSWORD MD5SUM: 312e1723a2f896fd002539c2a3f60ab8...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.7-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.7-CDH-HDP-MAPR-latest.rpm -u -u USERNAME PASSWORD MD5SUM: 312e1723a2f896fd002539c2a3f60ab8 " }, 
{ "title" : "Azure HDInsight", 
"url" : "downloads/download-4317.html#UUID-8b7385b2-d430-5fce-2c17-02c7d9ec8c9a_id_Unravelv4317-AzureHDinsight", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.7 \/ Azure HDInsight", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/Azure\/unravel-4.3.1.7-Azure-latest.rpm -o unravel-4.3.1.7-Azure-latest.rpm MD5SUM: e3d4c946747eb959352bf4b900ae10e8...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/Azure\/unravel-4.3.1.7-Azure-latest.rpm -o unravel-4.3.1.7-Azure-latest.rpm MD5SUM: e3d4c946747eb959352bf4b900ae10e8 " }, 
{ "title" : "Unravel 4.3.1.6", 
"url" : "downloads/download-4316.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.6", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4316...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4316 " }, 
{ "title" : "CDH, HDP, and MapR", 
"url" : "downloads/download-4316.html#UUID-672cd565-4d41-3c1e-f9a4-69595370bc7c_id_Unravelv4316-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.6 \/ CDH, HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.6-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.6-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 5774e247de2a81edb77e49c814305208...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.6-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.6-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 5774e247de2a81edb77e49c814305208 " }, 
{ "title" : "Amazon EMR", 
"url" : "downloads/download-4316.html#UUID-672cd565-4d41-3c1e-f9a4-69595370bc7c_id_Unravelv4316-4313-EMREMR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.6 \/ Amazon EMR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/EMR\/unravel-4.3.1.6-EMR-latest.rpm -o unravel-4.3.1.6-EMR-latest.rpm MD5SUM: d63f7bcbd8f1bd8c8d7c49f507f53898...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/EMR\/unravel-4.3.1.6-EMR-latest.rpm -o unravel-4.3.1.6-EMR-latest.rpm MD5SUM: d63f7bcbd8f1bd8c8d7c49f507f53898 " }, 
{ "title" : "Unravel 4.3.1.5", 
"url" : "downloads/download-4315.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.5", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4315...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4315 " }, 
{ "title" : "CDH , HDP, and MapR", 
"url" : "downloads/download-4315.html#UUID-f06ab0bb-c297-e292-2cfd-5373166db054_id_Unravelv4315-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.5 \/ CDH , HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.5-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.5-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 502de995cc0eee29f8cf0a117c602de3...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.5-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.5-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 502de995cc0eee29f8cf0a117c602de3 " }, 
{ "title" : "Unravel 4.3.1.4", 
"url" : "downloads/download-4314.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.4", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4314...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4314 " }, 
{ "title" : "CDH , HDP, and MapR", 
"url" : "downloads/download-4314.html#UUID-3ef5e4ae-0506-8e08-00a6-399400a76679_id_Unravelv4314-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.4 \/ CDH , HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.4-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.4-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: f46df2031a8503a1b69443465aa0bcf8...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.4-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.4-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: f46df2031a8503a1b69443465aa0bcf8 " }, 
{ "title" : "Unravel 4.3.1.3", 
"url" : "downloads/download-4313.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.3", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4313...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4313 " }, 
{ "title" : "CDH , HDP, and MapR", 
"url" : "downloads/download-4313.html#UUID-7a5fc8fe-dcc2-f701-6fb5-cd1b35bbc282_id_Unravelv4313-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.3 \/ CDH , HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.3-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.3-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c1ffb13206042614faa455b2b5e79af...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.3-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.3-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c1ffb13206042614faa455b2b5e79af " }, 
{ "title" : "Unravel 4.3.1.2", 
"url" : "downloads/download-4312.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.2", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4312...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4312 " }, 
{ "title" : "CDH , HDP, and MapR", 
"url" : "downloads/download-4312.html#UUID-26684eab-f1be-95e4-504f-2f0fed0c99d2_id_Unravelv4312-CDHHDPMapR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.2 \/ CDH , HDP, and MapR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.2-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.2-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 6e8960f6a1b63e930aa44afbef861c03...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.2-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.2-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 6e8960f6a1b63e930aa44afbef861c03 " }, 
{ "title" : "Unravel 4.3.1.1", 
"url" : "downloads/download-4311.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.1", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4311...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4311 " }, 
{ "title" : "CDH and HDP", 
"url" : "downloads/download-4311.html#UUID-d25457a4-c720-c8b8-e91e-2e35288b1275_id_Unravelv4311-CDHHDP", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.1 \/ CDH and HDP", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: c1ea729e77c065b0297ff04998dfc46f...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: c1ea729e77c065b0297ff04998dfc46f " }, 
{ "title" : "Unravel 4.3.1.0", 
"url" : "downloads/download-4310.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.0", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4310...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4310 " }, 
{ "title" : "CDH and HDP", 
"url" : "downloads/download-4310.html#UUID-9bce2979-1e7a-d9f3-5882-198a5e83bd36_id_Unravelv4310-CDHHDP", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.3.1.0 \/ CDH and HDP", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 25617daec072c9bd25c892b1eedd9fe1...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/CDH-HDP-MapR\/unravel-4.3.1-CDH-HDP-MAPR-latest.rpm -o unravel-4.3.1-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 25617daec072c9bd25c892b1eedd9fe1 " }, 
{ "title" : "Unravel 4.2.7", 
"url" : "downloads/download-4270.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.7", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4270...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4270 " }, 
{ "title" : "CDH, HDP, and MAPR", 
"url" : "downloads/download-4270.html#UUID-05a16678-ad14-78b8-e5a5-4a9c52f475bb_id_Unravelv427-CDHHDPMAPR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.7 \/ CDH, HDP, and MAPR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/CDH-HDP-MapR\/unravel-4.2.7-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.7-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 4efd830d6414d4934091813b9c82ff57...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/CDH-HDP-MapR\/unravel-4.2.7-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.7-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 4efd830d6414d4934091813b9c82ff57 " }, 
{ "title" : "Azure HDInsight", 
"url" : "downloads/download-4270.html#UUID-05a16678-ad14-78b8-e5a5-4a9c52f475bb_id_Unravelv427-Azure", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.7 \/ Azure HDInsight", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm -o unravel-4.2.7-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: f7eac26de1734317630f0989ab199297...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm -o unravel-4.2.7-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: f7eac26de1734317630f0989ab199297 " }, 
{ "title" : "Unravel 4.2.6", 
"url" : "downloads/download-4260.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.6", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4260...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4260 " }, 
{ "title" : "CDH, HDP, and MAPR", 
"url" : "downloads/download-4260.html#UUID-0ef3820a-d87b-c75f-0b2f-283419640439_id_Unravelv426-CDHHDPMAPR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.6 \/ CDH, HDP, and MAPR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/CDH-HDP-MapR\/unravel-4.2.6-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.6-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 6c0dc2c32f715379850dcb92e4f1fe27...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/CDH-HDP-MapR\/unravel-4.2.6-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.6-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: 6c0dc2c32f715379850dcb92e4f1fe27 " }, 
{ "title" : "Amazon EMR", 
"url" : "downloads/download-4260.html#UUID-0ef3820a-d87b-c75f-0b2f-283419640439_id_Unravelv426-EMR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.6 \/ Amazon EMR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/EMR\/unravel-4.2.6-EMR-latest.rpm -o unravel-4.2.6-EMR-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c0a968e8573a2e9938d42d5c84a0839...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/EMR\/unravel-4.2.6-EMR-latest.rpm -o unravel-4.2.6-EMR-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c0a968e8573a2e9938d42d5c84a0839 " }, 
{ "title" : "Azure HDInsight", 
"url" : "downloads/download-4260.html#UUID-0ef3820a-d87b-c75f-0b2f-283419640439_id_Unravelv426-Azure", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.6 \/ Azure HDInsight", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/AzureR\/unravel-4.2.6-Azure-latest.rpm -o unravel-4.2.6-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c0a968e8573a2e9938d42d5c84a0839 Currently the EMR and Azure RPMs are the same except that the Azure folder contains Unravel action scripts...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.6\/AzureR\/unravel-4.2.6-Azure-latest.rpm -o unravel-4.2.6-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: 2c0a968e8573a2e9938d42d5c84a0839 Currently the EMR and Azure RPMs are the same except that the Azure folder contains Unravel action scripts and ARM templates. " }, 
{ "title" : "Unravel 4.2.5", 
"url" : "downloads/download-4250.html", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.5", 
"snippet" : "To obtain the values for USERNAME PASSWORD Release notes rn-4250...", 
"body" : " To obtain the values for USERNAME PASSWORD Release notes rn-4250 " }, 
{ "title" : "CDH, HDP, and MAPR", 
"url" : "downloads/download-4250.html#UUID-26d3d45b-b1dc-817a-f697-f82966c413b9_id_Unravelv425-CDHHDPMAPR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.5 \/ CDH, HDP, and MAPR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/CDH-HDP-MapR\/unravel-4.2.5-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.5-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: d1f834cc0004ca7d786c54bd48493501...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/CDH-HDP-MapR\/unravel-4.2.5-CDH-HDP-MAPR-latest.rpm -o unravel-4.2.5-CDH-HDP-MAPR-latest.rpm -u USERNAME PASSWORD MD5SUM: d1f834cc0004ca7d786c54bd48493501 " }, 
{ "title" : "Amazon EMR", 
"url" : "downloads/download-4250.html#UUID-26d3d45b-b1dc-817a-f697-f82966c413b9_id_Unravelv425-EMR", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.5 \/ Amazon EMR", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/EMR\/unravel-4.2.5-EMR-latest.rpm -o unravel-4.2.5-EMR-latest.rpm -u USERNAME PASSWORD MD5SUM: 53d980e07e7c04d9925d0b4a14b7a180...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/EMR\/unravel-4.2.5-EMR-latest.rpm -o unravel-4.2.5-EMR-latest.rpm -u USERNAME PASSWORD MD5SUM: 53d980e07e7c04d9925d0b4a14b7a180 " }, 
{ "title" : "Azure HDInsight", 
"url" : "downloads/download-4250.html#UUID-26d3d45b-b1dc-817a-f697-f82966c413b9_id_Unravelv425-Azure", 
"breadcrumbs" : "Home \/ Downloads \/ Unravel 4.2.5 \/ Azure HDInsight", 
"snippet" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/AzureR\/unravel-4.2.5-Azure-latest.rpm -o unravel-4.2.5-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: 53d980e07e7c04d9925d0b4a14b7a180 Currently the EMR and Azure RPMs are the same except that the Azure folder contains Unravel action scripts...", 
"body" : "# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.5\/AzureR\/unravel-4.2.5-Azure-latest.rpm -o unravel-4.2.5-Azure-latest.rpm -u USERNAME PASSWORD MD5SUM: 53d980e07e7c04d9925d0b4a14b7a180 Currently the EMR and Azure RPMs are the same except that the Azure folder contains Unravel action scripts and ARM templates. " }, 
{ "title" : "Compatibility Matrix", 
"url" : "compatibility.html", 
"breadcrumbs" : "Home \/ Compatibility Matrix", 
"snippet" : "Unravel Version CDH HDP MapR AWS EMR Azure HDInsights Operating System MySQL version Unravel Sensor 4.1.0 upto 5.11 RedHat\/Centos 6.4 - 7.3 4.2.0 upto 5.11 up to 4.7 RedHat\/Centos 6.4 - 7.3 4.2.3 5.12 2.6 3.5 RedHat\/Centos 6.4 - 7.3 4.2.5 5.13 (incl. Spark 2.2.x) 2.6 (incl. Spark 2.2.x) upto 5.2 3.6...", 
"body" : " Unravel Version CDH HDP MapR AWS EMR Azure HDInsights Operating System MySQL version Unravel Sensor 4.1.0 upto 5.11 RedHat\/Centos 6.4 - 7.3 4.2.0 upto 5.11 up to 4.7 RedHat\/Centos 6.4 - 7.3 4.2.3 5.12 2.6 3.5 RedHat\/Centos 6.4 - 7.3 4.2.5 5.13 (incl. Spark 2.2.x) 2.6 (incl. Spark 2.2.x) upto 5.2 3.6 RedHat\/Centos 6.4 - 7.3 UNRAVEL_SENSOR-1.0.58 4.2.6 5.13 (incl. Spark 2.2.x) 2.6 (incl. Spark 2.2.x) upto 5.2 3.6 RedHat\/Centos 6.4 - 7.3 UNRAVEL_SENSOR-1.0.60 4.2.7 5.13 (incl. Spark 2.2.x) 2.6 (incl. Spark 2.2.x) RedHat\/Centos 6.4 - 7.3 UNRAVEL_SENSOR-1.0.61 4.3.0 5.13 (incl. Spark 2.2.x) 2.6 (incl. Spark 2.2.x) RedHat\/Centos 6.4 - 7.4 4.3.1.0 5.13 (incl. Spark 2.2.x) with Kerberos 2.6 with Kerberos & spnego enabled 5.2.2 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.63 4.3.1.1 5.13 (incl. Spark 2.2.x) with Kerberos 2.6 with Kerberos & spnego enabled 5.2.2 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.63 4.3.1.2 5.13 (incl. Spark 2.2.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 5.2.2 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.63 4.3.1.3 5.12, 5.13 & 5.14 (incl. Spark 2.2.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 5.2.2 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.63 4.3.1.4 5.12, 5.13 & 5.14 (incl. Spark 2.2.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.63 4.3.1.5 5.12, 5.13 & 5.14 (incl. Spark 2.2.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.64 4.3.1.6 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 EMR 5.13 & EMR 5.14 (incl. Hive 2.3.2 & Spark 2.3.0) RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.65 4.3.1.7 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 HDI 3.6 Hadoop 2.7.3 HDI 3.6 spark 2.1,2.2, 2.3 HDI 3.6 kafka 0.10.0 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.66 4.3.1.9 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.66 4.4.0 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 4.4.1.0 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enabled 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.4410001 4.4.1.1 5.12, 5.13, 5.14 & 5.15 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enable 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.4 UNRAVEL_SENSOR-1.0.4410001 4.4.2.0 5.12, 5.13, 5.14 & 5.15 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enable 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.5 4.4.2.1 5.12, 5.13, 5.14 & 5.15 (incl. Spark 2.3.x) with Kerberos 2.6.3.0 with Kerberos & spnego enable 6.0.0 with MapR Expansion Packs 4.1.1 RedHat\/Centos 6.4 - 7.5 4.5.0.0 6.0, 5.15 & 5.14 with Kerberos 2.6.5 & 2.6.4 with Kerberos & spnego enable 6.1.0, 6.0.1 & 6.0.0 with Kerberos RedHat\/Centos 6.6 - 7.5 5.5-5.7 (5.7 recommended) Driver: 5.1.47 4.5.0.2 6.0, 5.15 & 5.14 with Kerberos 2.6.5 & 2.6.4 with Kerberos & spnego enable 6.1.0, 6.0.1 & 6.0.0 with Kerberos RedHat\/Centos 6.6 - 7.5 5.5-5.7 (5.7 recommended) Driver: 5.1.47 4.5.0.3 6.0, 5.15 & 5.14 with Kerberos 2.6.5 & 2.6.4with Kerberos & spnego enable 6.1.0, 6.0.1 & 6.0.0 with Kerberos RedHat\/Centos 6.6 - 7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) 5.5-5.7 (5.7 recommended) Driver: 5.1.47 4.5.0.4 6.0, 5.15 & 5.14 with Kerberos 3.0, 2.6.5 & 2.6.4 with Kerberos & spnego enable 6.1.0, 6.0.1 & 6.0.0 with Kerberos RedHat\/Centos 6.6 - 7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) 5.5-5.7 (5.7 recommended) Driver: 5.1.47 " }, 
{ "title" : "", 
"url" : "newsletters.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Newsletters February 2019 March 2019 April 2019...", 
"body" : " Newsletters February 2019 March 2019 April 2019 " }, 
{ "title" : "Frequently Asked Questions", 
"url" : "faqs.html", 
"breadcrumbs" : "Home \/ Frequently Asked Questions", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "General", 
"url" : "faqs.html#UUID-db27fe20-f4b5-f999-cabd-b26b051b3d67_id_FrequentlyAskedQuestionsFAQ-General", 
"breadcrumbs" : "Home \/ Frequently Asked Questions \/ General", 
"snippet" : "How fast can I get value out of Unravel? Deploy the Unravel Server and configuration as outlined in Part 1 of the installation guides. After this, you should start seeing all your completed MR and Spark jobs and identify areas for improvement. To enable more features and capabilities of Unravel, see...", 
"body" : "How fast can I get value out of Unravel? Deploy the Unravel Server and configuration as outlined in Part 1 of the installation guides. After this, you should start seeing all your completed MR and Spark jobs and identify areas for improvement. To enable more features and capabilities of Unravel, see Advanced Topics Do I need special hardware or software deploy Unravel? The installation guides outline the hardware requirements for Unravel Server. Unravel Server is self-contained; in other words, everything related to Unravel Server is packaged as part of the RPM. How long does Unravel retain data? The retention period is configurable. The default retention period is 60 days. Is Unravel highly available (HA)? Unravel is designed with high availability and resiliency in mind. However, we're looking into true HA capabilities (such as those provided in tools like IBM Tivoli) as part of our product road map. How do I contact your support team? If you are a current Unravel customer, please contact https:\/\/customers.unraveldata.com " }, 
{ "title" : "Free Trial", 
"url" : "faqs.html#UUID-db27fe20-f4b5-f999-cabd-b26b051b3d67_id_FrequentlyAskedQuestionsFAQ-UnravelTrialProduct", 
"breadcrumbs" : "Home \/ Frequently Asked Questions \/ Free Trial", 
"snippet" : "How long is the free trial? The free trial is 30 days. What happens when the free trial is over? Post 30-day trial, Unravel stops ingesting new data. You can still see all the features of Unravel on your old data. Whom should I contact for an Enterprise license? Please send a request to license@unra...", 
"body" : "How long is the free trial? The free trial is 30 days. What happens when the free trial is over? Post 30-day trial, Unravel stops ingesting new data. You can still see all the features of Unravel on your old data. Whom should I contact for an Enterprise license? Please send a request to license@unraveldata.com What support is available during the free trial? Support is available through Unravel's mailing list. To sign up, visit https:\/\/groups.google.com\/d\/forum\/unravel-data-users unravel-data-users@googlegroups.com What types of applications does Unravel analyze? Unravel analyzes all MapReduce and Spark applications. This includes MapReduce, Hive, Pig, Spark SQL, and Spark ML. Unravel also analyzes workflows submitted through popular workflow engines such as Oozie, Airflow, CRON, ControlM, and more. To request for coverage for an application not listed here, contact team@unraveldata.com. " }
]
$(document).trigger('search.ready');
});