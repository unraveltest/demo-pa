$(document).ready(function () {indexDict['en'] = [{ "title" : "", 
"url" : "un40--unravel-4-0-4-1-.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "UN40 (Unravel 4.0-4.1)...", 
"body" : " UN40 (Unravel 4.0-4.1) " }, 
{ "title" : "Unravel 4.0-4.1", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Overview", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/overview.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Unravel Sensor for Hive...", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Unravel Sensor for Hive " }, 
{ "title" : "Installation Guides", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cloudera", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 " }, 
{ "title" : "Part 1: Install Unravel Server on CDH+CM", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Conf...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For ...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties All values in unravel.properties can be modified through the in Unravel UI's configuration wizard, for CDH. Furthermore, some properties can only Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 If Kerberos is Enabled: Add authentication for HDFS... (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties You can find the principal by using 'klist -kt KEYTAB_FILE' If Sentry is Enabled: Add these permissions... Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs read Spark event log hdfs:\/\/usr\/history\/done hdfs read MapReduce logs hdfs:\/\/tmp\/logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information Do Host-Specific Configuration Steps For CDH, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-on-cdh-cm.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel UI Configuration Wi...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure the cluster. Install the Unravel Server RPM on the cluster. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Enable additional data collection\/instrumentation. " }, 
{ "title" : "Requirements Checklist", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-RequirementsChecklist", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Requirements Checklist", 
"snippet" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in P...", 
"body" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH If Spark is in use, Spark client gateway Open signup or LDAP for Unravel UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/ Disk: \/srv For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Network Port 3000 (or 4020) for Unravel UI access HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) unless Hive-hook via HDFS option is used For Oozie, port 11000 open to Unravel Server(s) " }, 
{ "title" : "1. Configure the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-1ConfiguretheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 1. Configure the Cluster", 
"snippet" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because t...", 
"body" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because the EBS partition you create\/mount below must be in the same AZ. [HVD] Security Group Unravel Server works with multiple EMR clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the master node of each EMR cluster to Unravel Server. Simplest approach it to make Unravel server a member of the security group ElasticMapReduce-master when the instance running Unravel Server is first launched. If Unravel Server is already started or if you prefer more security groups, open ports 3000 (TCP) and 4041-4043 (TCP and UDP) from the ElasticMapReduce-master group to a new server security group called 'unravel'. Start ntpd Configure the Environment at First Login Disable selinux # sudo setenforce Permissive\n Edit the file to make sure setting persists after reboot. Make sure SELINUX=permissive # vi \/etc\/selinux\/config\n Install libaio.x86_64 # sudo yum -y install libaio.x86_64 Install lzop # sudo yum install lzop.x86_64 Configure Ephemeral Storage Find the available ephemeral storage (also called instance storage lsblk It can be convenient to use \/srv Find block devices with no mount point: # lsblk For each unmounted area do the following steps, substituting correct value for Z EPHEMERAL # sudo mkfs.ext4 \/dev\/xvd{Z}\n If necessary, create mount point and check if mounted, # mkdir $EPHEMERAL \n# echo \"\/dev\/xvd{Z} $EPHEMERAL ext4 defaults,noatime,nodiratime 1 2\" | sudo tee -a \/etc\/fstab \n# sudo mount -a\n# df -h $EPHEMERAL Make a note of the path to the mounted ephemeral store, referred to as UNRAVEL_EPHEMERAL Configure Durable Storage [HVD] In a PoC or test install, this step can be skipped if there is sufficient disk space (at least 100GB) on \/ or under \/srv mounted from an ephemeral ('instance storage') disk area. Here we create a \"Provisioned IOPS\" EBS volume, setting the maximum IOPS, based on the size 300GB. In AWS EC2 console, Volumes in the same AZ as the Unravel server On Unravel server, find the letter Z lsblk Use dd Z # sudo dd if=\/dev\/zero of=\/dev\/xvd{Z} bs=1M Reference:[ | http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html {+} http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html+ Format the volume as ext4 Z # sudo mkfs.ext4 \/dev\/xvd Z Mount the new volume (for example, as \/mnt\/unravel_durable UNRAVEL_DURABLE \/etc\/fstab mount -a Z # echo \"\/dev\/xvd Z UNRAVEL_DURABLE Check if the volume is mounted: # df -h $ UNRAVEL_DURABLE " }, 
{ "title" : "2. Install the Unravel Server RPM on the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-2InstalltheUnravelServerRPMontheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 2. Install the Unravel Server RPM on the Cluster", 
"snippet" : "The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: ...", 
"body" : " The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: # scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm\" . For the enterprise version: # scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm.$timestamp . Install the Unravel Server RPM # sudo rpm -U unravel-4.0*.x86_64.EMR.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password is stored in \/root\/unravel.install.include Grant Access to Unravel Server Assign elastic IP to Unravel Server using AWS Console [unless you use VPC] Adjust internal DNS for new IP [for access via a browser] Request reverse DNS change from AWS [if you plan on adding SSL] Restriction Do not make Unravel Server accessible on the public Internet because doing so would violate your licensing terms. Supply Credentials Needed for EMR and EC2 Create IAM User(s) and Credentials Open the AWS IAM (Identity and Access Management) console with your browser and then make the following users and credentials so that Unravel Server can access EMR logs stored in S3 and use EMR read-only permission to find EMR clusters for efficient data loading. These credentials are described separately, but can be combined into one user if desired. Multiple accounts can also be created per access kind here if Unravel Server is going to monitor multiple accounts; just create multiple credential files. S3 Read-Only Access Create a group named s3ro AmazonS3ReadOnlyAccess Create a user named s3unravel Add user s3unravel s3ro. EMR API Read-Only Access Create a group named emrro AmazonElasticMapReduceReadOnlyAccess Create a user named emrunravel Add user emrunravel emrro Cloudwatch API Read-Only Access Create a group named cwro AmazonCloudWatchReadOnlyAccess Create a user named cwunravel Add user cwunravel cwro " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc # scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unrav...", 
"body" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem cwro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc where: is your RSA key, rsa.pem is the LOCAL_IP of the Unravel Server, and root@xx.xxx.xx.xxx , s3ro.properties emrro.properties cwro.properties [default]\naws_access_key_id = {your access key}\naws_secret_access_key = {your secret key} You can create multiple credentials of the same type for multiple accounts by creating multiple files with the same base name and appending .1 .2 \/usr\/local\/unravel\/etc\/ s3ro.properties.1\nemrro.properties.1\ncwro.properties.1 All three files are required for each account. Open an SSH Session to Unravel Server Replace somefile.pem - must be a fully qualified path. UNRAVEL_HOST_IP # ssh -i {somefile.pem} root@$UNRAVEL_IP Set Correct Permissions on the Unravel Configuration Directory # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties \n# sudo chmod 644 *.properties Modify unravel.properties The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the following values in unravel.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \ncom.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \ncom.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties Adjust other values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.s3.profile.config.file.path Location of Unravel s3 read-only access & secret key filename s3ro.properties. com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties com.unraveldata.emr.profile.config.file.path Location of Unravel EMR read-only access & secret key filename emrro.properties. com.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties com.unraveldata.cloudwatch.profile.config.file.path Location of Unravel Cloud Watch read-only access & secret key filename cwro.properties. com.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties com.unraveldata.spark.s3.profilesToBuckets s3 profile associated to the s3 bucket For 1 bucket, follow example as follows: com.unraveldata.spark.s3.profilesToBuckets=<default>:<s3_bucket> For 2 buckets follow below example: com.unraveldata.spark.s3.profilesToBuckets=<s3_profile>:<bucket>,<s3_2nd_profile>:<2nd_s3_bucket> com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 # sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 # sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_hhw_2 \n# sudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_hhw_3 \n# sudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_hhw_4 \n# sudo chkconfig --add unravel_jcw2_4 Enable Collection from Hive Metastore If you have a central Hive Metastore, you can inform Unravel Server to enable more monitoring and analysis: For MySQL use avax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver instead of the postgresql driver Substitute thecorrect values for your site. # echo \" \n javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/10.0.0.10:7432\/hive_nqz \n javax.jdo.option.ConnectionDriverName=org.postgresql.Driver \n javax.jdo.option.ConnectionUserName=hive_nqz \n javax.jdo.option.ConnectionPassword=123456789abc \n \" | sudo tee -a \/usr\/local\/unravel\/unravel.properties Adjust Storage Locations [HVD] Prepare symlinks from \/srv\/unravel UNRAVEL_EPHEMERAL UNRAVEL_DURABLE \/srv Make sure daemons are stopped [HVD] # sudo \/etc\/init.d\/unravel_all.sh stop Check that all Unravel daemons are stopped: # ps -U unravel -f If any processes are owned by Unravel, stop them with a kill command. Check that destination areas are present [HVD] # df -h $ UNRAVEL_EPHEMERAL UNRAVEL_DURABLE Move files and create symlinks [HVD] # sudo \/bin\/mv \/srv\/unravel\/k_data $ UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE This zk # sudo \/bin\/mv \/srv\/unravel\/zk_3_data $ UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL Start Unravel Server # sudo \/etc\/init.d\/unravel_all.sh start Set Up an External DB [HVD] For performance and ease of management, using an RDS MySQL instead of the bundled mysql is recommended. Set Up RDS MySQL [HVD] RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group Create db instance Select multi-AZ Select db.m3.large Select provisioned IOPS 1000 Select SSD size (capacity depends on activity level) 770GB for 180 days retention (number of days set in unravel.properties) 1.54TB for 360 days retention No read-only replicas needed Prefer overlap with Unravel Server AZ Retain 7 days of snapshots Specify unravel ElasticMapReduce-master Name db instance unravel X Use MySQL 5.5.42 Disable auto-minor-upgrade Define a parameter group unravel key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Modify unravel X Use unravel Take effect immediately Create the unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c10)\" Log into RDS mysql instance from Unravel Server as admin\/master user and do the following commands, substituting the DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${ DB_PASSWORD Log into RDS mysql as user unravel DB_PASSWORD Dump Bundled DB with Schema [HVD] On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into RDS MySQL [HVD] Load the initial db with schema into the RDS MySQL instance, substituting RDS_HOST unravel # gunzip --stdout $DEST_FILE | \/usr\/local\/unravel\/mysql\/bin\/mysql --host=$ RDS_HOST Configure to Use RDS MySQL [HVD] Configure unravel.properties Edit the file: # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust existing properties to point to new RDS MySQL. Change example values as needed: unravel.jdbc.username=unravel \nunravel.jdbc.url=jdbc:mysql:\/\/unravelrds.something.REGION.rds.amazonaws.com:3306\/unravel_mysql_prod \nunravel.jdbc.password=****** Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the Us...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-5OptionalEnableAdditionalDataCollectionInstrumentation", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"snippet" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup...", 
"body" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup " }, 
{ "title" : "Part 3: Additional Topics for CDH", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/cloudera/part-3--additional-topics-for-cdh.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 3: Additional Topics for CDH", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "Hortonworks", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). This guide is compatible with: HDP 2.2-2.6...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). This guide is compatible with: HDP 2.2-2.6 " }, 
{ "title" : "Part 1: Install Unravel Server on HDP", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Workflow Summary Pre-...", 
"body" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR j...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.u...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised....", 
"body" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel UI's configuration wizard. com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel UI's configuration wizard. com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL yarn.resourcemanager.webapp.address=http:\/\/example.localdomain :8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: (a) Create a keytab for unravel for daemons that run as unravel \/etc\/keytabs\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct realm and \/hostname, if applicable): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/etc\/keytabs\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n You can find the principal by using 'klist -kt KEYTAB_FILE' If Ranger is Enabled: Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs read Spark event log hdfs:\/\/usr\/history\/done hdfs read MapReduce logs hdfs:\/\/tmp\/logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information " }, 
{ "title" : "4. Convert Your Unravel Installation to HDP", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-4ConvertYourUnravelInstallationtoHDP", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 4. Convert Your Unravel Installation to HDP", 
"snippet" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades....", 
"body" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades. " }, 
{ "title" : "5. Update Site-Specific HDP Properties", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-5UpdateSite-SpecificHDPProperties", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 5. Update Site-Specific HDP Properties", 
"snippet" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base=...", 
"body" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/user\/spark\/applicationHistory\/\n \n# Add Hive Metastore database information for Unravel Hive Config \njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/{hostname}:3306\/{database_name} \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName={HiveMetastoreUserName} \njavax.jdo.option.ConnectionPassword={HiveMetastorePassword} Verify above site-specific values using the Ambari Web UI (AWU): Verify com.unraveldata.job.collector.done.log.base In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Take note of the value of mapreduce.jobhistory.done-dir unravel.properties com.unraveldata.job.collector.done.log.base= Verify com.unraveldata.job.collector.log.aggregation.base In AWU, on the left-hand side, click YARN Configs Advanced Node Manager Take note of the value of yarn.nodemanager.remote-app-log-dir unravel.properties com.unraveldata.job.collector.log.aggregation.base= Verify Hive Metastore javax.jdo.option.Connection In AWU, on the left-hand side, click Hive Configs Advanced Hive Metastore Take note of following properties and their values to be entered into unravel.properties Database URL Database Host JDBC Driver Class Database Name Database Username Database Password Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-1--install-unravel-server-on-hdp.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ Introduction", 
"snippet" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1....", 
"body" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Highlighted text indicates where you must substitute your particular values. " }, 
{ "title" : "1. Start Unravel Server", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-1StartUnravelServer", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 1. Start Unravel Server", 
"snippet" : "Note: Unravel needs to be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start...", 
"body" : " Note: Unravel needs to be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-2InstallUnravelHiveHookandSparkSensorOntoHDPServers", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"snippet" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps # ensure wget is install and use below script to install sensors # \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh # from Unravel server (eg. edge node) # run...", 
"body" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps\n# ensure wget is install and use below script to install sensors \n# \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh \n# from Unravel server (eg. edge node)\n# run on each server that will use instrumentation:\nyum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\nsudo .\/unravel_hdp_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION Substitute valid values for: - fully qualified host name or IP address UNRAVEL_HOST_IP - target Spark version SPARK_VERSION - target Hive version HIVE_VERSION This installation creates the following files: Hive hook jar in \/usr\/local\/unravel_client\/ Spark jar in \/usr\/local\/unravel-spark\/jars\/ Once the files are installed under \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-spark\/ " }, 
{ "title" : "3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-3AddUnravelHiveHookhive-siteSettingstoAllofHDPsServersintheClusterUsingAWU", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"snippet" : "Completion of this step requires a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, Hive jobs could fail with a ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. Add AUX_CLASSPATH hive...", 
"body" : " Completion of this step requires a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, Hive jobs could fail with a ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env AUX_CLASSPATH=$ AUX_CLASSPATH Hold off on restarting any services until the next step. Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar You can optionally restart the services in Ambari, as prompted, at this point in order to verify that the environment change is done correctly. After the restart, look at hadoop-env.sh and hive-env.sh on an edge node and check the path to the jar file. Hint: find the files with sudo find \/etc -name '*env.sh' -newerct '1 hour ago' Add the contents of \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip UNRAVEL_HOST_IP Custom hive-site Hive In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook \ncom.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR \ncom.unraveldata.hive.hook.tcp=true \ncom.unraveldata.host= UNRAVEL_HOST_IP You should restart the services in Ambari now, as prompted, to test whether Hive queries can succeed after the above configuration change. If you get ClassNotFoundException during a query, then make corrections or revert. " }, 
{ "title" : "4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-4IfPossibleEnsurethathiveexecutionengineisSettoMapReduceinyourHivequery", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"snippet" : "set hive.execution.engine=mr;...", 
"body" : "set hive.execution.engine=mr; " }, 
{ "title" : "5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-5OptionallyforSparkonYARNEnableUnravelSparkInstrumentationonAllofHDPsServersintheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"snippet" : "Completion of this step will require a restart of all affected Spark services in Ambari UI. Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.se...", 
"body" : " Completion of this step will require a restart of all affected Spark services in Ambari UI. Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP " }, 
{ "title" : "6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-6OptionallyforMapReduce2MRJVMSensorCluster-Wide", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"snippet" : "IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size Leave a white space in-between the current and the following new property) On the top notification banner, click Save -javaagent:\/usr\/local\/unravel-spark\/jars\/btrace...", 
"body" : " IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size Leave a white space in-between the current and the following new property) On the top notification banner, click Save -javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site: Inside Custom mapred-site Add Property Bulk property add mode: On the top notification banner, click Save mapreduce.task.profile=true\nmapreduce.task.profile.maps=0-5\nmapreduce.task.profile.reduces=0-5\nmapreduce.task.profile.params=-javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP Following instructions are for Unravel rpm 4.0.x and 4.1.x Propagate Unravel MR JAR onto all the servers in the cluster as follows : Ensure you have already installed unzip curl ssh to the Unravel gateway host server and use guided steps to unzip the Unravel MR jars. With root or sudo access, change directory to \/usr\/local\/unravel-spark and run the below curl get command: cd \/usr\/local\/unravel-spark\ncurl http:\/\/localhost:3000\/hh\/unravel-sensor-for-mapreduce-bin.zip -o unravel-sensor-for-mapreduce-bin.zip\nunzip -d jars unravel-sensor-for-mapreduce-bin.zip Now, tar up the \/usr\/local\/unravel-spark cd \/usr\/local\/\ntar -cvf unravel-spark.tar .\/unravel-spark Copy unravel-spark.tar \/usr\/local unravel-spark " }, 
{ "title" : "Part 3: Additional Topics for HDP", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/hortonworks/part-3--additional-topics-for-hdp.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 3: Additional Topics for HDP", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "MapR", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on MapR. This guide is compatible with MapR 5.1...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on MapR. This guide is compatible with MapR 5.1 " }, 
{ "title" : "Part 1: Install Unravel Server on MapR", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on the MapR platform. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on the MapR platform. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel hadoop fs -mkdir \/user\/unravel hadoop fs -chown unravel:mapr \/user\/unravel If MapR...", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel\nhadoop fs -mkdir \/user\/unravel\nhadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr \/etc\/unravel_ctl Check available RAM to ensure availability: free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 (only change this setting on the Unravel gateway\/client machine). And the restart mfs. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs com.unraveldata.job.collector.done.log.base=\/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process com.unraveldata.job.collector.log.aggregation.base=\/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=maprfs:\/\/\/apps\/spark yarn.resourcemanager.webapp.address Resource Manager web app address yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=unravel\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n If Sentry is Enabled: Resource Principal Access hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\n echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. Check Unravel UI for MR jobs loading: on the Applications M...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. Check Unravel UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-1--install-unravel-server-on-mapr.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unrave...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the Unravel UI User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ Introduction", 
"snippet" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's ...", 
"body" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. " }, 
{ "title" : "1. Enable Additional Instrumentation on Unravel Server's Host", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-1EnableAdditionalInstrumentationonUnravelServersHost", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 1. Enable Additional Instrumentation on Unravel Server's Host", 
"snippet" : "Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_s...", 
"body" : " Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_setup.sh host1 sudo yum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\nsudo .\/unravel_mapr_setup.sh install -y --unravel-server $UNRAVEL_HOST:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION In the code above, substitute valid values for: UNRAVEL_HOST_IP - fully qualified host name or IP address SPARK_VERSION - target Spark version HIVE_VERSION - target Hive version " }, 
{ "title" : "2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-2EntercorrectvalueforHiveMetastoreResourceManagerandOozieproperties", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"snippet" : "Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore # ### Uncomment and enter correct HM java.jdo.option.Connection properties ### #javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive #javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_...", 
"body" : " Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore\n#\n### Uncomment and enter correct HM java.jdo.option.Connection properties ###\n#javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive\n#javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_zzzzzz\n#javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\n#javax.jdo.option.ConnectionDriverName=org.postgresql.Driver\n#javax.jdo.option.ConnectionUserName=hiveuser?\n#javax.jdo.option.ConnectionPassword=???????\n\n# optional selectivity of databases to analyze in metastore\n#com.unraveldata.metastore.databasePattern=s*|t*|d*\n\n# Resource Manager (RM)\n#\n# Enable https access to Resource Manager\n#https.protocols=TLSv1.2\n#\n### Uncomment and enter correct below properties RM ###\n#yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 \n\n# Resource Manager username to login\n#yarn.resourcemanager.webapp.username=foo\n\n# Resource Manager password to login\n#yarn.resourcemanager.webapp.password=?????\n\n#\n# oozie\n#\n### Uncomment below oozie properties when oozie is used ###\n# oozie.server.url=http:\/\/<oozie-hostname-IP-address>:11000\/oozie Restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "3. Confirm that Unravel Web UI Shows Additional Data", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-3ConfirmthatUnravelWebUIShowsAdditionalData", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 3. Confirm that Unravel Web UI Shows Additional Data", 
"snippet" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and ...", 
"body" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. Replace $ someUser sudo -u $ someUser This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. This script runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. " }, 
{ "title" : "4. Confirm and Adjust the Settings in yarn-site.xml", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-4ConfirmandAdjusttheSettingsinyarn-sitexml", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 4. Confirm and Adjust the Settings in yarn-site.xml", 
"snippet" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property> <name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value> <source>yarn-site.xml<\/source> <\/property> yarn.log-aggregation-enable <...", 
"body" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> " }, 
{ "title" : "5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-5EnableAdditionalInstrumentationonOtherHostsintheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"snippet" : "To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar ...", 
"body" : " To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar from Unravel to your existing mechanism. Run the shell script unravel_mapr_setup.sh Copy the newly edited (in the previous step 4) yarn-site.xml to all nodes. Do a rolling-restart of HiveServer2 " }, 
{ "title" : "Part 3: Additional Topics for MapR", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/mapr/part-3--additional-topics-for-mapr.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 3: Additional Topics for MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Amazon Elastic MapReduce and Qubole Clusters", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Amazon Elastic MapReduce (Amazon EMR) and Qubole clusters. This guide is compatible with Amazon EMR 3.4 - 4.7...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Amazon Elastic MapReduce (Amazon EMR) and Qubole clusters. This guide is compatible with Amazon EMR 3.4 - 4.7 " }, 
{ "title" : "Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure the cluster. Install the Unravel Server RPM on the cluster. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Enable additional data collection\/instrumentation. " }, 
{ "title" : "Requirements Checklist", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-RequirementsChecklist", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Requirements Checklist", 
"snippet" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in P...", 
"body" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH If Spark is in use, Spark client gateway Open signup or LDAP for Unravel UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/ Disk: \/srv For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Network Port 3000 (or 4020) for Unravel UI access HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) unless Hive-hook via HDFS option is used For Oozie, port 11000 open to Unravel Server(s) " }, 
{ "title" : "1. Configure the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-1ConfiguretheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 1. Configure the Cluster", 
"snippet" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because t...", 
"body" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because the EBS partition you create\/mount below must be in the same AZ. [HVD] Security Group Unravel Server works with multiple EMR clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the master node of each EMR cluster to Unravel Server. Simplest approach it to make Unravel server a member of the security group ElasticMapReduce-master when the instance running Unravel Server is first launched. If Unravel Server is already started or if you prefer more security groups, open ports 3000 (TCP) and 4041-4043 (TCP and UDP) from the ElasticMapReduce-master group to a new server security group named unravel Start ntpd Configure the Environment at First Login Disable selinux # sudo setenforce Permissive Edit the file to make sure setting persists after reboot. Make sure SELINUX=permissive # vi \/etc\/selinux\/config Install libaio.x86_64 # sudo yum -y install libaio.x86_64 Install lzop # sudo yum install lzop.x86_64 Configure Ephemeral Storage Find the available ephemeral storage (also called instance storage lsblk Tip It can be convenient to use \/srv Find block devices with no mount point: # lsblk For each unmounted area do the following steps, substituting correct value for Z EPHEMERAL # sudo mkfs.ext4 \/dev\/xvd Z If necessary, create mount point and check if mounted, # mkdir $ EPHEMERAL Z EPHEMERAL EPHEMERAL Make a note of the path to the mounted ephemeral store, referred to as UNRAVEL_EPHEMERAL Configure Durable Storage [HVD] In a PoC or test install, this step can be skipped if there is sufficient disk space (at least 100GB) on \/ or under \/srv mounted from an ephemeral ('instance storage') disk area. Here we create a \"Provisioned IOPS\" EBS volume, setting the maximum IOPS, based on the size 300GB. In AWS EC2 console, Volumes in the same AZ as the Unravel server On Unravel server, find the letter Z lsblk Use dd Z # sudo dd if=\/dev\/zero of=\/dev\/xvd{Z} bs=1M Reference:[ | http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html {+} http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html+ Format the volume as ext4 Z # sudo mkfs.ext4 \/dev\/xvd Z Mount the new volume (for example, as \/mnt\/unravel_durable UNRAVEL_DURABLE \/etc\/fstab mount -a Z # echo \"\/dev\/xvd Z UNRAVEL_DURABLE Check if the volume is mounted: # df -h $ UNRAVEL_DURABLE " }, 
{ "title" : "2. Install the Unravel Server RPM on the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-2InstalltheUnravelServerRPMontheCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 2. Install the Unravel Server RPM on the Cluster", 
"snippet" : "The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: ...", 
"body" : " The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: # scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm\" . For the enterprise version: # scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm.$timestamp . Install the Unravel Server RPM # sudo rpm -U unravel-4.0*.x86_64.EMR.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Grant Access to Unravel Server Assign elastic IP to Unravel Server using AWS Console [unless you use VPC] Adjust internal DNS for new IP [for access via a browser] Request reverse DNS change from AWS [if you plan on adding SSL] Restriction Do not make Unravel Server accessible on the public Internet because doing so would violate your licensing terms. Supply Credentials Needed for EMR and EC2 Create IAM User(s) and Credentials Open the AWS IAM (Identity and Access Management) console with your browser and then make the following users and credentials so that Unravel Server can access EMR logs stored in S3 and use EMR read-only permission to find EMR clusters for efficient data loading. These credentials are described separately, but can be combined into one user if desired. Multiple accounts can also be created per access kind here if Unravel Server is going to monitor multiple accounts; just create multiple credential files. S3 Read-Only Access Create a group named s3ro AmazonS3ReadOnlyAccess Create a user named s3unravel Add user s3unravel s3ro. EMR API Read-Only Access Create a group named emrro AmazonElasticMapReduceReadOnlyAccess Create a user named emrunravel Add user emrunravel emrro Cloudwatch API Read-Only Access Create a group named cwro AmazonCloudWatchReadOnlyAccess Create a user named cwunravel Add user cwunravel cwro " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc # scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unrav...", 
"body" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem cwro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc where: is your RSA key, rsa.pem is the LOCAL_IP of the Unravel Server, and root@xx.xxx.xx.xxx , s3ro.properties emrro.properties cwro.properties [default]\naws_access_key_id = {your access key}\naws_secret_access_key = {your secret key} You can create multiple credentials of the same type for multiple accounts by creating multiple files with the same base name and appending \".1\" for the second account, \".2\" for the third account, and so on. For example, using the file naming convention suggested above, copy these additional files into \/usr\/local\/unravel\/etc\/ s3ro.properties.1\nemrro.properties.1\ncwro.properties.1 All three files are required for each account. Open an SSH Session to Unravel Server Replace somefile.pem - must be a fully qualified path. UNRAVEL_HOST_IP # ssh -i somefile.pem Set Correct Permissions on the Unravel Configuration Directory # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties \n# sudo chmod 644 *.properties Modify unravel.properties The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the following values in unravel.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \ncom.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \ncom.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties Adjust other values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.s3.profile.config.file.path Location of Unravel s3 read-only access & secret key filename s3ro.properties. com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties com.unraveldata.emr.profile.config.file.path Location of Unravel EMR read-only access & secret key filename emrro.properties. com.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties com.unraveldata.cloudwatch.profile.config.file.path Location of Unravel Cloud Watch read-only access & secret key filename cwro.properties. com.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties com.unraveldata.spark.s3.profilesToBuckets s3 profile associated to the s3 bucket For 1 bucket, follow example as follows: com.unraveldata.spark.s3.profilesToBuckets=<default>:<s3_bucket> For 2 buckets follow below example: com.unraveldata.spark.s3.profilesToBuckets=<s3_profile>:<bucket>,<s3_2nd_profile>:<2nd_s3_bucket> com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 # sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 # sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_hhw_2 \n# sudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_hhw_3 \n# sudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_hhw_4 \n# sudo chkconfig --add unravel_jcw2_4 Enable Collection from Hive Metastore If you have a central Hive Metastore, you can inform Unravel Server to enable more monitoring and analysis: For MySQL use avax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver instead of the postgresql driver Substitute thecorrect values for your site. # echo \" \n javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/10.0.0.10:7432\/hive_nqz \n javax.jdo.option.ConnectionDriverName=org.postgresql.Driver \n javax.jdo.option.ConnectionUserName=hive_nqz \n javax.jdo.option.ConnectionPassword=123456789abc \n \" | sudo tee -a \/usr\/local\/unravel\/unravel.properties Adjust Storage Locations [HVD] Prepare symlinks from \/srv\/unravel $UNRAVEL_EPHEMERAL $UNRAVEL_DURABLE \/srv Make sure daemons are stopped [HVD] # sudo \/etc\/init.d\/unravel_all.sh stop Check that all Unravel daemons are stopped: # ps -U unravel -f If any processes are owned by Unravel, stop them with a kill command. Check that destination areas are present [HVD] # df -h $UNRAVEL_EPHEMERAL \n# df -h $UNRAVEL_DURABLE Move files and create symlinks [HVD] # sudo \/bin\/mv \/srv\/unravel\/k_data $UNRAVEL_DURABLE\/k_data \n# sudo ln -s $UNRAVEL_DURABLE\/k_data \/srv\/unravel\/k_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_1_data $UNRAVEL_DURABLE\/zk_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_1_data \/srv\/unravel\/zk_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_2_data $UNRAVEL_DURABLE\/zk_2_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_2_data \/srv\/unravel\/zk_2_data This zk # sudo \/bin\/mv \/srv\/unravel\/zk_3_data $UNRAVEL_EPHEMERAL\/zk_3_data \n# sudo ln -s $UNRAVEL_EPHEMERAL\/zk_3_data \/srv\/unravel\/zk_3_data \n\n# sudo \/bin\/mv \/srv\/unravel\/db_data $UNRAVEL_DURABLE\/db_data \n# sudo ln -s $UNRAVEL_DURABLE\/db_data \/srv\/unravel\/db_data \n\n# sudo \/bin\/mv \/srv\/unravel\/s_1_data $UNRAVEL_DURABLE\/s_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/s_1_data \/srv\/unravel\/s_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp $UNRAVEL_EPHEMERAL\/tmp \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp \/srv\/unravel\/tmp \n\n# sudo \/bin\/mv \/srv\/unravel\/log_hdfs $UNRAVEL_EPHEMERAL\/log_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/log_hdfs \/srv\/unravel\/log_hdfs \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp_hdfs $UNRAVEL_EPHEMERAL\/tmp_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp_hdfs \/srv\/unravel\/tmp_hdfs Start Unravel Server # sudo \/etc\/init.d\/unravel_all.sh start Set Up an External DB [HVD] For performance and ease of management, using an RDS MySQL instead of the bundled mysql is recommended. Set Up RDS MySQL [HVD] RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group Create db instance Select multi-AZ Select db.m3.large Select provisioned IOPS 1000 Select SSD size (capacity depends on activity level) 770GB for 180 days retention (number of days set in unravel.properties) 1.54TB for 360 days retention No read-only replicas needed Prefer overlap with Unravel Server AZ Retain 7 days of snapshots Specify unravel ElasticMapReduce-master Name db instance \" unravelX Use MySQL 5.5.42 Disable auto-minor-upgrade Define a parameter group unravel key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Modify unravelX Use unravel Take effect immediately create \" unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c10)\" Log into RDS mysql instance from Unravel Server as admin\/master user and do the following commands, substituting above DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into RDS mysql as user unravel DB_PASSWORD Dump Bundled DB with Schema [HVD] On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into RDS MySQL [HVD] Load the initial db with schema into the RDS MySQL instance, substituting $RDS_HOST unravel # gunzip --stdout $DEST_FILE | \/usr\/local\/unravel\/mysql\/bin\/mysql --host=$RDS_HOST -u unravel -p --port=3306 --force unravel_mysql_prod Configure to Use RDS MySQL [HVD] Configure unravel.properties Edit the file: # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust existing properties to point to new RDS MySQL. Change example values as needed: unravel.jdbc.username=unravel \nunravel.jdbc.url=jdbc:mysql:\/\/unravelrds.something.REGION.rds.amazonaws.com:3306\/unravel_mysql_prod \nunravel.jdbc.password=****** Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the Us...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-5OptionalEnableAdditionalDataCollectionInstrumentation", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"snippet" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup...", 
"body" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup " }, 
{ "title" : "Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"snippet" : "Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values....", 
"body" : " Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. " }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html#UUID-e74e34e8-930f-2c37-7305-8e186e2d77ec_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Introduction", 
"snippet" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already run...", 
"body" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already running), do a \"setup\" procedure: follow the steps 3 to 5 in Hive Bootstrap and Unravel Hive Hook Sensor Setup. " }, 
{ "title" : "Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html#UUID-e74e34e8-930f-2c37-7305-8e186e2d77ec_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-HiveBootstrapandUnravelHiveHookSensorSetup", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"snippet" : "- is the s3 location where the unravel hive hook jar will be accessed location_in_s3_where_unravel_jar_folder - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Unravel_Hostname_FQDN_Internal_IP Add a one-time Unravel Hive ...", 
"body" : " - is the s3 location where the unravel hive hook jar will be accessed location_in_s3_where_unravel_jar_folder - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Unravel_Hostname_FQDN_Internal_IP Add a one-time Unravel Hive Bootstrap into Qubole's control panel on the left-hand side in the \"Hive Bootstrap\" section, as follows: add jar s3n:\/\/ location_in_s3_where_unravel_jar_folder Unravel_Hostname_FQDN_Internal_IP Determine the Hive version that Qubole uses, and use that value for HIVE_VER To determine the Hive version Qubole uses, see http:\/\/docs.qubole.com\/en\/latest\/faqs\/hive\/version-hive-qubole-provide.html must be a Hive version that Unravel Server supports: either HIVE_VER 1.2.0 0.13.0 On the master node of the Qubole Hadoop2\/Hive cluster, check that Unravel Server is reachable. Ensure the security group on the master\/slave allows TCP port 3000 and 4043 accessible. # curl http:\/\/{ Unravel_Hostname_FQDN_Internal_IP If the version information is not visible, adjust security groups and routing and try again. SSH to the master server of the existing Qubole Hadoop2\/Hive cluster, run the following commands: Use curl # curl http:\/\/ Unravel_Hostname_FQDN_Internal_IP Ensure wget The installation creates these files: Hive hook jar is \/usr\/local\/unravel_client\/ Spark jar is \/usr\/local\/unravel-spark\/ Unravel ES is \/usr\/local\/unravel_es\/ On the master node, \/etc\/init.d\/unravel_es service # yum install -y wget\n# chmod 755 unravel_qubole_setup.sh \n# sudo .\/unravel_qubole_setup.sh install -y --unravel-server Unravel_Hostname_FQDN_Internal_IP Verify if the setup works in Qubole by invoking Analyze and execute following Hive test query: set hive.on.master=true ;\nselect count(*) from default_qubole_memetracker; " }, 
{ "title" : "Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Introduction", 
"snippet" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your parti...", 
"body" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your particular values. text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED must be a fully qualified DNS or the IP address. UNRAVEL_HOST_IP : is the absolute path to the location of the sensor jars. PATH_TO_SENSOR_JARS " }, 
{ "title" : "Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AddUnravelSparkInstrumentationtoNewQuboleSparkCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"snippet" : "Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties Substitute actual values for ACCESS_KEY_VALUEx SECRET_KEY_VALUEx [default] aws_access_key_id = ACCESS_KEY_VALUE1 SECRET_KEY_VALUE1 ACCESS_KEY_VALUE2 SECRET_KEY_VALUE2 Edit \/usr\/local\/unra...", 
"body" : " Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties Substitute actual values for ACCESS_KEY_VALUEx SECRET_KEY_VALUEx [default]\naws_access_key_id = ACCESS_KEY_VALUE1 SECRET_KEY_VALUE1 ACCESS_KEY_VALUE2 SECRET_KEY_VALUE2 Edit \/usr\/local\/unravel\/etc\/unravel.properties s3ro.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties\ncom.unraveldata.spark.s3.profilesToBuckets=<default>:<s3ro_bucket1>,<profile_name_2>:<s3r0_bucket2> Restart the Unravel ETL daemon(s): # sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Ensure ports 3000 (for web UI access) and 4043 (from cluster) are open for incoming traffic. These should not Verify that port 3000 is open by running a curl If the version information is not visible (request timeout), then adjust security groups, firewalls, etc. and try again. For VPCs, it might be necessary to add a route. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt Copy the Unravel Spark bootstrap file from Unravel Server to your Spark Qubole cluster, using curl Substitute the actual host for UNRAVEL_HOST_IP # curl http:\/\/ UNRAVEL_HOST_IP Copy Unravel Spark bootstrap file to your s3:\/\/ Bootstrap_location_for_Spark Qubole Cluster aws s3 cp unravel_qubole_bootstrap.sh s3:\/\/{Bootstrap_location_for_Spark Qubole cluster}\/unravel_qubole_bootstrap.sh In Qubole's Edit Cluster Setting do not unravel_qubole_bootstrap.sh In Qubole, scripts do not take input parameters. Therefore, Unravel's bootstrap script takes all the required parameters from the Hadoop configuration. You can customize your Hadoop configuration through specific parameters within the Override Hadoop Configuration Variables unravel-bootstrap Separate parameters with commas. Add these settings to unravel-bootstrap The parameters in square brackets [ ] below are optional. Their meanings are: SPARK_APP_LOAD_MODE Appendix WRAPPED_SCRIPT unravel_qubole_bootstrap.sh unravel-bootstrap=UNRAVEL_SERVER= UNRAVEL_HOST_AND_PORT SPARK_VER_XYZ SPARK_APP_LOAD_MODE WRAPPED_SCRIPT SPARK_VERSION_X.Y.Z Confirm that the unravel_es Open an SSH session to the Qubole master node. Run this command to check that the unravel_es service has been started: # ps -aux | grep unravel_es " }, 
{ "title" : "(Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-OptionalAddUnravelSparkInstrumentationtoanExistingQuboleSparkCluster", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ (Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"snippet" : "Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP To obtain Spark version 2.0.x zip file: # ...", 
"body" : " Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP To obtain Spark version 2.0.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP Unzip the archive unravel-sensor-for-spark-bin-1.6.zip PATH_TO_SENSOR_JARS Obtain EMR jar files and snippet script that should be added to the bootstrap action to start the unravel_es process: \n#wget http:\/\/ UNRAVEL_HOST_IP UNRAVEL_HOST_IP Ensure that unravel-emr-sensor.jar unravel-emr-sensor.jar PATH_TO_SENSOR_JARS Edit run-es.sh # Replace following two parameters into your Qubole Spark environment #\nUNRAVEL_HOST= UNRAVEL_HOST_IP PATH_TO_SENSOR_JARS Spark configuration can be provided in theQubole consoleat bootstrap or directly inside spark-defaults.conf spark.unravel.server.hostport UNRAVEL_HOST_IP PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS Edit zeppelin-env.sh ZEPPELIN_JAVA_OPTS export ZEPPELIN_JAVA_OPTS=\"-Dcom.sun.btrace.FileClient.flush=-1 -javaagent: PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS The Zeppelin configuration is located at \/usr\/lib\/zeppelin\/conf\/zeppelin-env.sh " }, 
{ "title" : "Appendix", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AppendixAppendix", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Appendix", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Application Loading Modes for Spark Applications: OPS, DEV, and BATCH", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-ApplicationLoadingModesforSparkApplicationsOPSDEVandBATCH", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Application Loading Modes for Spark Applications: OPS, DEV, and BATCH", 
"snippet" : "There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark application completes. BATCH mode loads applications for which the sen...", 
"body" : "There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark application completes. BATCH mode loads applications for which the sensor was not enabled at the time the application has been run. You can specify the application load mode for the bootstrap script by setting SPARK_APP_LOAD_MODE OPS DEV SPARK_APP_LOAD_MODE OPS " }, 
{ "title" : "When to Use Which Mode", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-WhentoUseWhichMode", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ When to Use Which Mode", 
"snippet" : "Unravel recommends using OPS mode as the cluster-side default for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOpt...", 
"body" : " Unravel recommends using OPS mode as the cluster-side default for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false DEV mode is useful during Spark application development. This mode shows a Spark application as soon as the first Spark job of the application finishes. For long running applications, this functionality is useful, as the application is shown in the UI while the application is running. In addition, DEV mode shows applications even when the Spark event log is not being persisted to HDFS or S3. This is an advantage in situations like Spark on Mesos. In a Qubole cluster that is using OPS mode as the default, DEV mode can be obtained for individual Spark applications by overriding spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false BATCH mode is always on and does not interfere with application performance in any way since the loading is entirely outside the application execution path. For details see the next section. " }, 
{ "title" : "Loading Applications in Batch Mode", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-LoadingApplicationsinBatchMode", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Loading Applications in Batch Mode", 
"snippet" : "In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is ...", 
"body" : "In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is helpful to load all of the applications that have been run in the past, before Unravel Sensor was installed. To enable the batch mode, add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.eventlog.location=hdfs:\/\/ NAMENODE_IP_PORT Currently, only one event log location is supported. " }, 
{ "title" : "Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-Introduction", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ Introduction", 
"snippet" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workf...", 
"body" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amazon EMR cluster(s). " }, 
{ "title" : "1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-1AddYourAWSAccountNumberstotheUnravelDataMainS3BucketPolicy", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"snippet" : "Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients...", 
"body" : " Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients " }, 
{ "title" : "2. Get the Bootstrap Script(s)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-2GettheBootstrapScripts", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 2. Get the Bootstrap Script(s)", 
"snippet" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/unraveldata-clients\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy...", 
"body" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/unraveldata-clients\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below policy into the Policy Document: {\n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n { \n \"Sid\": \"getunraveldataclients3files\", \n \"Effect\": \"Allow\", \n \"Action\": [ \n \"s3:ListBucket\", \n \"s3:Get*\" \n ], \n \"Resource\": [ \n \"arn:aws:s3:::unraveldata-clients\/*\" \n ] \n } \n ]\n} Save it by clicking Apply Policy When you create a new Amazon EMR cluster, be sure to add a bootstrap action as shown in the IAM screenshot below. You need to copy and paste the full pathname of the bootstrap action (script) into the Script location Important Note: Do not For guidance on which script to use, see the table below. File S3 Bucket Local Directory Applies To unravel_emr_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 3.x Hive unravel_emr4_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 4.x Hive " }, 
{ "title" : "3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-3IntegratetheBootstrapScriptsintoYourAmazonEMRClusters", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"snippet" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unrave...", 
"body" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unraveldata-clients\/unravel_emr_setup.sh aws s3 install_bin\/unraveldata-clients Copy unravel_emr_setup.sh \/tmp scp ssh hadoop Open an SSH session to the cluster's master node ( ssh hadoop hadoop cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --unravel-server LOCAL_IP To uninstall Hive instrumentation on an Amazon EMR cluster (perhaps because you want to upgrade the instrumentation), you simply run the same install script again with the uninstall cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --uninstall For Transient Amazon EMR Clusters Hive Applications: This is similar to the previous section on integrating an existing cluster except the script used as a bootstrap step is one of the following files: File S3 Bucket Local Directory Applies To unravel_emr_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 3.x Hive unravel_emr4_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 4.x Hive Spark Applications: The Unravel Server does not load data from a Spark cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unraveldata-clients\/unravel_emr_spark_setup.sh aws s3 install_bin\/unraveldata-clients Copy unravel_emr_setup.sh \/tmp scp ssh ec2-user Open an SSH session to the cluster's master node ( ssh ec2-user ec2-user cd \/tmp \nsudo .\/unravel_emr_spark_setup.sh --unravel-server \\ \n $LOCAL_IP:3000 --client Change --client --cluster " }, 
{ "title" : "Part 5: Additional Topics for EMR or Qubole", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/installation-guides/amazon-elastic-mapreduce-and-qubole-clusters/part-5--additional-topics-for-emr-or-qubole.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon Elastic MapReduce and Qubole Clusters \/ Part 5: Additional Topics for EMR or Qubole", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "User Guide", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide", 
"snippet" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations....", 
"body" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations. " }, 
{ "title" : "Getting Started", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide.html#UUID-163e93bc-2c0a-8bf9-9964-3c13466feec5_id_UserGuide-GettingStarteduser-guide-get-startedhtml", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started", 
"snippet" : "The Operations Tab The Applications Tab The Data Tab...", 
"body" : " The Operations Tab The Applications Tab The Data Tab " }, 
{ "title" : "Use Cases", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide.html#UUID-163e93bc-2c0a-8bf9-9964-3c13466feec5_id_UserGuide-UseCasesuser-guide-use-caseshtml", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases", 
"snippet" : "Optimizing the Performance of Spark Applications Detecting Resource Contention in the Cluster Identifying Rogue Applications...", 
"body" : " Optimizing the Performance of Spark Applications Detecting Resource Contention in the Cluster Identifying Rogue Applications " }, 
{ "title" : "Notifications", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide.html#UUID-163e93bc-2c0a-8bf9-9964-3c13466feec5_id_UserGuide-Notifications", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Notifications", 
"snippet" : "Setting Up Auto Actions (Alerts)...", 
"body" : " Setting Up Auto Actions (Alerts) " }, 
{ "title" : "Getting Started", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase1HowtoSearchforApplicationsandOptimizeTuneaHiveApplication", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase2HowtoRootCauseIssueswithaWorkflowthatMissedItsSLA", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #3: How to Debug Failed Applications", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase3HowtoDebugFailedApplications", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #3: How to Debug Failed Applications", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase4HowtoReviewSparkApplicationsandIdentifyAreasforPerformanceImprovements", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin " }, 
{ "title" : "Setting Up Access to Big Data Components", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-SettingUpAccesstoBigDataComponents", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Setting Up Access to Big Data Components", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-CreatingUsersandSettingUpEmailSMTPLDAPKerberos", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "The Operations Tab", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Dashboard", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Dashboard", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Dashboard", 
"snippet" : "To view the dashboard, click Operations Dashboard The dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a spe...", 
"body" : "To view the dashboard, click Operations Dashboard The dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a specific tile, click that tile. To configure the dashboard for a specific time range or cluster, select options from the pull-down menus in the banner. " }, 
{ "title" : "Charts", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Charts", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Charts", 
"snippet" : "To view charts, click Operations Charts Charts To see KPIs for resources, jobs, nodes, or services, select the appropriate secondary tab in the Charts To see a detailed view of any of KPI, click the tile for that KPI. To configure Charts...", 
"body" : "To view charts, click Operations Charts Charts To see KPIs for resources, jobs, nodes, or services, select the appropriate secondary tab in the Charts To see a detailed view of any of KPI, click the tile for that KPI. To configure Charts " }, 
{ "title" : "Reports", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Reports", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Reports", 
"snippet" : "To view reports, click Operations Reports...", 
"body" : "To view reports, click Operations Reports " }, 
{ "title" : "Chargeback", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Chargeback", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Chargeback", 
"snippet" : "Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback...", 
"body" : "Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback " }, 
{ "title" : "Cluster Summary", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-ClusterSummary", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Cluster Summary", 
"snippet" : "The Cluster Summary Reports...", 
"body" : "The Cluster Summary Reports " }, 
{ "title" : "Cluster Compare", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-ClusterCompare", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Cluster Compare", 
"snippet" : "The Cluster Compare Reports...", 
"body" : "The Cluster Compare Reports " }, 
{ "title" : "The Applications Tab", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab", 
"snippet" : "The Applications The performance and reliability of an application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort t...", 
"body" : "The Applications The performance and reliability of an application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems of an application. Unravel provides insights into an application. These insights are called events Key Performance Indicators " }, 
{ "title" : "Finding Applications", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-FindingApplications", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Applications", 
"snippet" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property,...", 
"body" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. You can also use the global search bar in the top banner to search by full job ID, user name, table name and cluster ID. Search results list individual jobs, and job IDs. If the job is part of a Hive query, Pig script, and\/or a workflow, then a link to that Hive query\/Pig script\/workflow page appears on the same line. To go to the job-specific page, click the job ID. To go to the application-specific manager for a query\/script\/job\/workflow, click the icon under its GoTo " }, 
{ "title" : "Application-Specific Managers", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-Application-SpecificManagers", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Application-Specific Managers", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Spark Application Manager", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-SparkApplicationManager", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Spark Application Manager", 
"snippet" : "The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor partitioning Detect and fix ...", 
"body" : "The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor partitioning Detect and fix inefficient and failed Spark apps Tune JVM settings for Spark drivers\/executors Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Spark application. The Spark Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Spark application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Spark application. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of stages that make up the Spark application and their status Number of Stages Sub-Tabs The Spark Application Manager contains multiple sub-tabs, each of which is described below. : A table of the stages associated with this application. To see details about a stage, click its row in the table. This displays the Spark Stage view, as illustrated in the image below. Navigation The Spark Stage view provides detailed information about each Spark stage. It includes: : General task\/slot statistics of the stage Graphs : Timeline and histogram of task attempts, duration, and bytes shuffled\/spilled. Timeline This information is very useful for identifying data skew. : Key\/value pairs of configuration settings for the application Configuration Click here to see sample screenshots... : Displays both the stage view and RDD view associated with this application. Execution Graph : A timeline of application stages Gantt Chart : The logical plan of the SparkSQL query Query Plan : Exceptions, errors, and warnings associated with this application Errors : Logs for the driver and executors of this application, and a skyline of task attempts within the stage Logs : Source code of a general purpose Spark application, or the SQL query for a SparkSQL query Program : Statistics about the task attempts that are executed as part of the current application Task Attempts : Utilization of slot containers over time Containers : Utilization of slot vcores over time Vcores : Utilization of slot memory over time Memory : Graphs of JVM-level metrics at the executor and driver level. To show the graph for a specific metric, select that metric from the Resource METRIC Get Data Resource Metrics " }, 
{ "title" : "Hive Application Manager", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-HiveApplicationManager", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Hive Application Manager", 
"snippet" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. You can use this view to resolve inefficiencies, bottlenecks and reasons for failure within applications. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). The Hive Application ...", 
"body" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. You can use this view to resolve inefficiencies, bottlenecks and reasons for failure within applications. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). The Hive Application Manager uses Unravel's Intelligence Engine to automatically identify certain inefficiencies with the application and to provide recommendations on how to improve efficiency. Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Hive query. The Hive Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Hive query depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Hive query. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of YARN apps that make up the Hive query Number of YARN apps Sub-Tabs The Hive Application Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the application and drill down into MapReduce jobs that make up the application. It includes information about MapReduce jobs such as duration, I\/O, resources, events, job ID, and job status. When you select a MapReduce job, its details are shown in a pane below it. This detailed view includes the MapReduce timeline, MapReduce task attempts, and slot usage graph. Navigation : Provides insights into the execution of the application. It shows detailed information about MapReduce stages and their relationship with one another. The execution view is split into two areas; the execution plan and the expanded info area. Execution Graph : Shows the relationship between MapReduce stages and high level information about each stage. The information shown in each stage box includes MapReduce job ID, base table name, time taken by the stage, percentage of total run time taken by the stage, and execution status. Gantt Chart Errors : Shows detailed information about a stage when selected. The expanded info shows each of the map reduce functions, tables usage information, timings of each function and input paths used by the stage. Query : Provides an easy way to understand efficiency and status of MapReduce task attempts by breaking down attempted tasks by successful, failed and killed. Task Attempts Attempts : Shows slot usage by Map and Reduce jobs over time. Slot Usage " }, 
{ "title" : "MapReduce Application Manager", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-MapReduceApplicationManager", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ MapReduce Application Manager", 
"snippet" : "The MapReduce Application Manager provides a detailed view into the behavior of MapReduce applications. It is used by Hadoop DBAs or application owners (engineers, BI team, analysts) to resolve inefficiencies, bottlenecks and reasons for failure within applications. The MapReduce Application Manager...", 
"body" : "The MapReduce Application Manager provides a detailed view into the behavior of MapReduce applications. It is used by Hadoop DBAs or application owners (engineers, BI team, analysts) to resolve inefficiencies, bottlenecks and reasons for failure within applications. The MapReduce Application Manager uses the Unravel intelligence engine to automatically identify certain inefficiencies with the application and provides solutions on how to the fix the problem. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Sub-Tabs The MapReduce Application Manager contains multiple sub-tabs, each of which is described below. : Provides a detailed view into a MapReduce job execution. The Timeline shows execution of each MapReduce task on the machine that those tasks ran on and allows drill-down into each task to obtain further information. The timeline view comes with filters which can be used to display only map tasks, reduce tasks and killed\/failed tasks. The histograms above the Timeline show the distribution of MapReduce tasks along time and data size. This histogram can also be used as a filter to zoom in on specific tasks. Timeline : Provides comprehensive information about each application including task and job logs. The logs section intelligently selects interesting tasks and presents its logs in an organized manner. Logs : Provides a complete list of configuration settings used during the application execution. Configuration " }, 
{ "title" : "Finding Workflows", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-FindingWorkflows", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Workflows", 
"snippet" : "To find workflows, select SHOW Workflow...", 
"body" : "To find workflows, select SHOW Workflow " }, 
{ "title" : "", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Workflow Manager", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-WorkflowManager", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Workflow Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automaticall...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automatically identify inefficiencies with the workflow and provides solutions on how to the fix the problem. The Workflow Manager helps pipeline owners easily maintain SLAs. The workflow header provides primary information about the workflow such as name, user name, queue that the workflow was submitted on, start time and tags that the workflow has been given. Key Performance Indicators (KPIs) : The number of Unravel recommendations or insights for this workflow. To see details, click the Events Events : Total time taken by the workflow to complete execution Duration : Total data read and written by the workflow Data I\/O Resources : The number of apps that make up this workflow Number of Apps Sub-Tabs The Workflow Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the workflow and drill down into the Hive queries, Spark jobs, and MapReduce jobs that make up the application. It includes information about duration, I\/O, resources, events, job IDs, and job status. Navigation : Shows the dependency between the various components and the time taken by each. It helps you to identify stuck or incomplete components which could be affecting the overall completion of the workflow. Gantt Chart : Clearly shows the dependencies between various components of the workflow and the status of components. To see details about status, hover over a status box to see the tool tips. DAG View : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer over instances displays top KPIs such as duration, data I\/O, resources, and the number of jobs in that instance. Clicking on a point on the chart loads that particular instance for inspection. Compare " }, 
{ "title" : "Events", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-EventsEvents", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Events", 
"snippet" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events...", 
"body" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events " }, 
{ "title" : "Error View", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-ErrorView", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Error View", 
"snippet" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details f...", 
"body" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details from the errors messages\/log data that can help developers\/operators quickly root cause issue. Examples of keywords include Oozie errors code(s), Java run time error(s) etc. " }, 
{ "title" : "Resource Metrics", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-applications-tab/resource-metrics.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time w...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time when the collection process started totalPhysicalMemory BYTES The total physical memory in the operating system freePhysicalMemory BYTES The free physical memory in the operating system committedVirtualMemory BYTES The committed virtual memory in the operating system freeSwap BYTES The free swap size availableMemory BYTES An estimate of memory available for launching new processes vmRss BYTES The resident set size of the complete process tree vmRssDir BYTES The resident set size of the process totalSwap BYTES The total swap size processCpuLoad PERCENTS Average process CPU load for the last minute (all cores) systemCpuLoad PERCENTS Average system CPU load for the last minute (all cores) fullGcCount COUNT Number of full GC runs minorGcCount COUNT Number of minor GC runs minorGcTime DURATION (nanoseconds) Accumulated time spent in minor GC fullGcTime DURATION (nanoseconds) Accumulated time spent in full GC gcEdenSurvivedAvg BYTES Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcSurvivorPromotedAvg BYTES Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg BYTES Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms allocatedBytes BYTES Accumulated number of allocated bytes edenPeakUsage BYTES Maximum memory usage in the eden space survivorPeakUsage BYTES Maximum memory usage in the survivor space oldPeakUsage BYTES Maximum memory usage in the old space avgMinorInterval DURATION (nanoseconds) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations avgFullGcInterval DURATION (nanoseconds) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms gcOldLiveAvg BYTES Average number of bytes alive in the old generation. Might not be available for particular GC algorithms initHeap BYTES Initial heap size maxHeap BYTES Maximum heap size usedHeap BYTES Used heap size committedHeap BYTES Committed heap size initNonHeap BYTES Initial non-heap size maxNonHeap BYTES Maximum non-heap size usedNonHeap BYTES Used non-heap size committedNonHeap BYTES Committed non-heap size currentThreadCpuTime DURATION (nanoseconds) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime DURATION (nanoseconds) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems " }, 
{ "title" : "The Data Tab", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-data-tab.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Table\/Partition Usage", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/the-data-tab.html#UUID-252ec35e-09cb-026a-9479-a05cb52df60e_id_TheDataTab-TablePartitionUsage", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab \/ Table\/Partition Usage", 
"snippet" : "The Data Details On the Details Click on any column to sort by that metric such as: Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read I\/O - to find tables which have performed most I\/O Attempts - to find tables which have had most number of read\/write att...", 
"body" : "The Data Details On the Details Click on any column to sort by that metric such as: Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read I\/O - to find tables which have performed most I\/O Attempts - to find tables which have had most number of read\/write attempts on them Apps - to find tables which have most number of applications using it Users - to find tables which have the most number of users using it Other Tables - shows other tables commonly used along with a particular table in applications Labels - view tables which have been labeled Hot, Warm or Cold Hover on columns like Users and Other Tables to know respectively which users accessed this table and which other tables are accessed along with this table. View trends of table usage and access by accessing the drop-down menu on the top right corner. Metrics available are Read I\/O, Total Users, Total Apps, Total Attempts as shown below. You can also search for a particular table by typing part of the table name in the table search box as shown below. In our example we are looking for tables with the word customer in it. Table Details To drill down into a particular table click its More Info This opens a Table Detail You can view different metric trends on this pane including: Read IO Number of Users Number of Apps Number of Attempts It also allows you to view the historical usage and access information about this table. The table detail view also shows a list of applications and users that accessed the table in the given time range. This list of applications and users is also sortable for easy search and browse capabilities. Partition Details To view detailed information about partitions associated with a table, click the Partition Detail Table Detailed View Partition Detail The partition histogram displays the access age of partitions (including the max access age). Unravel uses this information to calculate the number and size of reclaimable partitions as follows: Reclaimable partitions found = number of partitions with last access time < (current time - max access age of partitions). In the example above: No partition is accessed by any application more than 4 days 14 hours after it is created. Thus, the partitions created on Dec 1, 2015, will not be accessed after Dec 5, 2015. In Unravel, the max access age of partitions for the above table will be computed as 4 days 14 hrs based on the history of accesses to partitions in the above table. So on Dec 15, 2015, when you look at the table above: Reclaimable Partitions found = number of Partitions with Last Access Time < (Dec 14 2015 - 4days 14 hours) i.e., (Dec 10, 2015). Thus, the partitions created on any day before Dec 10, 2015 will be marked as a Reclaimable Partition. In this case, it will be all the 26 partitions. The page also list all partitions by key KPIs, including: Last access date\/time Create date\/time Current size Number of users accessing the partition Unravel provides an easy way to label tables\/partitions as HOT WARM COLD Once you have the rule configured as above, click SAVE RULES While the HOT WARM COLD As illustrated above, HOT rule can be \"Last Access <= 120 days\", WARN rule can be \"Last Access <= 175 days and > 120 days\" and COLD rule can be \"Last Access >= 176 days\" These rules are checked periodically (every 24 hours) and tables\/partition are labeled and classified accordingly. The Data overview page provides a quick summary information based on these labels. Operators can use this information to identify unused or frequently used tables\/partitions and take appropriate actions. " }, 
{ "title" : "Setting Up Auto Actions (Alerts)", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Auto Actions", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html#UUID-9fa98f37-b271-7594-bd19-35bf82f977ce_id_SettingUpAutoActionsAlerts-AutoActions", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts) \/ Auto Actions", 
"snippet" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin M...", 
"body" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin Manage On the Manage Auto Actions Click ADD NEW AUTO ACTION Select the desired Auto Action type, and then click Next Enter a name for the new Auto Action, and specify its prerequisite conditions, defining conditions, and actions: Prerequisite conditions: A set of boolean conditions such that when they are all met, Unravel will evaluate the defining conditions of the Auto Action. Examples include: whether this Auto Action should be evaluated during a given time, whether this Auto Action should be evaluated for a job belonging to a given user, etc. Defining conditions: A set of boolean conditions defining the Auto Action. When they are all met, the corresponding action defined as part of the Auto Action will be taken automatically. Examples include: Is this job running for too long? Does it use too many mappers? Actions: A set of actions to be taken when the defining conditions are all evaluated to be true. Examples include: send an email to admin (i.e., “alerting”), kill the job, etc. Click SAVE AUTO ACTION " }, 
{ "title" : "Use Cases", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Operations Dashboard INEFFICIENT APPLICATIONS", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/user-guide/use-cases/operations-dashboard-inefficient-applications.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Operations Dashboard INEFFICIENT APPLICATIONS", 
"snippet" : "The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the f...", 
"body" : "The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Unravel Server Reference", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/unravel-server-reference.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zooke...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zookeeper (on a custom port) unravel_k bundled Kafka (on a custom port) unravel_hhs Hive Hook Sensor unravel_hhw Hive Hook Worker unravel_hhwe Hive Hook Worker EMR unravel_hvw Hive Worker unravel_jcs1 Job Collector Sensor MRv1 unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw1_N Job Collector Sensor Worker MRv1 unravel_jcw2_N Job Collector Sensor Worker YARN unravel_lr Log Receiver unravel_mrw Map Reduce Worker unravel_ja \"Job Analyzer\" summarizes jobs unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_os3 Oozie v3 Sensor unravel_os4 Oozie v4 Sensor unravel_tw Table Worker unravel_pw Partition Worker unravel_ew_N Event Worker unravel_sw_N Spark Worker " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/local\/unravel\/logs Do not set directly; set UNRAVEL_LOG_DIR in etc\/unravel.ext.sh instead and this property will be derived from that com.unraveldata.zk.quorum 127.0.0.1:4181 embedded Zookeeper ensemble in form host1:port1,host2:port2, ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ com.unraveldata.kafka.broker_list 127.0.0.1:4091 embedded Kafka unravel.jdbc.username unravel MySQL (embedded or external) username for db unravel.jdbc.password random generated for bundled MySQL MySQL (embedded or external) password for db unravel.jdbc.url jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod This is JDBC URL without username and password com.unraveldata.hdfs.interactive.monitoring.interval.sec 30 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) com.unraveldata.hdfs.batch.monitoring.interval.sec 300 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) com.unraveldata.longest.job.duration.days 2 Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) oozie.server.url http:\/\/localhost:11000\/oozie URL for accessing Oozie to track workflows com.unraveldata.oozie.fetch.num 100 Max number of jobs to fetch during an interval com.unraveldata.oozie.fetch.interval.sec 120 seconds between intervals for fetching Oozie workflow status " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java JAVA_EXT_OPTS unset Last chance a...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java JAVA_EXT_OPTS unset Last chance arguments to jvm to override other settings HADOOP_CONF as discovered by running hadoop fs -ls The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml UNRAVEL_DATA_DIR \/srv\/unravel A base directory owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data zk_data UNRAVEL_LISTEN_PORT 3000 The Unravel UI port on the primary or standalone Unravel installation (service unravel_tc 0.0.0.0 com.unraveldata.tc UNRAVEL_LOG_DIR \/usr\/local\/unravel\/logs A destination dir. owned by user unravel for log files UNRAVEL_TC_SHUTDOWN_PORT 3005 An unoccupied port used for cleanly stopping the Unravel UI (service unravel_tc 127.0.0.1 " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs L...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~1.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file to preserve traceability \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/srv\/unravel\/log_hdfs log directory for daemons that run as user hdfs <2GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR \/srv\/unravel\/tmp_hdfs tmp directory for daemons that run as user hdfs <1GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR " }, 
{ "title" : "Advanced Topics", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Adminis...", 
"body" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires Un-select Password must be changed Select Password cannot be changed Right-click the created user, click Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Sensors", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/sensors.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-applications-submitted-through-spark-shell.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions ...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/ UNRAVEL_HOST VERSION To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION 2.0 1.6 1.5 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps executed via the spark-shell To intercept Spark apps executed via the spark-shell, you need to unzip the Unravel Sensor .zip file on the client node at a location on the local file system that is readable by all users, referred to as UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark- VERSION Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-shell always executed in client mode To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 SPARK_EVENT_LOG_DIR UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\n\n\/usr\/lib\/spark\/bin\/spark-shell \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n<<EOF\n\/\/ your spark-shell code snippet will follow here\n\/\/ For exemplifying, we use a snippet of RDDRelation below\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\ncase class Record(key: Int, value: String)\n\n val df = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\"))).toDF()\n\n \/\/ Any RDD containing case classes can be registered as a table. The schema of the table is\n \/\/ automatically inferred using scala reflection.\n df.registerTempTable(\"records\")\n\n \/\/ Once tables have been registered, you can run SQL queries over them.\n println(\"Result of SELECT *:\")\n sqlContext.sql(\"SELECT * FROM records\").collect().foreach(println)\n\n \/\/ Aggregation queries are also supported.\n val count = sqlContext.sql(\"SELECT COUNT(*) FROM records\").collect().head.getLong(0)\n println(s\"COUNT(*): $count\")\n\nexit\n\nEOF\n\n Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-applications-submitted-through-spark-shell-8218.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications executed via spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications executed via spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/ UNRAVEL_HOST VERSION To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION 2.0 1.6 1.5 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps executed via the spark-shell To intercept Spark apps executed via the spark-shell, you need to unzip the Unravel Sensor .zip file on the client node at a location on the local file system that is readable by all users, referred to as UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip\nunzip unravel-sensor-for-spark-bin.zip Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-shell always executed in client mode To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 SPARK_EVENT_LOG_DIR UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\n\n\/usr\/lib\/spark\/bin\/spark-shell \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n<<EOF\n\/\/ your spark-shell code snippet will follow here\n\/\/ For exemplifying, we use a snippet of RDDRelation below\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\ncase class Record(key: Int, value: String)\n\n val df = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\"))).toDF()\n\n \/\/ Any RDD containing case classes can be registered as a table. The schema of the table is\n \/\/ automatically inferred using scala reflection.\n df.registerTempTable(\"records\")\n\n \/\/ Once tables have been registered, you can run SQL queries over them.\n println(\"Result of SELECT *:\")\n sqlContext.sql(\"SELECT * FROM records\").collect().foreach(println)\n\n \/\/ Aggregation queries are also supported.\n val count = sqlContext.sql(\"SELECT COUNT(*) FROM records\").collect().head.getLong(0)\n println(s\"COUNT(*): $count\")\n\nexit\n\nEOF\n\n Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Hive Queries", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-hive-queries.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Hive Queries", 
"snippet" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar W...", 
"body" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Part 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-sensor-for-mapreduce-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set mapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; " }, 
{ "title" : "Workflows", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web ser...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web service are in the same cluster. If You Use Http For Airflow Web UIAccess Add the following 3 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.protocol=http\ncom.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart If You Use Https For Airflow Web UIAccess Add the following 4 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true\ncom.unraveldata.airflow.login.name={airflow web UI username}\ncom.unraveldata.airflow.login.password={airflow web UI password} Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Range of Monitoring By default, Unravel Server ingests all the workflows that started within the last 5 days. If you wish to change the date range to the last x Add the following configuration to \/usr\/local\/unravel\/etc\/unravel.properties Don't omit the “-” (minus sign) in the value. airflow.look.back.num.days=-x Restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import PythonOperator from da...", 
"body" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Oozie\/Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n 1. Add Hooks for Unravel Instrumentation The example below shows the contents of a bash script, example-hdp-client.sh spark-submit spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport Setting these parameters on a per-application spark-defaults.conf This script references the following variables, which you would need to edit: PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR 2. Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call 2016-08-30 3. Monitor the Workflow To see the new Oozie workflow in Unravel Web UI, select APPLICATIONS Workflows Add Workflow " }, 
{ "title" : "Tagging Workflows", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/workflows/tagging-workflows.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Tagging Workflows", 
"snippet" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utct...", 
"body" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: [Wrong usage] SET unravel.workflow.name=\"ETL-Workflow\"; [Correct usage] SET unravel.workflow.name=ETL-Workflow; Please note the following: Different runs of the same same unravel.workflow.name Different runs of the same different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name The example below shows a Hive query that is marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\n\nSELECT foo FROM table WHERE … [Rest of Hive Query text goes here] Easy Recipes for Tagging Workflows First, export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow: Export the workflow name: export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow Export the UTC timestamp for this run of the workflow. Here, we use bash's date export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Then follow the appropriate instructions below: Tagging a Hive query Tagging a Sqoop job Tagging a direct MapReduce job Tagging a Spark job Tagging a Pig job Tagging a Impala query How to Tag a Hive Query Using SET Commands in Hive hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=${env:WORKFLOW_NAME};\nSET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP};\nselect count(1) from lineitem; How to Tag a Sqoop Job Using –D Command Line Parameters sqoop export \\\n *-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar How to Tag a Direct MapReduce Job Using –D Command Line Parameters hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n*-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n-p \/wordcount.properties -input \/tmp\/soumitra\/data\/small -output \/tmp\/soumitra\/outsmoke How to Tag a Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name spark-submit \\\n * --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" *\n * --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" *\n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster How to Tag a Pig Job Using –param and SET Commands pig \\\n*-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP * \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME;\nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP;\n\nlines = LOAD '\/tmp\/soumitra\/data\/small' using PigStorage('|') AS (line:chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount; How to Tag a Impala Job Using –-query_options and SET Command Set DEBUG_ACTION in Impala script to pass the workflow tags into impala queries: SET DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}; According to this CDH article: https:\/\/www.cloudera.com\/documentation\/enterprise\/5-14-x\/topics\/impala_query_options.html impala-shell --query_option=DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}| -f impala.script Note: the tagging option should be turned on for this tagging to work. ie, the followings need to be set in unravel.properties: # Tagging\ncom.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path=\/tmp\/app_tag.py\ncom.unraveldata.app.tagging.script.method.name=get_tags Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Application Workflow " }, 
{ "title" : "Custom Configurations", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/configuring-multiple-hosts-for-unravel-server.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig 1. Stop Unravel Server On each Unravel host, run this command: sudo \/etc\/init.d\/unravel_all.sh stop 2. Modify unravel.properties on host1 Pick a machine to be host1 If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties host1 Replace 127.0.0.1 3316 unravel_mysql_prod To find your fully qualified hostname, type hostname -I unravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod 2. Copy host1's unravel.properties to Other Hosts Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh host1 host2 host3 # host1\nscp \/usr\/local\/unravel\/etc\/unravel.properties host2:\/usr\/local\/unravel\/etc\/\n# host1\nscp \/usr\/local\/unravel\/etc\/unravel.ext.sh host2:\/usr\/local\/unravel\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel Important Note The scripts invoked below will make an identical change to the unravel.properties 3. Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host): # host1\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh\\ \nhost1 host2 \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh\\ \nhost1 host2 For a 3-host ensemble (substitute host): # host1 sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh\\ \nhost1 host2 host3\n# host2 sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh\\ \nhost1 host2 host3\n# host3 sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh\\ \nhost1 host2 host3 These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id 4. Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order, on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: # host1\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host3\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: # host1 \nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15sec for Zookeeper quorum to settle: sleep 15 Start up Kafka ensemble: # host1\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-k Wait 10sec for Kafka coordination: sleep 10 Create the Kafka topics (only on one host): # host1 \nsudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh 5. Start Unravel Server Finish multi-host installation by starting up Unravel Server: # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host1\necho \"http:\/\/$(hostname -f):3000\/\" \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start 6. Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 7. Snapshot unravel.properties as new golden file " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 sudo chkconfig --add unravel_hhw_2 sudo ch...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: sudo \/etc\/init.d\/unravel_all.sh start\n " }, 
{ "title" : "Defining a Custom TC Port", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/defining-a-custom-tc-port.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom TC Port", 
"snippet" : "These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\ >>\/usr\/local\/unravel\/etc\/unravel.ext.sh...", 
"body" : " These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\\n>>\/usr\/local\/unravel\/etc\/unravel.ext.sh " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/integrating-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use Hi...", 
"body" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use HiveServer2 LDAP, then follow the steps below. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties Change QA ldap:\/\/LDAP_HOST For QA For ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST unravel\/jre\/ ldap:\/\/ ldap_host For Active Directory (AD): com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.Domain=QA Change LDAP_HOST and QA to appropriate value for your installation. For Open LDAP, example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 2: In this example, we expect a typical DN to be uid=%s,ou=myunit,dc=example,dc=com where %s is the login name as typed in the login form. In some cases 'cn' is used in place of 'uid'. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. 2. Restart unravel_tc Restart unravel_tc sudo \/etc\/init.d\/unravel_tc restart Advanced Hive Properties Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The process of authentication is described next. Authentication Process for Active Directory (AD) Bind as username + at sign + domain, using the given password, with simple LDAP auth mode verbose log will show Connecting and then Connected when bind is successful If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group filter is specified, it is checked if a user filter is specified, it is checked Authentication Process for Open LDAP Bind as cn or uid =username + baseDN using the given password, with simple LDAP auth mode the guidKey property determines whether cn or uid is used if userDNPattern is used, it takes precedence over baseDN, and each pattern is tried If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group pattern or filter is specified, it is checked if a user filter is specified, it is checked Property Description Example Value hive.server2.authentication.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. REQUIRED for advanced query except when setting custom query or groupDNPattern. uid or CN hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). , hiveuser impalauser hiveadmin hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member uniqueMember memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group groupOfNames groupOfUniqueNames com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_tc_webapp.log file under \/usr\/local\/unravel\/logs\/ ; when enabled, user names and group names can appear in this log, but raw passwords are not logged. Can be true or false or not set; default is false " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties admin Manage Configuration Core Retention unravel.properties When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires abo...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties admin Manage Configuration Core Retention unravel.properties When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 500KB of disk space. That means about 2000 jobs per 1GB of disk. In Unravel Web UI, select the Manage Configuration Core The TIME SERIES RETENTION DAYS unravel.properties: com.unraveldata.retention.max.days=90 The WEEKS TO SHOW FOR SEARCH RESULTS unravel.properties com.unraveldata.history.maxSize.weeks=7 This value should be no larger than the next setting minus 1. The WEEKS TO SHOW FOR DEEP SEARCH RESULTS unravel.properties com.unraveldata.recent.maxSize.weeks=14 This value should be at least 1 week more than the setting immediately above. After changing any of the settings above, restart unravel_td sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/custom-configurations/setting-up-email-for-auto-actions-and-collaboration.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. Web UI Property Value Description PORT mail.smtp.port 25 Port AUTHENTICATE mail.smtp.auth false Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw START TLS mail.smtp.starttls.enable false Use start-TLS? SSL ENABLE mail.smtp.ssl.enable false Use SSL right from the start? USER mail.smtp.user null Username for SMTP authentication USER PASSWORD mail.smtp.pw null Password for SMTP authentication HOST mail.smtp.host l Host for SMTP server FROM USER mail.smtp.from someone @example.com Use a From: LOCALHOST mail.smtp.localhost localhost.local A domain name for apparent sender; must have at least one dot (e.g. organization.com) DEBUG mail.smtp.debug false Enable debug mode? Set to true (temporarily) to see more details in logs. Unravel daemons to restart after email setup Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_tc restart \nsudo \/etc\/init.d\/unravel_all.sh stop-etl\nsudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n--> enter password from dist.unraveldata.com\n--> should see following output in terminal mode and if you see \"result is = null\", then, setup is correct.\n:\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add these properties to \/usr\/local\/unravel\/unravel.properties mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Disable unneeded daemons: sudo service unravel_os3 stop\nsudo chkconfig unravel_os3 off Restart daemons: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI....", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. " }, 
{ "title" : "For CDH+CM", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForCDHCM", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For CDH+CM", 
"snippet" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the RES...", 
"body" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/ CMGR_HOSTNAME_IP Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore 2. In Unravel UI, on the top right corner, click admin Manage 3. On the left tab, click Hive HIVE METASTORE URL HIVE METASTORE DRIVER HIVE METASTORE USER NAME HIVE METASTORE PASSWORD 4. Save the information when done: click Save Changes 5. Restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart 6. After restart, confirm that Hive queries appear in Unravel UI in the Application " }, 
{ "title" : "For HDP", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForHDP", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For HDP", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "For MapR", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForMapR", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For MapR", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-11c5f57d-7413-5d9d-c486-9a999fb8376e_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database you wish to monitor for FreeStorageSpace Next In the Alarm Threshold - for this Database ...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database you wish to monitor for FreeStorageSpace Next In the Alarm Threshold - for this Database Metrics (e.g. RDS_FreeStorageSpace_for_MySQL-A) Name - describe what the above database metrics name you entered (e.g. Disk space monitor of RDS MySQL-A) Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for 10 consecutiveperiod(s) Under Actions Send notifications to Note: this sns topic should already be setup before you add it. Click Create Alarm In Alarms Alarms To to create alarm metrics for RDS monitoring on Storage Space, click Create Alarm " }, 
{ "title" : "Creating Application Tags", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags", 
"snippet" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed....", 
"body" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. " }, 
{ "title" : "What is a Tag in Unravel?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-WhatisaTaginUnravel", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What is a Tag in Unravel?", 
"snippet" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ......", 
"body" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ... " }, 
{ "title" : "How Does Unravel Use Tags?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-HowDoesUnravelUseTags", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Does Unravel Use Tags?", 
"snippet" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights...", 
"body" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights " }, 
{ "title" : "What Types of Tags Are There?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-WhatTypesofTagsAreThere", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What Types of Tags Are There?", 
"snippet" : "There are two types of tags: Unravel tags and user-created tags....", 
"body" : "There are two types of tags: Unravel tags and user-created tags. " }, 
{ "title" : "Unravel Tags", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-UnravelTags", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Unravel Tags", 
"snippet" : "All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp...", 
"body" : "All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp " }, 
{ "title" : "User-Created Tags", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-User-CreatedTags", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ User-Created Tags", 
"snippet" : "You can create tags based on your use cases....", 
"body" : "You can create tags based on your use cases. " }, 
{ "title" : "How Do I Create Tags?", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-HowDoICreateTags", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Do I Create Tags?", 
"snippet" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them....", 
"body" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. " }, 
{ "title" : "Adding Tags to your Application's Configuration", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-AddingTagstoyourApplicationsConfiguration", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Adding Tags to your Application's Configuration", 
"snippet" : "Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,......", 
"body" : "Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,... " }, 
{ "title" : "Injecting Tags Through a Python Script", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-InjectingTagsThroughaPythonScript", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Injecting Tags Through a Python Script", 
"snippet" : "You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches ...", 
"body" : "You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches a consistent state.For example, Spark receives data from Resource Manager, event log file, YARN aggregated logs, and sensors. Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. " }, 
{ "title" : "Precedence of Tags", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-PrecedenceofTags", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Precedence of Tags", 
"snippet" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script...", 
"body" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script " }, 
{ "title" : "Sample Use Cases", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-SampleUseCases", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Sample Use Cases", 
"snippet" : "Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants...", 
"body" : " Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants " }, 
{ "title" : "Troubleshooting", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/troubleshooting.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting", 
"snippet" : "Provide solutions for commonly encountered problems. If you can't reach Unravel Server, ping LANS_DNS....", 
"body" : "Provide solutions for commonly encountered problems. If you can't reach Unravel Server, ping LANS_DNS. " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/troubleshooting/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are worki...", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (â€œSmoke Testsâ€?) Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run thespark_test_via_parcel.shscript. This script runs a Spark app. Itâ€™s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, even before you install and configure Unravel Sensor. You should be able to see the data generated by this Spark app on Unravel Web UI. \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server <unravel_host_IP_address> Note: You can run this script without installing and configuring Unravel Sensor. After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 Running Benchmarks We provide sample Spark, MapReduce, Hive, and WF apps that you can download from preview.unraveldata.com Spark Download our sample Spark app: curl https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz -o spark-benchmarks1.tgz This .tgz Run md5sum md5sum spark-benchmarks1.tgz Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 spark-benchmarks1.tgz Uncompress the .tgz tar -zxvf spark-benchmarks1.tgz Run the samples. Instructions on how to run the samples are included in the package itself, inside demo-benchmarks-for-spark\/benchmarks\/README After running the samples, check the Program Execution Graph Execution Graph " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/troubleshooting/running-verification-scripts-and-benchmarks-8237.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working...", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (“Smoke Tests”) Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run thespark_test_via_parcel.shscript. This script runs a Spark app. It’s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, even before you install and configure Unravel Sensor. You should be able to see the data generated by this Spark app on Unravel Web UI. \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server <unravel_host_IP_address> You can run this script without installing and configuring Unravel Sensor. After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 Running Benchmarks We provide sample Spark, MapReduce, Hive, and WF apps that you can download from preview.unraveldata.com Spark Download our sample Spark app: curl https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz -o spark-benchmarks1.tgz This .tgz Run md5sum md5sum spark-benchmarks1.tgz Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 spark-benchmarks1.tgz Uncompress the .tgz tar -zxvf spark-benchmarks1.tgz Run the samples. Instructions on how to run the samples are included in the package itself, inside demo-benchmarks-for-spark\/benchmarks\/README. After running the samples, check the Program and the Execution Graph tabs in Unravel Web UI. Click an RDD in the Execution Graph to see the corresponding line of code in the app. " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel\nsudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Upgrading Unravel Server", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/advanced-topics/upgrading-unravel-server.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Upgrading Unravel Server", 
"snippet" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1 sudo \/etc\/init.d\/unravel_all.sh stop...", 
"body" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host2\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host3\nsudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously # host1\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host2\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host3\nsudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties You must enter add license key to unravel.properties Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh After all the RPM upgrades finish, restart Unravel Server on each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Release Notes", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Release Notes: Version 4.1.941", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Software Version", 
"snippet" : "On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z...", 
"body" : " On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z " }, 
{ "title" : "New Features", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-NewFeatures", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ New Features", 
"snippet" : "Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics...", 
"body" : " Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics " }, 
{ "title" : "Tested Platforms", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-TestedPlatforms", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes", 
"snippet" : "Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\"...", 
"body" : " Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\" " }, 
{ "title" : "Robustness\/Reliability", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-RobustnessReliability", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Robustness\/Reliability", 
"snippet" : "Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common setti...", 
"body" : " Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common settings are all included and don't need to be specified by the user Improved Unravel Resource Metrics Sensor performance " }, 
{ "title" : "Spark Support", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-SparkSupport", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Spark Support", 
"snippet" : "Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configur...", 
"body" : " Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configuration from MySQL Accessing S3 log files fixes: Mapping multiple S3 buckets to the same S3 profile Set the S3 region to a custom one. Scripts and DSL API extension to generate large Oozie workflows programmatically " }, 
{ "title" : "Workflow Support", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-WorkflowSupport", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Workflow Support", 
"snippet" : "Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this fe...", 
"body" : " Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this feature, the Oozie workflows won't show up; instead, the selected (tagged) applications will be grouped into a single tagged workflow and presented in the Workflows tab. Airflow improvements " }, 
{ "title" : "MR Insights", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-MRInsights", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ MR Insights", 
"snippet" : "Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs...", 
"body" : " Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs " }, 
{ "title" : "Known Issues", 
"url" : "un40--unravel-4-0-4-1-/unravel-4-0-4-1/release-notes/release-notes--version-4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-KnownIssues", 
"breadcrumbs" : "Home \/ UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes \/ Release Notes: Version 4.1.941 \/ Known Issues", 
"snippet" : "Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (so...", 
"body" : " Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (source - unravel_emr_sensor.log in EMR cluster) Improve\/Rewrite EMR bootstrap script to avoid all manual steps. " }, 
{ "title" : "", 
"url" : "un42--un42-.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "UN42 (UN42)...", 
"body" : " UN42 (UN42) " }, 
{ "title" : "Unravel 4.2", 
"url" : "un42--un42-/unravel-4-2.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "This Documentation is for Unravel Version 4.2", 
"url" : "un42--un42-/unravel-4-2.html#UUID-d6261bd5-a853-ec75-7e65-bfd7ffe074c9_id_Unravel42-ThisDocumentationisforUnravelVersion42", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ This Documentation is for Unravel Version 4.2", 
"snippet" : "Documentation for Prior Versions For older documentation, contact Unravel Support. Downloads Compatibility Matrix Unravel Software Versions New Users Overview Frequently Asked Questions (FAQ) Release Notes Version: 4.2 Release Notes Version 4.2.3 Release Notes Version: 4.2.4 Release Notes Version: 4...", 
"body" : " \n \n \n Documentation for Prior Versions For older documentation, contact Unravel Support. Downloads Compatibility Matrix Unravel Software Versions \n New Users Overview Frequently Asked Questions (FAQ) Release Notes Version: 4.2 Release Notes Version 4.2.3 Release Notes Version: 4.2.4 Release Notes Version: 4.2.5 Release Notes Version: 4.2.6 Release Notes Version: 4.2.7 Release Notes \n advanced \n airflow \n auto-action-demos \n auto-actions \n autoactions \n azure \n btrace \n cdh \n cloudera \n cloudera-manager \n cm \n emr \n hadoop2 \n hdinisght \n hdinsight \n hdp \n hive \n hive-hook \n hortonworks \n impala \n installation \n kafka \n kb-how-to-article \n kerberos \n mapr \n mapreduce \n oozie \n qubole \n release-notes \n sensor \n spark \n troubleshooting \n unravel \n unravel-4-0 \n unravel-4-2 \n unravel-sensor \n user-guide \n workflow \n workflows \n yarn " }, 
{ "title" : "Overview", 
"url" : "un42--un42-/unravel-4-2/overview.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "un42--un42-/unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. \n The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "un42--un42-/unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. \n Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "un42--un42-/unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API....", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. " }, 
{ "title" : "Installation Guides", 
"url" : "un42--un42-/unravel-4-2/installation-guides.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides", 
"snippet" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3 (Optional): Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-o...", 
"body" : " Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3 (Optional): Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP MapR Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters Step 1: Install Unravel Server for Amazon EMR or Qubole Cluster Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup Unravel for Azure HDInsight clusters Install Unravel HDInsight app Installation Guide for Unravel VM " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/step-1--install-unravel-server-on-cdh-cm.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check. Configure the host machine. Install the Unravel Server RPM on the host...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check. Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility On-premises CDH up to v5.13, with Spark 2.2.x Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended, or 2x to 4x more RAM Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config. HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH. If Spark is in use, Spark client gateway. LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication. (Open signup by default.) On Unravel Edge-node server, please do not NTP should be running and in-sync with the cluster. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN’s “done dir” in HDFS YARN’s log aggregation directory in HDFS Spark event log directory in HDFS File sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive. 2. Install the Unravel Server RPM on the Host Machine The precise RPM filename will vary. The version has the structure x.y.b b x.y Get the Unravel Server RPM See instructions UPDATE_NEEDED_ADD LINK TO RPM. Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. 3. Configure Unravel Server (Basic\/Core Optional for CDH) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties All values in unravel.properties only optional Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay 1 com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.s3.batch.monitoring.interval.sec Optional 120 If Kerberos is Enabled Add Authentification for HDFS. Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab unravel:unravel chmod this can be the same principal and keytab as (2) if that is more convenient. Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab hdfs:hdfs chmod Tell Unravel Server about it in \/ usr\/local\/unravel\/etc\/unravel.ext.sh export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab\nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal by using ' klist -kt KEYTAB_FILE' If Sentry is Enabled Add These Permissions. For quicker setup, use the named principal. For more narrow privileges, define your own alt principal. Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alt read Spark event log hdfs:\/\/user\/history\/done hdfs or alt read MapReduce logs hdfs:\/\/tmp\/logs hdfs or alt read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alt read Obtain table partition sizes with \"stat\" only Hive Metastore GRANT for select hive or alt read Hive table information Please see Configure Permission for Unravel daemons on CDH Sentry Secured Cluste You can find and verify the principal the keytab by: # klist -kt KEYTAB_FILE\n If you are using KMS and HDFS encryption and are using the hdfs principal, you might need to adjust kms-acls.xml . If you are using \"JNI\" based groups for HDFS (a setting in CM), then you will need to add \" export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native\" to \/usr\/local\/unravel\/etc\/unravel.ext.sh Do Host-Specific Configuration Steps For CDH, there are no host-specific configuration steps. Restart Unravel Server After the edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. 4. Log into Unravel Web UI Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} admin\" \"unraveldata UNRAVEL_HOST_IP. For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 5. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/step-2--install-unravel-sensor-parcel-on-cdh-cm.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"snippet" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job...", 
"body" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. These instructions apply to Unravel Sensor 4.2. Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM \n Workflow Summary Obtain and distribute the parcel from Unravel Server. Put the Hive Hook JAR in AUX_CLASSPATH Configure the gateway automatic deployment of Hive instrumentation. Configure the gateway automatic deployment of Spark instrumentation. \n HIGHLIGHTED When Active Directory Kerberos is used, UNRAVEL_HOST_IP To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 1. Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, you can deactivate it now. Then click Activate 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh no subsitutions HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is Enabled: Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT {ROLE} # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE {ROLE} 3. Configure the Gateway Automatic Deployment of Hive Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways In Cloudera Manager (CM): Go to Hive service. Select the Configuration Search for hive-site.xml Add the xml snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks \n com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger, com.unraveldata.dataflow.hive.hook.HivePostHook \n IMPORTANT! Add the xml snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Save the changes with optional comment \"Unravel snippet in hive-site.xml \" Perform action Deploy Hive Client Configuration ) or by using the Actions Restart the Hive service. (Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. ) Again, monitor the situation to see if all Hive queries are failing with a class not found or permission problems. If they are failing hive-site.xml Troubleshooting Check Unravel Web UI If queries are running fine and appearing in Unravel Web UI, then you are done. 4. Configure the Gateway Automatic Deployment of Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \" spark-defaults In the Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf, On a multi-host Unravel Server deployment, use the fully qualified DNS or logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) \n spark-conf\/ UNRAVEL_HOST_IP SPARK_VERSION \n SPARK_VERSION \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true 5. Save changes. 6. Deploy client configuration by clicking the deploy glyph ( Actions Monitor the situation to see if all Spark queries are failing with a class not found or permission problems. If they are failing spark-defaults.conf 5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Set in Cloudera Manager In Cloudera Manager (CM): Go to YARN Select the Configuration Search for ApplicationMaster Java Opts Base ). Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr-Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043 Search for \n MapReduce Client Advanced Configuration Snippet (Safety Valve) for mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property><name>mapreduce.task.profile<\/name><value>true<\/value><\/property>\n<property><name>mapreduce.task.profile.maps<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.reduces<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> 6. Save changes. 7. Deploy client configuration by clicking the deploy glyph ( Actions 8. Cloudera Manager will specify a restart which is not necessary to effect these changes. (click Restart Stale Services Monitor the situation and you should see in Unravel UI a Resource Usage tab showing you mappers and reducers when you view the Application page for any completed MRjob. Restart is important for MR sensor to be picked up by queries submitted via Hiveserver2. 6. (Optional) Advanced Configuration Configuration for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Integrating LDAP Authentication for Unravel Web UI Troubleshooting \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR Verify that unravel \/user\/unravel\/ \n \n \n ClassNotFound com.unraveldata.dataflow.hive.hook.HivePreHook during Hive query execution \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST \n \n Permission denied writing to \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR during Hive query execution \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR\/* \n \n hadoop fs -chmod 777 \/user\/unravel\/HOOK_RESULT_DIR\/* \n OR Sentry command is needed to give permission \n OR revert to your previous hive-site.xml References \n {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "Step 3 (Optional): Enable Impala APM", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/step-3--optional---enable-impala-apm.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 3 (Optional): Enable Impala APM", 
"snippet" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM Workflow Summary If you want Unravel Server to...", 
"body" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM Workflow Summary If you want Unravel Server to retrieve Impala query data from ClouderaManager, start at Using CM as the Data Source. If you want Unravel Server to retrieve Impala query data from Impala daemons, start at Using Impalad as the Data Source Using CM as the Data Source Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.cloudera.manager.url CM internal URL. Must start with http:\/\/ com.unraveldata.cloudera.manager.port (Optional) CM port number. You only need to specify this if your ClouderaManager is not on port 7180. com.unraveldata.cloudera.manager.username CM username com.unraveldata.cloudera.manager.password CM password com.unraveldata.cloudera.manager.cluster.name.list A comma-separated list of cluster names com.unraveldata.cloudera.manager.version (Optional) CM version. You only need to specify this if your ClouderaManager is earlier than 5.8.0. Note For example, com.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret\ncom.unraveldata.cloudera.manager.username=mycmname\ncom.unraveldata.cloudera.manager.password=mycmpassword\ncom.unraveldata.cloudera.manager.cluster.name.list=cluster1,cluster2,cluster5\ncom.unraveldata.cloudera.manager.port=9997\ncom.unraveldata.cloudera.manager.version=5_7_0 If the Impala service name is not defaulted as \" impala unravel.properties: com.unraveldata.cloudera.manager.service.impala.name=myimpalaservicename Hints: To find out the cluster name: {cm_url}:{cm_port}\/api\/v13\/clusters\/ In the JSON, use the value of \"name\" in the \"items\" list as the cluster name. To find out the service name: {cm_url}:{cm_port}\/api\/v13\/clusters\/{cluster_name}\/services\/\n\n Substitute your particular values for bracketed ClouderaManager (CM) properties, i.e., {cm_port} Using Impalad as the Data Source Use this option if you want to import data from impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html UPDATE_NEEDED_LINK TO https:\/\/docs.google.com\/document\/d\/1KnkIrQ_lCTUU6dtvXbDETUWnIlx-DGWRDTjm8fnmu_c\/edit?ts=591dd404#heading=h.h6gaqdg1wspm Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to unravel.properties Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port For example, com.unraveldata.data.source=impalad\ncom.unraveldata.impalad.nodes=IP:port,IP:port,IP:port By default, the ImpalaSensor unravel.properties com.unraveldata.sensor.tasks.disabled=iw Change the Impala Lookback Window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us References http:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Hortonworks Data Platform (HDP)", 
"url" : "un42--un42-/unravel-4-2/installation-guides/hortonworks-data-platform--hdp-.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP)", 
"snippet" : "This guide is compatible with: On-premises HDP up to v2.6...", 
"body" : " This guide is compatible with: On-premises HDP up to v2.6 " }, 
{ "title" : "Ordered Steps", 
"url" : "un42--un42-/unravel-4-2/installation-guides/hortonworks-data-platform--hdp-.html#UUID-9add7f98-e1ed-a220-6f23-1128ae38399c_id_HortonworksDataPlatformHDP-OrderedSteps", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Ordered Steps", 
"snippet" : "Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : " Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "un42--un42-/unravel-4-2/installation-guides/hortonworks-data-platform--hdp-/step-1--install-unravel-server-on-hdp.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host mach...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility On-premises HDP up to v2.6 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config. HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway. LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication. (Open signup by default.) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN’s “done dir” in HDFS YARN’s log aggregation directory in HDFS Spark event log directory in HDFS File sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. 2. Install the Unravel Server RPM on the Host Machine The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks as needed to be more selective. Get the Unravel Server RPM See instructions UPDATE_NEEDED_ ADD LINK TO RPM. Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. 3. Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel Web UI's configuration wizard. com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel Web UI's configuration wizard. com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL yarn.resourcemanager.webapp.address=http:\/\/example.localdomain :8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: Create a keytab for unravel for daemons that run as unravel \/etc\/security\/unravel.keytab Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/security\/keytabs\/hdfs.keytab Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): # echo \" \\\nexport HDFS_KEYTAB_PATH=\/etc\/security\/keytabs\/hdfs.keytab \\\nexport HDFS_KERBEROS_PRINCIPAL=unravel\/myhost.mydomain@MYREALM \\\n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh Add properties for Kerberos: # echo \" \\\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM \\ \ncom.unraveldata.kerberos.keytab.path=\/etc\/security\/unravel.keytab \\\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n If Ranger is Enabled: Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/spark-history hdfs read Spark event log hdfs:\/\/spark-history hdfs read Spark2 event log hdfs:\/\/mr-history\/done hdfs read MapReduce logs hdfs:\/\/app-logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information 4. Convert Your Unravel Installation to HDP Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. 5. Log into Unravel Web UI Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 6. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2 (Optional): Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/installation-guides/hortonworks-data-platform--hdp-/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/installation-guides/hortonworks-data-platform--hdp-/empty-or-missing-topic-4829.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "MapR", 
"url" : "un42--un42-/unravel-4-2/installation-guides/mapr.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ MapR", 
"snippet" : "This guide is compatible with MapR 5.1, 5.2 Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR install-mapr...", 
"body" : " This guide is compatible with MapR 5.1, 5.2 Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR install-mapr " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/installation-guides/mapr/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "un42--un42-/unravel-4-2/installation-guides/mapr/step-2--enable-additional-data-collection---instrumentation-for-mapr.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unra...", 
"body" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. 1. Enable Additional Instrumentation on Unravel Server's Host Substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_ HIVE_VERSION_ Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client nodes that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Before unravel_mapr_setup.sh wget unzip hive spark-shell Run the shell script unravel_mapr_setup.sh host1 # sudo \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Hive hook jar is installed under: \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2) are made to: \/opt\/mapr\/spark\/spark-2.0.1\/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-env.sh Once the files are present on edge host where Unravel rpm is installed, you can tar these changes\/additions up and put on other hosts, if that is more convenient than running the script. In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). 2. Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server: someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 3. Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> 4. Enable Additional Instrumentation on Other Hosts in the Cluster To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml Run the shell script unravel_mapr_setup.sh Copy the newly edited (in the previous step 4) yarn-site.xml to all nodes. Do a rolling-restart of HiveServer2 " }, 
{ "title" : "Upgrade Unravel Sensor on MapR", 
"url" : "un42--un42-/unravel-4-2/installation-guides/mapr/upgrade-unravel-sensor-on-mapr.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Upgrade Unravel Sensor on MapR", 
"snippet" : "HIGHLIGHTED UNRAVEL_HOST_IP SPARK_VERSION _X.Y.Z HIVE_VERSION_X.Y.Z Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/ # sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unrav...", 
"body" : " HIGHLIGHTED UNRAVEL_HOST_IP SPARK_VERSION _X.Y.Z HIVE_VERSION_X.Y.Z Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client tar scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm--4834.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm--4834/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm--4834/step-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"snippet" : "Note Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. Introduction This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects informa...", 
"body" : " Note Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. Introduction This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already running), do a \"setup\" procedure: follow the steps 3 to 5 in Hive Bootstrap and Unravel Hive Hook Sensor Setup. HIGHLIGHTED UNRAVEL_HOST_IP UNRAVEL_Hostname_FQDN_Internal_IP Hive Bootstrap and Unravel Hive Hook Sensor Setup: Add a one-time Unravel Hive Bootstrap into Qubole's control panel on the left-hand side in the \"Hive Bootstrap\" section. location_in_s3_where_unravel_jar_folder add jar s3n:\/\/{location_in_s3_where_unravel_jar_folder}\/unravel-hive-0.13.0-hook.jar;\n\nset com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR;\nset hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook;\nset hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook;\nset hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook;\nset hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook;\nset com.unraveldata.host={UNRAVEL_Hostname_FQDN_Internal_IP};\nset com.unraveldata.hive.hook.tcp=true; Determine the Hive version that Qubole uses, and use that value for HIVE_VERSION__X.Y.Z - target Hive version (e.g. 1.2.0) To determine the Hive version Qubole uses, see http:\/\/docs.qubole.com\/en\/latest\/faqs\/hive\/version-hive-qubole-provide.html HIVE_VERSION_ _X.Y.Z 1.2.0 0.13.0 On the master node of the Qubole Hadoop2\/Hive cluster, check that Unravel Server is reachable. Note Ensure the security group on the master\/slave allows TCP port #3000 and #4043 accessible. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt If the version information is not visible, adjust security groups and routing and try again. SSH Use curl # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_setup.sh > unravel_qubole_setup.sh Ensure wget Installation Directories Hive hook jar - \/usr\/local\/unravel_client Spark jar - \/usr\/local\/unravel-spark Unravel ES - \/usr\/local\/unravel_es On the master node, \/etc\/init.d\/unravel_es # yum install -y wget\n# chmod 755 unravel_qubole_setup.sh \n# sudo .\/unravel_qubole_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 Verify if the setup works in Qubole by invoking Analyze and execute following Hive test query: set hive.on.master=true ;\nselect count(*) from default_qubole_memetracker; " }, 
{ "title" : "Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm--4834/step-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"snippet" : "Introduction Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 4.2+. Highlighted text indicates where you must substitute ...", 
"body" : "Introduction Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 4.2+. Highlighted text indicates where you must substitute your particular values. \n HIGHLIGHTED \n UNRAVEL_HOST_IP Unravel Spark Instrumentation Into NEW Qubole Spark Cluster Configure Unravel s3 read-only credentials into the \/usr\/local\/unravel\/etc\/s3ro.properties Following is an example of s3ro.properties [default]\naws_access_key_id = {ACCESS_KEY_VALUE1}\naws_secret_access_key = {SECRET_KEY_VALUE1}\n\n[profile_name_2]\naws_access_key_id = {ACCESS_KEY_VALUE2}\naws_secret_access_key = {SECRET_KEY_VALUE2} Edit \/usr\/local\/unravel\/etc\/unravel.properties s3ro.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties\ncom.unraveldata.spark.s3.profilesToBuckets=<default>:<s3ro_bucket1>,<profile_name_2>:<s3r0_bucket2> Restart the Unravel ETL daemon(s. # sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Ensure ports 3000 4043 not Verify that port 3000 curl If the version information is not visible (request timeout), then adjust security groups, firewalls, etc. and try again. For VPCs, it might be necessary to add a route. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt Copy the Unravel Spark bootstrap file from Unravel Server to your Spark Qubole cluster, using curl # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_bootstrap.sh > unravel_qubole_bootstrap.sh Copy Unravel Spark bootstrap file to your s3:\/\/ Bootstrap_location_for_Spark Qubole cluster # aws s3 cp unravel_qubole_bootstrap.sh s3:\/\/{Bootstrap_location_for_Spark Qubole cluster}\/unravel_qubole_bootstrap.sh In Qubole’s Edit Cluster Setting \n do not unravel_qubole_bootstrap.sh In Qubole, scripts do not take input parameters. Therefore, Unravel bootstrap script takes all the required parameters from the Hadoop configuration. You can customize your Hadoop configuration through specific parameters within the Override Hadoop Configuration Variables unravel-boostrap Note: Separate parameters with commas. Add these settings to unravel-bootstrap unravel-bootstrap=UNRAVEL_SERVER=UNRAVEL_HOST_AND_PORT, SPARK_VER_XYZ={SPARK_VERSION_X.Y.Z} [,SPARK_APP_LOAD_MODE={APP_LOAD_MODE},WRAPPED_SCRIPT={SCRIPT}] where: \n SPARK_VERSION_ The parameters in square brackets [ ] above are optional. \n APP_LOAD_MODE Application Loading Modes \n SCRIPT unravel_qubole_bootstrap.sh. Check the bootstrap log The bootstrap log will contain detailed information about the setup progress and also any possible failures. The log can be found on each cluster node: \/media\/ephemeral0\/unravel\/tmp\/unravel_qubole_bootstrap.sh.out Confirm that the unravel_es Open an SSH Check that the unravel_es # ps -aux | grep unravel_es (Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 Obtain following essential resource metrics sensor file from Unravel Server. # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip Unzip the archive unravel-agent-pack-bin.zip PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS Obtain jar files and snippet script that should be added to the bootstrap action to start the unravel-es # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/snippets\/run-es.sh\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-emr-sensor.jar\n Ensure that unravel-emr-sensor.jar unravel-emr-sensor.jar PATH_TO_SENSOR_JARS Edit the run-es.sh UNRAVEL_HOST={UNRAVEL_HOST_IP}\nUNRAVEL_EMR_SENSOR_JAR={PATH_TO_SENSOR_JARS} Spark configuration can be provided in theQubole consoleat bootstrap or directly inside spark-defaults.conf spark.unravel.server.hostport {UNRAVEL_HOST_IP}:4043\nspark.driver.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y} Edit zeppelin-env.sh ZEPPELIN_JAVA_OPTS export ZEPPELIN_JAVA_OPTS=\"-javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=bootClassPath=config=driver,libs=spark-SPARK_VERSION\" The Zeppelin configuration is located at \/usr\/lib\/zeppelin\/conf\/zeppelin-env.sh Application Loading Modes for Spark Applications: OPS, DEV, and BATCH There are three modes in which Spark applications can be loaded into Unravel Web UI: OPTS - shows applications in the UI after the application is done. DEV - shows applications in the UI as soon as the first job of the Spark application completes. BATCH - loads applications for which the sensor was not enabled at the time the application has been run. You can specify the application load mode for the bootstrap script by setting SPARK_APP_LOAD_MODE OPS DEV SPARK_APP_LOAD_MODE OPS When to Use Which Mode Unravel recommends using OPS mode as the cluster-side for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false DEV mode is useful during Spark application development. This mode shows a Spark application as soon as the first Spark job of the application finishes. For long running applications, this functionality is useful, as the application is shown in the UI while the application is running. In addition, DEV mode shows applications even when the Spark event log is not being persisted to HDFS or S3. This is advantageous in situations like Spark on Mesos. In a Qubole cluster that is using OPS mode as the default, DEV mode can be obtained for individual Spark applications by overriding spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false BATCH mode is always on and does not interfere with application performance in any way since the loading is entirely outside the application execution path. For details see the next section. Loading Applications in Batch Mode In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is helpful to load all of the applications that have been run in the past, before Unravel Sensor was installed. To enable the batch mode, add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.eventlog.location=hdfs:\/\/NAMENODE_IP_PORT\/user\/spark\/applicationHistory\/ \n Note: Currently, only one event log location is supported. " }, 
{ "title" : "Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"url" : "un42--un42-/unravel-4-2/installation-guides/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm--4834/step-4--modify-amazon-emr-cluster-bootstrap-setup.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"snippet" : "Follow these steps only Introduction This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server. Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amaz...", 
"body" : " Follow these steps only Introduction This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server. \n Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amazon EMR cluster(s). \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients 2. Get the Bootstrap Script(s) To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below policy into the Policy Document: {\n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n { \n \"Sid\": \"getunraveldataclients3files\", \n \"Effect\": \"Allow\", \n \"Action\": [ \n \"s3:ListBucket\", \n \"s3:Get*\" \n ], \n \"Resource\": [ \n \"arn:aws:s3:::unraveldata-clients\/*\" \n ] \n } \n ]\n} Save it by clicking Apply Policy When you create a new Amazon EMR cluster, be sure to add a bootstrap action as shown in the IAM screenshot below. You need to copy and paste the full pathname of the bootstrap action (script) into the Script location Important Note: Do not For guidance on which script to use, see the table below. \n \n \n File \n S3 Bucket \n Local Directory \n Applies To \n \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive \n \n unravel_emr5_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n install_bin\/unraveldata-clients\/ \n Amazon EMR 5.x Hive 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s) Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Follow the steps below to set up an existing cluster. Identify the UNRAVEL_HOST_IP Download s3:\/\/unraveldata-clients\/unravel_emr_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_setup.sh \/tmp hadoop Open an SSH session to the cluster's master node (ssh as user hadoop hadoop # cd \/tmp \n# aws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \n# chmod +x unravel_emr_setup.sh \n# .\/unravel_emr_setup.sh --unravel-server {UNRAVEL_HOST_IP}:3000 To uninstall Hive instrumentation on an Amazon EMR cluster (perhaps because you want to upgrade the instrumentation), you simply run the same install script again with the uninstall # cd \/tmp \n# aws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \n# chmod +x unravel_emr_setup.sh \n# .\/unravel_emr_setup.sh --uninstall Transient Amazon EMR Clusters Hive Applications: This is similar to the previous section on integrating an existing cluster except the script used as a bootstrap step is one of the following files: \n \n \n \n File \n \n S3 Bucket \n \n Local Directory \n \n Applies To \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive \n \n unravel_emr5_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 5.x Hive Spark Applications: The Unravel Server does not load data from a Spark cluster until the cluster is instrumented. Follow the steps below to set up an existing cluster Identify the UNRAVEL_HOST_IP Download s3:\/\/unraveldata-clients\/unravel_emr_spark_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_spark_setup.sh \/tmp ec2-user Open an ssh ssh user ec2-user ec2- user # cd \/tmp \n# sudo .\/unravel_emr_spark_setup.sh --unravel-server {UNRAVEL_HOST_IP}:3000 --client To use the Spark driver in cluster mode substitute \"– cluster client " }, 
{ "title" : "Unravel for Azure HDInsight clusters", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters", 
"snippet" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster. Unravel VM vs Unravel...", 
"body" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster. Unravel VM vs Unravel App: To monitor multiple HDInsight clusters on the same virtual network, Unravel VM is a preferable way. On the other hand, to quickly try out Unravel for your on demand HDInsight cluster, you can add the Unravel app during the cluster creation or add the app anytime after a cluster is created. Unravel app resides on the edge node of the target HDInsight cluster, and it can monitor one cluster's activities. " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters.html#UUID-55285332-3fb4-39e8-501c-f479800d98ee_id_UnravelforAzureHDInsightclusters-InstallationGuideforUnravelVM", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Templat...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions Page: Step 6: Updating Unravel Installation " }, 
{ "title" : "Installation Guide for Unravel App", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters.html#UUID-55285332-3fb4-39e8-501c-f479800d98ee_id_UnravelforAzureHDInsightclusters-InstallationGuideforUnravelApp", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel App", 
"snippet" : "Page: Install Unravel HDInsight app...", 
"body" : " Page: Install Unravel HDInsight app " }, 
{ "title" : "Install Unravel HDInsight app", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/install-unravel-hdinsight-app.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Install Unravel HDInsight app", 
"snippet" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to inst...", 
"body" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to install Unravel HDInsight app on a fresh Spark 2.1 cluster 1. Launch a Spark 2.1 Cluster Login to the Azure port. Click or choose HDInsight service. Create a new cluster; choosing Spark as the cluster type version Next. 2. Setup storage account configuration for the cluster Either create a new storage account or use existing one. Fill in the storage account information for the spark cluster. 3. Find Unravel app on search box or available app listing Enter UNRAVEL Click OK Create After you accepted the \"Terms of Use\" and \"Privacy Policy\" click Next 4. Review the summary and click Create to launch the cluster + Unravel app. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. 5. Access to Unravel app user interface After Unravel app and the spark2 cluster is successfully launched, go to HDinsight service, look for the spark2 cluster, and click on it. Click on Application In most cases, the Unravel HDInsight app user interface is in the following format https:\/\/<clusterName>-unr.apps.azurehdinsight.net\/ 6. Login to the Unravel app Start your browser and navigate to the Unravel app webpage URL https:\/\/clusterName-apps.azurehdinsight.net. The default admin login credential is admin unraveldata. Step 7. Unravel Dashboard When logging into Unravel, you will see the Dashboard. See the User Guide Step 8. Unravel daemons to the Unravel edge node, and check the unravel daemons' process status. ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Step 9. Licensing and support By default Unravel app doesn't contains any license keys, and runs without any issue during the inital 30 days trial period. To continue using Unravel app and technical support, contact our sales. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Step 10. Getting started to use Unravel Please read the Unravel User Guide Getting Started User Guide Step 11. Unravel API (special note for Unravel app) Unravel provides REST api to perform some operations. To try the api, click on the API tab on the dashboard An API page with available api command options are displayed and explained. You can try the API by clicking \"Try it out\" → Execute buttons; it will display the corresponding curl From the Unravel user interface, trying out the api will always has \"TypeError: Failed to fetch\". Because the generated curl command is not using https. Copy the generated curl commands and modify it to include default user credential and using https protocol. ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n The api output will be in JSON format shown below and the long numeric string displayed is the epoch time {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Templat...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions Page: Step 6: Updating Unravel Installation " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-edcd9be4-e3c2-da3e-f6c2-9c083e537e86", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2.X on Azure HDInsight Cluster. Workflow Summary Setup Unravel VM . Install the Unravel Server RPM on the VM. Start Unravel daemons Log into Unravel Web UI Requirements Checklist Platform Compatibility Azure HDinsight 3.3 - 3.6 Hadoop 1...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2.X on Azure HDInsight Cluster. \n Workflow Summary Setup Unravel VM . Install the Unravel Server RPM on the VM. Start Unravel daemons Log into Unravel Web UI Requirements Checklist Platform Compatibility Azure HDinsight 3.3 - 3.6 Hadoop 1.x - 2.x Kerberos (Windows AD) Hive 1.2 Spark 1.6, 2.0, 2.1 Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed \n SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH Open signup or LDAP for Unravel Web UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum OS Disk: \/ Data Disk: \/srv Network Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) HDFS ports open from Hadoop cluster to Unravel Server(s) Hive MetaStore DB port open to Unravel Server(s) for partition reporting For Oozie, port 11000 open to Unravel Server(s) \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Setup Unravel VM Setup Unravel VM on the same VNET and subnet of the target HDInsight cluster Provision a VM. VM size: Standard_E8s_v3 OS: centos7.3 or RHEL 7.4 image Publisher: OpenLogic Setup the VM on the same VNET and Subnet of the HDInsight cluster Start ntpd Security Group Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the head node of each HDInsight cluster to Unravel Server. Add inbound security policy allow ssh and 443 access to the unravel node. The default security policy already allow all access within the VNET. Default rule start with 6500. Configure the environment at first login. Disable selinux # sudo setenforce Permissive\n Edit the file to make sure the setting persists after reboot, be sure SELINUX=permissive. # vi \/etc\/selinux\/config\n Install libaio.x86_64. # sudo yum -y install libaio.x86_64 Install lzop. # sudo yum install lzop.x86_64 Disable the local Firewall. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L 2. Install the Unravel Server RPM on the VM The precise RPM filename will vary. The version has the structure \n x.y.b \n b \n x.y Replace the asterisks as needed to be more selective. Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM See instructions UPDATE_NEEDED_ADD LINK TO RPM. Install the Unravel Server RPM. # sudo rpm -U unravel-4.2.7-Azure-latest.rpm The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password is stored in \/root\/unravel.install.include Grant Access to Unravel Server. By Default Public IP should be assigned to the Unravel VM node. Create a security policy that allows ssh Use of either sshkey password ssh Restriction Do not make Unravel Server UI accessible on the public Internet because doing so would violate your licensing terms. 3. Start Unravel daemons and log into Unravel UI The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.ext.sh unravel.properties Open an SSH Session to Unravel Server Replace somefile.pem # ssh -i {somefile.pem} sshuser@$UNRAVEL_IP Set Correct Permissions on the Unravel Configuration Directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh (Required) # echo \"\nexport CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/* \\\n\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Modify unravel.properties. (Optional) The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust other values in unravel.properties \n \n \n Property \n Description \n Required by HDinsight cluster \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n Company_and_org \n \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n \n srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n 26 \n \n \n com.unraveldata.login.admins \n Unravel UI admin \n \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n Optional for Spark when HDinsight using blob storage storage account name for the HDinsight cluster \n using blob \n \n fs.azure.account.key. STORAGEACCOUNT \n \n \n com.unraveldat \n Primary storage account key \n using blob \n Ondaq2aYMpJf8pCdvtFJ\/zARJLhFr4Vf94PPJvMP1EsoFzBKp \n \n \n com.unraveldat a.hdinsight \n Optional for Spark when HDinsight using blob storage Storage account name for the HDinsight cluster (same as account-name-1 \n using blob \n \n fs.azure.account.key. STORAGEACCOUNT \n \n \n com.unraveldat a.hdinsight \n Secondary storage account key \n using blob \n aL3MFZ\/5hP4k0LxA+tn5\/NM6EkM1AZkFZzKmWjgEMqe0o6F \n \n \n com.unraveldat a.adl.accountFQDN \n The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net \n using Data Lake \n \n datalake0001.azuredatalakestore.net \n \n \n com.unraveldat a.adl.clientld \n An application ID. An application registration has to be created in the Azure Active Directory \n using Data Lake \n 5d19877f-3eb5-413a-9a41-7ae8a0048cfk \n \n \n com.unraveldat a.adl.clientKey \n An application access key which can be created after registering an application \n using Data Lake \n 6FMzo61+cKIRPFZRxzUxiLSuWc5YEsdZzYbtU5rMyUg= \n \n \n com.unraveldat a.adl.accessTokenEndpoint \n The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal \n using Data Lake \n \n https:\/\/login.microsoftonline.com\/bc745a0d-f282-4e99-b95f-1ecb477a209g\/oauth2\/token \n \n \n com.unraveldat a.adl.clientRootPath \n It is the path in the Data lake store where the target cluster has been given access. \n using Data Lake \n \/clusters\/CLUSTERNAME \n \n \n com.unraveldat a.ext.kafka.clusters \n Name of kafka cluster. The display name show on the Unravel UI to define kafka cluster. Other Unravel kafka properties depends on this name CLUSTERNAME \n \n kafka \n udkafka \n \n \n com.unraveldat a.ext.kafka.CLUSTERNAME.bootstrap_servers \n Kafka cluster bootstrap server and port (usually are two worker nodes) \n \n kafka \n wn0-UDKAFK:9092,wn1-UDKAFK:9092 \n \n \n com.unraveldat a.ext.kafka. \n Define kafka cluster broker servers names \n \n kafka \n broker-1,broker-2,broker-3 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 host \n \n kafka \n wn0-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 port \n \n kafka \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 host \n \n kafka \n wn1-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 port \n \n kafka \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 host \n \n kafka \n wn2-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 port \n \n kafka \n 9999 For HDinsight cluster using blob storage Please find and append the following properties in the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.unravelstorage03.blob.core.windows.net\ncom.unraveldata.hdinsight.primary-access-key=Ondaq2aYMpJf8pCdvtFJ\/zAR1LhFr4Vf94PPJvMP1DsoFzBKp\/\/4DVQi+hcL5+XsW2XFNI7p\ncom.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.unravelstorage03.blob.core.windows.net\ncom.unraveldata.hdinsight.secondary-access-key=aL3MFZ\/5hP4k0LxA+tn5\/NM6EkM1AZkFZzCmWjgEMqe0o6F33gJZxwfQABLaynxpatWY71 You will need to update the above blob Storage account name and access key For HDinsight cluster using Data Lake storage Please find and append the following properties in the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.adl.accountFQDN=datalake0001.azuredatalakestore.net\ncom.unraveldata.adl.clientId=5d19877f-3eb5-413k-9a41-7ae8a0048cfa\ncom.unraveldata.adl.clientKey=5FMzo61+cKIRPFZRxzUxiLSuWc5YEsdZzYbtU5r\ncom.unraveldata.adl.accessTokenEndpoint=https:\/\/login.microsoftonline.com\/bc745a0d-f282-4e99-b95f-1ecb477a209e\/oauth2\/token\ncom.unraveldata.adl.clientRootPath=\/clusters\/{CLUSTERNAME} You will need to update the above data like account name, clientId, clientKey, accessTokenEndpoint and clientRootPath Find the Data Lake storage account from HDinsight cluste Properties Find the Data Late clientRootPath \n HDinsight cluste Properties | Storage Accounts Find the user (Azure principal) that has access to the Data Lake storage path or folder from Azure portal. \n HDinsight cluste Properties | Storage Accounts Data Lake Data Explorer \n \n Access. Finding the clientId, clientKey, and accessTokenEndpoint On Azure portal → click Azure Active Directory | App Registrations Click the account principal and look for the Application ID and that is the client ID. Click the Keys key name expiration date client key SAVE Go back to App Registration screen and click Endpoints OAUTH 2.0 TOKEN ENDPOINT. \n \n Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" 4. Log into Unravel Web UI. Create a SSH # ssh -i somefile.pem sshuser@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Please proceed to Step2 For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "Step 2: Use Script Action to configure HDinsight cluster for Unravel", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-f6be08c2-4cee-d78f-4290-36f54cc9c0c3", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 2: Use Script Action to configure HDinsight cluster for Unravel", 
"snippet" : "1. Create a new Spark HDInsight cluster with Unravel's Script Action script. Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access, download the following files into your private blob store...", 
"body" : "1. Create a new Spark HDInsight cluster with Unravel's Script Action script. Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access, download the following files into your private blob store that are accessible by your secure HDInsight cluster. the unravel_hdi_spark_bootstrap_3.0.sh \n http:\/\/central.maven.org\/maven2\/org\/anarres\/lzo\/lzo-core\/1.0.5\/lzo-core-1.0.5.jar You use the Azure Portal to create a Spark 2.1 HDInsight Cluster. In Summary Confirm configuration On the Advanced settings When entering Virtual Network settings you must enter the VNET and Subnet where Unravel VM is located. Defining Script Actions On the Advanced setting screen click on the arrow to the right of script actions to bring up the Submit script action screen: Select Script type Name The Bash script URI uses the github URL https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh The Unravel script for Script Action is located on the Unravel VM \/usr\/local\/unravel\/webapps\/ROOT\/hh\/unraveldata-clients\/unravel_hdi_bootstrap.sh above instruction use github to store the Unravel script; and Azure script action script can also be uploaded to Azure blob storage with https URI Check the boxes “Head”, “Worker” and \"Edge\" ; so this custom script action will apply on both Head and worker nodes. In the parameter field enter the string \" --unravel-server\" follow by unravel node’s private IP address:port and spark or hive version ( see below); then click Create \n --unravel-server 10.10.1.10:3000 --spark-version 2.1.0 Azure portal will validate the script; if the script is validated you will be returned to you initial screen. validations it returns to the same screen and click Select. Now return back to Step 5 and Script Action is shown Configured; then click Next Other supported script arguments are --spark-version and --hive-version accepting the X.Y.Z format of the system version - eg. --spark-version 2.1.0 --hive-version 1.2.0 Summary configuration is displayed on Step 6; click Create The HDInsight cluster is shown on Azure dashboard with the status of Deploying. The Cluster creation process takes approximately 20-25 minutes. 2. Checking Script Action process Once the HDInsight cluster creation completed: check the cluster information from Azure dashboard Find out the Ambari URL and ssh access to first head node. And inspect script action if running success or not. Ambari URL = https:\/\/<Cluster_Name>. azurehdinsight.net ssh access to first head node = ssh sshuser@<Cluster_Name>- ssh.azurehdinsight.net Check Script Actions, by clicking the “Script actions” icon on the above window; and should be seeing the Script Action run “Succeeded” Login to Ambari UI and check the Ambari tasks; and there should be three main tasks and corresponding log files Customscript Action task Unravel call to stop Spark service Unravel call to start Spark service 3. Run Unravel script action on existing HDInsight cluster Script actions can be invoked after the cluster is up and running. In Azure portal of the cluster page select “Script actions”, “Submit new”. Enter the script action details. After the settings have been entered, and the settings saved the action will be run on the cluster nodes. Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, just submit a new script action script. Choose Script type: \"- Custom\" Name: Enter a name for this script Bash script URI: https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh If your HDInsight cluster has no public internet access please download the above script action script and upload to the blob storage that the HDInsight cluster has access to it. Node type: For Spark, and Hadoop HDInsight cluster, check \"Head\", \"Worker\" and \"Edge\" node if you have configured with edge node. For Kafka HDInsight cluster, the script action only applies to \"Head\" node. Script action also been applied to HDInsight cluster using Azure 1.0 command line, the following is an example of using Azure 1.0 CLI Install Azure 1.0 CLI use docker container is the quickest way described in this Azure online doc azure hdinsight script-action create estspk2rh74 -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_a.sh -p 'unravel-server 10.10.1.15:3000 --spark-version 2.1.0' -t \"headnode;workernode;edgenode\" The command structure is below azure hdinsight script-action create <CLUSTERNAME> -g <RESOURCEGROUP> -n <NAME> -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_a.sh -g = Resource Group name -n = Name of this script action task -u = script path -p = paramaters -t = node types \n Example screen capture is below The result of running this script action script via Azure 1.0 command line can also be checked from Azure portal Submitting script action for Kafka cluster azure hdinsight script-action create $CLUSTER_NAME -g $RESOURCE_GROUP -n $NAME_FOR_TASK -u $PATH_FOR_KAFKA_SCRIPT_ACTION -p '$UNRAVEL_VM_IP:3000' -t \"headnode\" For no internet access script action for kafka cluster please see this page under the section \"Using Script Actions with No Internet access cluster\" \n Example of screen capture shown below " }, 
{ "title" : "Step 3: Create Unravel VM using ARM template", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-5c9afdf0-90d9-ee07-e284-5765d91cc46b", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 3: Create Unravel VM using ARM template", 
"snippet" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or cr...", 
"body" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or create your own template files. This example template creates an Azure \"standard E8s V3\" VM in the existing VNET and subnet, and it adds a data disk on the VM for \"\/ srv You need You can change data disk size; it is currently set to 500G. If you change the disk size, update unravel-setup.sh For Centos 7.3 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.parameters.json For Redhat 7.4 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.parameters.json The parameter files have to be modified to fit your Azure environment; you have to fill in the blob storage account, access keys or ADLS account information. See here To download and install the Unravel RPM, you must download the ARM template which is embedded in the Azure Extension script. You can obtain the extension script here: Extension Script for CentOS 7.3 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/unravel-setup.sh Extension Script for CentOS 7.4 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/unravel-setup.sh The custom extension script fixes most of the basic unravel configuration; however, you must manually edit \/usr\/local\/unravel\/etc\/unravel.properties See Step 1 unravel.properties here If Unravel VM is deploying in a closed environment, download the unravel rpm file from UPDATE_NEEDED_ADD LINK TO RPM.4.2.7. unravel-setup.sh Below is the content of this extension script # Download unravel rpm\n\/usr\/bin\/wget http:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.2.7-Azure-latest.rpm\n\n\/usr\/bin\/sleep 5\n\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=1p6ed4s492012j5rb242rq3x3w702z1l455g501z2z4o2o4lo675555u3h\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start Download the ARM template and parameter JSON files into your configured Azure CLI workstation Azure CLI to deploy Unravel VM using this template and parameters json file # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json To Validate template before deployment # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually start unravel daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-6ab134c5-9a25-66fb-6c4f-d30fe4809892", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"snippet" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template will also run Unravel's Script Actions script to setup unravel sensors and configuration on header, worker and edge nodes. Th...", 
"body" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template will also run Unravel's Script Actions script to setup unravel sensors and configuration on header, worker and edge nodes. This ARM template uses the existing VNET, Subnet and Storage Account on the same resource group. You will need to update those values in parameter, variables to reflect your Azure environment. A Spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use Spark edge node for accessing the cluster, testing your client applications, and hosting your client applications. You will need to deploy unravel VM and update the script action parameters h--unravel-server UNRAVEL_IP:3000 --spark-version 2.1.0 You can change the VM size of header, worker and edge nodes; and currently they are all using VM size of \"Standard_D3_v2\" \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access. Please download the no dependency script below into your blob store that accessible by the HDInsight cluster. \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_nodep.sh After modify this template please validate it before applying and HDInsight cluster creation takes about 15 - 25 minutes Download the ARM template and parameter JSON files into your configured Azure CLI workstation. Validate template before deployment. az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Use the Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Apply Unravel Script Actions scripts manually on an existing Spark2 cluster Optionally, if you already have an existing spark2 cluster, you can apply Unravel's spark2 script via Azure portal. From Azure portal, click the resource of the target spark2 cluster under your resource group and click Script actions On script actions dialog box: Click Submit new Select script type \"- Custom\" Enter a Name for this script e.g. \" unravel-spark-setup. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Input parameters: --unravel-server UNRAVEL_VM_IP_address Check the box Persist this script action to rerun ... Click Create You can upload the unravel_hdi_bootstrap.sh Script Action will validate the script and then process it. Monitor the Azure portal until Scriptactions is completed. Then login to Ambari and check the Ambari task status. The checkbox for \"Persist this script action to rerun when new nodes are added ..\" will only affect new worker nodes when scale up the worker nodes. The Unravel script action script will not automaticallt apply because of checking this check box for newly added edge node from ARM template. Install additional edge node on existing HDinsight cluster ARM template for install edge node with Unravel Script Action only \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json ARM template to install the edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example) In this example, an edge node will be created first. Next, it runs emptynode-setup.sh unravel_hdi_bootstrap.sh \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires change in scriptActionUri path and application name in variables and also parameters for cluster name. Please adjust the ARM templates for your setup and validate it before using. " }, 
{ "title" : "Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-72ecd2eb-4556-4018-57cb-8a4594b5ce23", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"snippet" : "Unravel script action requires Internet Access to download the script action No Internet access cluster This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. It runs Unravel's Script Actions script to detect the Kafka cluster bootstrap server...", 
"body" : " Unravel script action requires Internet Access to download the script action No Internet access cluster This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. It runs Unravel's Script Actions script to detect the Kafka cluster bootstrap servers and jmx broker nodes; and update this information into the unravel.properties The Unravel script actions script for Kafka is specifically for Kafka cluster and is needed to run on header node only. This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. The worker nodes in this Kafka cluster uses two data disks per node. You will need to update those values (VNET, Subnet, Storage Account, Cluster name, ...etc ) in parameter, variables to reflect your Azure environment. You need to deploy unravel VM and update the script action parameters UNRAVEL_IP You can change the VM size of header, worker nodes; currently they are all using VM size of \"Standard_D3_v2\" After modifying this template please validate it before applying. The HDInsight cluster creation takes about 15 - 25 minutes Using Script Actions With Internet Access Download the ARM template and parameter JSON files into your configured Azure CLI workstation. To Validate template before deployment az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Use the Azure CLI to deploy Kafka cluster using the template and parameters JSON file. az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json After the Kafka cluster is successfully created, the unravel script actions script should apply the Kafka configuration on Unravel's VM \/user\/local\/unravel\/etc\/unravel.properties The following is the sample of lines appended to unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 And once unravel.properties unravel_km \/etc\/init.d\/unravel_km restart Unravel kafka ScriptAction scripts manually on existing kafka cluster Optionally, if you already have an existing kafka cluster, you can apply Unravel's kafka script via Azure portal. From Azure portal, click the resource of the target Kafka cluster under your resource group and click \"Script actions\" On script actions windows: Click Submit new Select script type, e.g., \"- Custom\" Enter a Name for this script e.g. \"unravel-kafka-setup\" Enter the script path from above e.g. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Input parameters: UNRAVEL_VM_IP_address:3000 Check the box \"Persist this script action to rerun ...\" you can upload the unravel_hdi_kafka_bootstrap.sh Script Action will validate the script and then process it. Monitor the Azure portal Script until it completes. Then login to Ambari of the cluster and check the Ambari task status. Login to unravel VM and restart the unravel Kafka monitor daemon, unravel_km \/etc\/init.d\/unravel_km restart Using Script Actions with No Internet access cluster For Kafka cluster script actions, it requires to get the jq package from azure content at ubuntu.com So for secured kafka cluster, you can access to a public VM (ubuntu) and do # apt-get install -d jq It will download the package of jq and dependency packages libonig2 on \/var\/cashe\/apt\/archives \n tar scp you can also get this jq and libonig2 deb files from unravel public blob store, you need to download it to your workstation then upload to hn0 and hn1 \n https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/jq_and_libonig2.tar Extract the jq and libonig2 files on the \/tmp folder of hn0 and hn1 # cd \/tmp\/\n# tar -xvf jq_and_libonig2.tar\n# sudo dpkg -i libonig2_5.9.6-1_amd64.deb jq_1.5+dfsg-1_amd64.deb Download the new kafka script actions https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap_nodep.sh The above kafka script action Submit the script actions \n Optional step script actions Edit the downloaded unravel_hdo_kafka_bootstrap_nodep.sh. ######################################################################################################\n# #\n# Get the jq from your own blob content #\n# You need to download and jq and libonig2 deb packages from #\n# https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/jq_and_libonig2.tar #\n# update and uncomment the following lines of script for jq install from your blob store #\n# #\n######################################################################################################\n#\n# wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar\n# tar -xvf \/tmp\/jq_and_libonig2.tar -C \/tmp\n# sudo dpkg -i \/tmp\/libonig2_5.9.6-1_amd64.deb\n# sudo dpkg -i \/tmp\/jq_1.5+dfsg-1_amd64.deb\n#\n#######################################################################################################\n# #\n# End of installing jq, uncomment above for jq installation from blob store #\n# #\n####################################################################################################### Uncomment and update the the https path for your jq_and_libonig2.tar wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar Uncomment the four lines below: wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar\ntar -xvf \/tmp\/jq_and_libonig2.tar -C \/tmp\nsudo dpkg -i \/tmp\/libonig2_5.9.6-1_amd64.deb\nsudo dpkg -i \/tmp\/jq_1.5+dfsg-1_amd64.deb You can now submit script actions must jq_and_libonig2.tar Script action can be submitted from Azure portal or using Azure Command Line 1.0 Submitting script action for kafka cluster azure hdinsight script-action create $CLUSTER_NAME -g $RESOURCE_GROUP -n $NAME_FOR_TASK -u $PATH_FOR_KAFKA_SCRIPT_ACTION -p '$UNRAVEL_VM_IP:3000' -t \"headnode\" -g = Resource Group name -n = Name of this script action task -u = script path -p = parameters -t = node types \n Example of screen capture shown below Testing your Kafka configuration You can follow the Azure documentation Download this two jar files from unravel public blob into your HDInsight cluster wget https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/kafka-streaming-1.0-SNAPSHOT.jar\nwget https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/kafka-producer-consumer-1.0-SNAPSHOT.jar Ssh into your kafka cluster headnode and running the following commands to define the variables CLUSTERNAME, KAFKAZKHOSTS, KAFKABROKERS. export CLUSTERNAME=<your_kafka_clustername>\n\nexport KAFKAZKHOSTS=`curl -sS -u admin:<ambari_password> -G https:\/\/$CLUSTERNAME.azurehdinsight.net\/api\/v1\/clusters\/$CLUSTERNAME\/services\/ZOOKEEPER\/components\/ZOOKEEPER_SERVER | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2`\n\nexport KAFKABROKERS=`curl -sS -u admin:<ambari_password> -G https:\/\/$CLUSTERNAME.azurehdinsight.net\/api\/v1\/clusters\/$CLUSTERNAME\/services\/KAFKA\/components\/KAFKA_BROKER | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2` Create the kafka topics by running the following commands on kafka headnode \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic test --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic wordcounts --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic RekeyedIntermediateTopic --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic wordcount-example-Counts-changelog --zookeeper $KAFKAZKHOSTS Verify created kafka topics \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --list --zookeeper $KAFKAZKHOSTS Start the streaming application as background java -jar kafka-streaming-1.0-SNAPSHOT.jar $KAFKABROKERS $KAFKAZKHOSTS & Send records to test topics java -jar kafka-producer-consumer-1.0-SNAPSHOT.jar producer $KAFKABROKERS at this moment observe the unravel UI → Operation page → Chart → Kafka; you should observe some metric data is being collected. Once the producer completes, use this command to view information stored in wordcounts topic. \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-console-consumer.sh --bootstrap-server $KAFKABROKERS --topic wordcounts --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --from-beginning Kill this process or command after a while, then observe unravel UI again " }, 
{ "title" : "Step 6: Updating Unravel Installation", 
"url" : "un42--un42-/unravel-4-2/installation-guides/unravel-for-azure-hdinsight-clusters/installation-guide-for-unravel-vm.html#UUID-88afcc45-0c4c-bd7c-14f4-904faa7b1d62", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 6: Updating Unravel Installation", 
"snippet" : "From time to time, Unravel Data will release new package with new features and improvement for customers to download and update their existing installations. Updating Unravel VM on Azure is simple; just download the new unravel RPM package and install it. Text with brackets ( { } ) indicate where yo...", 
"body" : "From time to time, Unravel Data will release new package with new features and improvement for customers to download and update their existing installations. Updating Unravel VM on Azure is simple; just download the new unravel RPM package and install it. Text with brackets ( { } ) indicate where you must substitute your particular values for the text including the brackets. 1. Download unravel rpm file # wget http:\/\/preview.unraveldata.com\/img\/{NEW-RPM-FILENAME.rpm} 2. Install new rpm file on unravel VM # rpm -U {NEW-RPM-FILENAME.rpm} Update unravel installation should not affect the connected HDinsight cluster operation and can be done at any time. However some unravel rpm updates require the sensor upgrade and this will require you to re-submit the unravel script action script to head, worker, and edge nodes. " }, 
{ "title" : "User Guide", 
"url" : "un42--un42-/unravel-4-2/user-guide.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started The Operations Tab The Applications Tab The Data Tab Setting Up Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Perf...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started The Operations Tab The Applications Tab The Data Tab Setting Up Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "un42--un42-/unravel-4-2/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "un42--un42-/unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "Case 1: How to Search for Applications and Optimize\/Tune a Hive Application Case 2: How to \"Root\" Cause\" Issues with a Workflow that Missed its SLA Case 3: How to Debug Failed Applications Case 4: How to Review Spark Applications and Identify Areas for Performance Improvements...", 
"body" : "[video] Case 1: How to Search for Applications and Optimize\/Tune a Hive Application Case 2: How to \"Root\" Cause\" Issues with a Workflow that Missed its SLA Case 3: How to Debug Failed Applications Case 4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "un42--un42-/unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin " }, 
{ "title" : "Setting Up Access to Big Data Components", 
"url" : "un42--un42-/unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-SettingUpAccesstoBigDataComponents", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Setting Up Access to Big Data Components", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"url" : "un42--un42-/unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-CreatingUsersandSettingUpEmailSMTPLDAPKerberos", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "The Operations Tab", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-operations-tab.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Operations Tab", 
"snippet" : "The Operations Tab provides synopsis of your clusters and its activities, and the ability to generate reports on the cluster activities. It has three sub-tabs: Dashboard, Charts and Reports. Typically you can configure the date range time period cluster(s)...", 
"body" : "The Operations Tab provides synopsis of your clusters and its activities, and the ability to generate reports on the cluster activities. It has three sub-tabs: Dashboard, Charts and Reports. Typically you can configure the date range time period cluster(s) " }, 
{ "title" : "Dashboard", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Dashboard", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, workflows, application inefficiencies, etc. By default the Dashboard is configured hourly all clusters 7 days Dashboard Tiles Fi...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, workflows, application inefficiencies, etc. By default the Dashboard is configured hourly all clusters 7 days Dashboard Tiles Finished and Running YARN applications, and Resources The first two tiles display the number of yarn applications and their states, and the third tile cluster resource usage. Clicking on the Open Section Finished YARN Application Detail This view is equivalent to Applications Finding Applications Finding Applications Running YARN Application Detail Displays the chart Operations Charts Jobs Resources Detail Displays the chart Operations Charts Resources Workflow Missing SLA Each row in this tile lists the workflow missing SLA and pertinent details including some KPIs. Clicking on the application name brings up the workflow manager. Inefficient Applications Its three sub-tabs, HIVE, MapReduce, and Spark Jobs each contain a list of inefficiencies that have occurred for jobs of that type. Click on the sub-tab to change the application type (1) or the event name to bring up the list of applications that experienced the event (2). Clicking on the application name brings up the application's information (3). Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. Selecting the event\/alerts brings up a Cluster Resource view ( Charts Resources Cluster View in Auto Actions add a new Auto Action or Alert " }, 
{ "title" : "Charts", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Charts", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Charts", 
"snippet" : "To view charts, click Operations Charts One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the various applications running in the clusters. Through Charts For example, Unravel can pinpoint the applications causing a sudden ...", 
"body" : "To view charts, click Operations Charts One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the various applications running in the clusters. Through Charts For example, Unravel can pinpoint the applications causing a sudden a spike in the total VCores or memory MB usage. This allows you to easily you drill down into these applications to understand their behavior. Whenever possible, Unravel provides recommendations and insights Common Features of Charts\/Graphs See the Resources chart below for an example of the following attributes. If is the graph is expandable it has Show More Most graphs\/charts allow you to chose how to display them (1). Typically your options are line, stacked area or grouped bar chart. When available, a chart glyph is displayed in the upper right corner of the graph\/chart. Click on the chart type to change the display. The first four charts: Resources, Jobs, Nodes and Impala: initially display their graphs in line mode, and, have relevant applications or queries listed at the bottom. When present, clicking on the application's name brings up its details. On the Resources, Jobs, and Impala tab you can chose how to Group by Placing your cursor within a chart\/graph brings up the information for that point in time in a text format (3). Clicking on the graph shows applications\/queries running at that point in time on the cluster. Click on the application name to bring up its details (4). Resources Vcores and memory MB are graphed by available\/allocated and allocated across app\/user\/queue. You can group the graphs by App Type User Queue Jobs Graphs the accepted, running, new, and pending jobs as applicable. You can Group State App Type User Queue Nodes This chart graphs Nodes by Total Active Lost Unhealthy Decommissioned Rebooted the Healthy Active nodes, and Active: the Unhealthy Active nodes. Unhealthy: Impala Graphs memory MB consumption and the number of queries. Queries can Grouped User Queue Services Graphs the system parameters. Use the Host Narrow Wide Kafka Lists all the configured Kafka clusters. Selecting the cluster name brings up the cluster view " }, 
{ "title" : "Reports", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Reports", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Reports", 
"snippet" : "To view reports, click Operations Reports This tab allows you to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Chargeback You can generate ChargeBack Application Type, User Queue Use the Group by User Application Type VCore\/Hour Memory MB\/Hour Update ...", 
"body" : "To view reports, click Operations Reports This tab allows you to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Chargeback You can generate ChargeBack Application Type, User Queue Use the Group by User Application Type VCore\/Hour Memory MB\/Hour Update Report The report consists of three sections: pie charts showing the top results, the chargeback report and a complete listing of the YARN applications contributing to the underlying report. The chargeback report (6) and applications (5) can be sorted in ascending or descending order on various reported metrics. Cluster Summary The Cluster Summary Applications User Queue VCore Memory Applications VCore Seconds Cluster Compare To compare clusters chose the initial Time Range Compare With Range Group By User Queue Any deviation in metrics across the time ranges is highlighted (4). A green red Time Compare With Group By " }, 
{ "title" : "The Applications Tab", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab", 
"snippet" : "The Applications cron Map-Reduce Hive (on Map-Reduce) Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL Impala Pig Cascading Kafka Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, sche...", 
"body" : "The Applications cron Map-Reduce Hive (on Map-Reduce) Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL Impala Pig Cascading Kafka Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. It takes significant expertise and effort to get to the root cause(s) of an application's problems. Unravel's Intelligence Engine provides insights into your application's run to help resolve it's problems\/inefficiencies. These insights are called events Event Panel Examples The Applications Tab has three views you can chose using Show , Applications and Templates, Workflows. " }, 
{ "title" : "Common Features across Tiles, Panels, Tabs, etc.", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-APMCommonCommonFeaturesacrossTilesPanelsTabsetc", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Common Features across Tiles, Panels, Tabs, etc.", 
"snippet" : "When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descendi...", 
"body" : " When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( Clicking on the app name\/id usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. The application status is color coded: There is an Auto Actions column ( When more information glyph ( A block glyph ( Graphs Frequently a pull-down menu, i.e., If you can chose how to display the graph, i.e., area, line, the options are displayed in the upper right corner ( Hovering within the chart\/graph or on a diagram item, i.e., line, execution, brings up the information for that point\/item in time in a text format ( " }, 
{ "title" : "Applications", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Applications", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Applications", 
"snippet" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filtering by app name app type status queue user cluster duration number of events By time period, Initially search results are ordered by the most recent start time. To...", 
"body" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filtering by app name app type status queue user cluster duration number of events By time period, Initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Search results list individual jobs and their IDs. If the job is part of a Hive query, Pig script, or a Workflow, a link to that Hive query\/Pig script\/workflow page is noted in the job's Go To GoTo " }, 
{ "title" : "Application Managers Basic Layout", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-ApplicationManagersBasicLayout", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Application Managers Basic Layout", 
"snippet" : "Below we are using the Spark Application Manager as the example. Many other tiles follow this format, MapReduce, Workflow, Fragments, Hive, etc. A black title bar notes the type of tile (Spark, Impala, MapReduce, Fragment, etc) and the job ID. The right side of the title bar are glyphs for adding a ...", 
"body" : "Below we are using the Spark Application Manager as the example. Many other tiles follow this format, MapReduce, Workflow, Fragments, Hive, etc. A black title bar notes the type of tile (Spark, Impala, MapReduce, Fragment, etc) and the job ID. The right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. Unravel's Intelligence Engine provides insights into an application and may provide recommendation, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains the Key Performance Indicators (KPIs) and general job information. [empty] : notes the number of events the job had. If there were no events Event icon No Events Event Panel Examples notes the job type and status. The box is colored code the same as the application status. Job icon: user, queue, start, stop time, etc. Job information: these vary by job type. KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation Common Sub-Tabs: Application Managers and views frequently contain the following sub-tabs. The contents will vary by job type. The below examples are again from the Spark Application Manager. Lists all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted. \"No errors found\" when there are none. Errors for each application are categorized by severity type and also include keywords and details associated with each. Keywords extract important details from the errors messages\/log data that can help developers\/operators quickly root cause issue. Examples of keywords include Oozie errors code(s), Java run time error(s), etc. Errors: : A list available logs. Click on the name to view the log information. \"No logs are available for this job.\" is listed if there are none. Logs : List the configuration parameters for the task\/job being displayed and their values. The parameters vary according to task\/job. Conf " }, 
{ "title" : "[empty]", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Application-SpecificManager", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ [empty]", 
"snippet" : "Spark Application Manager See the Spark Application Manager Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: resolve ineffici...", 
"body" : "Spark Application Manager See the Spark Application Manager Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O : The number of YARN apps making up the Hive query. Number of YARN apps Sub-Tabs By default the Hive APM opens showing the Navigation and Query tabs. The left sub-tabs are: : List all the MapReduce jobs associate with the query. Click on the job name bring up job in the Navigation MapReduce Application Manager : Shows detailed information about the MapReduce jobs and their relationship with one another. This view helps identify bottlenecks and inefficiencies. Execution Graph This graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other and some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). To close the box click close (2) or scroll within the tab. Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. : See the Gantt Chart here : Exceptions, errors, and warnings associated with this application. See Errors here The right sub-tabs are: : Shows the Hive Query. See the Hive Application Manager tab above for an example Hive Query window. Query A list of all the accessed Tables. Tables: Clicking on the table bring up the Table Detail. See here : Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the Task Attempts entire : Graphs the Map and Reduce tasks over the duration of the job. Attempts MapReduce Application Manager The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O Sub-Tabs By default the MapReduce APM opens in the Graphs | Attempts : Has four (4) sub tabs. Graphs : Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in on the Map\/Reduce slot duration is noted textually. Attempts andMemory: Graphs utilization of slot containers, Vcores, and memory over time. Containers, Vcores, : Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. It's divided into two (2) sections, a Timeline Distribution Timeline Selected Tasks Timeline Map Reduce Killed\/Failed Selected Tasks : The metrics, their definitions and values. Metrics : Logs for the driver and executors of this application. See Logs here The defined parameters and their values. See Configuration: here : Graphs JVM-level metrics at the executor and driver level. Resource Usage Initially only ten (10) of the series are graphed. You can select one or more series to use (1) and you can choose to show more or less of the series (3). Clicking on a name causes the graph to only display information for that series. Use the METRIC Get Data Resource Metrics : Exceptions, errors, and warnings associated with this application. See Errors here Impala Application Manager The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Operators Sub-Tabs By default the Impala APM opens showing the Fragments and Query tabs. The left sub-tabs are: : Displays a table with information about each fragment associated with this query. The Fragments Coordinator Operators More : Displays a list of all operators for all fragments. Operators You can search the operators name. Click on the operator to display its details. Scan HDFS details Aggregate Details Exchange Details : Charts the fragments. Gannt Chart : Shows the query plan in fragment or operator view. Query Plan Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. The right sub-tabs are: : Shows the query plan code. Click on Query Copy to copy the query. (See 3 in Impala APM view above.) Query : Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. Mem Usage [empty] The Kafka Application Manager provides Multi-Cluster support for monitoring : Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. | Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out\/sec Messages in\/sec Total Fetch Requests per \/sec Number of Active Controller Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafa Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators Number of Topics Number of Partitions The Topic lists displays the KPIs; when details are available a more info You can chose both the Partition Metric th offset Partition Details' The Kafka View has two tabs, Topic Detail Partition Detail Consumer Details' Kafka Topic Detail By default the Kafka Topic Detai Topic Detail Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is if: Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. if: Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: OK, the consumer is working, but falling behind, or Warning: : the consumer has stopped or stalled. Error Tez Application Manager The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/srv\/unravel\/log_hdfs\/unravel_us_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O Sub-Tabs By default the Tez APM opens showing the Navigation and Program Tabs. The left sub-tabs are: : List the Dag jobs with KPIs, Duration and I\/O. Navigation The DAG detail has six tabs: Displays the query. Query: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Graph: : Lists all the relevant counters for the Tez-DAG and their values. Counter Vertex Timeline Wall Clock Total Run All Vertices : List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task Id; Tasks containing the string will be displayed. All Task : List all attempts, their status (failed, success, etc.), vertex name and other relevant information. All Task Attempts : Lists all relevant parameters and their value. Changed Configuration : List the configuration parameters and their values. Configuration The right sub-tabs are: : Displays the query. Program : Has three (3) sub tabs. Graphs andMemory: Graphs utilization of slot containers, Vcores, and memory over time. Containers, Vcores, : Graphs the resources consumed. Resources By default the Resource systemCpuLoad Select series Metric Get Data " }, 
{ "title" : "Event Panel Examples", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-EventPanelEventPanelExamples", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Event Panel Examples", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. Recommendations Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. Efficiency 2: Tez DAG Resulted in one recommendation (#1). Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Templates", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Templates", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Templates", 
"snippet" : "Shows the templates in use. The templates with the highest average duration Showing Templates with the highest average duration Templates using most number of map tasks Templates using most number of reduce tasks Templates doing most DFS read I\/O Templates doing most DFS write I\/O Templates using mo...", 
"body" : "Shows the templates in use. The templates with the highest average duration Showing Templates with the highest average duration Templates using most number of map tasks Templates using most number of reduce tasks Templates doing most DFS read I\/O Templates doing most DFS write I\/O Templates using most reduce time Templates using most reduce time Workflow Manager The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications | Applications Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps Sub-Tabs The APM opens showing the Navigation Compare The left sub-tabs : Provides an easy way to understand the breakdown of the workflow the applications which comprise the Workflow, i.e., Hive, Spark, MapReduce, Oozie. Click on Navigation More Type : Exceptions, errors, and warnings associated with this application. See Errors here The right sub-tabs : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events : Displays charts for Map Task sand Reduce Tasks, broken down by success, failed, and killed as appropriate. Task Attempts : Graphs the attempts over the time interval in Wall Clock time and list the Map and Reduce Slot Duration in total time across all tasks. Attempts " }, 
{ "title" : "Spark Application Manager", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-applications-tab/spark-application-manager.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Spark Application Manager", 
"snippet" : "Overview The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use it to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark executors, Detect and fix poor partitioning, Detect and...", 
"body" : "Overview The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use it to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. There are multiple Spark applications types, for instance SQL, Streaming, PySpark, Shell, and Notebook. Currently Unravel's UI Spark Application Manager distinguishes between SQL, Streaming, and the remaining (PySpark, Shell and Notebook). The information contained with the Spark Application Manager can vary by the type of job. But, as with all Applications Managers, no matter what the job, the layout is similar and there are common tabs\/information across all types. Common Features across Tiles, Panels, Tabs, etc. When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( ). Click on a column name to use it for the sort. Clicking on a column already being used reverses the sort order. Clicking on the application name\/id usually brings up the application information, i.e., the Spark Application Manager or Job information. The application status is color coded ( , , , and ) when applicable. The Auto Action\/Tuning column ( )) notes if Unravel has tuning suggestions ( ) or the application has triggered an Auto Action\/alert ( ). There is an Auto Actions column ( ) when relevant. The number of auto actions (0-n) triggered is noted. A information glyph ( ) indicates when more information is available. Clicking on it brings up a new block or view as appropriate. A block glyph ( ) notes when the applications\/task\/job is open. Green indicates the block is open\/displayed, grey closed. Graphs Frequently a pull down menu, e.g., , above the graph offers display options to sort\/display the graph on, i.e., various metrics, filters, types, etc. If you can chose how to display the graph the options are shown in the upper right corner ( ). The current display will be highlighted in blue. Click on a graph type to change the display. Hovering within the chart\/graph\/diagram brings up the information for that point\/item in time in a text box ( ). When tables or lists are spread across multiple screen a page glyph ( )will be displayed. Click on the number to switch to that page. Spark Application Managers Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). The right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If it has a link to the parent, there will be an up arrow ( Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions or insights into how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains the Key Performance Indicators (KPIs) and general job information. \n Event icon No Events Event Panel Examples \n Job icon: application status \n Job information: \n Key Performance Indicators (KPIs): The last section, typically divided into two, has specific information related to the application\/job. The sections for a specific Spark Application (e.g Streaming) go into more detail. If the job is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Spark Application Managers Regardless of the type of Spark job, every Application Manager view is split into two sides. Every application type has the following three tabs on the left: \n Errors: \n Logs Below is a excerpt of the executor-20 log. \n Conf When choosing a configuration item to display, it is possible that some configurations parameters apply to other configurations. If that is the case, the applicable configurations are highlighted. In the example below, memory was selected. The memory's configuration also contains parameters relevant to: driver (spark.driver.memory), executor (spark.executor.memory) and resources (yarn.nodemanager.resource.memory-mb). Every job has the following four tabs on the right: \n Program \n Task Attempts The donut graphs shows the percentage of successful (green) and of failed (orange) tasks. The legend on the right lists the number of successes or failure. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination of failed and successful jobs. Hovering over the chart tells you the percentage of each. \n Graphs \n Attempt \n Containers \n Vcores \n Memory \n Resource By default the Resource systemCpuLoad Select series name Metric Get Data Below is executor-2's data displayed using JSON. Spark Streaming Enabling Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf. Spark 1.6.x After the Unravel sensor has been deployed and installed on the cluster open spark-defaults.conf spark.driver.extraJavaOptions -javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-1.6. Key Performance Indicators Unlike other Spark applications the Spark-Streaming APM lists no KPI's other than: \n Events Event Panel Examples The Application Manager has a Stream tab Graph: Attempts The left tabs are: Stream Tab This tab displays the core of an Streaming Application and from here you drill down into the batches, The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Total Delay Metric Processing Time Scheduling Delay. This graph is composed of two sections. By default, the upper and lower section (1) display the entire run. The table lists the stream batches relevant to the time period selected in the lower section of the graph as shown in the screenshot below. The table lists only the first three batches, but you can page through the table (2). By default the streams are sorted on start time in ascending order. The batch table is initially sorted by time in ascending order and lists KPIs for the batches, e.g., Input Records, Scheduling\/Processing\/Total Delaty and the number of Stream operations. You can chose the time window to display by clicking and dragging over a region of the lower section of the graph. You can expand\/contract the section to display by pulling\/pushing the edge ( The Stream Batch table is updated to reflect the time period selected. In this example the right three batch groups were selected from the graph above. In the Spark APM window above there is 43 pages of stream batches; with the narrowed time selection there are now only 10 pages of streams. In the example below, the right most 3 batches were selected, note the batches have decreased from 43 to 13. Clicking on a batch brings up the Spark Stream Batch. The batch window lists all the jobs associated with the batch and the batch's metadata. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. It has two tabs, Output Operations Metadata duration processing delay scheduling delay total delay Output Operations descriptions duration Job ID duration status The Metadata Clicking on a job id brings up the Spark Job window in the same block thereby replacing the Spark Stream Batch information. The title bar notes it is a Spark Job and part of a Spark Streaming application. In the title bar there is an up arrow glyph ( Duration # of Stages Stages Metadata Tasks The metadata and its associated values are shown. Click on the Batch ID Click on the Stage to bring up the Spark Stage panel. The Spark Stage is displayed in the same block thereby replacing the Spark Job information. The Spark Stage view is not specific to the job type. See Stage Details Execution Tab Currently no data is displayed for streaming applications since it would display thousands of nodes rendering the information useless. In Unravel v4.3.1 the Unravel UI will display the execution information at the stage level. Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf The right tabs are: Program Currently there is no information under this tab. Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resources Spark SQL-Query Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Stages The left tabs are: \n Navigation Lists the application's jobs with their relevant KPIs: Status Start Time Duration Paritions\/Tasks Read Write # Stages Start Time The job block lists the KPIs Duration # of Stages Stages Metadata Status Start Time Duration Partitions\/Tasks Read Write Input Output Start Time Spark Stage Details \n Execution A execution graph of the query. There are times when the DAG is too large to display and it will be noted. In v4.3.1 execution graphs will be displayed in the stage block. Hover over a stage for more information about the stage. \n Gantt Char Displays the stages using a Gantt Chart. The table is sorted on Start Time \n Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf The right tabs are: \n Program This tab connects all the pieces of a SQL query in one location. The table lists all queries with significant KPI's and, at most, the top five stages having the longest duration. For a particular query the SQL query text, the Program, the stages with the longest duration and KPIs are linked. The lower section contains two tabs, SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID to display it's SQL and program. Click on the stage to display its Spark Stage Detail. See Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. \n Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resources Other Spark Applications All other Spark Applications have a APM like the Spark SQL except there is no Query Spark Stage Details The Spark Stage block displays two KPIs, duration Data IO Graphs Timeline Attempts Task Attempts Program The default view is Graph Task Attempts Attempts. Program Details Clicking on the Source File Nam Call Site Line Number org.apache.spark.streaming.dstream.DStream.print(DStream.scala:757). \n \n Timeline Block The Timeline tab has two sections, \n Distribution \n Timeline Timeline Breakdown Selected Tasks Distribution Charts The Distribution Charts ShuffleMap Seconds Input KB Output KB Disk Bytes Spilled Memory Bytes Spilled Records Read Timeline You can chose the time period to display by clicking and dragging to select a time period. You can expand\/contract the section by pulling\/pushing the edge, or move the box by clicking on it and sliding it to the new location. Only the series relevant to that time period are displayed. In this example, the time period from 24-48 seconds has been selected, during which no series was running, so the Timeline \n \n Timeline The Timeline Tab has three sub-tabs: Timeline Timeline Breakdown Selected Tasks \n Timeline The timeline is a Gantt chart of the series which makes up the stage. See Distribution Charts above for an example of the graph. Only the series relevant to the time selected are displayed. You can filter the display by Tasks Killed\/Failed Show All Example Text Box T imeline Breakdown This is useful to identify bottlenecks. For each executor used in the current stage multiple metrics are graphed: Scheduler Delay, Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Executor Computing Time performing actual work thrashing, or waiting for scheduling. In this example, there is only one executor. \n Selected Tasks A list of tasks, if any, for the stage. " }, 
{ "title" : "The Data Tab", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab", 
"snippet" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. It has two tabs, Overview Details...", 
"body" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. It has two tabs, Overview Details " }, 
{ "title" : "Common Features across Tiles, Panels, Tabs, etc.", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-CommonFeaturesacrossTilesPanelsTabsetc", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Common Features across Tiles, Panels, Tabs, etc.", 
"snippet" : "When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Table\/Application list can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highligh...", 
"body" : " When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Table\/Application list can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( ). Click on a column name to use it for the sort. Clicking on a column already being used alters the sort order. When more information glyph ( ) appears clicking on it displays the information in a new view\/tile, i.e., APM, job block, etc. A check box is left of the table\/partition name if it must be selected in order to be graphed or make the more information glyph active. Clicking on the app name\/id usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. The table label is color coded, , Hot ), Warm ), or Co ld ) when applicable. The Auto Action\/Tuning column ( ) notes if Unravel has tuning suggestions ( ) or the application has triggered an Auto Action\/alert ( ). Graphs Frequently a pull down menu, i.e., , above the graph offers options to sort the graph on, i.e., various metrics, filters, types, etc. Hovering within the chart\/graph or on a diagram item, i.e., line, execution, brings up the information for that point\/item in time in a text format ( ). " }, 
{ "title" : "Overview", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-Overview", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Overview", 
"snippet" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of 3 sections. Tables Partitions More Info The 24 hour time period is noted in the upper right corner of the dashboard. Tables & Partitions Tiles The Tables Partitions Line Charts : Numb...", 
"body" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of 3 sections. Tables Partitions More Info The 24 hour time period is noted in the upper right corner of the dashboard. Tables & Partitions Tiles The Tables Partitions Line Charts : Number of Tables\/Partitions accessed. # Accessed : Number of Tables\/Partitions created. # Created : Size of Tables\/Partitions created. Size Created : Total Number of Tables\/Partitions currently in the system. Total Number Each line chart has the value of the metric for the last 24 hours overlaid on line graph which shows the historical values up to that point in time. Hovering over the line brings up a text box containing the value for that point in time. Donut Charts These charts display their information both in absolute values and as a percentage of the whole. : Displays Current Label Distribution Hot Warm, Cold policy configuration The Partitions : Total number of partitions and reclaimable partitions, and Partition Count : Total size of partitions and reclaimable amount. Partition Size The number and size of Reclaimable Partitions are calculated thusly: Reclaimable Partitions Found " }, 
{ "title" : "More Info", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-MoreInfo", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ More Info", 
"snippet" : "This section has three graphs. : Total number of queries accessing the tables. Accessed Queries : Total Read IO due to accessing the table. Total Read IO : Total number of users accessing the tables. Click on the information glyph to see the list of users. Number of Users...", 
"body" : "This section has three graphs. : Total number of queries accessing the tables. Accessed Queries : Total Read IO due to accessing the table. Total Read IO : Total number of users accessing the tables. Click on the information glyph to see the list of users. Number of Users " }, 
{ "title" : "Details Tab", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-DetailsTab", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Details Tab", 
"snippet" : "The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In the example below, the first three tables have been selected and are displayed. Choose the table(s) by selecting the check box (1) next to the table. Use the Metric Read IO, Total Users...", 
"body" : "The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In the example below, the first three tables have been selected and are displayed. Choose the table(s) by selecting the check box (1) next to the table. Use the Metric Read IO, Total Users Total Attempts, Total Size Reset Graph Table List You can Search Show Hot Warm Cold All Warm Read IO More Info Table Detail . Configuration Policy The sortable columns are: Hover over the name to see the full name. Table Name: : The table’s owner. Owner : Date\/Time of the last access to the table. Last Access : Date\/Time the table was created Created : Total table size. Size : Total data read by all applications that accessed the table. Read IO : Total number of attempts by all applications that accessed the table. Attempts : Total number of applications that accessed the table. Apps : Number of partitions in the table. Partitions : Number of reclaimable partitions in the table. RP (Count) : Total size of reclaimable partitions in the table. RP (size) : Total number ofusersthat accessed the table.Hovering over the user number lists the user(s) names. Users : Tables used along side this table as part of an application. Hovering over the table number lists the table name(s). Other Tables The two additional colums are: : Table age\/last access, Labels Hot Warm Cold warm cold. : A link to the More Info Table Detail " }, 
{ "title" : "Table Detail", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-TableDetail", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Table Detail", 
"snippet" : "This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the ...", 
"body" : "This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed, Read IO User # Apps, Attempts There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Read IO, Total Users, Total Attempts, Total Size Application Detail Application Tab page Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition S ize MR jobs By default the 100 latest partitions are loaded, and the first partition listed is graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View " }, 
{ "title" : "Configuration", 
"url" : "un42--un42-/unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-ConfigConfiguration", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Configuration", 
"snippet" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data Details From the pull down menus...", 
"body" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save " }, 
{ "title" : "Setting Up Auto Actions", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "About Auto Actions", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-AboutAutoActions", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ About Auto Actions", 
"snippet" : "Unravel's Auto Actions The Unravel Server processes Auto Actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical ex...", 
"body" : "Unravel's Auto Actions The Unravel Server processes Auto Actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate this rule in order to detect violation, and Action(s) for Unravel Server to execute whenever it detects a violation of the rule. " }, 
{ "title" : "How to Create Auto Actions", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-CreateAAHowtoCreateAutoActions", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ How to Create Auto Actions", 
"snippet" : "Click the Admin pull-down menu, and select Manage. On the Manage page, select the Auto Actions tab. The Auto Actions To add a new action, click ADD NEW AUTO ACTION. The Add New Action Map Reduce Jobs Long running MR job Too many mappers for an MR job Too many reducers for an MR jo Resource contentio...", 
"body" : " Click the Admin pull-down menu, and select Manage. On the Manage page, select the Auto Actions tab. The Auto Actions To add a new action, click ADD NEW AUTO ACTION. The Add New Action Map Reduce Jobs \n Long running MR job \n Too many mappers for an MR job \n Too many reducers for an MR jo Resource contention \n Resource contention in cluster \n Resource contention in queu Rogue Identification \n Rogue user \n Rogue application Long Running Jobs \n Long running YARN application \n Long running Hive query \n Long running workflow Expert mode This is a very powerful mode of operation. You can take full advantage of all the Unravel Auto Actions engine's features running the scene via defining rules and actions in “free form” using the Auto Actions JSON language. Consult with the Unravel team before attempting to use the Expert Mode. In this mode you must specify prerequisite conditions, defining conditions, and actions: \n Prerequisite conditions \n Defining conditions \n Actions Define an Auto Action Using predefined templates. Select an Auto Action Template and click NEXT The Auto Action Template Enter the name of the rule and a description. The name is required and is used to identify the rule in the UI. The name is included in every message the Auto Action sends\/posts upon violation. The name should be short and clearly identify the purpose of the rule definition. The description is optional but highly recommended. In it you can provide an explanation of what the rule is designed to do, i.e. monitor metrics and actions of a user. The description is mainly for your reference but can be included in Auto Actions messages. Specify the rule and its applicable scope. A rule has two conditions: \n Prerequisite conditions \n Defining conditions: \n User \n Queue \n Cluster Note: this rule is applicable only to multi-cluster Unravel monitoring configuration. \n Application Name \n Time Specify the Auto Actions executed for rule violations. Once a violation is triggered, you have a choice of actions to take. The Auto Actions engine automatically logs and flags violations As with the rules, the actions vary between templates. Typically, you have the following available: \n Send email email \n HTTP post HTTP endpoint \n Move to queue \n Kill application Caution: This is a destructive action that may affect the cluster performance and availability to the users. \n Send an Email Select the Send Email ADD RECIPIENT INCLUDE OWNER email cluster history charts dashboard \n HTTP post sent to Slack This action requires support through Slack “hooks”. Select the HTTP Post ADD URL HTTP post \n Kill the Application Select the Kill App . \n \n Caution: This is a destructive action that may affect the cluster performance and availability to the users. Applications targeted to be killed must have: Directly caused the rule violation, and Allocated resources, i.e. in allocated or running states. \n Kill App Move app to queue Move app Kill App . \n Move the application to another queue Select the Move app to queue \n Caution: While a non-destructive action that should not affect the cluster performance and its availability to the user, we suggest moving with caution. Applications targeted to be moved must have: Directly caused the rule violation, and Allocated resources, i.e. in allocated or running state. \n Move app to queue Kill App Move app Kill App Using theExpert ModeTemplate. The Auto Actions engine is capable of much more than is available through the templates. Using the Auto Actions Expert Mode Section 4.1 Expert Mode \n Caution: This flexiblity and power also makes this mode dangerous and capable of wreaking havoc. Consult with Unravel team before attempting to use the Expert Mode. Select Expert Mode from the Auto Actions template screen. Use the JSON language enter the rule(s) definition(s) in the Rule text box and any Actions in the Actions text box. See Sample Auto Actions " }, 
{ "title" : "5. Click SAVE AUTO ACTION.", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-5ClickSAVEAUTOACTION", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ 5. Click SAVE AUTO ACTION.", 
"snippet" : "A link is generated to cluster history charts dashboard at the point when this violation occurred +\/- 5 minutes (a 10 minute time slice). The link is included in every message (email or HTTP post) and violation log event. A \"bell\" badge is placed the UI next to each application that triggered a viol...", 
"body" : "A link is generated to cluster history charts dashboard at the point when this violation occurred +\/- 5 minutes (a 10 minute time slice). The link is included in every message (email or HTTP post) and violation log event. A \"bell\" badge is placed the UI next to each application that triggered a violation. Example cluster history charts dashboard Example Email Auto Action policy \"ROGUE APPLICATION #1\" violation detected.\n\nPolicy description: \"Identify applications that are using too much of the cluster resources\"\n\nApplication \"application_1498514199803_2411\" has 1 violation:\n\n1. Sum of memory in MB allocated to containers is 1GB >= 1MB\n\nTimestamp: 07\/19\/2017 08:41:35 +0000\n\nReach Unravel server at http:\/\/localhost:3000\/\n\nSee cluster history at http:\/\/localhost:3000\/ops_dashboard\/charts\/resources?from=1499817549571&to=1499817669570&at=1499817609571&interval=1m Example HTTP Post Example Event Log: " }, 
{ "title" : "Understanding the Snooze Feature", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-UnderstandingtheSnoozeFeature", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Understanding the Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions being repeated for the same violation context during a specified period of time If, and only if, the action adds no further information to the violation, i.e., is essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert...", 
"body" : "The snooze function prevents automatic actions being repeated for the same violation context during a specified period of time If, and only if, the action adds no further information to the violation, i.e., is essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert adds no new context the user A's violation. Snooze is set the first time the user violates the rule. Though actions are snoozed for some users, the Auto Action continues to run uninterrupted and will take action for those not \"snoozing\" at the time of violation. Snooze is irrelevant if the action is Kill App Move App For example: the rule\/action: if user uses memory > 1 GB send email two users: A & B snooze time: 30 minutes \n at 20:00 user A > 1GB → email is sent + snooze set (runs until 20:30). user B < 1GB → nothing is done. \n at 20:10 user A > 1GB → 'snoozing', no action is taken. user B > 1GB → email is sent + snooze set (runs until 20:40). \n at 20:20 user A > 1GB → 'snoozing', no action is taken user B > 1GB → 'snoozing', no action is taken \n at 20:35 user A > 1GB → email is sent + snooze set (runs until 21:05) user B > 1GB → 'snoozing', no action is taken The snooze property, com.unraveldata.auto.action.snooze.period.sec \/usr\/local\/unravel\/etc\/unravel.properties . Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions/running-auto-action-demos.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with Auto Actions and their triggering. Example Auto Actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED U...", 
"body" : "The Demos program provides you a way to understand and experiment with Auto Actions and their triggering. Example Auto Actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory. DEMO_PATH auto-actions-demos # ls auto-actions-demos\ndemos\/ setup\/ Go to DEMO_PATH Open .\/settings Execute the .\/setup-all script. # .\/setup-all The Auto Action rules that include time specification will be automatically adjusted to the current time period, i.e. from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Admin Manage Auto Actions You should see all the Auto-Actions demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag, e.g., AA-Spark-1c, Map-1b, AA-Hive-1a. Executing the demos Go to DEMO_PATH\/demos directory. For each Auto Actions rules listed in Admin->Manage->Auto Actions there is a corresponding script in the demo's directory. Each script will trigger the corresponding Auto Action demo. For example, in the UI you will see an Auto Action named AA_Spark-1c. demo-Spark-1c. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Go to DEMO_PATH\/demos directory. For each Auto Actions rules listed in Admin->Manage->Auto Actions there is a corresponding script in the demo's directory. Each script will trigger the corresponding Auto Action demo. For example, in the UI you will see an Auto Action named AA_Spark-1c. demo-Spark-1c. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute “.\/demo-tag” script to trigger the corresponding “AA-tag” rule. Each script is designed to simulate violation conditions for the corresponding Auto Action on the target Hadoop cluster, i.e., to trigger AA-Spark-1c you run the demo-Spark-1c script. Some Auto Actions demo scripts cause multiple Auto Actions to trigger. While this a side effect that is unavoidable, it is also what can happen when running your defined Auto Actions. This is because defined Auto Actions can have overlapping definitions. . Cleaning up demos Go to DEMO_PATH\/setup directory. Run .\/clean-all script. # .\/clean-all This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 Auto Actions Demos List Application type Use case Auto Action Triggering Script Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vcores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Will also trigger MR-1a as well. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. AA-MR-2b Demo-MR-2b Will also trigger MR-2a as well. Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. Spark Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark application has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark application allocated vcores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark application is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Alert if a workflow is reading more data than expected. Related articles Page: Setting Up Auto Actions Page: Running Auto Action Demos " }, 
{ "title" : "Sample Auto Actions", 
"url" : "un42--un42-/unravel-4-2/user-guide/setting-up-auto-actions/sample-auto-actions.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Sample Auto Actions", 
"snippet" : "Supported cluster metrics AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master also maintains various counter. Users can use these metrics and the counter...", 
"body" : "Supported cluster metrics AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master also maintains various counter. Users can use these metrics and the counters when defining Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on: \"live\" running applications allowing to take proactive actions when violations are detected, and on MapReduce AM metrics only when the user specifies a rule requiring the polling\/aggregation of a metric. YARN Resource Manager metrics appCount - total number of applications elapsedTime - total elapsed time of containers in milliseconds allocatedMB - sum of memory in MB allocated to containers allocatedVCores - sum of virtual cores allocated to containers runningContainers - number of containers currently running memorySeconds - amount of allocated memory in MB-seconds vcoreSeconds - amount of allocated CPU resources in virtual core-seconds For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API MapReduce Application Master metrics elapsedAppTime - time since the application was started in milliseconds mapsCompleted - number of completed maps reducesTotal - total number of reduces reducesCompleted - number of completed reduces mapsPending - number of maps still to be run mapsRunning - number of running maps reducesPending - number of reduces still to be run reducesRunning - number of running reduces newReduceAttempts - number of new reduce attempts runningReduceAttempts - number of running reduce attempts failedReduceAttempts - number of failed reduce attempts killedReduceAttempts - number of killed reduce attempts successfulReduceAttempts - number of successful reduce attempts newMapAttempts - number of new map attempts runningMapAttempts - number of running map attempts failedMapAttempts - number of failed map attempts killedMapAttempts - number of killed map attempts successfulMapAttempts - number of successful map attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduce Application Master counters \n Shuffle Errors badId - total number of errors related with the interpretations of IDs from shuffle headers connection - total number of established network connections ioError - total number of errors related with reading and writing intermediate data wrongLength - total number of errors related to compression and decompression of intermediate data wrongMap - total number of errors related to duplication of the mapper output data wrongReduce - total number of errors related to the attempts of shuffling data for wrong reducer \n File System Counters fileBytesRead - mount of data read from local file system fileBytesWritten - amount of data written to local file system fileReadOps - number of read operations from local file system fileLargeReadOps - number of read operations of large files from local file system fileWriteOps - number of write operations from local file system hdfsBytesRead - amount of data read from HDFS hdfsBytesWritten - amount of data written to HDFS hdfsReadOps - number of read operations from HDFS hdfsLargeReadOps - number of read operations of large files from HDFS hdfsWriteOps - number of write operations to HDFS \n Map-Reduce Framework Counters mapInputRecords - total number of records processed by all of the mappers mapOutputRecords - total number of records produced by by all of the mappers mapOutputBytes - total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes - amount of (compressed) data which was actually written to disk splitRawBytes - amount of data consumed for metadata representation during splits combineInputRecords - total number of records processed by combiners combineOutputRecords - total number of records produced by combiners reduceInputGroups - total number of unique keys reduceShuffleBytes - of data processed in shuffle and reduce phase reduceInputRecords - total number of records processed by all reducers reduceOutputRecords - total number of records produced by all reducers spilledRecords - total number of map and reduce records that were spilled to disk shuffledMaps - total number of mappers which undergone through shuffle phase failedShuffle - total number of mappers which failed to undergo through shuffle phase mergedMapOutputs - total number of mapper output files undergone through shuffle phase gcTimeMillis - wall time spent in Java Garbage Collection cpuMilliseconds - cumulative CPU time for all tasks physicalMemoryBytes - total physical memory used by all tasks including spilled data virtualMemoryBytes - total virtual memory used by all tasks committedHeapBytes - total amount of memory available for JVM \n Job Counters totalLaunchedMaps - total number of launched map tasks totalLaunchedReduces - total number of launched reduce tasks dataLocalMaps - number of map tasks which were launched on the nodes containing required data slotsMillisMaps - total time spent by all executing maps in occupied slots slotsMillisReduces - total time spent by all executing reduces in occupied slots millisMaps - total time spent by all map tasks millisReduces - total time spent by all reduce tasks vcoresMillisMaps - total vcore-seconds taken by all map tasks vcoresMillisReduces - total vcore-seconds taken by all reduce tasks mbMillisMaps - total megabyte-seconds taken by all map tasks mbMillisReduces - total megabyte-seconds taken by all reduce tasks \n File Input Format Counters bytesRead - amount of data read by every tasks for every filesystem \n File Output Format Counters bytesWritten - amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Spark Metrics In addition to the metric set supported by MapReduce applications, Spark application, Spark applications can be polled on: inputRecords outputRecords outputToInputRecordsRatio totalJoinInputRowCount totalJoinOutputRowCount inputPartitions outputPartitions Hive\/Workflow Metrics duration - total time taken by the application totalDfsBytesRead totalDfsBytesWritten Information on Demo Auto Actions can be found here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Alert for Rogue application - any application which is consuming a major portion of cluster resources. a. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue You can use the Rogue Application or memory Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Set Time rule condition as: Set Move app rule as: b. If any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application or memory Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n}\n or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\":\"allocated_mb\",\n \"compare\": \">\",\n \"value\": [X]\n} Set Queue rule conditions as: Set Move app action as: Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. a. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n UserAlert for Rogue User - Any user consuming a major portion of cluster resources. \n a. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Workflow Alert if a workflow is taking longer than expected. a. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} b. Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} " }, 
{ "title" : "Use Cases", 
"url" : "un42--un42-/unravel-4-2/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "un42--un42-/unravel-4-2/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Go to Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Go to Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster_the spike's timestamp,_the bottom of the page. When you see many applications in the ACCEPTED RUNNING RUNNING ACCEPTED Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "un42--un42-/unravel-4-2/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster_the spike's timestamp,_the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "un42--un42-/unravel-4-2/user-guide/use-cases/optimizing-the-performance-of-spark-applications.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web ...", 
"body" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec. In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec. Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Kafka Insights", 
"url" : "un42--un42-/unravel-4-2/user-guide/use-cases/kafka-insights.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, \n OK \n Lagging \n Stalled A Topic's status is set to the lowest status among it's Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Examples 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Note: Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric offset, test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI’s. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. " }, 
{ "title" : "Advanced Topics", 
"url" : "un42--un42-/unravel-4-2/advanced-topics.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics", 
"snippet" : "Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery, and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting ...", 
"body" : " Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery, and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Using a Private Certificate Authority with Unravel Connecting to\/Configuration of a Kafka Stream Connecting to a Hive Metastore Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Running Verification Scripts and Benchmarks Supported Roles Unravel Servers and Sensors Installing Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Uninstalling Unravel Server Upgrading the Unravel Server and Sensors Uploading the Spark Program(s) to Unravel Using an External MySQL or Compatible Database for Unravel Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Backing-up, Disaster Recovery, and Reverting to Prior Version", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version", 
"snippet" : "It is best practice to create a snapshot (backup) of your Unravel Server and to test your snapshot on a failover server daily or at a minimum, weekly; in addition, it is absolutely essential to create a snapshot before any upgrade in case you need to revert to the previous Unravel version....", 
"body" : "It is best practice to create a snapshot (backup) of your Unravel Server and to test your snapshot on a failover server daily or at a minimum, weekly; in addition, it is absolutely essential to create a snapshot before any upgrade in case you need to revert to the previous Unravel version. " }, 
{ "title" : "Dump of Unravel Server's State", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-DumpofUnravelServersState", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Dump of Unravel Server's State", 
"snippet" : "Creating a snapshot of your Unravel Server involves dumping its state and databases to a specified location. You create a snapshot by running the dump_unravel.sh # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh $DEST The result is a subdirectory under $DEST unravel_yyyymmddthhmmZ If you use an external...", 
"body" : "Creating a snapshot of your Unravel Server involves dumping its state and databases to a specified location. You create a snapshot by running the dump_unravel.sh # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh $DEST The result is a subdirectory under $DEST unravel_yyyymmddthhmmZ If you use an external database in production, that database is not dumped; you must replicate or back up the externally managed database separately. " }, 
{ "title" : "Load of Unravel Server's State", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-LoadofUnravelServersState", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Load of Unravel Server's State", 
"snippet" : "You can load a saved state into Unravel Server by specifying a source directory, $SRC, on the invocation of the load_unravel.sh script replaces Very important: this operation wipes out existing data in the installed Unravel Server on the current host. Data in an external database is unaffected. # su...", 
"body" : "You can load a saved state into Unravel Server by specifying a source directory, $SRC, on the invocation of the load_unravel.sh script replaces Very important: this operation wipes out existing data in the installed Unravel Server on the current host. Data in an external database is unaffected. # sudo \/usr\/local\/unravel\/bin\/load_unravel.sh $SRC The result is that the saved state $SRC " }, 
{ "title" : "Disaster Recovery Scenario", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-DisasterRecoveryScenario", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Disaster Recovery Scenario", 
"snippet" : "For disaster recovery, you can prepare a cold failover or alternate server, and periodically update it with a snapshot. In this case, you would create a snapshot periodically on your normally live Unravel Server, and load that snapshot onto your alternate server. For a cold failover (to the alternat...", 
"body" : "For disaster recovery, you can prepare a cold failover or alternate server, and periodically update it with a snapshot. In this case, you would create a snapshot periodically on your normally live Unravel Server, and load that snapshot onto your alternate server. For a cold failover (to the alternate server), there is no need to start all the Unravel daemons until switching over–in other words, after the disaster in the normal data center. " }, 
{ "title" : "Revert to Prior Version Scenario", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-ReverttoPriorVersionScenario", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Revert to Prior Version Scenario", 
"snippet" : "Best practice is to execute rolling upgrades of Unravel Server: first upgrade the normal server, then upgrade the alternate server. In this fashion you always have an easy way to completely revert an upgrade to the prior version. To revert, do the following steps. If you use an external database in ...", 
"body" : "Best practice is to execute rolling upgrades of Unravel Server: first upgrade the normal server, then upgrade the alternate server. In this fashion you always have an easy way to completely revert an upgrade to the prior version. To revert, do the following steps. If you use an external database in production, you cannot revert it with this procedure; you might need to revert it separately. Create a snapshot (saved state). Dump Unravel Server state $DEST before upgrade. For example, # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh \/mnt\/someplace Note the backup subdirectory created. For example, \/mnt\/someplace\/unravel_yyyymmddthhmmZ\/ Try the new version of Unravel Server. Upgrade Unravel Server Start Unravel Server: # \/etc\/init.d\/unravel_all.sh start (Optional) Revert to the previous version of Unravel Server. Stop Unravel Server: # \/etc\/init.d\/unravel_all.sh stop Uninstall the RPM: # rpm -e unravel This command's output is a list of all data files. Remove all data files listed in the output of the \" rpm -e Install the previous version of Unravel Server: # rpm -U unravel*rpm* Load the snapshot (saved state) you saved in $DEST #\/usr\/local\/unravel\/bin\/load_unravel.sh \/mnt\/someplace\/unravel_yyyymmddthhmmZ\/ Start Unravel Server: # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Cluster Wide Report", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/cluster-wide-report.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of the recommendations revolve around these parameters: MapSplitSizeParams MHiveExecReducersBytesParam MHiveExecParallelParam , and MMapReduceSlowStartParam . MMapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/cluster-wide-report.html#UUID-45a459b7-2296-b3c5-1a23-15babf5d95c2_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSe...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/ usr\/local\/unravel\/install_bin\/ClusterReport. cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz to cd dbin Input.txt # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report # sudo -u hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Custom Configurations", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel...", 
"body" : " Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-3f2575f8-3b6b-ad4b-2b97-bc2dc766563c", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # ...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-630ee792-432e-cd68-b3b9-495c206cf2df", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig 1. Stop Unravel Server On each Unravel host, run this command: sudo \/etc\/init.d\/unravel_all.sh stop 2. Modify unravel.properties on host1 Pick a machine to be host1 If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties host1 Replace 127.0.0.1 3316 unravel_mysql_prod To find your fully qualified hostname, type hostname -I unravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod 2. Copy host1's unravel.properties to Other Hosts Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh host1 host2 host3 # host1\n# scp \/usr\/local\/unravel\/etc\/unravel.properties host2:\/usr\/local\/unravel\/etc\/\n# scp \/usr\/local\/unravel\/etc\/unravel.ext.sh host2:\/usr\/local\/unravel\/etc\/\n# scp \/etc\/unravel_ctl host2:\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel Important Note The scripts invoked below will make an identical change to the unravel.properties 3. Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host): # host1\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh\\ \nhost1 host2 \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh\\ \nhost1 host2 For a 3-host ensemble (substitute host): # host1 sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh\\ \nhost1 host2 host3\n# host2 sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh\\ \nhost1 host2 host3\n# host3 sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh\\ \nhost1 host2 host3 These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id 4. Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order, on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: # host1\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host3\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: # host1 \nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15sec for Zookeeper quorum to settle: sleep 15 Start up Kafka ensemble: # host1\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-k Wait 10sec for Kafka coordination: sleep 10 Create the Kafka topics (only on one host): # host1 \nsudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh 5. Start Unravel Server Finish multi-host installation by starting up Unravel Server: # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host1\necho \"http:\/\/$(hostname -f):3000\/\" \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start 6. Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 7. Snapshot unravel.properties " }, 
{ "title" : "Defining a Custom TC Port", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-aa6e1641-db0c-3155-33d0-9072c68cf574", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom TC Port", 
"snippet" : "These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 # echo 'export UNRAVEL_LISTEN_PORT={18080}' \\ >>\/usr\/local\/unravel\/etc\/unravel.ext.sh...", 
"body" : " These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 # echo 'export UNRAVEL_LISTEN_PORT={18080}' \\\n>>\/usr\/local\/unravel\/etc\/unravel.ext.sh " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-c4c09c84-0d79-e869-c598-928e396d475e", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration Core Retention Manage Configuration unravel.properties will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-r...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration Core Retention Manage Configuration unravel.properties will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space. That means about 1000 jobs per 1GB of disk. In Unravel Web UI, select the Manage Configuration Core \/usr\/local\/unravel\/etc\/unravel.properties The TIME SERIES RETENTION DAYS unravel.properties: com.unraveldata.retention.max.days=30 This setting is the most significant factor in controlling disk space usage in the database used by Unravel. The WEEKS TO SHOW FOR SEARCH RESULTS unravel.properties: com.unraveldata.recent.maxSize.weeks=2 This value should be no larger than the next setting minus 1. If recent activity in Unravel gives you too many results, consider making this smaller. It should not be less than 2. The WEEKS TO SHOW FOR DEEP SEARCH RESULTS unravel.properties: com.unraveldata.history.maxSize.weeks=4 This value should be at least 1 week more than the setting immediately above, so that com.unraveldata.recent.maxSize.weeks < com.unraveldata.history.maxSize.weeks is true. After changing any of the settings above, restart unravel_td sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-c21ccf0c-4dc7-95a5-875e-45c0fe495ce8", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. \n \n \n Web UI \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 25 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n false \n Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw \n \n START TLS \n mail.smtp.starttls.enable \n false \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n null \n Username for SMTP authentication \n \n USER PASSWORD \n mail.smtp.pw \n null \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n l \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone @example.com \n Use a From: \n \n LOCALHOST \n mail.smtp.localhost \n localhost.local \n A domain name for apparent sender; must have at least one dot (e.g. organization.com) \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. GMail SMTP Example These settings are for our internal use. Do not \n \n \n Config Label \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 587 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n true \n Enable SMTP authentication? \n \n START TLS \n mail.smtp.starttls.enable \n true \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n \n s \n Username for SMTP authentication \n \n USER PASSWORD \n \n mail.smtp.pw \n ******** \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n smtp.gmail.com \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone@example.com \n This sets the From header \n \n LOCALHOST \n mail.smtp.localhost \n \n e \n A domain name for apparent sender; must have at least one dot \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. Debug mode under \"Advance SMTP\" section Unravel daemons to restart after email setup Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_tc restart \nsudo \/etc\/init.d\/unravel_all.sh stop-etl\nsudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n--> enter password from dist.unraveldata.com\n--> should see following output in terminal mode and if you see \"result is = null\", then, setup is correct.\n:\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add following properties to \/usr\/local\/unravel\/unravel.properties on Unravel Server: mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Disable unneeded daemons: sudo service unravel_os3 stop\nsudo chkconfig unravel_os3 off Restart daemons: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Spark Properties for Spark Work daemon @ Unravel", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/custom-configurations.html#UUID-da2df4f9-2880-7d6c-9f18-cee0f3e9f035", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Work daemon @ Unravel", 
"snippet" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an ap...", 
"body" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:_1,_1.lz4,_1.snappy,_1.inprogress,,.lz4,.snappy, .inprogress,_2,_2.lz4,_2.snappy,_2.inprogress com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) Executor log processing Property Definition Default com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) Tagging Property Definition Default com.unraveldata.tagging.enabled Enables tagging functionality. True com.unraveldata.tagging.script.enabled Enables tagging. False com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False EMR\/HDInsight specific properties Property Definition Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False True Wasb block storage specific properties (for HDInsight): The following properties are set with information obtained from Microsoft Azure. MiFor a storage account with the name storagename storagename blob.core.windows.net Property Definition Default com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Login in to Azure. How to locate your storage account name At the Azure Dashboard Select Storage Accounts Copy the Storage Name ( STORAGE_NAME) fs.azure.account.key. STORAGE_NAME blob.core.windows.net com.unraveldata.hdinsight.storage-account-name-1 com.unraveldata.hdinsight.storage-account-name-2 How to locate your access keys. From the home page select Storage Accounts Access Keys. key1 com.unraveldata.hdinsight.primary-access-ke key2 com.unraveldata.hdinsight.secondary-access-key =<key2>. S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g. , \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_ke aws_secret_access_key access_key\/secretKey - Data Lake (ADL) specific data properties The following properties are set with information obtained from the ADL properties in Microsoft Azure. For further information on how to retrieve these Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster “spk21utj02” has been given access to “\/clusters\/spk21utj02” on Data Lake store. retrieve from Microsoft Azure Login in to Azure. How to locate your account Fully Qualified Domain Name Select Data Lake Store | Data Explorer Data Explorer Click on Folder properties Properties PATH FQDN com.unraveldata.adl.accountFQDN=<FQDN> How to locate your OAUTH 2.0 Token Endpoint, Application ID and Application Key. From Home page, select Azure Active Director App Registration. Select Endpoints. OAUTH 2.0 Token Endpoint com.unraveldata. adl.accessTokenEndpoint=<OAUTH 2.0 Token Endpoint> Locate Application ID and Application Key. Select View all applications, If your application is listed select it. If you need to create a new application select + New application registration Create Whether your selected an existing application or created a new one, you will see the the registered App Application ID com.unraveldata.adl.clientID=<Application ID>. Select Settings Keys. Description Duration. Save. Copy and save the key Value . Set com.unraveldata.adl.clientKey=<Key Value>. How to grant access to a newly create application. Select Data Lake Store | Data Explorer Select Access +Add Access Click OK Select Grant the permissions OK Your app is now listed under Assigned Permission " }, 
{ "title" : "Security Configurations", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-78bbc81b-23a1-c3c2-de40-b53b207e919a", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-630ed6f0-86ff-2777-0e91-f7d7f17c1059", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_tc which listens on port 3000 in Enabling TLS to Unravel Web UI Directly These ste...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_tc which listens on port 3000 in Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4. Install needed packages. # sudo yum install httpd mod_ssl Note: There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf unravelhost_FQDN SSLCertificate* <VirtualHost *:80>\n ServerName unravelhost_FQDN unravelhost_FQDN unravelhost_FQDN \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt \/etc\/certs\/wildcard_unravelhost_RSA_private.key\n \/etc\/certs\/IntermediateCA.crt Adjust or add com.unraveldata.advertised.url \/usr\/local\/unravel\/etc\/unravel.properties :port# com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_tc # sudo service unravel_tc restart Start the httpd # sudo service httpd start Visit https:\/\/unravelhost_FQDN Troubleshooting To enable verbose logging in Apache2, add these lines. ProxyHTMLLogVerbose On\nLogLevel debug\n where LogLevel debug trace1 trace8 Note: Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ add this line after ServerName line in the virt. host httpd RequestHeader set X-FORWARDED-PROTO 'https'\n " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-d677c501-8713-5c25-cad7-465ad1badd66", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in Step 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in Step 2 of the installation guide. HIGHLIGHTED 1. Check HDFS Default umask For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see subdirectories and read files. Check the value of dfs.umaskmode and make sure the middle digit is 2 or 0. This controls the group mask and ACLs are masked using this default group mode. 2. Enable ACL Inheritance In Cloudera Manager, HDFS Configuration, search for namenode advanced configuration snippet dfs.namenode.posix.acl.inheritance.enabled hdfs-site.xml https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 3. Restart Cluster When you are ready, restart the cluster to effect a change in dfs.namenode.posix.acl.inheritance.enabled 4. Change ACL of Target HDFS Directories Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented). The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # set ACL for future directories\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse\n\n# set ACL for existing directories\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse 5. Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target users 'unravel' and 'hdfs', with a valid kerberos ticket corresponding to keytab principal (substitute your values for KEYTAB_FILE KEYTAB_FILE2 PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n\n# sudo -u hdfs kdestroy\n# sudo -u hdfs kinit -kt {KEYTAB_FILE2} {PRINCIPAL}\n# sudo -u hdfs hadoop fs -ls \/user\/history\n# sudo -u hdfs hadoop fs -ls \/tmp\/logs\n# sudo -u hdfs hadoop fs -ls \/user\/hive\/warehouse " }, 
{ "title" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-39b1d725-914a-c07f-f2d6-0ab8e52e9a1d", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"snippet" : "HDFS Permission The following instruction applies to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl expor...", 
"body" : "HDFS Permission The following instruction applies to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export RUN_AS=$user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH please see the Cloudera Documentation HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI Go to Manage Configuration HIVE : JDBC URL Hive Metastore URL use the pull-down to specify the type of database mysql or postgresql, your host, port, and name of your hive database. : JDBC driver Hive Metastore Driver : JDBC User name Hive Metastore User Name : JDBC password Hive Metastore Password " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-8518f666-90bd-49c0-9d69-7d5fa5c16a07", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Here is how to directly enable TLS (SSL) to unravel_tc which is listening on port 3000. Alternatively, you can add an apache2 reverse proxy in Adding SSL and TLS to Unravel Web UI These steps work with Unravel 4.2.x and earlier. In 4.3 the mechanism will change. Note that you must take steps to reap...", 
"body" : "Here is how to directly enable TLS (SSL) to unravel_tc which is listening on port 3000. Alternatively, you can add an apache2 reverse proxy in Adding SSL and TLS to Unravel Web UI These steps work with Unravel 4.2.x and earlier. In 4.3 the mechanism will change. Note that you must take steps to reapply these changes after an upgrade, so be sure to save copies of the changed and added files. Insert this snippet of xml into \/usr\/local\/unravel\/conf\/server.xml.orig in place of the commented-out one containg <Connector port=\"8443\" : <Connector port=\"3003\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" SSLEnabled=\"true\"\n scheme=\"https\" secure=\"true\"\n address=\"0.0.0.0\"\n connectionTimeout=\"20000\" \n maxHttpHeaderSize=\"8192\"\n executor=\"tomcatThreadPool\"\n URIEncoding=\"UTF-8\" server=\"Unravel\/4.2\"\n enableLookups=\"false\" disableUploadTimeout=\"true\"\n socket.appWriteBufSize=\"32768\"\n acceptCount=\"90\" socket.processorCache=\"10\" socket.bufferPoolSize=\"1048576\" socket.bufferPool=\"10\" socket.keyCache=\"10\"\n allowTrace=\"false\" compression=\"on\" compressionMinSize=\"16384\"\n noCompressionUserAgents=\"gozilla, traviata\"\n compressableMimeType=\"text\/html,text\/xml,text\/plain\"\n clientAuth=\"false\" sslProtocol=\"TLSv1.2\"\n sslEnabledProtocols=\"TLSv1.2\" \n ciphers=\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDH_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDH_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDH_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDH_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDH_RSA_WITH_AES_256_CBC_SHA,TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDH_RSA_WITH_AES_128_CBC_SHA,TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA\"\n keyAlias=\"unravel.example.com\" \n keystoreFile=\"\/usr\/local\/unravel\/etc\/keystore.p12\" \n keystoreType=\"PKCS12\"\n keystorePass=\"verysecret\"\n \/> Note: you need to provide the PKCS12 keystore file in this example, specify the key store password you plan to set below, and specify the port you want used (3003 for example). Listening on port 443 is not supported directly, but could be implemented via iptables prerouting and redirect. The ciphers are listed explicitly in order to avoid allowing deprecated, weak encryption. Change the \/usr\/local\/unravel\/conf\/server.xml.orig Here we create a PKCS12 keystore, the most widely used keystore format. The inputs are expected to be the CA chain, the signed certificate, and the private server key in PKCS8 format (if in base64 \"PEM\" encoding, it will have \"BEGIN ENCRYPTED PRIVATE KEY\"). In this example, we refer to an imaginary domain unravel.example.com with a suggested file naming convention. Create a keystore.p12 file using openssl cat my_root_ca.cert IntermediateCA.crt unravel_example_com_certificate.crt > combined.crt\n\nopenssl pkcs12 -export -in unravel_example_com_certificate.crt \\\n -inkey unravel_example_com_RSA_private.key -out \/usr\/local\/unravel\/etc\/keystore.p12 \\\n -name unravel.example.com -CAfile combined.crt -caname root -chain\n\nchown unravel.unravel \/usr\/local\/unravel\/etc\/keystore.p12 Note: Substitute local values for my_root_ca.cert, IntermediateCA.crt unravel_example_com_certificate.crt example.com unravel_example_com_RSA_private.key as needed. For additional info see https:\/\/knowledge.rapidssl.com\/support\/ssl-certificate-support\/index?page=content&actp=CROSSLINK&id=SO17070 Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/unravel.example.com:3003 Restart Unravel web UI: sudo service unravel_tc restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-ad56e728-bfe5-9639-7791-865b332953ed", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you pre...", 
"body" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter (Return key), it will emit something like: ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-0215733e-d5dd-b279-33ab-02197e86679a", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use Hi...", 
"body" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use HiveServer2 LDAP, then follow the steps below. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties Change QA ldap:\/\/LDAP_HOST For QA For ldap:\/\/LDAP_HOST ldaps:\/\/LDAP_HOST unravel\/jre\/ ldap:\/\/ldap_host:9999 For Active Directory (AD), example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.Domain=QA Change LDAP_HOST and QA (use the Microsoft \"domain\", not DNS) to appropriate value for your installation. For Active Directory (AD) with base DN defined, example 2: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldaps:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myou,dc=domain,dc=com\nhive.server2.authentication.ldap.guidKey=uid Change LDAP_HOST, ou=myou,dc=domain,dc=com to appropriate value for your installation. For guidKey, the default username attribute in windows AD is uid but could also be sAMAccountName . For Open LDAP, example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 2: In this example, we expect a typical DN to be uid=%s,ou=myunit,dc=example,dc=com where %s is the login name as typed in the login form. In some cases 'cn' is used in place of 'uid'. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 3: In this example, we use a manager or admin account to bind, and then search for user logins. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.search_bind_authentication=true\ncom.unraveldata.ldap.base=ou=myunit,dc=example,dc=com\ncom.unraveldata.ldap.bind_dn=cn=Manager,dc=example,dc=com\ncom.unraveldata.ldap.bind_pw=????????\ncom.unraveldata.ldap.url=ldap:\/\/localhost:389\ncom.unraveldata.ldap.user_name_attr=uid Change the example ou=myunit,dc=example,dc=com , bind_pw, url, bind_dn and base to appropriate values for your installation. 2. Restart unravel_tc Restart unravel_tc # sudo \/etc\/init.d\/unravel_tc restart Advanced Hive Properties Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The process of authentication is described next. Authentication Process for Active Directory (AD) Bind as username + at sign + domain, using the given password, with simple LDAP auth mode verbose log will show Connecting Connected If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group filter is specified, it is checked if a user filter is specified, it is checked Authentication Process for Open LDAP Bind as cn or uid =username + baseDN using the given password, with simple LDAP auth mode the guidKey property determines whether cn or uid is used if userDNPattern is used, it takes precedence over baseDN, and each pattern is tried If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group pattern or filter is specified, it is checked if a user filter is specified, it is checked Property Description Example Value hive.server2.authentication.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. REQUIRED for advanced query except when setting custom query or groupDNPattern. uid or CN hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). hiveuser,impalauser,hiveadmin,hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member, uniqueMember or memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group, groupOfNames or groupOfUniqueNames com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_tc_webapp.log file under \/usr\/local\/unravel\/logs\/ ; when enabled, user names and group names can appear in this log, but raw passwords are not logged. Can be true or false or not set; default is false " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/configurations/security-configurations.html#UUID-a2895be8-5a7b-6b94-64e6-d0d23eda0f97", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with sudo \/etc\/init.d\/unravel_all.sh restart Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: chmod 444 \/path\/to\/jks_keystore\nln -s \/path\/to\/jks_keystore \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Substitute the path for your local settings for \/path\/to\/jks_keystore and ensure that the target file is updated whenever your CA certificates are updated. Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts or jssecacerts file it contains is up-to-date. If this is convenient, you can edit \/usr\/local\/unravel\/etc\/unravel.ext.sh For example: export JAVA_HOME \/usr\/java\/jdk1.8 Substitute your local settings for \/usr\/java\/jdk1.8 Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts to jssecacerts so that an upgrade of Unravel will preserve your change: cd \/usr\/local\/unravel\/jre\/lib\/security\nsudo cp -p cacerts jssecacerts List contents of the jssecacerts keystore: sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias mycompanyca -file something.cer Substitute your local values for mycompanyca and something.cer when you execute this command. " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI....", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. " }, 
{ "title" : "For CDH+CM", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForCDHCM", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For CDH+CM", 
"snippet" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the RES...", 
"body" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/CMGR_HOSTNAME_IP:7180\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore In Unravel Web UI, on the top right-hand corner, click Admin Manage On the left tab, click Hive Encrypting Passwords in Unravel Properties and Settings HIVE METASTORE URL HIVE METASTORE DRIVER HIVE METASTORE USER NAME HIVE METASTORE PASSWORD Save the information when done: click Save Changes Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in the Application " }, 
{ "title" : "For HDP", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForHDP", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For HDP", 
"snippet" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "For MapR", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForMapR", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For MapR", 
"snippet" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrat...", 
"body" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires Uncheck Password must be changed. Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-36fe406d-0d0c-e524-e55f-71f023a65295_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storag...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storage of 20% left to alert contact under \" Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for \" 10 consecutive period(s) Under Actions Send notifications to Note: this sns topic should already be setup before you add it. Click \" Create Alarm Now, you will see in \" Alarms ALARM Alarms Click \" Create Alarm advanced-topics-create-aws-rds-cloudwatch-alarm-for-free-storage-space " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/empty-or-missing-topic-4887.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "UPDATE_NEEDED_FIX_EXTERNAL LNKS_TO_4.2 This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "\n UPDATE_NEEDED_FIX_EXTERNAL LNKS_TO_4.2 This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel ...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages \n \n \n Package Name \n Location \n \n Benchmarks 1.6.x \n \n https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz \n \n Benchmarks 2.0.x \n \n https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where {LOCATION} is full pathname of the benchmark (see above) and {FNAME} is the package name. # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz\n\n71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , \n cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ \n lib: \n scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh \n src: \n tpch-query-instances: \n cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. \n Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \n \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. \n \n example3 \n A Scala program containing a SparkSQL \n How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. \n \n example4 \n A Scala-based application. This application generates its input and applies multiple transformations to the generated data. \n How Unravel helps to root-cause a failed \n \n example5 \n A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group \n Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only \n Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n see example1 in Benchmarks for Spark 1.6.x \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce \n How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. \n \n example3 example4 example5 \n see example3 - example5 in Benchmarks for Spark 1.6.x \n \n example6 \n A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. " }, 
{ "title" : "Supported Roles", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/supported-roles.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel currently supports the following roles:", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/supported-roles.html#UUID-6445e693-14e6-dcf5-6e8d-4949d84c672c_id_SupportedRoles-Unravelcurrentlysupportsthefollowingroles", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles \/ Unravel currently supports the following roles:", 
"snippet" : "Full Admin This role is for operators. It provides access to all features and the ability to invoke every action within Unravel, including setting up Auto Actions, moving jobs to different queues, etc. You have full and complete access to the \"Admin\" pages. Read-only This role is for end-users, e.g....", 
"body" : "Full Admin This role is for operators. It provides access to all features and the ability to invoke every action within Unravel, including setting up Auto Actions, moving jobs to different queues, etc. You have full and complete access to the \"Admin\" pages. Read-only This role is for end-users, e.g., Data Scientists\/Engineers. Access to Unravel features is available in \"read-only\" mode. The \"Admin\" pages are not accessible. " }, 
{ "title" : "Adding Users and Specifying Roles", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/supported-roles.html#UUID-6445e693-14e6-dcf5-6e8d-4949d84c672c_id_SupportedRoles-AddingUsersandSpecifyingRoles", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles \/ Adding Users and Specifying Roles", 
"snippet" : "To add Admins see Adding More Admins to Unravel Web UI. To set up integration with your LDAP in order to easily specify a user's role see Integrating LDAP Authentication of Unravel Web UI....", 
"body" : "To add Admins see Adding More Admins to Unravel Web UI. To set up integration with your LDAP in order to easily specify a user's role see Integrating LDAP Authentication of Unravel Web UI. " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel Sensors", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/unravel-sensors.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/unravel-sensors.html#UUID-19a09628-51a9-570f-2a20-1a74157e0ab4", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute you...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Within this directory, locate the sensor file: unravel-agent-pack-bin.zip 2. Run the Sensor to Intercept Spark Apps Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip\ncd $UNRAVEL_SENSOR_PATH \nhdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\nset UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace # UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR : Parent directory of the Unravel Sensor .zip file, #UNRAVEL_SENSOR_PATH unravel-agent-pack-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the #UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. #SPARK_EVENT_LOG_DIR : #PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit #SPARK_VERSION:Spark version to be instrumented. Valid options are 1.3 1.5 1.6 2.0 For example, export UNRAVEL_SENSOR_PATH=#UNRAVEL_SENSOR_PATH\nexport UNRAVEL_SERVER_IP_PORT=#UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=#SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=#PATH_TO_SPARK_EXAMPLE_JAR\nexport SPARK_VERSION=#SPARK_VERSION\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. Option B: If You Run Spark Apps in yarn-client Mode To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip\nunzip unravel-agent-pack-bin.zip Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace # UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR # UNRAVEL_SENSOR_PATH UNRAVEL_SENSOR_PATH # UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 # SPARK_EVENT_LOG_DIR # PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit spark-examples.jar #UNZIPPED_ARCHIVE_DEST: Directory on the local file system that contains the unzipped content of unravel-agent-pack-bin.zip unravel-agent-pack-bin.zip #SPARK_VERSION:Spark version to be instrumented. Valid options are 1.3 1.5 1.6 2.0 For example, export UNZIPPED_ARCHIVE_DEST=#UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=#UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=#SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=#PATH_TO_SPARK_EXAMPLE_JAR\nexport SPARK_VERSION=#SPARK_VERSION\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Hive Queries", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/unravel-sensors.html#UUID-f1e91ce8-1ff1-db7e-270b-a2ac0d50b6ee", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors \/ Installing Unravel Sensor for Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip text and text with brackets ( { } ), unless otherwise noted, indicates where you must...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel\nsudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Servers and Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Uploading the Spark Program(s) to Unravel", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/unravel-servers-and-sensors/uploading-the-spark-program-s--to-unravel.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading the Spark Program(s) to Unravel", 
"snippet" : "Unravel has the capability to upload Spark program(s) to the Unravel UI. Currently we support the uploading of Java, Scala and Python source code. We do not currently support JVM byte code. HIGHLIGHTED In order to upload a program you must: Package all relevant sources\/program files into a zip archi...", 
"body" : "Unravel has the capability to upload Spark program(s) to the Unravel UI. Currently we support the uploading of Java, Scala and Python source code. We do not currently support JVM byte code. HIGHLIGHTED In order to upload a program you must: Package all relevant sources\/program files into a zip archive. It is advisable to keep the archive small by including only the relevant Driver source files. Provide the following information in a spark-submit zip.archive \/home\/adrian\/benchmarks\/scripts\/sources.zip --files zip.archive --conf spark.unravel.program.zip zip.archive the path the to Spark Example Jar Example: FULLY_QUALIFIED_ZIP_PATH FULLY_QUALIFIED_JAR_PATH export PROGRAM_FILES_ZIP={FULLY_QUALIFIED_ZIP_PATH} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $PROGRAM_FILES_ZIP \\\n --conf \"spark.unravel.program.zip=$PROGRAM_FILES_ZIP\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR After the Spark application has completed, we can visualize the Spark program(s) under the Program tab, which is situated on the right hand side of the application page. Unravel also highlights the line of code corresponding to a given RDD node of the execution graph when a RDD node is selected (by clicking on it). In the illustrative example shown below, MapPartitionsRDD was evaluated on line 324 of QueryDriver.scala. " }, 
{ "title" : "Using an External MySQL or Compatible Database for Unravel", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Compatibility", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-Compatibility", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Compatibility", 
"snippet" : "Unravel default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server uses the mariadb connector. The following versions of MySQL are known to work: MySQL 5.5...", 
"body" : "Unravel default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server uses the mariadb connector. The following versions of MySQL are known to work: MySQL 5.5.x or AWS RDS or MariaDB or Percona equivalent The following MySQL versions are NOT yet certified by our QA department: MySQL 5.6.x MySQL 5.7.x Replication is not required, but it can be used in order to support online backup via mysqldump or LVM or Percona XtraBackup. Unravel will only communicate with the master. " }, 
{ "title" : "Configuration Requirements for MySQL", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-ConfigurationRequirementsforMySQL", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Configuration Requirements for MySQL", 
"snippet" : "Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size= 64M thre...", 
"body" : " Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size= 64M thread_concurrency = 6 max_connections = 500 max_connect_errors=2000000000 open_files_limit=10000 port-open-timeout=121 expire-logs-days=2 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_open_files=2000 innodb_file_per_table=1 innodb_data_file_path = ibdata1:100M:autoextend innodb_buffer_pool_size = 1G innodb_flush_method=O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency=20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format=mixed if SSD disk is used, then also set: innodb_io_capacity = 4000 Note The innodb_buffer_pool_size depends on load and cluster size. On a dedicated machine, it can be 50% of the RAM size. Using 1G is absolute minimum. For a large cluster we use 48G. " }, 
{ "title" : "Set Up Steps", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-SetUpSteps", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Set Up Steps", 
"snippet" : "Install the database Setting Retention Verify that the database host Create \" unravel \" user Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c8)\" Log into your mysql instance as admin\/master user and do the following commands, substituting above { DB_...", 
"body" : " Install the database Setting Retention Verify that the database host Create \" unravel \" user Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c8)\" Log into your mysql instance as admin\/master user and do the following commands, substituting above { DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into the mysql using the mysql commandline client as user unravel DB_PASSWORD Stop Unravel server: # sudo \/etc\/init.d\/unravel_all.sh stop Configure unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Modify the property lines below so that they reflect your particular values: unravel.jdbc.username=unravel\nunravel.jdbc.password=ENC(QioHJsum9rmqKROG0DRqbU51)\nunravel.jdbc.url=jdbc:mysql:\/\/10.0.0.99:3306\/unravel_mysql_prod\n Use the actual values your set in the steps above. Use an ip address if you want to avoid DNS lookups, but hostname is also okay. The database password can be encrypted Dump Bundled DB with Schema On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \\\n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \\\n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \\\n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \\\n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into MySQL Load the initial db with schema into the MySQL instance, substituting HOST unravel # gunzip --stdout {DEST_FILE} | \/usr\/local\/unravel\/mysql\/bin\/mysql --host={HOST} -u unravel -p --port=3306 --force unravel_mysql_prod Disable bundled db in Unravel server: # sudo chkconfig unravel_db off Start Unravel server: # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Workflows", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web ser...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web service are in the same cluster. If You Use Http For Airflow Web UIAccess Add the following 3 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace { airflow_web_url com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= airflow_web_url com.unraveldata.airflow.available=true Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart If You Use Https For Airflow Web UIAccess Add the following 4 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace { airflow_web_url com.unraveldata.airflow.server.url= airflow_web_url com.unraveldata.airflow.available=true Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart If You Enabled login security For Airflow Web UIAccess com.unraveldata.airflow.login.name= airflow_web_UI_UserName com.unraveldata.airflow.login.password={airflow_web_UI_password} Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Range of Monitoring By default, Unravel Server ingests all the workflows that started within the last 5 days. If you wish to change the date range to the last {X} Add the following configuration to \/usr\/local\/unravel\/etc\/unravel.properties airflow.look.back.num.days=-{X} Restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import PythonOperator from da...", 
"body" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Oozie\/Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n 1. Add Hooks for Unravel Instrumentation The example below shows the contents of a bash script, example-hdp-client.sh spark-submit spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport Setting these parameters on a per-application spark-defaults.conf This script references the following variables, which you would need to edit: PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR 2. Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call 2016-08-30 3. Monitor the Workflow To see the new Oozie workflow in Unravel Web UI, select APPLICATIONS Workflows Add Workflow " }, 
{ "title" : "Empty or missing topic", 
"url" : "un42--un42-/unravel-4-2/advanced-topics/workflows/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "un42--un42-/unravel-4-2/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "un42--un42-/unravel-4-2/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If it is not possible for email to be sent directly to unraveldata.com (blocked SMTP port or unapproved recipient domain), then pick Download Diagnostics and email the produced gz file to support people at UnravelData.com. " }, 
{ "title" : "Unravel HDInsight App Support", 
"url" : "un42--un42-/unravel-4-2/cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/unravel-hdinsight-app-support.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Unravel HDInsight App Support", 
"snippet" : "For any technical issues in launching the Unravel HDInsight app or using it, please feel free to contact us at azuresupport@unraveldata.com Unravel app Installation Guide Unravel app User Guide For Licensing and guided PoC, please contact our sales team at sales@unraveldata.com Unraveldata Main numb...", 
"body" : "For any technical issues in launching the Unravel HDInsight app or using it, please feel free to contact us at azuresupport@unraveldata.com Unravel app Installation Guide Unravel app User Guide For Licensing and guided PoC, please contact our sales team at sales@unraveldata.com Unraveldata Main number: (650) 741-3442 " }, 
{ "title" : "Appendices", 
"url" : "un42--un42-/unravel-4-2/appendices.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices", 
"snippet" : "Event List Resource Metrics Some Keywords and Error Messages Unravel Server Reference...", 
"body" : " Event List Resource Metrics Some Keywords and Error Messages Unravel Server Reference " }, 
{ "title" : "Event List", 
"url" : "un42--un42-/unravel-4-2/appendices/event-list.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Event List", 
"snippet" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception HIVE Application failure HIveFailureIncorrectHea...", 
"body" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception HIVE Application failure HIveFailureIncorrectHeaderEvent Indicates that the Hive query fails with \"incorrect header check\" exception HIVE Application failure HiveFailureReturnCodeEvent Indicates that the Hive query fails and shows return code HIVE Application failure HiveMapJoinMemoryExhaustionEvent Indicates that the Hive query fails because it is out of memory in map join, and recommends turning off mapjoin HIVE Application failure HiveOutOfMemoryErrorEvent Indicates that the Hive query fails because it is out of memory HIVE Informational HiveKillFailEvent Indicates that the Hive query is killed or failed with lots of wasted work HIVE Informational HiveSuccWithKillFailEvent Indicates that the Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources HIVE Informational HiveShuffleBytesEvent Indicates that the Hive query has lots of data shuffle from map to reduce side HIVE Informational HiveTimeBreakdownEvent Identifies where time is spent on for the query and points out significant events, including MR-level skew events HIVE Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true HIVE Speedup HiveSingleReduceCountDistinct Indicates that the Hive query has a job with a long single reducer because the query has \"count distinct\" HIVE Speedup HiveSingleReduceLongWait Indicates that the Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side HIVE Speedup HiveSingleReduceOrderBy Indicates that the Hive query has a job with a long single reducer because the query has \"order by\" HIVE Speedup HiveTooFewReduceEvent Indicates that the Hive query is using too few reducers HIVE Speedup\/Resource Utilization HiveTooLargeMapEvent Indicates that mappers in the Hive query are requesting too much memory HIVE Speedup\/Resource Utilization HiveTooLargeReduceEvent Indicates that reducers in the Hive query are requesting too much memory HIVE Speedup\/Resource Utilization HiveTooManyMapEvent Indicates that the Hive query is using too many mappers HIVE Speedup\/Resource Utilization HiveTooManyReduceEvent Indicates that the Hive query is using too many reducers IMPALA Informational\/ Speedup\/ Resource Utilization BottleneckOperatorEvent Displays the longest phase in the Impala query Displays the longest operator in the Impala query. If applicable, this event also shows insights as to why this operator takes the most time, and makes tuning suggestions IMPALA Application failure ImpalaFailureEvent Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible) IMPALA Speedup ImpalaSkewExecutionEvent Indicates whether one of the instances takes much longer time than the other instances for the same operator IMPALA Speedup ImpalaUnderestimatedCounfOfRowsEvent Indicates that an estimate is outdated and should be refreshed MR Application Failure MRClassNotFoundEvent Indicates that MR job fails due to a \"class not found\" exception MR Application Failure MRFailureCompressLibNotAvailable Indicates that MR job fails due to a \"compression library not available\" exception MR Application Failure MRFailureFileNotFoundEvent Indicates that MR job fails due to a \"file not found\" exception MR Application Failure MRFailureIllegalArgumentEvent Indicates that MR job fails due to an \"illegal argument\" exception MR Application Failure MRFailureNumberFormatEvent Indicates that MR job fails due to a \"number format\" exception MR Application Failure MRFailureTimeOutEvent Indicates that MR job fails due to a \"time out\" exception MR Application Failure MRGcOverheadLimitExceededMapEvent Indicates that the MR job fails because the GC overhead limit is exceeded on map side MR Application Failure MRGcOverheadLimitExceededReduceEvent Indicates that the MR job fails because the GC overhead limit is exceeded on reduce side MR Application Failure MRJavaOutOfMemoryMapEvent Indicates that MR job fails because it is out of memory on map side MR Application Failure MRJavaOutOfMemoryReduceEvent Indicates that MR job fails because it is out of memory on reduce side MR Informational MRKillFailEvent Indicates that the MR job is killed or failed with lots of wasted work MR Informational MRSuccWithKillFailEvent Indicates that the MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources MR Informational MRShuffleBytesEvent Indicates that the MR job has lots of data shuffle from map to reduce side MR Informational MRTimeBreakdownEvent Identifies where time is spent on for the job and points out significant events MR Speedup MRLongTasksFromSlowNodeEvent Indicates that the MR job has long-running map\/reduce tasks from slow nodes MR Speedup MRMapSkewDataIOEvent Indicates that the map phase of the MR job has a time skew with strong correlation with IO MR Speedup MRReduceSkewDataIOEvent Indicates that the reduce phase of the MR job has a time skew with strong correlation with IO MR Speedup MRTooFewReduceEvent Indicates that the MR job is using too few reducers MR Speedup\/Resource Utilization MRTooLargeMapEvent Indicates that mappers in the MR job are requesting too much memory MR Speedup\/Resource Utilization MRTooLargeReduceEvent Indicates that reducers in the MR job are requesting too much memory MR Speedup\/Resource Utilization MRTooManyMapEvent Indicates that the MR job is using too many mappers MR Speedup\/Resource Utilization MRTooManyReduceEvent Indicates that the MR job is using too many reducers SPARK Application Failure DriverOomeEvent Indicates that a driver failed with \"OutOfMemory\" error SPARK Application Failure ExecutorOomeEvent Indicates that an executor failed with \"OutOfMemory\" error SPARK Application Failure YarnContainerKilledEvent Indicates that there are containers killed by YARN SPARK Resource Utilization ContainerSizingUnderutiliztionEvent Indicates that container resources are underutilized SPARK Resource Utilization InefficientInputSplitSizeEvent Indicates an inefficient input split size SPARK Resource Utilization TooFewPartitionsEvent Indicates that there are too few partitions with respect to available parallelism SPARK Resource Utilization UnderutilizedCpuEvent Indicates that there is low utilization of CPU resources SPARK Resource Utilization UnderutilizedNodeMemoryEvent Indicates that there is low utilization of memory resources SPARK Resource Utilization UnderutilizedStorageMemoryEvent Indicates that the Spark storage memory has low utilization. More RDDs can be cached in memory SPARK Speedup CachingOpportunityEvent Indicates that there is an (unused) opportunity for RDD caching SPARK Speedup ContendedCpuEvent Indicates that there is contention for CPU resources SPARK Speedup ExcessiveGcEvent Indicates that there is high garbage collection overhead SPARK Speedup ExecutorImbalanceEvent Indicates that there is load imbalance among executors SPARK Speedup ExhaustedStorageEvent Indicates that the Spark storage memory is getting exhausted SPARK Speedup LightExecutorEvent Indicates that there is large idle time for one or several executors SPARK Speedup LongStageEvent Indicates that there is load imbalance among tasks for the longest stage of the application Workflow Informational WorkflowTimeBreakdownEvent Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. Workflow Informational\/ Application Failure WorkflowGeneralFailureEvent For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. Workflow Informational\/ Resource Utilization WorkflowIRSummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources Workflow Informational\/SLA analysis WorkflowDurationAnomalousEvent If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. Workflow Informational\/ Speedup WorkflowIASummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time TEZ Resource Utilization\/Inefficiency TezNoDAGEvent Indicates that the TEZ session was created and TEZ DAG was not submitted. TEZ Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true TEZ Speedup\/Resource Utilization MapVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few mapper task TEZ Speedup\/Resource Utilization MapVertexTooManyTaskEvent Indicates that the TEZ DAG is using too few mapper task TEZ Speedup\/Resource Utilization ReduceVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few reducers task TEZ Speedup\/Resource Utilization ReduceVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many reducers task Note: In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook. " }, 
{ "title" : "Resource Metrics", 
"url" : "un42--un42-/unravel-4-2/appendices/resource-metrics.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time w...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started totalPhysicalMemory bytes The total physical memory in the operating system freePhysicalMemory bytes The free physical memory in the operating system committedVirtualMemory bytes The committed virtual memory in the operating system freeSwap bytes The free swap size availableMemory bytes An estimate of memory available for launching new processes vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process totalSwap bytes The total swap size processCpuLoad PERCENT Average process CPU load for the last minute (all cores) systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) fullGcCount COUNT Number of full GC runs minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms allocatedBytes bytes Accumulated number of allocated bytes edenPeakUsage bytes Maximum memory usage in the eden space survivorPeakUsage bytes Maximum memory usage in the survivor space oldPeakUsage bytes Maximum memory usage in the old space avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms initHeap bytes Initial heap size maxHeap bytes Maximum heap size usedHeap bytes Used heap size committedHeap bytes Committed heap size initNonHeap bytes Initial non-heap size maxNonHeap bytes Maximum non-heap size usedNonHeap bytes Used non-heap size committedNonHeap bytes Committed non-heap size currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "un42--un42-/unravel-4-2/appendices/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type....", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. " }, 
{ "title" : "Spark Keywords", 
"url" : "un42--un42-/unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-SparkKeywords", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Keywords", 
"snippet" : "Spark Key Term Explanation spark.executor.memory Related to executor memory spark.default.parallelism Default number of partitions spark.yarn.executor.memoryOverhead YARN memory overhead spark.sql.shuffle.partitions Number of SparkSQL partitions spark.dynamicAllocation.enabled Enables dynamic alloca...", 
"body" : " Spark Key Term Explanation spark.executor.memory Related to executor memory spark.default.parallelism Default number of partitions spark.yarn.executor.memoryOverhead YARN memory overhead spark.sql.shuffle.partitions Number of SparkSQL partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Executor Process launched by the application on a worker node Driver Process that coordinates the application execution SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SQLContext Main Spark SQL entry point SparkConf Spark configuration object StreamingContext Main Spark Streaming entry point " }, 
{ "title" : "Spark Error Messages", 
"url" : "un42--un42-/unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-SparkErrorMessages", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Error Messages", 
"snippet" : "Spark Error Messages Explanation java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" sho...", 
"body" : " Spark Error Messages Explanation java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow " }, 
{ "title" : "MapReduce\/Hive Keywords", 
"url" : "un42--un42-/unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-MapReduceHiveKeywords", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ MapReduce\/Hive Keywords", 
"snippet" : "Key Term Explanation mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into hive.exec.reducers.bytes.per.reducer Size per reducer mapreduce.job.reduces Default num...", 
"body" : " Key Term Explanation mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into hive.exec.reducers.bytes.per.reducer Size per reducer mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. hive.exec.parallel Whether to execute jobs in parallel. " }, 
{ "title" : "Unravel Server Reference", 
"url" : "un42--un42-/unravel-4-2/appendices/unravel-server-reference.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference", 
"snippet" : "install-hdi-unravel-vm This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "install-hdi-unravel-vm This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "un42--un42-/unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_ngui REST API unravel_db bundled db (on a custom port) unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_ngui REST API unravel_db bundled db (on a custom port) unravel_zk_N bundled Zookeeper (on a custom port) unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_hhs Hive Hook Sensor unravel_hhwe Hive Hook Worker EMR unravel_jcs1 Job Collector Sensor MRv1 unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw1_N Job Collector Sensor Worker MRv1 unravel_jcw2_N Job Collector Sensor Worker YARN unravel_lr Log Receiver unravel_ja \"Job Analyzer\" summarizes jobs unravel_s_N Search engine unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_os3 Oozie v3 Sensor unravel_os4 Oozie v4 Sensor unravel_tw Table Worker unravel_pw Partition Worker unravel_ew_N Event Worker unravel_sw_N Spark Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "un42--un42-/unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/u...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/local\/unravel\/logs Do not set directly; set UNRAVEL_LOG_DIR in etc\/unravel.ext.sh instead and this property will be derived from that com.unraveldata.zk.quorum 127.0.0.1:4181 embedded Zookeeper ensemble in form host1:port1,host2:port2, … com.unraveldata.kafka.broker_list 127.0.0.1:4091 embedded Kafka unravel.jdbc.username unravel MySQL (embedded or external) username for db unravel.jdbc.password random generated for bundled MySQL MySQL (embedded or external) password for db unravel.jdbc.url jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod This is JDBC URL without username and password com.unraveldata.hdfs.interactive.monitoring.interval.sec 30 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) com.unraveldata.hdfs.batch.monitoring.interval.sec 300 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) com.unraveldata.longest.job.duration.days 2 Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) oozie.server.url http:\/\/localhost:11000\/oozie URL for accessing Oozie to track workflows com.unraveldata.oozie.fetch.num 100 Max number of jobs to fetch during an interval com.unraveldata.oozie.fetch.interval.sec 120 seconds between intervals for fetching Oozie workflow status " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "un42--un42-/unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset L...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset Last chance arguments to jvm to override other settings HADOOP_CONF as discovered by running \"hadoop fs -ls \" The directory containing the hadoop config files core-site.xml, hdfs-site.xml, and mapred-site.xml UNRAVEL_DATA_DIR \/srv\/unravel A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data (db_data, k_data, zk_data) and also tmp data if property com.unraveldata.tmpdir is not set. UNRAVEL_LISTEN_PORT 3000 The Web UI port on the primary or standalone Unravel installation (service unravel_tc) which listens on 0.0.0.0 ; the property com.unraveldata.tc UNRAVEL_LOG_DIR \/usr\/local\/unravel\/logs A destination dir. owned by user unravel for log files UNRAVEL_TC_SHUTDOWN_PORT 3005 An unoccupied port used for cleanly stopping the Web UI (service unravel_tc) which listens on 127.0.0.1 " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "un42--un42-/unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs L...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~1.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file to preserve traceability \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME or other settings as shown in table above n\/a Optional; example syntax: export JAVA_HOME=\/path \/srv\/unravel\/log_hdfs log directory for daemons that run as user hdfs (for YARN, when applicable) <2GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR \/srv\/unravel\/tmp_hdfs tmp directory for daemons that run as user hdfs (for YARN, when applicable) <1GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR " }, 
{ "title" : "Release Notes", 
"url" : "un42--un42-/unravel-4-2/release-notes.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes", 
"snippet" : "Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7...", 
"body" : " Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7 " }, 
{ "title" : "Release Notes: Version 4.2", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2 \/ Software Version", 
"snippet" : "On-premise RPM EMR RPM...", 
"body" : " On-premise RPM EMR RPM " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2 \/ New Features", 
"snippet" : "Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD...", 
"body" : " Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD " }, 
{ "title" : "Tested Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-TestedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2 \/ Improvements\/Bugfixes", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Known Issues", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-KnownIssues", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2 \/ Known Issues", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Release Notes: Version 4.2.3", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ Software Version", 
"snippet" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM...", 
"body" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-HowtoDownload", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec " }, 
{ "title" : "Tested Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-TestedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional...", 
"body" : " Optional " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ New Features", 
"snippet" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed ac...", 
"body" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed acyclic graph (DAG). DAG details include query text, DAG graphs, and information on vertices, tasks, and attempts. For more information, see the User Guide Spark Pipeline This release includes an improved pipeline for collecting data from Spark applications. The pipeline provides: Additional metadata Stage level updates: Previously, the Spark APM in Unravel Web UI updated the application's page only once, when the application was finished. With this release, as soon as a stage completes, the Spark APM in Unravel Web UI shows stage level information. This improvement allows you to interact with the Spark APM more often. Smaller memory footprint Smarter events: The event generation algorithm has been updated with additional triggers. Events are generated only if an application's original suggested Workflow tagging: Unravel Web UI now displays tagged workflows identically, irrespective of the mode in which the Spark applications were loaded. Supported loading modes remain the same: OPS mode BATCH mode Applications are tagged even when the event log files are unavailable. Consistent display of information about each Spark application, even if the event log file is not available. At a minimum, a Spark application's page displays the application metadata, taken from the resource manager. Multi-Cluster Kafka Support This release supports data collection for multi-cluster Kafka topics, and Kafka application performance management through Unravel Web UI. Kafka support includes: Multi cluster support Multi cluster metrics monitoring Multi cluster consumer offset\/lag monitoring Consumer groups Single view for CG status\/stats across topics it consumes Additional metrics added for brokers Insights Tunable sliding window algorithm Consumer group lagging\/stalled " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ Improvements and Bug Fixes", 
"snippet" : "Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries ...", 
"body" : " Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries insights improvements Improved time breakdown event Improved suggestions on hash joins Fixed the issue “Unravel not loading eventlog.inprogress file” Fixed the issue “Impala queries not coming up in UI” Fixed the issue “Page refresh changes the sorting order” Fixed the issue “High latency in loading MR jobs”.JCS2 now uses “hdfs ls” as opposed to “du” Fixed the issue “Issue with unravel_us_1 demon” ES migration script to migrate ES mappings for Impala (runs during upgrade of 4.1 or older version to 4.2) " }, 
{ "title" : "Known Issues", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-KnownIssues", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ Known Issues", 
"snippet" : "Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support...", 
"body" : " Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support " }, 
{ "title" : "What's New", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-WhatsNew", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.3 \/ What's New", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Release Notes: Version 4.2.4", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ Software Version", 
"snippet" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM...", 
"body" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-HowtoDownload", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4 " }, 
{ "title" : "Tested Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-TestedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ Unravel Sensor Upgrade", 
"snippet" : "Yes ( See details below)...", 
"body" : " Yes ( See details below) " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.4 \/ Improvements and Bug Fixes", 
"snippet" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creat...", 
"body" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creating rules for Tez Apps AutoActions now can read Application Master job counter metrics and use it in Expert Auto Action rules (ref - https:\/\/hadoop.apache.org\/docs\/r2.4.1\/hadoop-yarn\/hadoop-yarn-site\/MapredAppMasterRest.html#Job_Counters_API Sensors FD leak and HttpUrlConnection issues fixed in spark sensor Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394 ) " }, 
{ "title" : "Release Notes: Version 4.2.5", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5 \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional One improvement in sensors; details under Improvements and Bug Fixes....", 
"body" : " Optional One improvement in sensors; details under Improvements and Bug Fixes. " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5 \/ New Features", 
"snippet" : "UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides reten...", 
"body" : " UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides retention for long running Spark apps (streaming, shells, notebooks) by keeping a maximum number of annotation entries inside the DB and ES. UNRAVEL-2429, UNRAVEL-2430: Support for showing both the Spark program and SQL query in the UI for a SparkSQL query.For applications with multiple queries added a table with KPIs sorted by query runtime to help identify the longest running query and the longest stage of a query. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.5 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server s...", 
"body" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server settings. Bug Fixes UNRAVEL-3070: Fixed memory leak in Spark Worker. UNRAVEL-3030: init script for mysql leaves a process owned by root running. UNRAVEL-3173: Fixed very rare but possible ConcurrentModificationException in Auto Action internal metric stream. UNRAVEL-3427: Fixed. In the cases that query memory metrics are not available, Impala event generator produces NPE. UNRAVEL-3229: Fixed Airflow http connection problem. Sensors UNRAVEL-2858 : Allows slowing down metrics collection in sensor agent. Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394). " }, 
{ "title" : "Release Notes: Version 4.2.6", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6 \/ Software Version", 
"snippet" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6 \/ Unravel Sensor Upgrade", 
"snippet" : "Not Required, recommended for high volume...", 
"body" : " Not Required, recommended for high volume " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6 \/ New Features", 
"snippet" : "UNRAVEL-3576: SLES Parcels support - beta...", 
"body" : " UNRAVEL-3576: SLES Parcels support - beta " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.6 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM us...", 
"body" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM usage UNRAVEL-3552: collect only minimal set of resource usage metrics by default UNRAVEL-3543: Improved reliability in Unravel Impala worker Bug Fixes UNRAVEL-3520: For N\/A type Impala query, fixed NPE in missing fields in query profile Security Audit Issues UNRAVEL-3416: Security Fixes to prevent SQL injection UNRAVEL-3418: XSS - Cross-Site Scripting UNRAVEL-3444: State of settings from db lost in running processes if properties edited by hand UNRAVEL-3583: Email related fixes in Unravel Auto-Action UNRAVEL-3597: Improvements in Auto-Action page loading UNRAVEL-3536: Prevent jackson lib conflict in hive-hook sensor UNRAVEL-3546: Prevent re-submitting application data from unravel_es Known Issues Secure cookies and headers (UNRAVEL-3419, UNRAVEL-3420) " }, 
{ "title" : "Release Notes: Version 4.2.7", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-SoftwareVersion", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ Software Version", 
"snippet" : "Release 04\/11\/2018 For details on downloading updates see here...", 
"body" : "Release 04\/11\/2018 For details on downloading updates see here " }, 
{ "title" : "Certified Platforms", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix...", 
"body" : " CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ Unravel Sensor Upgrade", 
"snippet" : "Recommended. See bug fixes below for further information....", 
"body" : " Recommended. See bug fixes below for further information. " }, 
{ "title" : "New Features", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-NewFeatures", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ New Features", 
"snippet" : "TEZLLAP-5: TEZ APM support on CDH Platform...", 
"body" : " TEZLLAP-5: TEZ APM support on CDH Platform " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by defau...", 
"body" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by default. TEZLLAP-19: Hive Template Changes: Failed and killed Tez DAGs are linked with hive now. Bug Fixes Stability and Robustness Fixes across features Sensor Fixes for Robustness SENSOR-20: Improved sensor configuration problem detection routine SENSOR-17: Fixed RSS calculation for process tree USPARK-15, USPARK-16, USPARK-38 & USPARK-39: Spark Scalability Bug Fixes. Optimized configuration settings for Spark worker daemon. See New Configuration Settings below. UIX-7,UIX-8, UIX-14, UIX-20: Miscellaneous UI Bug Fixes dor stability and correctness. TEZLLAP-6, TEZLLAP-7, TEZLLAP-19 : UNRAVEL-3583, UNRAVEL-3598:Auto Action Bug Fixes Impala tagging via Python script is overridden by Impala Query Options on CDH 5.13 Repackaged embedded Jackson libs " }, 
{ "title" : "Known Issues", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-KnownIssues", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ Known Issues", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "New configuration settings", 
"url" : "un42--un42-/unravel-4-2/release-notes/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-Newconfigurationsettings", 
"breadcrumbs" : "Home \/ UN42 (UN42) \/ Unravel 4.2 \/ Release Notes \/ Release Notes: Version 4.2.7 \/ New configuration settings", 
"snippet" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveld...", 
"body" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveldata.spark.time.histogram " }
]
$(document).trigger('search.ready');
});