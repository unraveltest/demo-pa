$(document).ready(function () {indexDict['en'] = [{ "title" : "Unravel 4.0-4.1", 
"url" : "unravel-4-0-4-1.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1", 
"snippet" : "This documentation is for Unravel versions 4.0-4.1. For Unravel 4.2 documentation, see Unravel 4.2 Unravel Support advanced airflow autoactions btrace cdh cloudera cm daemons diagnostics emr featured hdp hive hortonworks hosts impala insights installation kerberos mapr mapreduce metastore oozie pig ...", 
"body" : " This documentation is for Unravel versions 4.0-4.1. For Unravel 4.2 documentation, see Unravel 4.2 Unravel Support \n advanced \n airflow \n autoactions \n btrace \n cdh \n cloudera \n cm \n daemons \n diagnostics \n emr \n featured \n hdp \n hive \n hortonworks \n hosts \n impala \n insights \n installation \n kerberos \n mapr \n mapreduce \n metastore \n oozie \n pig \n python \n qubole \n retention \n sensor \n spark \n spark-submit \n sqoop \n tagging \n tags \n unravel-4-0 \n unravel-sensor \n user-guide \n workers \n workflow \n workflows \n yarn " }, 
{ "title" : "Overview", 
"url" : "unravel-4-0-4-1/overview.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-88f6d89c-f684-43cd-3425-4052e4c7f7dc_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. \n The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-88f6d89c-f684-43cd-3425-4052e4c7f7dc_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. \n Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-88f6d89c-f684-43cd-3425-4052e4c7f7dc_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API....", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. " }, 
{ "title" : "Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0...", 
"body" : " This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-.html#UUID-e491725f-d884-0f0f-8c6f-c06bd531dcef_id_InstallationGuideforClouderaDistributionIncludingApacheHadoopCDHwithClouderaManagerCM-OrderedSteps", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Ordered Steps", 
"snippet" : "Page: Part 1: Install Unravel Server on CDH+CM Page: Part 2: Install Unravel Sensor Parcel on CDH+CM Page: Part 3: Additional Topics for CDH...", 
"body" : " Page: Part 1: Install Unravel Server on CDH+CM Page: Part 2: Install Unravel Sensor Parcel on CDH+CM Page: Part 3: Additional Topics for CDH " }, 
{ "title" : "Part 1: Install Unravel Server on CDH+CM", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Conf...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). \n Table of Contents \n Introduction \n Pre-Installation Check \n Platform Compatibility \n Hardware \n Software \n Access Permissions \n Network \n 1. Configure the Host Machine \n Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access \n 2. Install the Unravel Server RPM on the Host Machine \n Get the Unravel Server RPM \n Install the Unravel Server RPM \n Do Host-Specific Post-Installation Actions \n 3. Configure Unravel Server (Basic\/Core Options) \n Enable\/Disable Optional Daemons \n Modify unravel.properties \n If Kerberos is Enabled: \n If Sentry is Enabled: \n Do Host-Specific Configuration Steps \n Restart Unravel Server \n 4. Log into Unravel Web UI \n Congratulations! \n 5. (Optional) Configure Unravel Server (Advanced Options) \n Enable Additional Data Collection\/Instrumentation \n Run the Unravel Web UI Configuration Wizard " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-Pre-InstallationCheck", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For ...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to:\n Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-1ConfiguretheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version:\n scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \n The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh \n The RPM installation also creates an HDFS directory for Hive Hook information collection. \n During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties All values in unravel.properties can be modified through the in Unravel Web UI's configuration wizard, for CDH. Furthermore, some properties can only Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n com.unraveldata.customer.organization=Company_and_org \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n com.unraveldata.history.maxSize.weeks=26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay \n \n com.unraveldata.hive.hook.topic.num.threads=1 \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.s3.batch.monitoring.interval.sec \n . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional \n \n com.unraveldata.s3.batch.monitoring.interval.sec=120 If Kerberos is Enabled: Add authentication for HDFS... (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties \n You can find the principal by using 'klist -kt KEYTAB_FILE' If Sentry is Enabled: Add these permissions... \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR \n * \n read+write \n data transfer from Hive jobs when Unravel is not up \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n hdfs \n read \n Spark event log \n \n \n hdfs:\/\/usr\/history\/done \n hdfs \n read \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n hdfs \n read \n YARN aggregation folder \n \n \n hdfs:\/\/user\/hive\/warehouse \n hdfs \n read \n Obtain table partition sizes \n \n Hive Metastore GRANT \n hive \n read \n Hive table information Do Host-Specific Configuration Steps For CDH, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel Web UI", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-4LogintoUnravelWebUI", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ 4. Log into Unravel Web UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-1--install-unravel-server-on-cdh-cm.html#UUID-d4ad11d0-bf9e-3f4a-64bd-618a876ee4c7_id_Part1InstallUnravelServeronCDHCM-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 1: Install Unravel Server on CDH+CM \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel Web UI Configuratio...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 2: Install Unravel Sensor Parcel on CDH+CM", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-_17zqoqqgx4oqIntroduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ Introduction", 
"snippet" : "This topic explains how to install the Unravel Sensor 4.0 parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performa...", 
"body" : "This topic explains how to install the Unravel Sensor 4.0 parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. These instructions apply to Unravel Sensor 4.0. For older versions of Unravel Server, contact Unravel Support. Before following these instructions, follow the steps in Part 1: Install Unravel Server on CDH+CM \n Workflow Summary Obtain and distribute the parcel from Unravel Server. Put the Hive Hook JAR in AUX_CLASSPATH. Configure the gateway automatic deployment of Hive instrumentation. Configure the gateway automatic deployment of Spark instrumentation. Highlighted text below indicates where you must substitute your particular values. When Active Directory Kerberos is used, UNRAVEL_HOST_IP In this section... Table of Contents \n Introduction \n 1. Obtain and Distribute the Parcel from Unravel Server \n 2. Put the Hive Hook JAR in AUX_CLASSPATH \n If Sentry is Enabled: \n 3. Configure the Gateway Automatic Deployment of Hive Instrumentation \n Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways \n Check Unravel Web UI \n 4. Configure the Gateway Automatic Deployment of Spark Instrumentation \n 5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide \n Set in Cloudera Manager \n Troubleshooting \n References " }, 
{ "title" : "1. Obtain and Distribute the Parcel from Unravel Server", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-1ObtainandDistributetheParcelfromUnravelServer", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ 1. Obtain and Distribute the Parcel from Unravel Server", 
"snippet" : "In Cloudera Manager, go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs + Add http:\/\/UNRAVEL_HOST_IP:3000\/parcels\/cdhX.Y\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In...", 
"body" : " In Cloudera Manager, go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs + Add http:\/\/UNRAVEL_HOST_IP:3000\/parcels\/cdhX.Y\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, you can deactivate it now. Then click Activate " }, 
{ "title" : "2. Put the Hive Hook JAR in AUX_CLASSPATH", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-2PuttheHiveHookJARinAUX_CLASSPATH", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ 2. Put the Hive Hook JAR in AUX_CLASSPATH", 
"snippet" : "In Cloudera Manager, for the target cluster, click Hive Configuration In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configurati...", 
"body" : " In Cloudera Manager, for the target cluster, click Hive Configuration In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is Enabled: Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT $ROLE GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE $ROLE " }, 
{ "title" : "3. Configure the Gateway Automatic Deployment of Hive Instrumentation", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-3ConfiguretheGatewayAutomaticDeploymentofHiveInstrumentation", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ 3. Configure the Gateway Automatic Deployment of Hive Instrumentation", 
"snippet" : "Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Note: On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to...", 
"body" : "Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \n Note: On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways In Cloudera Manager (CM): Go to Hive service. Select the Configuration Search for hive-site.xml Add the xml snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks , com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger com.unraveldata.dataflow.hive.hook.HivePostHook Add the xml snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Save the changes with optional comment \"Unravel snippet in hive-site.xml\". Perform action Deploy Hive Client Configuration ) or by using the Actions Restart the Hive and YARN services (click Restart Stale Services \n Again, monitor the situation to see if all Hive queries are failing with a class not found or permission problems. If they are failing hive-site.xml Troubleshooting Check Unravel Web UI If queries are running fine and appearing in Unravel Web UI, then you are done. " }, 
{ "title" : "4. Configure the Gateway Automatic Deployment of Spark Instrumentation", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-4ConfiguretheGatewayAutomaticDeploymentofSparkInstrumentation", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ 4. Configure the Gateway Automatic Deployment of Spark Instrumentation", 
"snippet" : "In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \"spark-defaults\". In the Spark Client Advanced Configuration Snippet (Safety Valve) spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use logical host2 for UNRAVEL_H...", 
"body" : " In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \"spark-defaults\". In the Spark Client Advanced Configuration Snippet (Safety Valve) spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) spark-conf\/spark-defaults.conf UNRAVEL_HOST_IP spark.unravel.server.hostport=UNRAVEL_HOST_IP:4043 \n \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-boot.jar,systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true \n \nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-boot.jar,systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1 \n \nspark.eventLog.enabled=true Notice that in this code block, the blank lines separate only one full line of text that is wrapped due to length. 5. Save changes. 6. Deploy client configuration by clicking the deploy glyph ( Actions Monitor the situation to see if all Spark queries are failing with a class not found or permission problems. If they are failing spark-defaults.conf " }, 
{ "title" : "5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-ConfigureJVMSensorForYARN5Optional-ConfigureYARN-MapReduceMRJVMSensorCluster-Wide", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ 5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide", 
"snippet" : "Set in Cloudera Manager In Cloudera Manager (CM): Go to YARN Select the Configuration Search for ApplicationMaster Java Opts Base ). -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPat...", 
"body" : "Set in Cloudera Manager In Cloudera Manager (CM): Go to YARN Select the Configuration Search for ApplicationMaster Java Opts Base ). -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dcom.sun.btrace.FileClient.flush=-1 -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Notice that in this code block, ensure \"-\" is a minus sign and customer need to modify the value of UNRAVEL_HOST_IP Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) for mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property><name>mapreduce.task.profile<\/name><value>true<\/value><\/property>\n<property><name>mapreduce.task.profile.maps<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.reduces<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dcom.sun.btrace.FileClient.flush=-1 -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043<\/value><\/property> Notice that in this code block, ensure \"-\" is a minus sign and customer need to modify the value of UNRAVEL_HOST_IP 6. Save changes. 7. Deploy client configuration by clicking the deploy glyph ( Actions 8. Restart the YARN services (click Restart Stale Services Monitor the situation and you should see in Unravel UI a Resource Usage tab showing you mappers and reducers when you view the Application page for any completed MRjob. Restart is important for MR sensor to be picked up by queries submitted via Hiveserver2. " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-_troubleshootingTroubleshooting", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ Troubleshooting", 
"snippet" : "Symptom Problem Remedy hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed Install...", 
"body" : " \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ \n shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR \n Verify that unravel \/user\/unravel\/ \n \n error for ClassNotFound com.unraveldata.dataflow.hive.hook.HivePreHook \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR \n Put the Unravel hive-hook JAR corresponding to $HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST \n \n Permission denied writing to hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR \n directories are not writable hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR\/* \n \n hadoop fs -chmod 777 \/user\/unravel\/HOOK_RESULT_DIR\/* \n OR \n Sentry command is needed to give permission \n OR \n revert to your previous hive-site.xml " }, 
{ "title" : "References", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-2--install-unravel-sensor-parcel-on-cdh-cm.html#UUID-e73e92c5-20c5-afb0-da7f-a176de44d51f_id_Part2InstallUnravelSensorParcelonCDHCM-References", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 2: Install Unravel Sensor Parcel on CDH+CM \/ References", 
"snippet" : "see Creating Permanent Functions. {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+...", 
"body" : " see Creating Permanent Functions. {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "Part 3: Additional Topics for CDH", 
"url" : "unravel-4-0-4-1/installation-guide-for-cloudera-distribution-including-apache-hadoop--cdh-,-with-cloudera-manager--cm-/part-3--additional-topics-for-cdh.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Part 3: Additional Topics for CDH", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "Installation Guide for MapR", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR", 
"snippet" : "This guide is compatible with MapR 5.1...", 
"body" : " This guide is compatible with MapR 5.1 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr.html#UUID-5204ceab-4d91-8f7c-84cf-03b60db3dc52_id_InstallationGuideforMapR-OrderedSteps", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Ordered Steps", 
"snippet" : "Page: Part 1: Install Unravel Server on MapR Page: Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Page: Part 3: Additional Topics for MapR...", 
"body" : " Page: Part 1: Install Unravel Server on MapR Page: Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Page: Part 3: Additional Topics for MapR " }, 
{ "title" : "Part 1: Install Unravel Server on MapR", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on the MapR converged data platform. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM ...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on the MapR converged data platform. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). \n Table of Contents \n Introduction \n Pre-Installation Check \n Platform Compatibility \n Hardware \n Software \n Access Permissions \n Network \n 1. Configure the Host Machine \n Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access \n Configure the Host Before installing the RPM \n 2. Install the Unravel Server RPM on the Host Machine \n Get the Unravel Server RPM \n Install the Unravel Server RPM \n Do Host-Specific Post-Installation Actions \n 3. Configure Unravel Server (Basic\/Core Options) \n Enable\/Disable Optional Daemons \n Modify unravel.properties \n If Kerberos is Enabled: \n If Sentry is Enabled: \n Do Host-Specific Configuration Steps \n Restart Unravel Server \n 4. Log into Unravel Web UI \n Congratulations! \n 5. (Optional) Configure Unravel Server (Advanced Options) \n Enable Additional Data Collection\/Instrumentation \n Run the Unravel Web UI Configuration Wizard " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-Pre-InstallationCheck", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to:\n Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-1ConfiguretheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel hadoop fs -mkdir \/user\/unravel hadoop fs -chown unravel:mapr \/user\/unravel If MapR...", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel\nhadoop fs -mkdir \/user\/unravel\nhadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr \/etc\/unravel_ctl Check available RAM to ensure availability: free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 (only change this setting on the Unravel gateway\/client machine). And the restart mfs. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version:\n scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \n The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh \n The RPM installation also creates an HDFS directory for Hive Hook information collection. \n During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n com.unraveldata.customer.organization=Company_and_org \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n com.unraveldata.tmpdir=\/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n com.unraveldata.history.maxSize.weeks=26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay \n com.unraveldata.hive.hook.topic.num.threads=1 \n \n \n com.unraveldata.job.collector.done.log.base \n HDFS path to \"done\" directory of MR logs \n com.unraveldata.job.collector.done.log.base=\/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done \n \n \n com.unraveldata.job.collector.log.aggregation.base \n An HDFS path that helps locate MR job logs to process \n com.unraveldata.job.collector.log.aggregation.base=\/tmp\/logs\/*\/logs\/ \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n com.unraveldata.spark.eventlog.location=maprfs:\/\/\/apps\/spark \n \n \n yarn.resourcemanager.webapp.address \n Resource Manager web app address \n yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 \n \n \n oozie.server.url \n Oozie URL \n oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: Add authentication for HDFS... (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=unravel\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n If Sentry is Enabled: Add these permissions... \n \n \n Resource \n Principal \n Access \n \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR \n * \n read+write Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\n echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel Web UI", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-4LogintoUnravelWebUI", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ 4. Log into Unravel Web UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. Check Unravel Web UI for MR jobs loading: on ...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. Check Unravel Web UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-1--install-unravel-server-on-mapr.html#UUID-cfa5f807-5fa9-ca14-2b9e-e645949388ff_id_Part1InstallUnravelServeronMapR-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 1: Install Unravel Server on MapR \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unrave...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the Unravel Web UI User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ Introduction", 
"snippet" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's ...", 
"body" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. " }, 
{ "title" : "1. Enable Additional Instrumentation on Unravel Server's Host", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-1EnableAdditionalInstrumentationonUnravelServersHost", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 1. Enable Additional Instrumentation on Unravel Server's Host", 
"snippet" : "Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_s...", 
"body" : " Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_setup.sh host1 sudo yum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\nsudo .\/unravel_mapr_setup.sh install -y --unravel-server $UNRAVEL_HOST:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION \n Note: In the code above, substitute valid values for: \n UNRAVEL_HOST_IP - fully qualified host name or IP address \n SPARK_VERSION - target Spark version \n HIVE_VERSION - target Hive version " }, 
{ "title" : "2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-2EntercorrectvalueforHiveMetastoreResourceManagerandOozieproperties", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"snippet" : "Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore # ### Uncomment and enter correct HM java.jdo.option.Connection properties ### #javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive #javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_...", 
"body" : "Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore\n#\n### Uncomment and enter correct HM java.jdo.option.Connection properties ###\n#javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive\n#javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_zzzzzz\n#javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\n#javax.jdo.option.ConnectionDriverName=org.postgresql.Driver\n#javax.jdo.option.ConnectionUserName=hiveuser?\n#javax.jdo.option.ConnectionPassword=???????\n\n# optional selectivity of databases to analyze in metastore\n#com.unraveldata.metastore.databasePattern=s*|t*|d*\n\n# Resource Manager (RM)\n#\n# Enable https access to Resource Manager\n#https.protocols=TLSv1.2\n#\n### Uncomment and enter correct below properties RM ###\n#yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 \n\n# Resource Manager username to login\n#yarn.resourcemanager.webapp.username=foo\n\n# Resource Manager password to login\n#yarn.resourcemanager.webapp.password=?????\n\n#\n# oozie\n#\n### Uncomment below oozie properties when oozie is used ###\n# oozie.server.url=http:\/\/<oozie-hostname-IP-address>:11000\/oozie Then restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "3. Confirm that Unravel Web UI Shows Additional Data", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-3ConfirmthatUnravelWebUIShowsAdditionalData", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 3. Confirm that Unravel Web UI Shows Additional Data", 
"snippet" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and ...", 
"body" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. sudo -u $someUser \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. \n Note: Replace $someUser with a user that can create tables in the default database. If you need to use a different database, copy the script and edit it to change the target database. This script runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. " }, 
{ "title" : "4. Confirm and Adjust the Settings in yarn-site.xml", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-4ConfirmandAdjusttheSettingsinyarn-sitexml", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 4. Confirm and Adjust the Settings in yarn-site.xml", 
"snippet" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml : yarn.resourcemanager.webapp.address <property> <name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value> <source>yarn-site.xml<\/source> <\/property> : yarn.log-aggregation-enab...", 
"body" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml : yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> : yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> " }, 
{ "title" : "5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-2--enable-additional-data-collection---instrumentation-for-mapr.html#UUID-f92dddcf-579c-b392-23cb-3b2600b03b87_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-5EnableAdditionalInstrumentationonOtherHostsintheCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"snippet" : "To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar ...", 
"body" : " To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar from Unravel to your existing mechanism. Run the shell script unravel_mapr_setup.sh Copy the newly edited (in the previous step 4) yarn-site.xml to all nodes. Do a rolling-restart of HiveServer2 " }, 
{ "title" : "Part 3: Additional Topics for MapR", 
"url" : "unravel-4-0-4-1/installation-guide-for-mapr/part-3--additional-topics-for-mapr.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for MapR \/ Part 3: Additional Topics for MapR", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters", 
"snippet" : "This guide is compatible with Amazon EMR 3.4 - 4.7...", 
"body" : " This guide is compatible with Amazon EMR 3.4 - 4.7 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters.html#UUID-9a17adfa-8c96-d07d-8020-55ae3afecd31_id_InstallationGuideforAmazonElasticMapReduceAmazonEMRandQuboleClusters-OrderedSteps", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Ordered Steps", 
"snippet" : "Page: Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster Page: Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Page: Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Page: Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup Page: Part 5: Addi...", 
"body" : " Page: Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster Page: Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Page: Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Page: Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup Page: Part 5: Additional Topics for EMR or Qubole " }, 
{ "title" : "Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"snippet" : "Table of Contents Introduction Requirements Checklist Platform Compatibility Software Hardware Network 1. Configure the Cluster Provision an Instance Configure the Environment at First Login Configure Ephemeral Storage Configure Durable Storage [HVD] 2. Install the Unravel Server RPM on the Cluster ...", 
"body" : "\n Table of Contents \n Introduction \n Requirements Checklist \n Platform Compatibility \n Software \n Hardware \n Network \n 1. Configure the Cluster \n Provision an Instance \n Configure the Environment at First Login \n Configure Ephemeral Storage \n Configure Durable Storage [HVD] \n 2. Install the Unravel Server RPM on the Cluster \n Get the Unravel Server RPM \n Install the Unravel Server RPM \n Grant Access to Unravel Server \n Supply Credentials Needed for EMR and EC2 \n Create IAM User(s) and Credentials \n S3 Read-Only Access \n EMR API Read-Only Access \n Cloudwatch API Read-Only Access \n 3. Configure Unravel Server (Basic\/Core Options) \n Copy S3 and EMR Credential Files to Unravel Server \n Open an SSH Session to Unravel Server \n Set Correct Permissions on the Unravel Configuration Directory \n Modify unravel.properties \n Enable\/Disable Optional Daemons \n Enable Collection from Hive Metastore \n Adjust Storage Locations [HVD] \n Start Unravel Server \n Set Up an External DB [HVD] \n Set Up RDS MySQL [HVD] \n Dump Bundled DB with Schema [HVD] \n Load DB with Schema Into RDS MySQL [HVD] \n Configure to Use RDS MySQL [HVD] \n Restart Unravel Server \n 4. Log into Unravel Web UI \n Congratulations! \n 5. (Optional) Enable Additional Data Collection\/Instrumentation " }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. \n Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. \n Workflow Summary Configure the cluster. Install the Unravel Server RPM on the cluster. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Enable additional data collection\/instrumentation. " }, 
{ "title" : "Requirements Checklist", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-RequirementsChecklist", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Requirements Checklist", 
"snippet" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in P...", 
"body" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH If Spark is in use, Spark client gateway Open signup or LDAP for Unravel Web UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/ Disk: \/srv For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Network Port 3000 (or 4020) for Unravel Web UI access HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) unless Hive-hook via HDFS option is used For Oozie, port 11000 open to Unravel Server(s) " }, 
{ "title" : "1. Configure the Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-1ConfiguretheCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 1. Configure the Cluster", 
"snippet" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because t...", 
"body" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because the EBS partition you create\/mount below must be in the same AZ. [HVD] Security Group\n Unravel Server works with multiple EMR clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the master node of each EMR cluster to Unravel Server. Simplest approach it to make Unravel server a member of the security group ElasticMapReduce-master when the instance running Unravel Server is first launched. If Unravel Server is already started or if you prefer more security groups, open ports 3000 (TCP) and 4041-4043 (TCP and UDP) from the ElasticMapReduce-master group to a new server security group called 'unravel'. Start ntpd Configure the Environment at First Login Disable selinux # sudo setenforce Permissive\n Edit the file to make sure setting persists after reboot. Make sure SELINUX=permissive # vi \/etc\/selinux\/config\n Install libaio.x86_64 # sudo yum -y install libaio.x86_64 Install lzop # sudo yum install lzop.x86_64 Configure Ephemeral Storage Find the available ephemeral storage (also called instance storage lsblk Tip \n It can be convenient to use \/srv Find block devices with no mount point: # lsblk For each unmounted area do the following steps, substituting correct value for { Z} EPHEMERAL # sudo mkfs.ext4 \/dev\/xvd{Z}\n If necessary, create mount point and check if mounted, # mkdir $EPHEMERAL \n# echo \"\/dev\/xvd{Z} $EPHEMERAL ext4 defaults,noatime,nodiratime 1 2\" | sudo tee -a \/etc\/fstab \n# sudo mount -a\n# df -h $EPHEMERAL \n Make a note of the path to the mounted ephemeral store, referred to as UNRAVEL_EPHEMERAL Configure Durable Storage [HVD] \n In a PoC or test install, this step can be skipped if there is sufficient disk space (at least 100GB) on \/ or under \/srv mounted from an ephemeral ('instance storage') disk area. Here we create a \"Provisioned IOPS\" EBS volume, setting the maximum IOPS, based on the size 300GB. In AWS EC2 console, Volumes in the same AZ as the Unravel server On Unravel server, find the letter { Z} lsblk Use dd Z} # sudo dd if=\/dev\/zero of=\/dev\/xvd{Z} bs=1M Reference:[ | http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html {+} http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html+ Format the volume as ext4 Z # sudo mkfs.ext4 \/dev\/xvd{Z} Mount the new volume (for example, as \/mnt\/unravel_durable UNRAVEL_DURABLE \/etc\/fstab mount -a Z # echo \"\/dev\/xvd{Z} $UNRAVEL_DURABLE ext4 defaults,noatime,nodiratime 1 2\" | sudo tee -a \/etc\/fstab \n# sudo mount -a\n Check if the volume is mounted: # df -h $UNRAVEL_DURABLE " }, 
{ "title" : "2. Install the Unravel Server RPM on the Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-2InstalltheUnravelServerRPMontheCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 2. Install the Unravel Server RPM on the Cluster", 
"snippet" : "The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: ...", 
"body" : " The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: # scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm\" . For the enterprise version:\n # scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm.$timestamp . Install the Unravel Server RPM # sudo rpm -U unravel-4.0*.x86_64.EMR.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \n The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh \n During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password is stored in \/root\/unravel.install.include Grant Access to Unravel Server Assign elastic IP to Unravel Server using AWS Console [unless you use VPC] Adjust internal DNS for new IP [for access via a browser] Request reverse DNS change from AWS [if you plan on adding SSL] Restriction \n Do not make Unravel Server accessible on the public Internet because doing so would violate your licensing terms.\n Supply Credentials Needed for EMR and EC2 Create IAM User(s) and Credentials Open the AWS IAM (Identity and Access Management) console with your browser and then make the following users and credentials so that Unravel Server can access EMR logs stored in S3 and use EMR read-only permission to find EMR clusters for efficient data loading. These credentials are described separately, but can be combined into one user if desired. \n Multiple accounts can also be created per access kind here if Unravel Server is going to monitor multiple accounts; just create multiple credential files. S3 Read-Only Access Create a group named s3ro AmazonS3ReadOnlyAccess Create a user named s3unravel Add user s3unravel s3ro. EMR API Read-Only Access Create a group named emrro AmazonElasticMapReduceReadOnlyAccess Create a user named emrunravel Add user emrunravel emrro Cloudwatch API Read-Only Access Create a group named cwro AmazonCloudWatchReadOnlyAccess Create a user named cwunravel Add user cwunravel cwro " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc # scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unrav...", 
"body" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem cwro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc where: is your RSA key, rsa.pem is the LOCAL_IP of the Unravel Server, and root@xx.xxx.xx.xxx , s3ro.properties emrro.properties cwro.properties [default]\naws_access_key_id = {your access key}\naws_secret_access_key = {your secret key} You can create multiple credentials of the same type for multiple accounts by creating multiple files with the same base name and appending \".1\" for the second account, \".2\" for the third account, and so on. For example, using the file naming convention suggested above, copy these additional files into \/usr\/local\/unravel\/etc\/ s3ro.properties.1\nemrro.properties.1\ncwro.properties.1 All three files are required for each account. Open an SSH Session to Unravel Server Replace { somefile.pem} - must be a fully qualified path. UNRAVEL_HOST_IP # ssh -i {somefile.pem} root@$UNRAVEL_IP Set Correct Permissions on the Unravel Configuration Directory # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties \n# sudo chmod 644 *.properties Modify unravel.properties \n The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the following values in unravel.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \ncom.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \ncom.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties Adjust other values in unravel.properties \n \n \n Property \n Description \n Example \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n com.unraveldata.customer.organization=Company_and_org \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n com.unraveldata.tmpdir=\/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n com.unraveldata.history.maxSize.weeks=26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay \n \n com.unraveldata.hive.hook.topic.num.threads=1 \n \n com.unraveldata.s3.profile.config.file.path \n Location of Unravel s3 read-only access & secret key filename s3ro.properties. \n com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \n \n com.unraveldata.emr.profile.config.file.path \n Location of Unravel EMR read-only access & secret key filename emrro.properties. \n com.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \n \n com.unraveldata.cloudwatch.profile.config.file.path \n Location of Unravel Cloud Watch read-only access & secret key filename cwro.properties. \n com.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties \n \n com.unraveldata.spark.s3.profilesToBuckets \n s3 profile associated to the s3 bucket \n For 1 bucket, follow example as follows: com.unraveldata.spark.s3.profilesToBuckets=<default>:<s3_bucket> For 2 buckets follow below example: com.unraveldata.spark.s3.profilesToBuckets=<s3_profile>:<bucket>,<s3_2nd_profile>:<2nd_s3_bucket> \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.s3.batch.monitoring.interval.sec \n . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional \n \n com.unraveldata.s3.batch.monitoring.interval.sec=120 \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ \n \n \n oozie.server.url \n Oozie URL \n oozie.server.url=http:\/\/example.localdomain :11000\/oozie Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 # sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 # sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_hhw_2 \n# sudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_hhw_3 \n# sudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_hhw_4 \n# sudo chkconfig --add unravel_jcw2_4 Enable Collection from Hive Metastore If you have a central Hive Metastore, you can inform Unravel Server to enable more monitoring and analysis: For MySQL use avax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver instead of the postgresql driver Substitute thecorrect values for your site. # echo \" \n javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/10.0.0.10:7432\/hive_nqz \n javax.jdo.option.ConnectionDriverName=org.postgresql.Driver \n javax.jdo.option.ConnectionUserName=hive_nqz \n javax.jdo.option.ConnectionPassword=123456789abc \n \" | sudo tee -a \/usr\/local\/unravel\/unravel.properties Adjust Storage Locations [HVD] Prepare symlinks from \/srv\/unravel $UNRAVEL_EPHEMERAL $UNRAVEL_DURABLE \/srv Make sure daemons are stopped [HVD] # sudo \/etc\/init.d\/unravel_all.sh stop Check that all Unravel daemons are stopped: # ps -U unravel -f If any processes are owned by Unravel, stop them with a kill command. Check that destination areas are present [HVD] # df -h $UNRAVEL_EPHEMERAL \n# df -h $UNRAVEL_DURABLE Move files and create symlinks [HVD] # sudo \/bin\/mv \/srv\/unravel\/k_data $UNRAVEL_DURABLE\/k_data \n# sudo ln -s $UNRAVEL_DURABLE\/k_data \/srv\/unravel\/k_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_1_data $UNRAVEL_DURABLE\/zk_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_1_data \/srv\/unravel\/zk_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_2_data $UNRAVEL_DURABLE\/zk_2_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_2_data \/srv\/unravel\/zk_2_data This zk # sudo \/bin\/mv \/srv\/unravel\/zk_3_data $UNRAVEL_EPHEMERAL\/zk_3_data \n# sudo ln -s $UNRAVEL_EPHEMERAL\/zk_3_data \/srv\/unravel\/zk_3_data \n\n# sudo \/bin\/mv \/srv\/unravel\/db_data $UNRAVEL_DURABLE\/db_data \n# sudo ln -s $UNRAVEL_DURABLE\/db_data \/srv\/unravel\/db_data \n\n# sudo \/bin\/mv \/srv\/unravel\/s_1_data $UNRAVEL_DURABLE\/s_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/s_1_data \/srv\/unravel\/s_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp $UNRAVEL_EPHEMERAL\/tmp \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp \/srv\/unravel\/tmp \n\n# sudo \/bin\/mv \/srv\/unravel\/log_hdfs $UNRAVEL_EPHEMERAL\/log_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/log_hdfs \/srv\/unravel\/log_hdfs \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp_hdfs $UNRAVEL_EPHEMERAL\/tmp_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp_hdfs \/srv\/unravel\/tmp_hdfs Start Unravel Server # sudo \/etc\/init.d\/unravel_all.sh start Set Up an External DB [HVD] For performance and ease of management, using an RDS MySQL instead of the bundled mysql is recommended. Set Up RDS MySQL [HVD] RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group Create db instance\n Select multi-AZ Select db.m3.large Select provisioned IOPS 1000 Select SSD size (capacity depends on activity level)\n 770GB for 180 days retention (number of days set in unravel.properties) 1.54TB for 360 days retention No read-only replicas needed Prefer overlap with Unravel Server AZ Retain 7 days of snapshots Specify unravel ElasticMapReduce-master Name db instance \" unravelX Use MySQL 5.5.42 Disable auto-minor-upgrade Define a parameter group \" unravel key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Modify unravelX Use unravel Take effect immediately create \" unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c10)\" Log into RDS mysql instance from Unravel Server as admin\/master user and do the following commands, substituting above DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into RDS mysql as user unravel DB_PASSWORD Dump Bundled DB with Schema [HVD] On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into RDS MySQL [HVD] Load the initial db with schema into the RDS MySQL instance, substituting $RDS_HOST unravel # gunzip --stdout $DEST_FILE | \/usr\/local\/unravel\/mysql\/bin\/mysql --host=$RDS_HOST -u unravel -p --port=3306 --force unravel_mysql_prod Configure to Use RDS MySQL [HVD] Configure unravel.properties Edit the file: # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust existing properties to point to new RDS MySQL. Change example values as needed: unravel.jdbc.username=unravel \nunravel.jdbc.url=jdbc:mysql:\/\/unravelrds.something.REGION.rds.amazonaws.com:3306\/unravel_mysql_prod \nunravel.jdbc.password=****** Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" " }, 
{ "title" : "4. Log into Unravel Web UI", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-4LogintoUnravelWebUI", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 4. Log into Unravel Web UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel Web ...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-1--install-unravel-server-for-amazon-emr-or-qubole-cluster.html#UUID-4083b9b2-b9ea-b6fc-bbee-b369c04f8de1_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-5OptionalEnableAdditionalDataCollectionInstrumentation", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"snippet" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup...", 
"body" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup " }, 
{ "title" : "Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"snippet" : "Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values....", 
"body" : " Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. " }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html#UUID-ce59cb31-0b1b-ab82-d5a5-5af34ffdc8a7_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Introduction", 
"snippet" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already run...", 
"body" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already running), do a \"setup\" procedure: follow the steps 3 to 5 in Hive Bootstrap and Unravel Hive Hook Sensor Setup. " }, 
{ "title" : "Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-2--install-unravel-hive-sensor-on-qubole-hadoop2-hive-cluster.html#UUID-ce59cb31-0b1b-ab82-d5a5-5af34ffdc8a7_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-HiveBootstrapandUnravelHiveHookSensorSetup", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"snippet" : "{location_in_s3_where_unravel_jar_folder} - is the s3 location where the unravel hive hook jar will be accessed {Unravel_Hostname_IP} - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Add a one-time Unravel Hive Bootstrap ...", 
"body" : " {location_in_s3_where_unravel_jar_folder} - is the s3 location where the unravel hive hook jar will be accessed {Unravel_Hostname_IP} - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Add a one-time Unravel Hive Bootstrap into Qubole's control panel on the left-hand side in the \"Hive Bootstrap\" section, as follows: add jar s3n:\/\/{location_in_s3_where_unravel_jar_folder} \/unravel-hive-0.13.0-hook.jar;\n\nset com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR;\nset hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook;\nset hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook;\nset hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook;\nset hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook;\nset com.unraveldata.host=<Unravel_Hostname_FQDN_Internal_IP>;\nset com.unraveldata.hive.hook.tcp=true; Determine the Hive version that Qubole uses, and use that value for HIVE_VER To determine the Hive version Qubole uses, see http:\/\/docs.qubole.com\/en\/latest\/faqs\/hive\/version-hive-qubole-provide.html must be a Hive version that Unravel Server supports: either HIVE_VER 1.2.0 0.13.0 On the master node of the Qubole Hadoop2\/Hive cluster, check that Unravel Server is reachable. Ensure the security group on the master\/slave allows TCP port #3000 and #4043 accessible. # curl http:\/\/{Unravel_Hostname_IP}:3000\/version.txt If the version information is not visible, adjust security groups and routing and try again. SSH to the master server of the existing Qubole Hadoop2\/Hive cluster, run the following commands: Use curl # curl http:\/\/{Unravel_Hostname_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_setup.sh > unravel_qubole_setup.sh Ensure wget Files will be installed under: Hive hook jar is \/usr\/local\/unravel_client\/. Spark jar is \/usr\/local\/unravel-spark\/. Unravel ES is \/usr\/local\/unravel_es\/. On the master node, \/etc\/init.d\/unravel_es service will be defined and started. # yum install -y wget\n# chmod 755 unravel_qubole_setup.sh \n# sudo .\/unravel_qubole_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 Verify if the setup works in Qubole by invoking Analyze and execute following Hive test query: set hive.on.master=true ;\nselect count(*) from default_qubole_memetracker; " }, 
{ "title" : "Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-d21298d1-03a5-42e9-cb35-852185917689_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Introduction", 
"snippet" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your parti...", 
"body" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your particular values. text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED must be a fully qualified DNS or the IP address. UNRAVEL_HOST_IP : is the absolute path to the location of the sensor jars. PATH_TO_SENSOR_JARS " }, 
{ "title" : "Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-d21298d1-03a5-42e9-cb35-852185917689_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AddUnravelSparkInstrumentationtoNewQuboleSparkCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"snippet" : "Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties [default] aws_access_key_id = {ACCESS_KEY_VALUE1} aws_secret_access_key = {SECRET_KEY_VALUE1} [profile_name_2] aws_access_key_id = {ACCESS_KEY_VALUE2} aws_secret_access_key = {SECRET_KEY_...", 
"body" : " Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties [default]\naws_access_key_id = {ACCESS_KEY_VALUE1}\naws_secret_access_key = {SECRET_KEY_VALUE1}\n\n[profile_name_2]\naws_access_key_id = {ACCESS_KEY_VALUE2}\naws_secret_access_key = {SECRET_KEY_VALUE2} \n Note: Substitute actual values for ACCESS_KEY_VALUEx and SECRET_KEY_VALUEx. Edit \/usr\/local\/unravel\/etc\/unravel.properties s3ro.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties\ncom.unraveldata.spark.s3.profilesToBuckets=<default>:<s3ro_bucket1>,<profile_name_2>:<s3r0_bucket2> Restart the Unravel ETL daemon(s): # sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Ensure ports 3000 (for web UI access) and 4043 (from cluster) are open for incoming traffic. These should not Verify that port 3000 is open by running a curl If the version information is not visible (request timeout), then adjust security groups, firewalls, etc. and try again. For VPCs, it might be necessary to add a route. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt Copy the Unravel Spark bootstrap file from Unravel Server to your Spark Qubole cluster, using curl # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_bootstrap.sh > unravel_qubole_bootstrap.sh \n Note: Substitute the actual host for UNRAVEL_HOST_IP. Copy Unravel Spark bootstrap file to your s3:\/\/ Bootstrap_location_for_Spark Qubole Cluster aws s3 cp unravel_qubole_bootstrap.sh s3:\/\/{Bootstrap_location_for_Spark Qubole cluster}\/unravel_qubole_bootstrap.sh In Qubole’s Edit Cluster Setting do not unravel_qubole_bootstrap.sh In Qubole, scripts do not take input parameters. Therefore, Unravel's bootstrap script takes all the required parameters from the Hadoop configuration. You can customize your Hadoop configuration through specific parameters within the Override Hadoop Configuration Variables unravel-bootstrap \n Note: Separate parameters with commas. Add these settings to unravel-bootstrap unravel-bootstrap=UNRAVEL_SERVER=UNRAVEL_HOST_AND_PORT, SPARK_VER_XYZ=SPARK_VER_XYZ[,SPARK_APP_LOAD_MODE=SPARK_APP_LOAD_MODE,WRAPPED_SCRIPT=WRAPPED_SCRIPT] \n SPARK_VERSION_X.Y.Z- target Spark version (eg. 1.3.0, 1.6.0, 2.0.1, etc.) Note: \n The parameters in square brackets [ ] above are optional. Their meanings are: is either OPS or DEV mode, and if not explicitly specified defaults to OPS mode. For details on application loading modes, see SPARK_APP_LOAD_MODE Appendix is the full S3 path to your original bootstrap script. Your bootstrap script is invoked from WRAPPED_SCRIPT unravel_qubole_bootstrap.sh Confirm that the unravel_es Open an SSH session to the Qubole master node. Run this command to check that the unravel_es service has been started: # ps -aux | grep unravel_es " }, 
{ "title" : "(Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-d21298d1-03a5-42e9-cb35-852185917689_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-OptionalAddUnravelSparkInstrumentationtoanExistingQuboleSparkCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ (Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"snippet" : "Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-sensor-for-spark-bin-1.6....", 
"body" : " Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-sensor-for-spark-bin-1.6.zip\n To obtain Spark version 2.0.x zip file: # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-sensor-for-spark-bin-2.0.zip Unzip the archive unravel-sensor-for-spark-bin-1.6.zip PATH_TO_SENSOR_JARS Obtain EMR jar files and snippet script that should be added to the bootstrap action to start the unravel_es process: \n#wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/snippets\/run-es.sh\n#wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-emr-sensor.jar\n Ensure that unravel-emr-sensor.jar unravel-emr-sensor.jar PATH_TO_SENSOR_JARS Edit run-es.sh # Replace following two parameters into your Qubole Spark environment #\nUNRAVEL_HOST={UNRAVEL_HOST_IP}\nUNRAVEL_EMR_SENSOR_JAR={PATH_TO_SENSOR_JARS} Spark configuration can be provided in theQubole consoleat bootstrap or directly inside spark-defaults.conf spark.unravel.server.hostport {UNRAVEL_HOST_IP}:4043\nspark.driver.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=bootClassPath={PATH_TO_SENSOR_JARS}\/unravel-boot.jar,systemClassPath=<PATH_TO_SENSOR_JARS>\/unravel-sys.jar,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1\nspark.executor.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=bootClassPath={PATH_TO_SENSOR_JARS}\/unravel-boot.jar,systemClassPath={PATH_TO_SENSOR_JARS}\/unravel-sys.jar,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1 Edit zeppelin-env.sh ZEPPELIN_JAVA_OPTS export ZEPPELIN_JAVA_OPTS=\"-Dcom.sun.btrace.FileClient.flush=-1 -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=bootClassPath={PATH_TO_SENSOR_JARS}\/unravel-boot.jar,systemClassPath={PATH_TO_SENSOR_JARS}\/unravel-sys.jar,script=DriverProbe.class:SQLProbe.class\" The Zeppelin configuration is located at \/usr\/lib\/zeppelin\/conf\/zeppelin-env.sh " }, 
{ "title" : "Appendix", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-3--install-unravel-spark-sensor-on-new-or-existing-qubole-spark-cluster.html#UUID-d21298d1-03a5-42e9-cb35-852185917689_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AppendixAppendix", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Appendix", 
"snippet" : "Application Loading Modes for Spark Applications: OPS, DEV, and BATCH There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark ...", 
"body" : "Application Loading Modes for Spark Applications: OPS, DEV, and BATCH There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark application completes. BATCH mode loads applications for which the sensor was not enabled at the time the application has been run. You can specify the application load mode for the bootstrap script by setting SPARK_APP_LOAD_MODE OPS DEV SPARK_APP_LOAD_MODE OPS When to Use Which Mode Unravel recommends using OPS mode as the cluster-side default for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false DEV mode is useful during Spark application development. This mode shows a Spark application as soon as the first Spark job of the application finishes. For long running applications, this functionality is useful, as the application is shown in the UI while the application is running. In addition, DEV mode shows applications even when the Spark event log is not being persisted to HDFS or S3. This is an advantage in situations like Spark on Mesos. In a Qubole cluster that is using OPS mode as the default, DEV mode can be obtained for individual Spark applications by overriding spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false BATCH mode is always on and does not interfere with application performance in any way since the loading is entirely outside the application execution path. For details see the next section. Loading Applications in Batch Mode In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is helpful to load all of the applications that have been run in the past, before Unravel Sensor was installed. To enable the batch mode, add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.eventlog.location=hdfs:\/\/NAMENODE_IP_PORT\/user\/spark\/applicationHistory\/ Note: Currently, only one event log location is supported. " }, 
{ "title" : "Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"snippet" : "Follow these steps only if you have an Amazon EMR cluster....", 
"body" : " Follow these steps only if you have an Amazon EMR cluster. " }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-c1b297b6-a3a2-7cfc-3fad-78b5e812a4e1_id_Part4ModifyAmazonEMRClusterBootstrapSetup-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ Introduction", 
"snippet" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workf...", 
"body" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. \n Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. \n Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amazon EMR cluster(s). \n Table of Contents \n Introduction \n 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy \n 2. Get the Bootstrap Script(s) \n 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s) \n For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters \n Hive Applications: \n For Transient Amazon EMR Clusters \n Hive Applications: \n Spark Applications: " }, 
{ "title" : "1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-c1b297b6-a3a2-7cfc-3fad-78b5e812a4e1_id_Part4ModifyAmazonEMRClusterBootstrapSetup-1AddYourAWSAccountNumberstotheUnravelDataMainS3BucketPolicy", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"snippet" : "Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients...", 
"body" : " Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients " }, 
{ "title" : "2. Get the Bootstrap Script(s)", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-c1b297b6-a3a2-7cfc-3fad-78b5e812a4e1_id_Part4ModifyAmazonEMRClusterBootstrapSetup-2GettheBootstrapScripts", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 2. Get the Bootstrap Script(s)", 
"snippet" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below pol...", 
"body" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below policy into the Policy Document: {\n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n { \n \"Sid\": \"getunraveldataclients3files\", \n \"Effect\": \"Allow\", \n \"Action\": [ \n \"s3:ListBucket\", \n \"s3:Get*\" \n ], \n \"Resource\": [ \n \"arn:aws:s3:::unraveldata-clients\/*\" \n ] \n } \n ]\n} Save it by clicking Apply Policy When you create a new Amazon EMR cluster, be sure to add a bootstrap action as shown in the IAM screenshot below. You need to copy and paste the full pathname of the bootstrap action (script) into the Script location \n Important Note: Do not For guidance on which script to use, see the table below. \n \n \n File \n S3 Bucket \n Local Directory \n Applies To \n \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive " }, 
{ "title" : "3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-4--modify-amazon-emr-cluster-bootstrap-setup.html#UUID-c1b297b6-a3a2-7cfc-3fad-78b5e812a4e1_id_Part4ModifyAmazonEMRClusterBootstrapSetup-3IntegratetheBootstrapScriptsintoYourAmazonEMRClusters", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"snippet" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the LOCAL_IP address of your Unravel Server. Download s3:\/\/unraveldata-clients\/...", 
"body" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the LOCAL_IP address of your Unravel Server. Download s3:\/\/unraveldata-clients\/unravel_emr_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_setup.sh \/tmp hadoop Open an SSH session to the cluster's master node (ssh as user hadoop hadoop cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --unravel-server $LOCAL_IP:3000 To uninstall Hive instrumentation on an Amazon EMR cluster (perhaps because you want to upgrade the instrumentation), you simply run the same install script again with the uninstall cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --uninstall For Transient Amazon EMR Clusters Hive Applications: This is similar to the previous section on integrating an existing cluster except the script used as a bootstrap step is one of the following files: \n \n \n \n File \n \n S3 Bucket \n \n Local Directory \n \n Applies To \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive Spark Applications: The Unravel Server does not load data from a Spark cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the LOCAL_IP address of your Unravel Server Download s3:\/\/unraveldata-clients\/unravel_emr_spark_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_setup.sh \/tmp ec2-user Open an SSH session to the cluster's master node (ssh user is ec2-user ec2-user cd \/tmp \nsudo .\/unravel_emr_spark_setup.sh --unravel-server \\ \n $LOCAL_IP:3000 --client \n Change --client --cluster " }, 
{ "title" : "Part 5: Additional Topics for EMR or Qubole", 
"url" : "unravel-4-0-4-1/installation-guide-for-amazon-elastic-mapreduce--amazon-emr--and-qubole-clusters/part-5--additional-topics-for-emr-or-qubole.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters \/ Part 5: Additional Topics for EMR or Qubole", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "Installation Guide for Hortonworks Data Platform (HDP)", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP)", 
"snippet" : "This guide is compatible with: HDP 2.2-2.6...", 
"body" : " This guide is compatible with: HDP 2.2-2.6 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-.html#UUID-8c527b92-5389-8ada-a439-7d82e0010de8_id_InstallationGuideforHortonworksDataPlatformHDP-OrderedSteps", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Ordered Steps", 
"snippet" : "Page: Part 1: Install Unravel Server on HDP Page: Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Page: Part 3: Additional Topics for HDP...", 
"body" : " Page: Part 1: Install Unravel Server on HDP Page: Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Page: Part 3: Additional Topics for HDP " }, 
{ "title" : "Part 1: Install Unravel Server on HDP", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. For older versions of...", 
"body" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. For older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). \n Table of Contents \n Introduction \n Pre-Installation Check \n Platform Compatibility \n Hardware \n Software \n Access Permissions \n Network \n 1. Configure the Host Machine \n Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access \n 2. Install the Unravel Server RPM on the Host Machine \n Get the Unravel Server RPM \n Install the Unravel Server RPM \n Do Host-Specific Post-Installation Actions \n 3. Configure Unravel Server (Basic\/Core Options) \n Modify unravel.properties \n If Kerberos is Enabled: \n If Ranger is Enabled: \n 4. Convert Your Unravel Installation to HDP \n 5. Update Site-Specific HDP Properties \n Restart Unravel Server \n 4. Log into Unravel Web UI \n Congratulations! \n 5. (Optional) Configure Unravel Server (Advanced Options) \n Enable Additional Data Collection\/Instrumentation \n Run the Unravel Web UI Configuration Wizard " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-Pre-InstallationCheck", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR j...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to:\n Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-1ConfiguretheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.u...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version:\n scp $USER@dist.unraveldata.com:unravel-4.*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \n The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh \n The RPM installation also creates an HDFS directory for Hive Hook information collection. \n During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised....", 
"body" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n com.unraveldata.customer.organization=Company_and_org \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n com.unraveldata.tmpdir=\/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n com.unraveldata.history.maxSize.weeks=26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay \n \n com.unraveldata.hive.hook.topic.num.threads=1 \n \n \n com.unraveldata.job.collector.done.log.base \n Only modifiable through Unravel Web UI's configuration wizard. \n com.unraveldata.job.collector.done.log.base=\/mr-history\/done \n \n \n com.unraveldata.job.collector.log.aggregation.base \n Only modifiable through Unravel Web UI's configuration wizard. \n com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.s3.batch.monitoring.interval.sec \n . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional \n \n com.unraveldata.s3.batch.monitoring.interval.sec=120 \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ \n \n \n yarn.resourcemanager.webapp.address \n YARN resource manager web address URL \n yarn.resourcemanager.webapp.address=http:\/\/example.localdomain :8088 \n \n \n oozie.server.url \n Oozie URL \n oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: Add authentication for HDFS... (a) Create a keytab for unravel for daemons that run as unravel \/etc\/keytabs\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct realm and \/hostname, if applicable): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/etc\/keytabs\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n \n You can find the principal by using 'klist -kt KEYTAB_FILE' If Ranger is Enabled: Add these permissions... \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR \n * \n read+write \n data transfer from Hive jobs when Unravel is not up \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n hdfs \n read \n Spark event log \n \n \n hdfs:\/\/usr\/history\/done \n hdfs \n read \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n hdfs \n read \n YARN aggregation folder \n \n \n hdfs:\/\/user\/hive\/warehouse \n hdfs \n read \n Obtain table partition sizes \n \n Hive Metastore GRANT \n hive \n read \n Hive table information " }, 
{ "title" : "4. Convert Your Unravel Installation to HDP", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-4ConvertYourUnravelInstallationtoHDP", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 4. Convert Your Unravel Installation to HDP", 
"snippet" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades....", 
"body" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh \n Note: This change will stick after RPM upgrades.\n " }, 
{ "title" : "5. Update Site-Specific HDP Properties", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-5UpdateSite-SpecificHDPProperties", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 5. Update Site-Specific HDP Properties", 
"snippet" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base=...", 
"body" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/user\/spark\/applicationHistory\/\n \n# Add Hive Metastore database information for Unravel Hive Config \njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/{hostname}:3306\/{database_name} \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName={HiveMetastoreUserName} \njavax.jdo.option.ConnectionPassword={HiveMetastorePassword} Verify above site-specific values using the Ambari Web UI (AWU): Verify com.unraveldata.job.collector.done.log.base In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Take note of the value of mapreduce.jobhistory.done-dir unravel.properties com.unraveldata.job.collector.done.log.base= Verify com.unraveldata.job.collector.log.aggregation.base In AWU, on the left-hand side, click YARN Configs Advanced Node Manager Take note of the value of yarn.nodemanager.remote-app-log-dir unravel.properties com.unraveldata.job.collector.log.aggregation.base= Verify Hive Metastore javax.jdo.option.Connection In AWU, on the left-hand side, click Hive Configs Advanced Hive Metastore Take note of following properties and their values to be entered into unravel.properties \n Database URL \n Database Host \n JDBC Driver Class \n Database Name \n Database Username (if visible) Database Password Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel Web UI", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-4LogintoUnravelWebUI", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 4. Log into Unravel Web UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-1--install-unravel-server-on-hdp.html#UUID-382d54e8-4dba-61db-d5ec-178627548447_id_Part1InstallUnravelServeronHDP-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 1: Install Unravel Server on HDP \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-Introduction", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ Introduction", 
"snippet" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI...", 
"body" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI " }, 
{ "title" : "1. Start Unravel Server", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-1StartUnravelServer", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 1. Start Unravel Server", 
"snippet" : "Note: Unravel needs to be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start...", 
"body" : " \n Note: Unravel needs to be up for the next step to complete.\n sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-2InstallUnravelHiveHookandSparkSensorOntoHDPServers", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"snippet" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps # ensure wget is install and use below script to install sensors # \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh # from Unravel server (eg. edge node) # run...", 
"body" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps\n# ensure wget is install and use below script to install sensors \n# \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh \n# from Unravel server (eg. edge node)\n# run on each server that will use instrumentation:\nyum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\nsudo .\/unravel_hdp_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION Substitute valid values for: UNRAVEL_HOST_IP - fully qualified host name or IP address SPARK_VERSION - target Spark version HIVE_VERSION - target Hive version Files will be installed under: Hive hook jar is located in \/usr\/local\/unravel_client\/. Spark jar is located in \/usr\/local\/unravel-spark\/jars\/. Once the files are installed under \/usr\/local\/unravel_client\/ & \/usr\/local\/unravel-spark\/ on edge host where Unravel rpm is installed, you can tar these two directories up and put on other hosts, if that is more convenient than running the script. In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). " }, 
{ "title" : "3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-3AddUnravelHiveHookhive-siteSettingstoAllofHDPsServersintheClusterUsingAWU", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"snippet" : "Completion of this step will require a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, then Hive jobs could fail with ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. IMPORTANT: Add ...", 
"body" : " Completion of this step will require a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, then Hive jobs could fail with ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. IMPORTANT: Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \n Hold off on restarting any services until the next step. Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \n You can optionally restart the services in Ambari, as prompted, at this point in order to verify that the environment change is done correctly. After the restart, look at hadoop-env.sh and hive-env.sh on an edge node and check the path to the jar file. Hint: find the files with sudo find \/etc -name '*env.sh' -newerct '1 hour ago' Add the contents of \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Custom hive-site Hive In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook \ncom.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR \ncom.unraveldata.hive.hook.tcp=true \ncom.unraveldata.host=UNRAVEL_HOST_IP \n \n-- Find below properties as it may already exists, concatenate it with a comma & no spaces -- \nhive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook \nhive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook \nhive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook \n You should restart the services in Ambari now, as prompted, to test whether Hive queries can succeed after the above configuration change. If you get ClassNotFoundException during a query, then make corrections or revert. " }, 
{ "title" : "4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-4IfPossibleEnsurethathiveexecutionengineisSettoMapReduceinyourHivequery", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"snippet" : "set hive.execution.engine=mr;...", 
"body" : "set hive.execution.engine=mr; " }, 
{ "title" : "5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-5OptionallyforSparkonYARNEnableUnravelSparkInstrumentationonAllofHDPsServersintheCluster", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"snippet" : ": Completion of this step will require a restart of all affected Spark services in Ambari UI. IMPORTANT Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spar...", 
"body" : " : Completion of this step will require a restart of all affected Spark services in Ambari UI.\n IMPORTANT Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport=UNRAVEL_HOST_IP:4043\n\nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class \n\nspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class\n\n\nspark.eventLog.enabled=true Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace \"UNRAVEL_HOST_IP\" with your Unravel server's IP address. " }, 
{ "title" : "6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-2--enable-additional-data-collection---instrumentation-for-hdp.html#UUID-a9c375f1-db08-480d-cf3c-f2862de61c31_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-6OptionallyforMapReduce2MRJVMSensorCluster-Wide", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"snippet" : ": Completion of this step will require a restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advancedmapred-site Search for MR AppMaster Java Heap Size (Note: please leave a white space in-between t...", 
"body" : " : Completion of this step will require a restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI.\n IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advancedmapred-site Search for MR AppMaster Java Heap Size \n (Note: please leave a white space in-between the current and the following new property) On the top notification banner, click Save -javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 -Dcom.sun.btrace.FileClient.flush=-1 Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace \"UNRAVEL_HOST_IP\" with your Unravel server's IP address. In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site: Inside Custom mapred-site Add Property : Bulk property add mode On the top notification banner, click Save mapreduce.task.profile=true\nmapreduce.task.profile.maps=0-5\nmapreduce.task.profile.reduces=0-5\nmapreduce.task.profile.params=-javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 -Dcom.sun.btrace.FileClient.flush=-1 Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace \"UNRAVEL_HOST_IP\" with your Unravel server's IP address. Following instructions are for Unravel rpm 4.0.x and 4.1.x Please propagate Unravel MR jaronto all the servers in the cluster as follows : \n Ensure you have already installed unzip, curl, and Unravel port# 3000 must be open\n ssh to the Unravel gateway host server and use guided steps to unzip the Unravel MR jars. With root or sudo access, change directory to \/usr\/local\/unravel-spark and run the below curl get command: cd \/usr\/local\/unravel-spark\ncurl http:\/\/localhost:3000\/hh\/unravel-sensor-for-mapreduce-bin.zip -o unravel-sensor-for-mapreduce-bin.zip\nunzip -d jars unravel-sensor-for-mapreduce-bin.zip Now, tar up the \/usr\/local\/unravel-spark directory, and propagate to all the servers in the HDP cluster cd \/usr\/local\/\ntar -cvf unravel-spark.tar .\/unravel-spark\n---> copy the unravel-spark.tar file to all your servers, and untar to \/usr\/local directory because when you untar unravel-spark directory will appear " }, 
{ "title" : "Part 3: Additional Topics for HDP", 
"url" : "unravel-4-0-4-1/installation-guide-for-hortonworks-data-platform--hdp-/part-3--additional-topics-for-hdp.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Installation Guide for Hortonworks Data Platform (HDP) \/ Part 3: Additional Topics for HDP", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-0-4-1/user-guide.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide", 
"snippet" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations....", 
"body" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations. " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-0-4-1/user-guide.html#UUID-08d4030d-6c2b-9e00-c3e7-d3cf3b40181d_id_UserGuide-GettingStartedGetting-Started_53157411html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started", 
"snippet" : "The Operations Tab The Applications Tab The Data Tab...", 
"body" : " The Operations Tab The Applications Tab The Data Tab " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-0-4-1/user-guide.html#UUID-08d4030d-6c2b-9e00-c3e7-d3cf3b40181d_id_UserGuide-UseCasesUse-Cases_53314582html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases", 
"snippet" : "Optimizing the Performance of Spark Applications Detecting Resource Contention in the Cluster Identifying Rogue Applications...", 
"body" : " Optimizing the Performance of Spark Applications Detecting Resource Contention in the Cluster Identifying Rogue Applications " }, 
{ "title" : "Notifications", 
"url" : "unravel-4-0-4-1/user-guide.html#UUID-08d4030d-6c2b-9e00-c3e7-d3cf3b40181d_id_UserGuide-Notifications", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Notifications", 
"snippet" : "Setting Up Auto Actions (Alerts)...", 
"body" : " Setting Up Auto Actions (Alerts) " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started", 
"snippet" : "Table of Contents Use Case Videos Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Use Case #3: How to Debug Failed Applications Use Case #4: How to Review Spark Applications and Identify Area...", 
"body" : "\n Table of Contents \n Use Case Videos \n Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application \n Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA \n Use Case #3: How to Debug Failed Applications \n Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements \n Running the Configuration Wizard \n Setting Up Access to Big Data Components \n Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos " }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-ce145bad-4d5b-fdf7-9ada-6a072871d0a3_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Use Case #3: How to Debug Failed Applications Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements...", 
"body" : "Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Use Case #3: How to Debug Failed Applications Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-ce145bad-4d5b-fdf7-9ada-6a072871d0a3_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin Setting Up Access to Big Data Components Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos " }, 
{ "title" : "The Operations Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab", 
"snippet" : "Table of Contents Dashboard Charts Reports Chargeback Cluster Summary Cluster Compare...", 
"body" : "\n Table of Contents \n Dashboard \n Charts \n Reports \n Chargeback \n Cluster Summary \n Cluster Compare " }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-54c6ab0c-255b-a813-c511-fb74c8a811c8_id_TheOperationsTab-Dashboard", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a spe...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a specific tile, click that tile. To configure the Dashboard for a specific time range or cluster, select options from the pull-down menus in the banner. " }, 
{ "title" : "Charts", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-54c6ab0c-255b-a813-c511-fb74c8a811c8_id_TheOperationsTab-Charts", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Charts", 
"snippet" : "To view charts, click Operations Charts address one of the big challenges in managing a multi-tenant Hadoop clusters: understanding how resources are being used or abused by the various applications running in the clusters. Unravel Web UI provides a unique forensic view into each cluster's key perfo...", 
"body" : "To view charts, click Operations Charts address one of the big challenges in managing a multi-tenant Hadoop clusters: understanding how resources are being used or abused by the various applications running in the clusters. Unravel Web UI provides a unique forensic view into each cluster's key performance indicators (KPIs) over time and how they are related to the applications running in the cluster. Charts To see KPIs for resources, jobs, nodes, or services, select the appropriate secondary tab in the Charts To see a detailed view of any of KPI, click the tile for that KPI. To configure Charts For example, if you notice a sudden spike in the total vCores or memory usage of the cluster, using Unravel you can correctly pinpoint the applications that resulted in this spike. More importantly, you can then drill down into these specific applications to understand their behavior and apply any recommendations that Unravel suggests for optimizing them. " }, 
{ "title" : "Reports", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-54c6ab0c-255b-a813-c511-fb74c8a811c8_id_TheOperationsTab-Reports", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Reports", 
"snippet" : "To view reports, click Operations Reports Chargeback Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback For example, you can generate reports catego...", 
"body" : "To view reports, click Operations Reports Chargeback Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback For example, you can generate reports categorized first by application type and secondarily by users and queue; then you can specify base costs (VCore\/Hour ($) and Memory MB\/Hour ($)) to estimate actual costs across various dimensions. Cluster Summary The Cluster Summary Reports Cluster Compare The Cluster Compare Reports " }, 
{ "title" : "The Applications Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab", 
"snippet" : "Topics Finding Applications Application-Specific Managers Spark Application Manager Key Performance Indicators (KPIs) Sub-Tabs Hive Application Manager Key Performance Indicators (KPIs) Sub-Tabs MapReduce Application Manager Sub-Tabs Finding Workflows Workflow Manager Key Performance Indicators (KPI...", 
"body" : "\n Topics \n Finding Applications \n Application-Specific Managers \n Spark Application Manager \n Key Performance Indicators (KPIs) \n Sub-Tabs \n Hive Application Manager \n Key Performance Indicators (KPIs) \n Sub-Tabs \n MapReduce Application Manager \n Sub-Tabs \n Finding Workflows \n Workflow Manager \n Key Performance Indicators (KPIs) \n Sub-Tabs \n Events \n Error View The Applications The performance and reliability of an application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems of an application. Unravel provides insights into an application. These insights are called events Key Performance Indicators " }, 
{ "title" : "Finding Applications", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-FindingApplications", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Applications", 
"snippet" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property,...", 
"body" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. You can also use the global search bar in the top banner to search by full job ID, user name, table name and cluster ID. Search results list individual jobs, and job IDs. If the job is part of a Hive query, Pig script, and\/or a workflow, then a link to that Hive query\/Pig script\/workflow page appears on the same line. To go to the job-specific page, click the job ID. To go to the application-specific manager for a query\/script\/job\/workflow, click the icon under its GoTo " }, 
{ "title" : "Application-Specific Managers", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-Application-SpecificManagers", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Application-Specific Managers", 
"snippet" : "Spark Application Manager The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor pa...", 
"body" : "Spark Application Manager The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor partitioning Detect and fix inefficient and failed Spark apps Tune JVM settings for Spark drivers\/executors Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Spark application. The Spark Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Spark application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Spark application. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of stages that make up the Spark application and their status Number of Stages Sub-Tabs The Spark Application Manager contains multiple sub-tabs, each of which is described below. : A table of the stages associated with this application. To see details about a stage, click its row in the table. This displays the Spark Stage view, as illustrated in the image below. Navigation The Spark Stage view provides detailed information about each Spark stage. It includes: : General task\/slot statistics of the stage Graphs : Timeline and histogram of task attempts, duration, and bytes shuffled\/spilled. Timeline This information is very useful for identifying data skew. : Key\/value pairs of configuration settings for the application Configuration Click here to see sample screenshots... : Displays both the stage view and RDD view associated with this application. Execution Graph : A timeline of application stages Gantt Chart : The logical plan of the SparkSQL query Query Plan : Exceptions, errors, and warnings associated with this application Errors : Logs for the driver and executors of this application, and a skyline of task attempts within the stage Logs : Source code of a general purpose Spark application, or the SQL query for a SparkSQL query Program : Statistics about the task attempts that are executed as part of the current application Task Attempts : Utilization of slot containers over time Containers : Utilization of slot vcores over time Vcores : Utilization of slot memory over time Memory : Graphs of JVM-level metrics at the executor and driver level. To show the graph for a specific metric, select that metric from the Resource METRIC Get Data Resource Metrics Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. You can use this view to resolve inefficiencies, bottlenecks and reasons for failure within applications. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). The Hive Application Manager uses Unravel's Intelligence Engine to automatically identify certain inefficiencies with the application and to provide recommendations on how to improve efficiency. Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Hive query. The Hive Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Hive query depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Hive query. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of YARN apps that make up the Hive query Number of YARN apps Sub-Tabs The Hive Application Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the application and drill down into MapReduce jobs that make up the application. It includes information about MapReduce jobs such as duration, I\/O, resources, events, job ID, and job status. When you select a MapReduce job, its details are shown in a pane below it. This detailed view includes the MapReduce timeline, MapReduce task attempts, and slot usage graph. Navigation : Provides insights into the execution of the application. It shows detailed information about MapReduce stages and their relationship with one another. The execution view is split into two areas; the execution plan and the expanded info area. Execution Graph : Shows the relationship between MapReduce stages and high level information about each stage. The information shown in each stage box includes MapReduce job ID, base table name, time taken by the stage, percentage of total run time taken by the stage, and execution status. Gantt Chart \n Errors : Shows detailed information about a stage when selected. The expanded info shows each of the map reduce functions, tables usage information, timings of each function and input paths used by the stage. Query : Provides an easy way to understand efficiency and status of MapReduce task attempts by breaking down attempted tasks by successful, failed and killed. Task Attempts \n Attempts : Shows slot usage by Map and Reduce jobs over time. Slot Usage MapReduce Application Manager The MapReduce Application Manager provides a detailed view into the behavior of MapReduce applications. It is used by Hadoop DBAs or application owners (engineers, BI team, analysts) to resolve inefficiencies, bottlenecks and reasons for failure within applications. The MapReduce Application Manager uses the Unravel intelligence engine to automatically identify certain inefficiencies with the application and provides solutions on how to the fix the problem. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Sub-Tabs The MapReduce Application Manager contains multiple sub-tabs, each of which is described below. : Provides a detailed view into a MapReduce job execution. The Timeline shows execution of each MapReduce task on the machine that those tasks ran on and allows drill-down into each task to obtain further information. The timeline view comes with filters which can be used to display only map tasks, reduce tasks and killed\/failed tasks. The histograms above the Timeline show the distribution of MapReduce tasks along time and data size. This histogram can also be used as a filter to zoom in on specific tasks. Timeline : Provides comprehensive information about each application including task and job logs. The logs section intelligently selects interesting tasks and presents its logs in an organized manner. Logs : Provides a complete list of configuration settings used during the application execution. Configuration " }, 
{ "title" : "Finding Workflows", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-FindingWorkflows", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Workflows", 
"snippet" : "To find workflows, select SHOW Workflow...", 
"body" : "To find workflows, select SHOW Workflow " }, 
{ "title" : "Workflow Manager", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-WorkflowManager", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Workflow Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automaticall...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automatically identify inefficiencies with the workflow and provides solutions on how to the fix the problem. The Workflow Manager helps pipeline owners easily maintain SLAs. The workflow header provides primary information about the workflow such as name, user name, queue that the workflow was submitted on, start time and tags that the workflow has been given. Key Performance Indicators (KPIs) : The number of Unravel recommendations or insights for this workflow. To see details, click the Events Events : Total time taken by the workflow to complete execution Duration : Total data read and written by the workflow Data I\/O \n Resources : The number of apps that make up this workflow Number of Apps Sub-Tabs The Workflow Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the workflow and drill down into the Hive queries, Spark jobs, and MapReduce jobs that make up the application. It includes information about duration, I\/O, resources, events, job IDs, and job status. Navigation : Shows the dependency between the various components and the time taken by each. It helps you to identify stuck or incomplete components which could be affecting the overall completion of the workflow. Gantt Chart : Clearly shows the dependencies between various components of the workflow and the status of components. To see details about status, hover over a status box to see the tool tips. DAG View : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer over instances displays top KPIs such as duration, data I\/O, resources, and the number of jobs in that instance. Clicking on a point on the chart loads that particular instance for inspection. Compare " }, 
{ "title" : "Events", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-EventsEvents", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Events", 
"snippet" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events...", 
"body" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events " }, 
{ "title" : "Error View", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-ac64a42d-1f8a-ac24-b85a-1a247a5b92fb_id_TheApplicationsTab-ErrorView", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Error View", 
"snippet" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details f...", 
"body" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details from the errors messages\/log data that can help developers\/operators quickly root cause issue. Examples of keywords include Oozie errors code(s), Java run time error(s) etc. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-68cd4742-d1a5-ae4f-0a18-7073d5371d05", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time w...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time when the collection process started totalPhysicalMemory BYTES The total physical memory in the operating system freePhysicalMemory BYTES The free physical memory in the operating system committedVirtualMemory BYTES The committed virtual memory in the operating system freeSwap BYTES The free swap size availableMemory BYTES An estimate of memory available for launching new processes vmRss BYTES The resident set size of the complete process tree vmRssDir BYTES The resident set size of the process totalSwap BYTES The total swap size processCpuLoad PERCENTS Average process CPU load for the last minute (all cores) systemCpuLoad PERCENTS Average system CPU load for the last minute (all cores) fullGcCount COUNT Number of full GC runs minorGcCount COUNT Number of minor GC runs minorGcTime DURATION (nanoseconds) Accumulated time spent in minor GC fullGcTime DURATION (nanoseconds) Accumulated time spent in full GC gcEdenSurvivedAvg BYTES Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcSurvivorPromotedAvg BYTES Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg BYTES Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms allocatedBytes BYTES Accumulated number of allocated bytes edenPeakUsage BYTES Maximum memory usage in the eden space survivorPeakUsage BYTES Maximum memory usage in the survivor space oldPeakUsage BYTES Maximum memory usage in the old space avgMinorInterval DURATION (nanoseconds) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations avgFullGcInterval DURATION (nanoseconds) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms gcOldLiveAvg BYTES Average number of bytes alive in the old generation. Might not be available for particular GC algorithms initHeap BYTES Initial heap size maxHeap BYTES Maximum heap size usedHeap BYTES Used heap size committedHeap BYTES Committed heap size initNonHeap BYTES Initial non-heap size maxNonHeap BYTES Maximum non-heap size usedNonHeap BYTES Used non-heap size committedNonHeap BYTES Committed non-heap size currentThreadCpuTime DURATION (nanoseconds) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime DURATION (nanoseconds) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems " }, 
{ "title" : "The Data Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-data-tab.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab", 
"snippet" : "Table of Contents Table\/Partition Usage Table Details Partition Details...", 
"body" : "\n Table of Contents \n Table\/Partition Usage \n Table Details \n Partition Details " }, 
{ "title" : "Table\/Partition Usage", 
"url" : "unravel-4-0-4-1/user-guide/the-data-tab.html#UUID-0accca0a-8b66-0544-a45f-6403030c167d_id_TheDataTab-TablePartitionUsage", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab \/ Table\/Partition Usage", 
"snippet" : "The Data tab provides a quick snapshot of the tables and partitions in the cluster. To see table and partition details, click the Details On the Details Click on any column to sort by that metric such as: Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read ...", 
"body" : "The Data tab provides a quick snapshot of the tables and partitions in the cluster. To see table and partition details, click the Details On the Details Click on any column to sort by that metric such as:\n Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read I\/O - to find tables which have performed most I\/O Attempts - to find tables which have had most number of read\/write attempts on them Apps - to find tables which have most number of applications using it Users - to find tables which have the most number of users using it Other Tables - shows other tables commonly used along with a particular table in applications Labels - view tables which have been labeled Hot, Warm or Cold Hover on columns like Users and Other Tables to know respectively which users accessed this table and which other tables are accessed along with this table. View trends of table usage and access by accessing the drop-down menu on the top right corner. Metrics available are Read I\/O, Total Users, Total Apps, Total Attempts as shown below. You can also search for a particular table by typing part of the table name in the table search box as shown below. In our example we are looking for tables with the word customer in it. Table Details To drill down into a particular table click its More Info This opens a Table Detail You can view different metric trends on this pane including: Read IO Number of Users Number of Apps Number of Attempts It also allows you to view the historical usage and access information about this table. The table detail view also shows a list of applications and users that accessed the table in the given time range. This list of applications and users is also sortable for easy search and browse capabilities. Partition Details To view detailed information about partitions associated with a Table, click the Partition Detail tab in the Table Detailed View and you will open up a Partition detail page as shown below. The Partition histogram provide the Access Age of Partitions (including the Max Access Age). This information is used to calculate the number and size of Reclaimable Partitions. This can be explained as follows: Reclaimable Partitions Found = number of Partitions with Last Access Time < (Current Time - Max Access Age of Partitions). In the example above: No partition is accessed by any application more than 4 days 14 hrs. after it is created. Thus, the partitions created on Dec 1, 2015, will not be accessed after Dec 5, 2015. In Unravel, the Max Access Age of Partitions for the above Table will be computed as 4 days 14 hrs. based on the history of accesses to partitions in the above Table So on Dec 15, 2015, if the above Table is reviewed: Reclaimable Partitions found = number of Partitions with Last Access Time < (Dec 14 2015 - 4days 14 hrs.) i.e., (Dec 10, 2015). Thus, the partitions created on any day before Dec 10, 2015 will be marked as a Reclaimable Partition. In this case, it will be all the 26 partitions. The page also list all Partitions by key KPI’s, including: Last access date\/time Create date\/time Current Size # of Users accessing the Partition Unravel provides an easy way to label Tables\/Partitions as “HOT”, “WARM” and “COLD” based on access patterns – last access time or age (last access time – create time). Once you have the rule configured as above, please click on the \"SAVE RULES\" and you will see a confirmation as below. While the \"HOT\", \"WARM\" and \"COLD\" labels are associated with the Tables right away, the main dashboard page with appropriate pie chart values are typically populated in 24 hrs. As illustrated above, HOT rule can be “Last Access <= 120 days”, WARN rule can be “Last Access <= 175 days and > 120 days” and COLD rule can be “Last Access >= 176 days” These rules are checked periodically (every 24 hrs.) and tables\/partition are labeled and classified accordingly. The Data overview page provides a quick summary information based on these labels. Operators can use this information to identify unused or frequently used tables\/partitions and take appropriate actions. " }, 
{ "title" : "Setting Up Auto Actions (Alerts)", 
"url" : "unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Auto Actions", 
"url" : "unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html#UUID-df98ceda-7f89-372a-fb63-1f0bd8a2177b_id_SettingUpAutoActionsAlerts-AutoActions", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts) \/ Auto Actions", 
"snippet" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin M...", 
"body" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin Manage On the Manage Auto Actions Click ADD NEW AUTO ACTION Select the desired Auto Action type, and then click Next Enter a name for the new Auto Action, and specify its prerequisite conditions, defining conditions, and actions: Prerequisite conditions: A set of boolean conditions such that when they are all met, Unravel will evaluate the defining conditions of the Auto Action. Examples include: whether this Auto Action should be evaluated during a given time, whether this Auto Action should be evaluated for a job belonging to a given user, etc. Defining conditions: A set of boolean conditions defining the Auto Action. When they are all met, the corresponding action defined as part of the Auto Action will be taken automatically. Examples include: Is this job running for too long? Does it use too many mappers? Actions: A set of actions to be taken when the defining conditions are all evaluated to be true. Examples include: send an email to admin (i.e., “alerting”), kill the job, etc. Click SAVE AUTO ACTION " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-0-4-1/user-guide/use-cases.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-0-4-1/user-guide/use-cases.html#UUID-e028b8d0-f3e2-5c51-fae2-7d3e37c2be4c", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB \n Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-0-4-1/user-guide/use-cases.html#UUID-76c5f559-aa3e-0756-6eba-7bd6be321604", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB \n Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "unravel-4-0-4-1/user-guide/use-cases.html#UUID-6b44a74d-619a-d195-bc08-7f3f04257122", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web ...", 
"body" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Unravel Server Reference", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : " This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-97f67c8b-8003-5763-6265-f26a75d0bb26_id_UnravelServerReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zooke...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zookeeper (on a custom port) unravel_k bundled Kafka (on a custom port) unravel_hhs Hive Hook Sensor unravel_hhw Hive Hook Worker unravel_hhwe Hive Hook Worker EMR unravel_hvw Hive Worker unravel_jcs1 Job Collector Sensor MRv1 unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw1_N Job Collector Sensor Worker MRv1 unravel_jcw2_N Job Collector Sensor Worker YARN unravel_lr Log Receiver unravel_mrw Map Reduce Worker unravel_ja \"Job Analyzer\" summarizes jobs unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_os3 Oozie v3 Sensor unravel_os4 Oozie v4 Sensor unravel_tw Table Worker unravel_pw Partition Worker unravel_ew_N Event Worker unravel_sw_N Spark Worker " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-97f67c8b-8003-5763-6265-f26a75d0bb26_id_UnravelServerReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/u...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/local\/unravel\/logs Do not set directly; set UNRAVEL_LOG_DIR in etc\/unravel.ext.sh instead and this property will be derived from that com.unraveldata.zk.quorum 127.0.0.1:4181 embedded Zookeeper ensemble in form host1:port1,host2:port2, … com.unraveldata.kafka.broker_list 127.0.0.1:4091 embedded Kafka unravel.jdbc.username unravel MySQL (embedded or external) username for db unravel.jdbc.password random generated for bundled MySQL MySQL (embedded or external) password for db unravel.jdbc.url jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod This is JDBC URL without username and password com.unraveldata.hdfs.interactive.monitoring.interval.sec 30 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) com.unraveldata.hdfs.batch.monitoring.interval.sec 300 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) com.unraveldata.longest.job.duration.days 2 Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) oozie.server.url http:\/\/localhost:11000\/oozie URL for accessing Oozie to track workflows com.unraveldata.oozie.fetch.num 100 Max number of jobs to fetch during an interval com.unraveldata.oozie.fetch.interval.sec 120 seconds between intervals for fetching Oozie workflow status " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-97f67c8b-8003-5763-6265-f26a75d0bb26_id_UnravelServerReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset L...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset Last chance arguments to jvm to override other settings HADOOP_CONF as discovered by running \"hadoop fs -ls \" The directory containing the hadoop config files core-site.xml, hdfs-site.xml, and mapred-site.xml UNRAVEL_DATA_DIR \/srv\/unravel A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data (db_data, k_data, zk_data) and also tmp data if property com.unraveldata.tmpdir is not set. UNRAVEL_LISTEN_PORT 3000 The Web UI port on the primary or standalone Unravel installation (service unravel_tc) which listens on 0.0.0.0 ; the property com.unraveldata.tc UNRAVEL_LOG_DIR \/usr\/local\/unravel\/logs A destination dir. owned by user unravel for log files UNRAVEL_TC_SHUTDOWN_PORT 3005 An unoccupied port used for cleanly stopping the Web UI (service unravel_tc) which listens on 127.0.0.1 " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-97f67c8b-8003-5763-6265-f26a75d0bb26_id_UnravelServerReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs L...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~1.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file to preserve traceability \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME or other settings as shown in table above n\/a Optional; example syntax: export JAVA_HOME=\/path \/srv\/unravel\/log_hdfs log directory for daemons that run as user hdfs (for YARN, when applicable) <2GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR \/srv\/unravel\/tmp_hdfs tmp directory for daemons that run as user hdfs (for YARN, when applicable) <1GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-0-4-1/advanced-topics.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-shell Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows Custom Configurations Adding More Admins to Unravel Web UI Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom TC Port Integrating LDAP Authentication for Unravel Web UI Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Connecting to a Hive Metastore Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Troubleshooting Running Verification Scripts and Benchmarks Sending Diagnostics to Unravel Support Uninstalling Unravel Server Upgrading Unravel Server " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-8ecedd56-f03d-93ee-96c0-cd4b9f0a388f_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-8ecedd56-f03d-93ee-96c0-cd4b9f0a388f_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-8ecedd56-f03d-93ee-96c0-cd4b9f0a388f_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Adminis...", 
"body" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires check UN Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Sensors", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors.html#UUID-598685a4-9c33-5a5d-155e-31be4ae78280", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOpt...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION for Spark 2.0.x 2.0 for Spark 1.6.x 1.6 for Spark 1.5.x 1.5 for Spark 1.3.x 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps executed via the spark-shell To intercept Spark apps executed via the spark-shell, you need to unzip the Unravel Sensor .zip file on the client node at a location on the local file system that is readable by all users, referred to as UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip\nunzip unravel-sensor-for-spark-bin.zip Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-shell always executed in client mode To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR : Parent directory of the Unravel Sensor .zip file, UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. SPARK_EVENT_LOG_DIR : Directory on the local file system that contains the unzipped content of UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\n\n\/usr\/lib\/spark\/bin\/spark-shell \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n<<EOF\n\/\/ your spark-shell code snippet will follow here\n\/\/ For exemplifying, we use a snippet of RDDRelation below\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\ncase class Record(key: Int, value: String)\n\n val df = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\"))).toDF()\n\n \/\/ Any RDD containing case classes can be registered as a table. The schema of the table is\n \/\/ automatically inferred using scala reflection.\n df.registerTempTable(\"records\")\n\n \/\/ Once tables have been registered, you can run SQL queries over them.\n println(\"Result of SELECT *:\")\n sqlContext.sql(\"SELECT * FROM records\").collect().foreach(println)\n\n \/\/ Aggregation queries are also supported.\n val count = sqlContext.sql(\"SELECT COUNT(*) FROM records\").collect().head.getLong(0)\n println(s\"COUNT(*): $count\")\n\nexit\n\nEOF\n\n Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors.html#UUID-ba5ea5aa-c317-f007-51bf-4b433a4f17d7", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-s...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION for Spark 2.0.x 2.0 for Spark 1.6.x 1.6 for Spark 1.5.x 1.5 for Spark 1.3.x 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-$VERSION\/unravel-sensor-for-spark-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-$VERSION\/unravel-sensor-for-spark-bin.zip\ncd $UNRAVEL_SENSOR_PATH \nhdfs fs -copyFromLocal unravel-sensor-for-spark-bin.zip \/tmp\nset UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR : Parent directory of the Unravel Sensor .zip file, UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. SPARK_EVENT_LOG_DIR : PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit For example, export UNRAVEL_SENSOR_PATH=$UNRAVEL_SENSOR_PATH\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=$PATH_TO_SPARK_EXAMPLE_JAR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. Option B: If You Run Spark Apps in yarn-client Mode To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip\nunzip unravel-sensor-for-spark-bin.zip Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR : Parent directory of the Unravel Sensor .zip file, UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. SPARK_EVENT_LOG_DIR : PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit spark-examples.jar : Directory on the local file system that contains the unzipped content of UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=$PATH_TO_SPARK_EXAMPLE_JAR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Hive Queries", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors.html#UUID-707f3984-91e4-4b43-0fdb-2a658c7a3ab5", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Hive Queries", 
"snippet" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar W...", 
"body" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Part 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-sensor-for-mapreduce-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set mapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; " }, 
{ "title" : "Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows.html#UUID-65de762e-fdc9-77c9-6b3e-c6e295c1a890", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web ser...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web service are in the same cluster. If You Use Http For Airflow Web UIAccess Add the following 3 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.protocol=http\ncom.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart If You Use Https For Airflow Web UIAccess Add the following 4 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true\ncom.unraveldata.airflow.login.name={airflow web UI username}\ncom.unraveldata.airflow.login.password={airflow web UI password} Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Range of Monitoring By default, Unravel Server ingests all the workflows that started within the last 5 days. If you wish to change the date range to the last x Add the following configuration to \/usr\/local\/unravel\/etc\/unravel.properties airflow.look.back.num.days=-x Restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows.html#UUID-26fde14b-e820-e7e0-09d1-fcd5b8f8bcd5", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import PythonOperator from da...", 
"body" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Oozie\/Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n 1. Add Hooks for Unravel Instrumentation The example below shows the contents of a bash script, example-hdp-client.sh spark-submit spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport Setting these parameters on a per-application spark-defaults.conf This script references the following variables, which you would need to edit: PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR 2. Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call 2016-08-30 3. Monitor the Workflow To see the new Oozie workflow in Unravel Web UI, select APPLICATIONS Workflows Add Workflow " }, 
{ "title" : "Tagging Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows.html#UUID-420bc84e-d12f-443f-56a0-3bb29526ab8e", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Tagging Workflows", 
"snippet" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utct...", 
"body" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: [Wrong usage] SET unravel.workflow.name=\"ETL-Workflow\"; [Correct usage] SET unravel.workflow.name=ETL-Workflow; Please note the following: Different runs of the same same unravel.workflow.name Different runs of the same different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name The example below shows a Hive query that is marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\n\nSELECT foo FROM table WHERE … [Rest of Hive Query text goes here] Easy Recipes for Tagging Workflows First, export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow: Export the workflow name: export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow \n Export the UTC timestamp for this run of the workflow. Here, we use bash's date export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Then follow the appropriate instructions below: \n Tagging a Hive query \n Tagging a Sqoop job \n Tagging a direct MapReduce job \n Tagging a Spark job \n Tagging a Pig job \n Tagging a Impala query How to Tag a Hive Query Using SET Commands in Hive hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=${env:WORKFLOW_NAME};\nSET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP};\nselect count(1) from lineitem; How to Tag a Sqoop Job Using –D Command Line Parameters sqoop export \\\n *-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar How to Tag a Direct MapReduce Job Using –D Command Line Parameters hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n*-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n-p \/wordcount.properties -input \/tmp\/soumitra\/data\/small -output \/tmp\/soumitra\/outsmoke How to Tag a Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name spark-submit \\\n * --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" *\n * --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" *\n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster How to Tag a Pig Job Using –param and SET Commands pig \\\n*-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP * \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME;\nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP;\n\nlines = LOAD '\/tmp\/soumitra\/data\/small' using PigStorage('|') AS (line:chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount; How to Tag a Impala Job Using –-query_options and SET Command User can set DEBUG_ACTION in Impala script to pass the workflow tags into impala queries: SET DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}; According to this CDH article: https:\/\/www.cloudera.com\/documentation\/enterprise\/5-14-x\/topics\/impala_query_options.html impala-shell --query_option=DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}| -f impala.script Note: the tagging option should be turned on for this tagging to work. ie, the followings need to be set in unravel.properties: # Tagging\ncom.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path=\/tmp\/app_tag.py\ncom.unraveldata.app.tagging.script.method.name=get_tags Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Application Workflow " }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-764b721f-037e-be6b-7dfb-8d2d464491f5", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-cfc70b61-7c58-bc56-14fd-9f3e3f74a3aa", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db is identical on all Unravel hosts in the ensemble. \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig 1. Stop Unravel Server On each Unravel host, run this command: sudo \/etc\/init.d\/unravel_all.sh stop 2. Modify unravel.properties on host1 Pick a machine to be host1 , edit If the bundled db is in use \/usr\/local\/unravel\/etc\/unravel.properties host1 Replace 127.0.0.1 3316 unravel_mysql_prod To find your fully qualified hostname, type hostname -I unravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod 2. Copy host1's unravel.properties to Other Hosts Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh host1 host2 host3 # host1\nscp \/usr\/local\/unravel\/etc\/unravel.properties host2:\/usr\/local\/unravel\/etc\/\n# host1\nscp \/usr\/local\/unravel\/etc\/unravel.ext.sh host2:\/usr\/local\/unravel\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel Important Note The scripts invoked below will make an identical change to the unravel.properties 3. Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host): # host1\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh\\ \nhost1 host2 \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh\\ \nhost1 host2 For a 3-host ensemble (substitute host): # host1 sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh\\ \nhost1 host2 host3\n# host2 sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh\\ \nhost1 host2 host3\n# host3 sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh\\ \nhost1 host2 host3 These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties .properties unravel.id 4. Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order, on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: # host1\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host3\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: # host1 \nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15sec for Zookeeper quorum to settle: sleep 15 Start up Kafka ensemble: # host1\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-k Wait 10sec for Kafka coordination: sleep 10 Create the Kafka topics (only on one host): # host1 \nsudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh 5. Start Unravel Server Finish multi-host installation by starting up Unravel Server: # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host1\necho \"http:\/\/$(hostname -f):3000\/\" \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start 6. Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 7. Snapshot unravel.properties as new golden file " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-81af64ff-c467-b6a1-edb7-bef17b33e603", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 sudo chkconfig --add unravel_hhw_2 sudo ch...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: sudo \/etc\/init.d\/unravel_all.sh start\n " }, 
{ "title" : "Defining a Custom TC Port", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-fee8fb8c-5f6a-60e0-e8b7-5f3c47093850", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom TC Port", 
"snippet" : "These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\ >>\/usr\/local\/unravel\/etc\/unravel.ext.sh...", 
"body" : " These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\\n>>\/usr\/local\/unravel\/etc\/unravel.ext.sh " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-c1415426-d768-22da-2292-845afa23d51b", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use Hi...", 
"body" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use HiveServer2 LDAP, then follow the steps below. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties Change QA ldap:\/\/LDAP_HOST For QA For ldap:\/\/LDAP_HOST ldaps:\/\/LDAP_HOST unravel\/jre\/ ldap:\/\/ldap_host:9999 For Active Directory (AD): com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.Domain=QA Change LDAP_HOST and QA to appropriate value for your installation. For Open LDAP, example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 2: In this example, we expect a typical DN to be uid=%s,ou=myunit,dc=example,dc=com where %s is the login name as typed in the login form. In some cases 'cn' is used in place of 'uid'. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. 2. Restart unravel_tc Restart unravel_tc sudo \/etc\/init.d\/unravel_tc restart Advanced Hive Properties Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The process of authentication is described next. Authentication Process for Active Directory (AD) Bind as username + at sign + domain, using the given password, with simple LDAP auth mode verbose log will show Connecting and then Connected when bind is successful If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group filter is specified, it is checked if a user filter is specified, it is checked Authentication Process for Open LDAP Bind as cn or uid =username + baseDN using the given password, with simple LDAP auth mode the guidKey property determines whether cn or uid is used if userDNPattern is used, it takes precedence over baseDN, and each pattern is tried If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group pattern or filter is specified, it is checked if a user filter is specified, it is checked Property Description Example Value hive.server2.authentication.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. REQUIRED for advanced query except when setting custom query or groupDNPattern. uid or CN hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). hiveuser,impalauser,hiveadmin,hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member, uniqueMember or memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group, groupOfNames or groupOfUniqueNames com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_tc_webapp.log file under \/usr\/local\/unravel\/logs\/ ; when enabled, user names and group names can appear in this log, but raw passwords are not logged. Can be true or false or not set; default is false " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-26f9c2ab-feb7-5de2-5208-dfa65a305f22", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon Manage Configuration Core Retention Manage Configuration will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 500KB of disk space. That m...", 
"body" : "To adjust the retention time ( time horizon Manage Configuration Core Retention Manage Configuration will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 500KB of disk space. That means about 2000 jobs per 1GB of disk. In Unravel Web UI, select the Manage Configuration Core The TIME SERIES RETENTION DAYS unravel.properties: com.unraveldata.retention.max.days=90 The WEEKS TO SHOW FOR SEARCH RESULTS unravel.properties: com.unraveldata.history.maxSize.weeks=7 This value should be no larger than the next setting minus 1. The WEEKS TO SHOW FOR DEEP SEARCH RESULTS unravel.properties: com.unraveldata.recent.maxSize.weeks=14 This value should be at least 1 week more than the setting immediately above. After changing any of the settings above, restart unravel_td service: sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html#UUID-b2acdcb5-c71e-1f8f-4c78-5f8fc44ff436", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. \n \n \n Web UI \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 25 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n false \n \n Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw \n \n START TLS \n mail.smtp.starttls.enable \n false \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n null \n Username for SMTP authentication \n \n USER PASSWORD \n mail.smtp.pw \n null \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n ocalhost l \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone @example.com \n Use a From: \n \n LOCALHOST \n mail.smtp.localhost \n localhost.local \n A domain name for apparent sender; must have at least one dot (e.g. organization.com) \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. GMail SMTP Example These settings are for our internal use. Do not \n \n \n Config Label \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 587 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n true \n Enable SMTP authentication? \n \n START TLS \n mail.smtp.starttls.enable \n true \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n omeone@organization.com\n s \n Username for SMTP authentication \n \n USER PASSWORD \n \n mail.smtp.pw \n ******** \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n smtp.gmail.com \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone@example.com \n This sets the From header \n \n LOCALHOST \n mail.smtp.localhost \n xample.com\n e \n A domain name for apparent sender; must have at least one dot \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. Debug mode under \"Advance SMTP\" section Unravel daemons to restart after email setup Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_tc restart \nsudo \/etc\/init.d\/unravel_all.sh stop-etl\nsudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n--> enter password from dist.unraveldata.com\n--> should see following output in terminal mode and if you see \"result is = null\", then, setup is correct.\n:\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add following properties to \/usr\/local\/unravel\/unravel.properties on Unravel Server: mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Disable unneeded daemons: sudo service unravel_os3 stop\nsudo chkconfig unravel_os3 off Restart daemons: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI....", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. " }, 
{ "title" : "For CDH+CM", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-a5abea26-3de6-4481-2353-fd337e649e32_id_ConnectingtoaHiveMetastore-ForCDHCM", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For CDH+CM", 
"snippet" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the RES...", 
"body" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/CMGR_HOSTNAME_IP:7182\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore 2. In Unravel Web UI, on the top right-hand corner, click Admin Manage 3. On the left tab, click Hive HIVE METASTORE URL HIVE METASTORE DRIVER HIVE METASTORE USER NAME HIVE METASTORE PASSWORD 4. Save the information when done: click Save Changes 5. Restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart 6. After restart, confirm that Hive queries appear in Unravel UI in the Application " }, 
{ "title" : "For HDP", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-a5abea26-3de6-4481-2353-fd337e649e32_id_ConnectingtoaHiveMetastore-ForHDP", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For HDP", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "For MapR", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-a5abea26-3de6-4481-2353-fd337e649e32_id_ConnectingtoaHiveMetastore-ForMapR", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For MapR", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-695e55b7-c776-d16b-754f-e427366971fa_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storag...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storage of 20% left to alert contact under \" Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for \" 10 consecutiveperiod(s) Under Actions Send notifications to \n Note: this sns topic should already be setup before you add it. Click \" Create Alarm Now, you will see in \" Alarms Alarms Click \" Create Alarm " }, 
{ "title" : "Creating Application Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags", 
"snippet" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed....", 
"body" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. " }, 
{ "title" : "What is a Tag in Unravel?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-WhatisaTaginUnravel", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What is a Tag in Unravel?", 
"snippet" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ......", 
"body" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ... " }, 
{ "title" : "How Does Unravel Use Tags?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-HowDoesUnravelUseTags", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Does Unravel Use Tags?", 
"snippet" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights...", 
"body" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights " }, 
{ "title" : "What Types of Tags Are There?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-WhatTypesofTagsAreThere", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What Types of Tags Are There?", 
"snippet" : "There are two types of tags: Unravel tags and user-created tags. Unravel Tags All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp User-Created Tags You can create tags based on your use cases....", 
"body" : "There are two types of tags: Unravel tags and user-created tags. Unravel Tags All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp User-Created Tags You can create tags based on your use cases. " }, 
{ "title" : "How Do I Create Tags?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-HowDoICreateTags", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Do I Create Tags?", 
"snippet" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,.....", 
"body" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,... Injecting Tags Through a Python Script You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches a consistent state.For example, Spark receives data from Resource Manager, event log file, YARN aggregated logs, and sensors. Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. " }, 
{ "title" : "Precedence of Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-PrecedenceofTags", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Precedence of Tags", 
"snippet" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script...", 
"body" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script " }, 
{ "title" : "Sample Use Cases", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-9f04b592-2a59-cfdc-f480-4bfbf296789f_id_CreatingApplicationTags-SampleUseCases", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Sample Use Cases", 
"snippet" : "Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants...", 
"body" : " Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting", 
"snippet" : "Add troubleshooting article Troubleshooting article Provide solutions for commonly encountered problems. <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround:...", 
"body" : "Add troubleshooting article Troubleshooting article Provide solutions for commonly encountered problems. <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround: " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting.html#UUID-c201d798-6123-2a65-d900-dc5ea5d6a4aa", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working...", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (“Smoke Tests”) Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run thespark_test_via_parcel.shscript. This script runs a Spark app. It’s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, even before you install and configure Unravel Sensor. You should be able to see the data generated by this Spark app on Unravel Web UI. \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server <unravel_host_IP_address> Note: You can run this script without installing and configuring Unravel Sensor. After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 Running Benchmarks We provide sample Spark, MapReduce, Hive, and WF apps that you can download from preview.unraveldata.com Spark Download our sample Spark app: curl https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz -o spark-benchmarks1.tgz This .tgz Runmd5sumon spark-benchmarks1.tgz to ensure it is intact: md5sum spark-benchmarks1.tgz Confirm that the output ofmd5sumis exactly as shown on the line below: ff8e56b4d5abfb0fb9f9e4a624eeb771 spark-benchmarks1.tgz Uncompress the .tgz tar -zxvf spark-benchmarks1.tgz Run the samples. Tip: Instructions on how to run the samples are included in the package itself, inside demo-benchmarks-for-spark\/benchmarks\/README. After running the samples, check the Program and the Execution Graph tabs in Unravel Web UI. Click an RDD in the Execution Graph to see the corresponding line of code in the app. " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting.html#UUID-88eb4b72-4cc3-120b-2c38-1e38025249fe", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/uninstalling-unravel-server.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel\nsudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Upgrading Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/upgrading-unravel-server.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Upgrading Unravel Server", 
"snippet" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1 sudo \/etc\/init.d\/unravel_all.sh stop...", 
"body" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host2\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host3\nsudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously # host1\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host2\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host3\nsudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties You must enter add license key to unravel.properties Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh After all the RPM upgrades finish, restart Unravel Server on each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Release Notes: Version 4.1.941", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941", 
"snippet" : "Contents Software Version New Features Tested Platforms Improvements\/Bugfixes Robustness\/Reliability Spark Support Workflow Support MR Insights Known Issues...", 
"body" : " Contents Software Version New Features Tested Platforms Improvements\/Bugfixes Robustness\/Reliability Spark Support Workflow Support MR Insights Known Issues " }, 
{ "title" : "Software Version", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-SoftwareVersion", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Software Version", 
"snippet" : "On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z...", 
"body" : " On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z " }, 
{ "title" : "New Features", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-NewFeatures", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ New Features", 
"snippet" : "Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics...", 
"body" : " Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics " }, 
{ "title" : "Tested Platforms", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-TestedPlatforms", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-ImprovementsBugfixes", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes", 
"snippet" : "Fixed the issue of “Done directory setting via UI” Fixed the issue of “Config wizard not showing the value of spark event log location”...", 
"body" : " Fixed the issue of “Done directory setting via UI” Fixed the issue of “Config wizard not showing the value of spark event log location” " }, 
{ "title" : "Robustness\/Reliability", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-RobustnessReliability", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes \/ Robustness\/Reliability", 
"snippet" : "Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common setti...", 
"body" : " Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common settings are all included and don't need to be specified by the user Improved Unravel Resource Metrics Sensor performance " }, 
{ "title" : "Spark Support", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-SparkSupport", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes \/ Spark Support", 
"snippet" : "Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configur...", 
"body" : " Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configuration from MySQL Accessing S3 log files fixes: Mapping multiple S3 buckets to the same S3 profile Set the S3 region to a custom one. Scripts and DSL API extension to generate large Oozie workflows programmatically " }, 
{ "title" : "Workflow Support", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-WorkflowSupport", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes \/ Workflow Support", 
"snippet" : "Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this fe...", 
"body" : " Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this feature, the Oozie workflows won't show up; instead, the selected (tagged) applications will be grouped into a single tagged workflow and presented in the Workflows tab. Airflow improvements " }, 
{ "title" : "MR Insights", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-MRInsights", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Improvements\/Bugfixes \/ MR Insights", 
"snippet" : "Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs...", 
"body" : " Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-0-4-1/release-notes--version-4-1-941.html#UUID-7cba538b-fd92-f5e0-3f38-b251ff6d1522_id_ReleaseNotesVersion41941-KnownIssues", 
"breadcrumbs" : "UN40 (Unravel 4.0-4.1) \/ Unravel 4.0-4.1 \/ Release Notes: Version 4.1.941 \/ Known Issues", 
"snippet" : "Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (so...", 
"body" : " Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (source - unravel_emr_sensor.log in EMR cluster) Improve\/Rewrite EMR bootstrap script to avoid all manual steps. " }
]
$(document).trigger('search.ready');
});