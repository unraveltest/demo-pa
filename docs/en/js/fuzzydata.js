$(document).ready(function () {indexDict['en'] = [{ "title" : "", 
"url" : "unravel-4-5.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.5 •...", 
"body" : " Unravel 4.5 • " }, 
{ "title" : "Overview", 
"url" : "unravel-4-5/overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Overview", 
"snippet" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, ...", 
"body" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, collecting data from every pipeline, every job, wherever those workloads are running. Unravel creates a correlated data model that provides the full context you need on your apps and resources to properly plan, manage, and improve performance. Unravel enables you to: \n \n Uncover \n \n Understand \n \n Unravel Unravel goes beyond raw visibility to provide concrete, providing AI-driven advice such as: Code you can use. Specific settings you can tweak. Recommendations you can immediately implement or automate to fix issues and optimize performance. The Unravel Data Operations Platform helps operations engineers, application developers, and enterprise architects reduce the complexity of delivering reliable application performance – providing unified visibility and operational intelligence to optimize your entire ecosystem. In summary, problems solved by Unravel: APM for Big Data AI for DataOps Cloud Migration Resource and Cost Optimization Troubleshooting and Root Cause Analysis " }, 
{ "title" : "Where Does Unravel Reside", 
"url" : "unravel-4-5/overview.html#UUID-98969c33-8b3d-d5d9-2229-78ffd5600eae_id_Overview-WhereDoesUnravelReside", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Overview \/ Where Does Unravel Reside", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services ...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS. Unravel analyzes this information and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Installing OnDemand for report generation. " }, 
{ "title" : "Features", 
"url" : "unravel-4-5/overview.html#UUID-98969c33-8b3d-d5d9-2229-78ffd5600eae_id_Overview-Features", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Overview \/ Features", 
"snippet" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. Dashboards Application Program Manager (APM) Auto-Actions & Alerts Reporting Insights and Recommendations Support Role Based Access Cont...", 
"body" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. \n Dashboards \n Application Program Manager (APM) \n Auto-Actions & Alerts \n Reporting \n Insights and Recommendations \n Support Role Based Access Control See here " }, 
{ "title" : "Installation Guides", 
"url" : "unravel-4-5/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides", 
"snippet" : "Cloudera Hortonworks Amazon Elastic MapReduce (EMR) Amazon Athena MapR Azure HDInsight MySQL OnDemand...", 
"body" : " Cloudera Hortonworks Amazon Elastic MapReduce (EMR) Amazon Athena MapR Azure HDInsight MySQL OnDemand " }, 
{ "title" : "Cloudera", 
"url" : "unravel-4-5/install/install-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Cloudera", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM)....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-5/install/install-cdh.html#UUID-7f2a0fe2-fbde-f775-e6ce-c8621a0ee9e3_id_Cloudera-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Ordered Steps", 
"snippet" : "Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor and Configure Impala The following reports require that you install the OnDemand service: Sessions Cluster Optimization, Capacity Forecasting,and Small Files For installation instructions, see OnD...", 
"body" : " Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor and Configure Impala The following reports require that you install the OnDemand service: Sessions Cluster Optimization, Capacity Forecasting,and Small Files For installation instructions, see OnDemand " }, 
{ "title" : "Cloudera Pre-Installation Check", 
"url" : "unravel-4-5/install/install-cdh/install-cdh-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Cloudera Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos Hard...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.6-7.5 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark Unravel host should be a client\/gateway (Hadoop and Hive commands in PATH) If Spark2 service is installed, Unravel host should be a client\/gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not NTP should be running and in-sync with the cluster MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions Local user unravel:unravel is created during installation, but can be changed later If Kerberos is in use, a keytab for an Unravel Kerberos principal is required for access to: Access to YARN’s \"done directory\" in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) For Impala support, a read-only account for Cloudera Manager API is needed Network The following ports must be open on the Unravel edge node: Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI If you plan to use Cloudera Manager to install Unravel Sensors, the Cloudera Manager service must also be able to reach the Unravel edge node on port 3000. 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "unravel-4-5/install/install-cdh/install-cdh-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create th...", 
"body" : "This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive, and optionally Spark2. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. Update Site-Specific Properties in unravel.properties \/\/ Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/\n 3. Configure Unravel Server (Basic\/Core Optional for CDH) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data If Kerberos is Enabled, Add Authentication for HDFS: Create Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE. unravel Run Unravel Daemons with Custom User If Sentry is Enabled, Add These Permissions: Define your own alt principal with narrow privileges. The alt principal can be unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal read Spark event log hdfs:\/\/user\/spark\/spark2ApplicationHistory Your alt principal read Spark 2 event log hdfs:\/\/user\/history Your alt principal read MapReduce logs hdfs:\/\/tmp\/logs Your alt principal read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse Your alt principal read Obtain table partition sizes with \"stat\" only Please see Configure Permission for Unravel daemons on CDH Sentry Secured Cluste You can find the principal by using 'klist -kt KEYTAB_FILE' If you are using KMS and HDFS encryption and are using the hdfs principal, you might need to adjust kms-acls.xml . If you are using \"JNI\" based groups for HDFS (a setting in CM), then you will need to add \" export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native\" to \/usr\/local\/unravel\/etc\/unravel.ext.sh Switch User Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Hive Metastore Access Hive metastore is accessed by Unravel server to analyze table usage in conjunction with Hive job instrumentation. Information is gathered using a Hive API that works very much like beeline connections which leverage the jdbc database connection protocol. As a quick-start approach, you can set Unravel to use the already-defined 'hive' user that is also used by HiveServer2. Alternatively, you can define a read-only metastore database user. If you want a custom user, then do the following steps for the particular kind of database that is used for Hive metastore: Connect to the Hive metastore using the normal conversational interface (mysql or psql, etc.) as an admin that can create new users. Create a user, e.g., unravel Grant select on all table in the hive database. As the new user, use the conversational interface (mysql or psql, etc.) from the Unravel server to verify their access. To complete the integration of the Hive metastore with the user, follow the steps in Hive Metastore Access Restart Unravel Server After the edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. 4. Log into Unravel Web UI Using a web browser, navigate to http:\/\/{UNRAVEL_HOST_IP} admin unraveldata UNRAVEL_HOST_IP. For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 5. Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor and Configure Impala " }, 
{ "title" : "Step 2: Install Unravel Sensor and Configure Impala", 
"url" : "unravel-4-5/install/install-cdh/install-cdh-part2-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Step 2: Install Unravel Sensor and Configure Impala", 
"snippet" : "Table of Contents Introduction 1. Obtain and Distribute the Parcel from Unravel Server 2. Put the Hive Hook JAR in AUX_CLASSPATH 3. For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path 4. Deploy the Hive Hook Instrumentation 5. Deploy the Spark Instrumentation 6. Optional - ...", 
"body" : "\n Table of Contents \n Introduction \n 1. Obtain and Distribute the Parcel from Unravel Server \n 2. Put the Hive Hook JAR in AUX_CLASSPATH \n 3. For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path \n 4. Deploy the Hive Hook Instrumentation \n 5. Deploy the Spark Instrumentation \n 6. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide \n 7. Enable Impala APM with CM as the Data Source \n 8. (Optional) Advanced Configuration \n Troubleshooting \n References Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 \n HIGHLIGHTED When Active Directory Kerberos is used, UNRAVEL_HOST_IP 1. Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ \n X.Y \n UNRAVEL_HOST_IP unravel_lr If you are running more than one version of CDH (multiple clusters) in Cloudera Manager, you can add more than one parcel directory from the UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, deactivate it now. Click Activate 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is enabled, grant privileges on the JAR files to the Sentry roles that run Hive queries. Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT {ROLE} # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE {ROLE} 3. For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath 4. Deploy the Hive Hook Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use the \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip In Cloudera Manager, go to the Hive service. Select the Configuration Search for hive-site.xml Add the snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook \n IMPORTANT! Add the snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Like the step above, if the hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook \n IMPORTANT! Save the changes with optional comment \"Unravel snippet in hive-site.xml \n \" Deploy the Hive Client configuration by clicking the deploy glyph ( ) or by using the Actions Restart the Hive service. Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. Check Unravel UI to see if all Hive queries are running. If queries are running fine and appearing in Unravel Web UI, you are done. If queries are failing with a \"class not found\" Undo the hive-site.xml Deploy the hive client configuration. Restart the Hive service. Follow the steps in Troubleshooting 5. Deploy the Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \" spark-defaults In the Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use the fully qualified DNS or logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) \n spark-conf\/ UNRAVEL_HOST_IP SPARK_VERSION-X.Y \n SPARK_VERSION -X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true Save changes. Deploy the client configuration by clicking the deploy glyph ( ) or by using the Actions Enable Spark Streaming. The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y. javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-X.Y. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited.\\ Check Unravel UI to see if all Spark jobs are running. If jobs are running fine and appearing in Unravel Web UI, you are done. If queries are failing with a \"class not found\" Undo the spark-defaults.conf Deploy the client configuration. Investigate and fix the issue. Follow the steps in Troubleshooting In the case of yarn-client mode applications, the default Spark configuration is not sufficient, because the driver JVM starts before the configuration set through the SparkConf is applied. For more information, see Apache's Spark Configuration here per-application profiling cluster-wide profiling 6. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide For instructions, see CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) 7. Enable Impala APM with CM as the Data Source Configure Unravel Server to retrieve Impala query data from Cloudera Manager (CM) as follows: Add com.unraveldata.data.source=cm \/usr\/local\/unravel\/etc\/unravel.properties Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n \n com.unraveldata.cloudera.manager.url \n CM internal URL. Must start with http:\/\/ \n \n com.unraveldata.cloudera.manager.port \n (Optional) CM port number. You only need to specify this if your Cloudera Manager is not on port 7180. \n \n com.unraveldata.cloudera.manager.username \n CM username \n \n com.unraveldata.cloudera.manager.password \n CM password For example: \n \n \n \n com.unraveldata.data.source=cm \n com.unraveldata.cloudera.manager.url= http:\/\/mycm.somewhere.secret \n com.unraveldata.cloudera.manager.port=9997 \n com.unraveldata.cloudera.manager.username=mycmname \n com.unraveldata.cloudera.manager.password=mycmpassword Make sure that the CM user in com.unraveldata.cloudera.manager.username curl \n curl --user CM_username: password CM CM \n curl --user CM_username:CM_password 'http:\/\/ CM _url: CM By default, the ImpalaSensor task is enabled. To disable it, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties \n \n \n com.unraveldata.sensor.tasks.disabled=iw (Optional) Change the Impala lookback window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us 8. (Optional) Advanced Configuration Configure Unravel Server for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Enabling LDAP Authentication for Unravel Web UI Troubleshooting \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR Verify that unravel \/user\/unravel\/ \n \n \n ClassNotFound com.unraveldata.dataflow.hive.hook.UnravelHiveHook \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST References \n {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "Hortonworks", 
"url" : "unravel-4-5/install/install-hdp.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Hortonworks", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP)....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-5/install/install-hdp.html#UUID-765512ba-2a1b-f017-5983-e467ab26fd74_id_Hortonworks-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ Ordered Steps", 
"snippet" : "HDP Pre-Installation Check Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see ...", 
"body" : " HDP Pre-Installation Check Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "HDP Pre-Installation Check", 
"url" : "unravel-4-5/install/install-hdp/install-hdp-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ HDP Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache ver. equiv.) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB ...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache ver. equiv.) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway Verify that all clients are installed by checking that RPMs are installed; Unravel utilizes clients and associated libraries You need to register edge node to Ambari LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for an Unravel Kerberos principal is required for access to: Access to YARN's \"done dir\" in HDFS Access to YARN's log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active If you plan to use the Auto Actions feature (move \/ kill), you'll need to add the Unravel username to YARN yarn.admin.acl JDBC access to the Hive Metastore (read-only user is sufficient) Application Timeline Server (ATS) read-only Network The following ports must be open on Unravel Server's host machine(s): Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "unravel-4-5/install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "This topic explains how to deploy Unravel Server on HDP. For this installation you need to allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari Web UI to create a gateway node configuration. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Acces...", 
"body" : "This topic explains how to deploy Unravel Server on HDP. For this installation you need to allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari Web UI to create a gateway node configuration. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access. For HDP, use Ambari Web UI to create a Gateway node configuration. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh Done The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. Configure Unravel Server (Basic\/Core Options) Enable optional daemons. Depending on your workload volume or kind of activity, you can enable optional daemons at this point. For more information, see Creating Multiple Workers for High Volume Data Edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Example Values com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel Web UI's configuration wizard. \/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel Web UI's configuration wizard. \/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.s3.batch.monitoring.interval.sec Optional. Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. 120 com.unraveldata.spark.eventlog.location Where to find Spark event logs hdfs:\/\/\/user\/spark\/applicationHistory\/, hdfs:\/\/\/user\/spark\/sparkApplicationHistory\/, hdfs:\/\/\/user\/spark\/spark2ApplicationHistory\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL http:\/\/example.localdomain:8088\" oozie.server.url Oozie URL http:\/\/example.localdomain:11000\/oozie If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using properties klist -kt KEYTAB_FILE The keytab file should have chmod bits 500 and be owned by unravel Run Unravel Daemons with Custom User If Ranger is Enabled, Add These Permissions: Define your own alt principal with narrow prvileges. The alt user can be unravel rpm X in the switch user section below. Resource Principal Access Purpose hdfs:\/\/spark-history Your alt principal read+execute Spark event log hdfs:\/\/spark2-history Your alt principal read+execute Spark2 event log hdfs:\/\/mr-history\/done Your alt principal read+execute MapReduce logs hdfs:\/\/app-logs Your alt principal read+execute YARN aggregation folder hdfs:\/\/apps\/hive\/warehouse (Default value of hive.metastore.warehouse.dir Your alt principal read+execute Obtain table partition sizes Hive Metastore database GRANT hive read+execute Hive table information You must set yarn properties in Ambari. Log into Ambari and from the dashboard select YARN Configs Advanced Set yarn.acl.enable Add the Unravel user specified in com.unraveldata.kerberos.principal above yarn.admin.acl Save your changes. Disable Impala Sensor Impala is not officially supported on HDP clusters. Therefore you should disable the Impala Sensor by setting\/adding com.unraveldata.sensor.tasks.disabled=iw \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw Convert Your Unravel Installation to HDP Run the following commands on Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh This change is persistent through subsequent RPM upgrades; it does not need to be done each time. Switch \"user\" Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y where X Y switch_to_user Restart Unravel Server. After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "unravel-4-5/install/install-hdp/install-hdp-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "Table of Contents Introduction 1. Convert Your Unravel Installation to HDP 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties 3. Start Unravel Server 4. Install Unravel hive hook and spark sensor onto HDP servers 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs t...", 
"body" : "\n Table of Contents \n Introduction \n 1. Convert Your Unravel Installation to HDP \n 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties \n 3. Start Unravel Server \n 4. Install Unravel hive hook and spark sensor onto HDP servers \n 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path. \n 6. Add Unravel Hive Hook hive-site settings to all of HDP's Servers in the cluster using AWU. \n 7. Optionally for Tez, enable Unravel Tez instrumentation on all of HDP's Services in the cluster. \n 8. Optionally for Spark on YARN \n 9. Optionally for YARN \n 10. Confirm that Unravel Web UI Shows Tez Data Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Convert Your Unravel Installation to HDP Note: This change remains after RPM upgrades. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties Add these properties to\/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n com.unraveldata.yarn.timeline-service.webapp.address \n The http address of the Timeline service web application \n \n http:\/\/localhost \n \n com.unraveldata.yarn.timeline-service.port \n Timeline service port \n 8188 Example \n \n \n \n com.unraveldata.yarn.timeline-service.webapp.address= http:\/\/172.16.1.101 If the Application Timeline Server requires user authentication the following properties need also be specified in: \/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n yarn.ats.webapp.username \n Username required for authentication to the Application Timeline Server (if authentication is required) \n \n yarn.ats.webapp.password \n Password required for authentication to the Application Timeline Server (if authentication is required) Open \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh If you are using Spark1 and Spark2 you must com.unraveldata.spark.eventlog.location com.unraveldata.spark.eventlog.location = hdfs:\/\/\/spark2-history\/ \n \n \n \n \/\/ Repoint Unravel application logs directory com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/ \n \/\/ Add Hive Metastore database information for Unravel Hive Config javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/UNRAVEL_HOST_IP:3306\/database_name javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName=HiveMetastoreUserName javax.jdo.option.ConnectionPassword=HiveMetastorePassword Log into Ambari Web UI (AWU) to verify the above properties have been set correctly in unravel.properties On the left-hand side of AWU's dashboard, click MapReduce2 Configs Advanced Advanced mapred-site Verify com.unraveldata.job.collector.done.log.base mapreduce.jobhistory.done-dir. On the left-hand side of AWU's dashboard, click YARN Configs Advanced Node Manager Verify com.unraveldata.job.collector.done.log. aggregation.base yarn.nodemanager.remote-app-log-dir On the left-hand side of AWU's dashboard, click Hive Configs Advanced Hive Metastore Verify javax.jdo.option.ConnectionURL Database Host Database URL Verify javax.jdo.option.ConnectionDriverName JDBC Driver Class Verify javax.jdo.option.ConnectionUserName Database Username 3. Start Unravel Server Note Unravel must be up for the next step to complete. # sudo \/etc\/init.d\/unravel_all.sh start 4. Install Unravel hive hook and spark sensor onto HDP servers Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows (substitute the correct fully qualified host name or IP address for UNRAVEL_HOST_IP Login as root wget # yum install -y wget From Unravel server (eg. edge node) run on each server that will use instrumentation. Be sure to substitute valid values for: \n UNRAVEL_HOST_IP \n SPARK_VERSION_X.Y.Z \n HIVE_VERSION # cd \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/\n# sudo python2 unravel_hdp_setup.py --sensor-only --unravel-server {Unravel Host}:3000 --spark-version {SPARK_VERSION} --hive-version {HIVE_VERSION} --ambari-server {Ambari Host} Files are installed locally on the edge node under: \n \/usr\/local\/unravel_client (Hive hook jar) \n \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor jars) \n Manually copy host2 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path. Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar oozie.libpath 6. Add Unravel Hive Hook hive-site settings to all of HDP's Servers in the cluster using AWU. Completion of this step requires a restart of all affected Hive services in Ambari UI. In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. \n \n \n \n hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR com.unraveldata.hive.hook.tcp=true com.unraveldata.host={add unravel gateway internal IP hostname} \n \/\/ Find below properties as it may already exists, concatenate it with a comma & no spaces hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook If LLAP is enabled copy the above settings in Custom hive-interactive-site Manual edit hive-site.xml (no AWU) \n hive-site.xml \/etc\/hive\/conf\/ Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env \n \n \n \n export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar If LLAP is enabled copy above line of code into Advanced hive-interactive-env You can manually edit hive-env.sh without using AWU. The hive-env.sh \/etc\/hive\/conf\/ Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env export HADOOP_CLASSPATH \n \n \n \n export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar 7. Optionally for Tez, enable Unravel Tez instrumentation on all of HDP's Services in the cluster. Completion of this step requires a restart of all affected Hive services in Ambari UI. Confirm that hive-execution.engine is set to tez. \n \n \n set hive.execution.engine=tez; Using the Ambari Web UI (AWU), configure the Btrace agent for Tez: Append the below java options to tez.am.launch.cmd-opts tez.task.launch.cmd-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment you need to modify tez.am.view-acls 8. Optionally for Spark on YARN Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster Completion of this step requires a restart of all affected Spark services in Ambari UI. Be sure to substitute valid values.for UNRAVEL_HOST_IP SPARK_VERSION_X.Y. \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark- \n \n 2.2 spark- \n \n 2.3 Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults You can manually edit spark-defaults.conf without using AWU. The default location for spark-defaults.conf \/usr\/hdp\/current\/SPARK_VERSION_X.Y The cluster only has one spark 1.X version: \/usr\/hdp\/current\/spark-client\/conf For spark 2.X version: \/usr\/hdp\/current\/spark2-client\/conf Inside Custom spark-defaults Add Property Bulk property add mode \n \n \n \n spark.unravel.server.hostport=UNRAVEL_HOST_IP spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs=SPARK_VERSION_X.Yspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs=SPARK_VERSION_X.Yspark.eventLog.enabled=true Enable Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y. javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-X.Y. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited.\\ 9. Optionally for YARN If yarn.acl.enable=true \n yarn.acl.enable=false, \n yarn.admin.acl=userName Unravel only requires yarn.admin.acl 10. Confirm that Unravel Web UI Shows Tez Data Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Unravel Web UI may take a few seconds to load Tez data. " }, 
{ "title" : "Amazon Elastic MapReduce (EMR)", 
"url" : "unravel-4-5/install/install-emr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR)", 
"snippet" : "Ordered Steps Introduction Prerequisites and Requirements Architecture Planning Guidance Step 1: Provision and Configure an Unravel EC2 Instance Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Step 3: Set Up AWS RDS for Unravel DB (Optional) Step 4: Set Up VPC Peering for Unra...", 
"body" : " Ordered Steps Introduction Prerequisites and Requirements Architecture Planning Guidance Step 1: Provision and Configure an Unravel EC2 Instance Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Step 3: Set Up AWS RDS for Unravel DB (Optional) Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional) Testing and Troubleshooting Operational Guidance The following reports require that you install the OnDemand service: Sessions Cluster Optimization, Capacity Forecasting,and Small Files For installation instructions, see OnDemand " }, 
{ "title" : "Introduction", 
"url" : "unravel-4-5/install/install-emr/install-emr-intro.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Introduction", 
"snippet" : "Unravel is full-stack performance management for modern data apps and systems. Use Unravel to Monitor, Manage and Optimize such apps & systems With full-stack visibility & AI-powered guidance Unravel helps customers to Run apps more reliably, improve performance (improving SLA) Run apps more efficie...", 
"body" : "Unravel is full-stack performance management for modern data apps and systems. Use Unravel to Monitor, Manage and Optimize such apps & systems With full-stack visibility & AI-powered guidance Unravel helps customers to Run apps more reliably, improve performance (improving SLA) Run apps more efficiently (lowering costs) Detect, troubleshoot and fix issues quickly (lowering MTTR) Dashboards and Reports (usage, chargeback\/show back, resource usage etc.) Includes support for any modern data stack in the cloud (AWS or Azure) or on-premise (with Cloudera, HDP or MapR) Please find a more detailed Unravel Overview here This guide is focussed on the Unravel for AWS EMR Unravel deployment involves creating a new EC2 instance and setting up RDS (optionally followed by installing Unravel server on it, configuring it and connecting it to the customer’s EMR deployment that will be monitored. (This is what happens in all the there deployment options mentioned next). There are three deployment options for Unravel in this case: Provision an Unravel EC2 instance using our CloudFormation Template Provision an Unravel EC2 instance using our Amazon Machine Image (AMI) Provision and configure an Unravel EC2 instance manually (RPM based deployment) More details of each of these three mechanisms will we discussed later in the guide. A typical deployment and configuration of Unravel that would monitor an EMR cluster should take less than an hour. However, in some cases, it could take a bit longer depending on the complexity of the setup in terms of security\/VPC settings, various permissions setup etc. " }, 
{ "title" : "Prerequisites and Requirements", 
"url" : "unravel-4-5/install/install-emr/install-emr-reqs.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Prerequisites and Requirements", 
"snippet" : "Unravel for AWS EMR We expect that someone operating the AWS EMR will have the requisite skills to setup Unravel. Specifically, the expectation is that the person deploying and configuring Unravel has sufficient knowledge to be able to: Provision EC2 instances, RDS instances Create and configure the...", 
"body" : " Unravel for AWS EMR We expect that someone operating the AWS EMR will have the requisite skills to setup Unravel. Specifically, the expectation is that the person deploying and configuring Unravel has sufficient knowledge to be able to: Provision EC2 instances, RDS instances Create and configure the required IAM roles, Security groups etc. Working knowledge of AWS networking concepts e.g VPC, subnets etc. Run Ansible scripts, basic Unix commands, AWS CLI Note Person deploying does not need to create any scripts or be familiar with any specific programming\/scripting language etc. The instructions are self-contained and someone with basic knowledge of AWS should be able to follow those. Does not require expert level knowledge for AWS. Since this solution is for monitoring, managing and optimizing modern data applications running on AWS EMR, so the requirement for an AWS account to be in place will implicitly be met. There are no specific requirements for a client operating system to work on the Unravel deployment process. The person deploying just needs to be able to connect to AWS for the deployment process. As for AWS permissions and access needed for the person deploying, they should be able to create EC2 instances, connect to those and install software on them. Create security groups, IAM roles and update those for the EMR cluster and the corresponding S3 storage. In case they want to deploy Unravel for a new EMR cluster, they should also have the required permissions to create an EMR cluster and necessary S3 buckets, create and configure VPC etc. As for licensing, Unravel comes with a 30-day trial license and so the deployment can be completed and the product can be used for 30 days without any additional licenses. The process to obtain licenses post this time period is listed in the Planning Guidance#Costs " }, 
{ "title" : "Architecture", 
"url" : "unravel-4-5/install/install-emr/install-emr-arch.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Architecture", 
"snippet" : "In order to manage, monitor and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics data, configuration information as wel...", 
"body" : " In order to manage, monitor and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics data, configuration information as well as logs. Parts of this data are pushed to Unravel and some are pulled by the daemons in the Unravel server. In order for this to available, there is a need for both inbound and outbound access between the Unravel Server (on the EC2 instance) and the EMR cluster. More details on this are shared here About backups and snapshots: We will discuss more about in the Backup and Recovery section " }, 
{ "title" : "Planning Guidance", 
"url" : "unravel-4-5/install/install-emr/install-emr-planning.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Planning Guidance", 
"snippet" : "Security About settings\/configurations required related to IAM roles, Security Groups etc. Security Aspects related to the Unravel Application itself Unravel (Application) Users - Authentication and Authorization Risk Audit Mechanism Costs Sizing Security Note: here About settings\/configurations req...", 
"body" : " Security About settings\/configurations required related to IAM roles, Security Groups etc. Security Aspects related to the Unravel Application itself Unravel (Application) Users - Authentication and Authorization Risk Audit Mechanism Costs Sizing Security Note: here About settings\/configurations required related to IAM roles, Security Groups etc. In order to manage, monitor and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the data apps running on the cluster. This information includes metrics data, configuration information as well as logs. Parts of this data are pushed to Unravel and some are pulled by the daemons in the Unravel server. In order for this to available, there is a need for both inbound and outbound access between the Unravel Server (on the EC2 instance) and the EMR cluster. Unravel server is set up on an EC2 instance and connects to the customer’s EMR cluster. The Unravel EC2 instance must be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. There are two possible scenarios: Both EMR cluster and Unravel node are created in the same VPC, same subnet; and the security group allows all traffic from the same subnet or The EMR cluster is located on a different VPC than the Unravel server, then you must configure VPC peering, route table creation, and security policy update. Instructions for that are available here The Unravel application has a UI which is what Unravel users connect to and use. It is configured to be available on port 3000. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node Create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address (The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes) or Put the member of security group used on EMR cluster in this rule. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes need to allow all traffic from the Unravel node. If it is not possible to Unravel EC2 access all traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Create a S3 ReadAccess only IAM role and assign it to Unravel EC2 node to READ the archive logs on the S3 bucket configured for the EMR cluster (create an IAM role that contains the policy that can only READ the specific S3 bucket used on EMR cluster. Then create a EC2 instance profile and add the IAM role to it) All the above points are adequately placed and discussed in the context of the appropriate setup and configuration steps. Refer to this this Security Aspects related to the Unravel Application itself As mentioned previously, Unravel UI and API are the user-facing Unravel components of this setup. Instructions for enabling TLS (SSL) for the Unravel UI are shared here Unravel (Application) Users - Authentication and Authorization Users access Unravel via the Unravel UI. There is one default admin account created automatically on installation. Follow this process For integrating LDAP authentication for Unravel UI, follow these steps here Unravel also allows for Role Based Access Control (RBAC) to restrict the views and applications various users can see when they login to the Unravel UI. Steps for setting and configuring RBAC are described here Risk Audit Mechanism The logging of actions done on the Unravel applications is recorded on the logs on the same instance that the Unravel server is deployed on. Specifically, this is the file to look into to find out various actions that have occurred in case you need to trace down an incident: .\/usr\/local\/unravel\/logs\/unravel_ngui.log Costs There are two components of the cost to the user in using Unravel: Cost of AWS Components: Amazon EC2 Pricing Amazon EBS Pricing Amazon RDS Pricing Cost of Unravel license contact Unravel Note: AWS generated tags user-defined tags here Sizing EC2 Instance Specifications: Minimum: r4.2xlarge (61 GiB RAM) Maximum: r4.8xlarge (244 GiB RAM) Recommended: r4.4xlarge (122 GiB RAM) Virtualization type: HVM EBS Volume Specifications: Provision 200 GiB or above with Volume Type = Provisioned IOPS SSD ( io1 The Baseline IOPS (3 IOPS per GiB with a minimum of 100 IOPS, burstable to 3000 IOPS) is sufficient for Unravel (Optional) RDS Specifications (in case the user chooses this): DB instance class: db.r3.xlarge — 4 vCPU, 30.5 GiB RAM Storage type: Provisioned IOPS (SSD) Allocated storage: 200 GiB or above Provisioned IOPS: 1000 Note: AWS Service Limits Virtual Private Cloud (Amazon VPC) Limits Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template " }, 
{ "title" : "Step 1: Provision and Configure an Unravel EC2 Instance", 
"url" : "unravel-4-5/install/install-emr/install-emr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance", 
"snippet" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR deployment. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Im...", 
"body" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR deployment. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision and Configure an Unravel EC2 Instance Manually " }, 
{ "title" : "Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"url" : "unravel-4-5/install/install-emr/install-emr-part1/install-emr-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Ma...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Allowed port for inbound access to EMR master node: Port 8020 (Name node access for spark event log) Allowed port for inbound access to EMR core node: Port 50010 (Data node access for spark event log) Port 50020 AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our CloudFormation Template Download our CloudFormation template from the following public S3 bucket into your local \/tmp folder: https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_aws_marketspace_01.json Use AWS CLI or the AWS Marketplace console to create the Unravel EC2 instance: From AWS CLI, run the aws cloudformation Syntax: aws cloudformation create-stack --stack-name UnravelEC2 \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue= AWS_ZONE_NAME AWS_INSTANCE_TYPE my_ssh_key AWS_VPC_BLOCK AWS_SSH_BLOCK Example: aws cloudformation create-stack --stack-name UnravelEC2number \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue=us-east-1d \\ \nParameterKey=InstanceType,ParameterValue=r4.2xlarge \\ \nParameterKey=KeyName,ParameterValue=topcat \\ \nParameterKey=UnravelVPCBlock,ParameterValue=10.12.0.0\/16 \\ \nParameterKey=TrustedSshIPBlock, ParameterValue=0.0.0.0\/0 \\ \n--capabilities CAPABILITY_IAM | tee UnravelEC2-stack.json From AWS Marketplace ( https:\/\/aws.amazon.com\/marketplace a. Type unravel b. In the search results, select Unravel APM for EMR Continue to Subscribe c. Click Continue to Configuration d. On the Configure this software Fulfillment Option Software Version Region e. Click Continue to Launch f. On the Launch this software Usage Instructions g. In the Choose Action Launch CloudFormation h. Click Launch You are now on the CloudFormation portal ( https:\/\/console.aws.amazon.com\/cloudformation\/home....). i. In the Choose a template section, select Specify an Amazon S3 template URL Next You are now on the Specify Details j.On the Specify Details settings for this EC2 instance Next k. On the Options Next l. On the Review Create The deployed application together with its virtual resources is called a CloudFormation \"stack\" and is named UnravelEC2number If you don't see your new stack in the list, wait 30-60 seconds and refresh the page. When the status of the stack changes to CREATE_COMPLETE UnravelEC2number Outputs UnravelBackupS3 UnravelAutoScalingGroupId VPC SecurityGroup For example: Get the public IP address and private IP address of this EC2 instance. You can do this through aws CLI or through the EC2 console, ( https:\/\/console.aws.amazon.com\/ec2\/ To use aws CLI, run the ec2 describe-instances UnravelAutoScalingGroupId Outputs AUTOSCALING_GROUP=UnravelAutoScalingGroupId_value_from_Outputs_tab \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PublicIpAddress |head -1 \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PrivateIpAddress |head -1 To use the EC2 console, highlight this EC2 instance and look at its IP addresses in its Description For example: Log into Unravel UI Navigate to URL http:\/\/EC2_Public_IP:3000 : admin unraveldata. connect to your existing or new EMR clusters " }, 
{ "title" : "Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"url" : "unravel-4-5/install/install-emr/install-emr-part1/install-emr-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM)...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our Amazon Machine Image (AMI) Navigate to the EC2 console ( https:\/\/console.aws.amazon.com\/ec2\/ From the menu on the left, select IMAGES AMIs In search box pull-down menu, select Public images Search for ami-08d8b2a645bdc7482 Select this AMI and click Launch On the Choose an Instance Type R4.2xlarge Next: Configure Instance Details (Optional) Modify the configuration of the EC2 instance: On the Configure Instance Details only Network Subnet IAM role Click Next: Add Storage (Optional) Increase the storage capacity of the EC2 instance to a maximum of 500GiB, depending on the number of clusters, the number of jobs running on those clusters, and whether you plan to enable debug logging. Click Next: Add Tags (Optional) Add tags. Click Next: Configure Security Group On the Configure Security Group Rule Type Protocol Port(s) Source Inbound: SSH TCP 22 Your trusted CIDR for SSH access Inbound: Custom TCP TCP 3000 0.0.0.0\/0 or EMR security group ID or EMR subnet IP block Inbound: Custom TCP TCP 4043 EMR security group ID or EMR subnet IP block Outbound: All traffic All All 0.0.0.0\/0 Security policy required for all nodes in the EMR cluster (master, core, and task nodes) All All Unravel EC2 node security group ID or Unravel EC2 node private IP address Security reminder: Don't make Unravel UI accessible on the public Internet. Click Review and Launch Review your settings and click Launch Enter the name of your key pair file and click Launch Instances Verify that you see the following notice: Click the instance. Find the public IP address of the instance in its Description Log into Unravel UI Using the public IP address you found in the step above, open a web browser window and navigate to URL http:\/\/EC2_Public_IP:3000 admin unraveldata. Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR clusters " }, 
{ "title" : "Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"url" : "unravel-4-5/install/install-emr/install-emr-part1/install-emr-part1-option3.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"snippet" : "This topic explains how to deploy Unravel Server 4.5.X on an AWS EC2 instance used to monitor existing or newly created EMR clusters. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Ce...", 
"body" : "This topic explains how to deploy Unravel Server 4.5.X on an AWS EC2 instance used to monitor existing or newly created EMR clusters. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI. For example, ami-02e98f78 Instance type: Minimum: R4.2xlarge (60 GB Ram) Virtualization type: HVM Virtualization type: HVM Root device type: EBS Root disk space: Minimum: 100GB.     In a PoC or evaluation, the minimal root disk size 100GB should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend to set minimal root disk from 300 - 500GB \"Provisioned IOPS\" EBS volume with 3000 IOPS. For production unravel use case, 200GB root disk Provisioned IOPS EBS and RDS are recommended. Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Create an EC2 Instance Base OS: See the table above. EC2 instance's type and size: See the table above. EC2 instance's security group \/ IAM role: See the table above. For instance, create an IAM role that contains the policy that only reads Creating IAM Role Create Instance Profile CLI Base OS: See the table above. Ports: See the table above. Networking: The EC2 instance must Security reminder: Don't make Unravel UI accessible on the public Internet. Security group or policy required for the Unravel EC2 instance: Create an S3 ReadAccess only IAM role and assign it to Unravel EC2 node to read the archive logs on the S3 bucket configured for the EMR cluster. Unravel EC2 node works with multiple EMR clusters, including existing and newly created clusters. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node. You must Create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address Put the member of security group used on EMR cluster in this rule. A sample security group used for Unravel EC2 node. Inbound Rule Type Protocol Port Range Source All traffic All All SG ID of this group or Subnet IP block (e.g. 10.10.0.0\/16) SSH TCP 22 0.0.0.0\/0 or trusted public IP for ssh access Custom TCP Rule TCP 3000 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Custom TCP Rule TCP 4043 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Outbound Rule Type Protocol Port Range Source All traffic All All 0.0.0.0\/0 The Unravel EC2 node should have all TCP access to the EMR cluster (master or slave) nodes. You can grant access by inserting a security policy into both SG (security group) of EMR master and slave with (All TCP, All port range) and the source is the SG ID of the unravel VM. (see screen capture below) If it's not possible to Unravel EC2 access All traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Configure the EC2 Instance at First Login Disable selinux # sudo setenforce Permissive Edit \/etc\/selinux\/config SELINUX=permissive # vi \/etc\/selinux\/config Install libaio.x86_64 lzop.x86_64 # sudo yum install -y libaio.x86_64\n# sudo yum install -y lzop.x86_64 Start ntpd # sudo service ntpd start\n# sudo ntpq -p Create a new user named hadoop # sudo useradd hadoop Install the Unravel RPM on the EC2 Instance Get the Unravel Server RPM; see Download Unravel Software Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. # sudo rpm -U unravel-4.5.0.*-EMR-latest.rpm Run the await_fixups.sh In a routine upgrade, it is okay to start all Unravel daemons, but do not stop or restart them until the await_fixups.sh Done # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n# \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Append the following line to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem=false For monitoring EMR Spark service, add the following properties to unravel.properties com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates the following items:   User unravel Initial internal database and other durable states in \/srv\/unravel\/   scripts for controlling services, and \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh Log into Unravel UI Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Create an ssh # ssh -i ssh_key.pem centos@ UNRAVEL_HOST_IP Using a web browser, navigate to http:\/\/127.0.0.1:3000 Log into the Unravel UI with username admin unraveldata Congratulations! Unravel Server is up and running. Proceed to Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster " }, 
{ "title" : "Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"url" : "unravel-4-5/install/install-emr/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"snippet" : "Table of Contents Introduction Connect the Unravel EC2 Instance to a New EMR Cluster Connect the Unravel EC2 Instance to an Existing EMR Cluster Sanity Check Troubleshooting Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring...", 
"body" : "\n Table of Contents \n Introduction \n Connect the Unravel EC2 Instance to a New EMR Cluster \n Connect the Unravel EC2 Instance to an Existing EMR Cluster \n Sanity Check \n Troubleshooting Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes allow all traffic from the Unravel node . Both EMR cluster and Unravel node are created in same VPC, same subnet; and the security group allows all traffic from the same subnet. For existing EMR cluster connection located on a different VPC, you must configure VPC peering, route table creation, and security policy update. For details, see Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional) Network ACL on VPC allows all traffic. Connect the Unravel EC2 Instance to a New EMR Cluster To connect the Unravel EC2 instance to a new EMR cluster, follow the steps below to run the Unravel EMR bootstrap script on all nodes in the cluster. The bootstrap script does the following: On the master node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf Updates \/etc\/hadoop\/conf\/mapred-site.xml Updates \/etc\/hadoop\/conf\/yarn-site.xml If TEZ is installed, it updates \/etc\/tez\/conf\/tez-site.xml Installs and starts the unravel_es daemon \/usr\/local\/unravel_es Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs the Hive hook sensor in \/usr\/lib\/hive\/lib\/ On all other nodes: Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs Hive sensors in \/usr\/lib\/hive\/lib Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py # curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the Unravel EMR bootstrap script unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce # aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce In the AWS console, select the EMR service and click Create cluster In the Create Cluster - Quick Options Go to advanced options . Select Step 1: Software and Steps emr-5.14.0 Release Next. Select Step 2: Hardware Set Network EC2 Subnet If you created the Unravel EC2 node from our CloudFormation template, then a new VPC was generated, named Unravel_VPC. This VPC comes with one configured subnet, and by default has a CIDR \/ network address block of 10.10.0.0\/16 (but you might have changed this during stack creation). If you created the Unravel EC2 node from our Amazon Machine Image (AMI), you must create the EMR cluster on the same VPC and same subnet as the Unravel EC2 node. Modify the instance type and enter the desired instance count for core (slave) node(s). Click \n Next. Select Step 3: General Cluster Settings Cluster name S3 folder Add bootstrap action Custom action Configure and add For details on how to set up your EMR cluster, see https:\/\/docs.aws.amazon.com\/emr\/latest\/ManagementGuide\/emr-gs-launch-sample-cluster.html In the Add Bootstrap Action Script location step 2 Optional arguments Add \n \n \n Sample script location \n \n s3:\/\/aws-logs-account_number-region\/elasticmapreduce \n \n Optional arguments (mandatory here) \n \n --unravel-server UNRAVEL_EC2_IP --bootstrap In the Bootstrap Actions Nex Advanced Options Select Step 4: Security Choose the EC2 key pair Select the EC2 security groups In this example, the security group picked for both Maste Core & Task You must choose the security group that includes the Unravel EC2 instance, otherwise bootstrap will fail. Click Create cluster If everything was entered correctly, your new EMR cluster should finish the bootstrap process and be in the Waiting Once your new EMR cluster is up and running, you can run some jobs and log into the Unravel EC2 node's web UI to see the metrics collected by the Unravel node. Connect the Unravel EC2 Instance to an Existing EMR Cluster To connect the Unravel EC2 instance to an existing EMR cluster, follow the steps below to run the Unravel EMR Ansible playbook either or Substitute your local values for text in {red brackets} Whenever you upgrade Unravel Server, repeat the steps below to upgrade Unravel Sensors as well. Prerequisites Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 <key> Option 1: Run Our Ansible Playbook on the EMR Master Node Download unravel-emr-ansible.zip $ curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip --output unravel-emr-ansible.zip\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 11708 100 11708 0 0 66541 0 --:--:-- --:--:-- --:--:-- 66902 Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --ssh-key path \n --unravel-host hostname \n --cluster-name displayname $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible on the EMR master node: $ sudo pip install ansible (Optional) Determine what directory Ansible was installed in, and add that directory to the $PATH variable in ~\/.bashrc $ which ansible\n\/usr\/local\/bin\/ansible In ~\/.bashrc export PATH=\/usr\/local\/bin\/:$PATH Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Option 2: Run Our Ansible Playbook on Your Personal Workstation (Mac or Linux Only) Set up AWS CLI. For instructions, see https:\/\/aws.amazon.com\/cli\/ Make sure AWS CLI has permission to list EMR clusters: $ aws emr list-instances --cluster-id {cluster id} Download unravel-emr-ansible.zip $ wget https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --cluster-id string \n --region string \n --inventory path \n --ssh-key path \n --ssh-user string hadoop \n --unravel-host hostname \n --cluster-name displayname \n --use-public $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible: $ sudo pip install ansible Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Sanity Check After you connect the Unravel EC2 instance to your EMR cluster, run some jobs on the EMR cluster and monitor the information displayed in Unravel UI ( http:\/\/unravel_ec2_node_public_IP:3000) Troubleshooting Check Ansible playbook logs in \/tmp\/unravel\/unravel_sensor_ansible.log If the EMR cluster is created in a different VPC, see Step 4 for configuring VPC peering connection for an Unravel node. " }, 
{ "title" : "Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"url" : "unravel-4-5/install/install-emr/install-emr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"snippet" : "Table of Contents Introduction Configuration Requirements for RDS Set Up MySQL RDS in AWS Connecting Unravel Node to the Unravel RDS Instance Create Unravel db Schema in RDS Unravel Database Start Unravel Daemon Introduction Unravel's default installation uses a bundled database for part of it's sto...", 
"body" : "\n Table of Contents \n Introduction \n Configuration Requirements for RDS \n Set Up MySQL RDS in AWS \n Connecting Unravel Node to the Unravel RDS Instance \n Create Unravel db Schema in RDS Unravel Database \n Start Unravel Daemon Introduction Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group. Create new DB subnet group Create new DB parameter group Set Up MySQL RDS in AWS Create Unravel RDS instance In AWS portal → RDS, click Create database Choose MySQL Next Choose Production - MySQL Change the following properties, leave all others as the default. \n License model \n \n DB engine version \n \n DB instance class \n \n Multi-AZ deployment \n \n Storage type \n \n Allocated storage \n \n Provisioned IOPS Create a new DB instance and Master user and password. Click Next \n DB Instance identifier \n Master username \n Master password In the Advanced Settings . \n \n Network & Security Settings \n Virtual Private Cloud \n Subnet group \n Public accessibility \n \n \n Availability zone \n VPC security group Create a new DB subnet group in advance. It is required for Multi-AZ deployment. The VPC should at least contains two subnets in at least two Availability zones in a given region. For further information please check AWS documentation. Screenshot for DB subnet group. Create a new DB parameter group in advance, and this group is based on mysql 5.5. Alter the parameters base on the custom db parameters. \n \n \n key_buffer_size = 268435456 \n max_allowed_packet = 33554432 \n table_open_cache = 256 \n read_buffer_size = 262144 \n read_rnd_buffer_size = 4194304 \n max_connect_errors=2000000000 \n net-read-timeout = 300 \n net-write-timeout = 600 \n open_files_limit=9000 \n innodb_open_files=9000 \n character_set_server=utf8 \n collation_server = utf8_unicode_ci \n innodb_autoextend_increment=100 \n innodb_additional_mem_pool_size = 20971520 \n innodb_log_file_size = 134217728 \n innodb_log_buffer_size = 33554432 \n innodb_flush_log_at_trx_commit = 2 \n innodb_lock_wait_timeout = 50 \n \n \n Database Options Settings \n Database name \n Port \n DB parameter group For other RDS options such as Encryption Backup Monitoring Maintenance Create database Connecting Unravel Node to the Unravel RDS Instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. Locate the MySQL database endpoint in the RDS dashboard. Look for the security group used for unravel RDS instance from RDS dashboard. Edit the inbound rule of the security group. Add a new rule to allow connections from either Unravel node's Security Group Subnet IP block which unravel node located The SG or IP block works provided the RDS instance is located on the same region as the VPC. Verify the MySQL connection from the Unravel Node. # \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Click here to see a sample screenshot. Verify that the database unravel_mysql_prod # CREATE DATABASE IF NOT EXISTS unravel_mysql_prod; Create Unravel db Schema in RDS Unravel Database Stop Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop Configure the following properties in unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Locate and modify the properties below so that they reflect your particular values. If the property isn't found, add it. Use the actual values you set in the above steps, here here Encrypting Passwords in Unravel Properties and Settings unravel.jdbc.username=unravel\nunravel.jdbc.password={unraveldata}\nunravel.jdbc.url=jdbc:mysql:\/\/unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod\n Ensure the schema is up to date using the schema upgrade utility provided by Unravel server. The script step connects to the database and applies schema deltas in-order until the schema is up to date. The success or failure of the update is noted. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user admin # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Start Unravel Daemon Disable the bundled db on Unravel Server. Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command produces an error that can be ignored. # sudo chkconfig unravel_db off\n# sudo chkconfig unravel_pg off Start Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"url" : "unravel-4-5/install/install-emr/install-emr-part4.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"snippet" : "Table of Contents Introduction Create VPC Peering in VPC Dashboard Accept the VPC Peering Request Create Routes Between Peered VPC Update Security Groups Verify Connection Between Unravel EC2 Node and EMR Master Node Follow these steps only Introduction This topic explains how to resolve connectivit...", 
"body" : "\n Table of Contents \n Introduction \n Create VPC Peering in VPC Dashboard \n Accept the VPC Peering Request \n Create Routes Between Peered VPC \n Update Security Groups \n Verify Connection Between Unravel EC2 Node and EMR Master Node Follow these steps only Introduction This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. \n Assumptions The VPC where Unravel EC2 located is in the same region where the EMR cluster located (e.g., us-east-1). The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in EMR cluster. Network ACL on both VPC for Unravel EC2 and EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. In the following steps, we have both Unravel EC2 node and EMR cluster located in us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. \n \n \n Resources \n Internal IP Address \n Subnet ID \n Subnet IP Block \n VPC ID (Name) \n IP block in VPC \n Security Group ID (Name) \n \n Unravel EC2 node \n 10.10.0.7 \n subnet-03b82c56b2c26dbd1 \n 10.10.0.0\/24 \n vpc-0b0e17b01c4a3b54a (Unravel_VPC) \n 10.10.0.0\/16 \n sg-0e0a03084398287c9 (Unravel-EC2_SG) \n \n EMR Cluster Master node \n 10.11.0.53 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) \n \n EMR Cluster Core nodes \n 10.11.0.76 10.11.0.130 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) Create VPC Peering in VPC Dashboard From the AWS console → VPC services → Peering Connections Create Peering Connection VPC (Requester) VPC (Accepter) Create Peering Connection. A success message should appear in the screen. Click OK Accept the VPC Peering Request In the VPC Dashboard Pending Acceptance Action Accept Request Click Yes Accept Close Create Routes Between Peered VPC Go to VPC Dashboard → Route Tables Edit Add another route Find the Unravel_VPC route table. Enter the IP block of EMR VPC, e.g., 10.11.0.0\/16 In the Destination Target Save. Find the Test_EMR_VPC route table. Set the Destination Target Save. pcx-0a57a978ef9a525e2 Target Save Update Security Groups Go to VPC Dashboard → Security Group Add another rule Type ALL traffic Protocol ALL. Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block, e.g., 10.11.0.0\/16 in the Source Save. Locate the security group used on EMR cluster node. Enter the Unravel VPC IP block, e.g., 10.10.0.0\/16 in the Source Save Verify Connection Between Unravel EC2 Node and EMR Master Node \n ssh ALL Traffic Then on Unravel EC2 node, telnet On the EMR master node, telnet . If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network acl, security groups, and route tables used on both VPC. \n See unsupported VPC peering configurations on AWS documentation. " }, 
{ "title" : "Testing and Troubleshooting", 
"url" : "unravel-4-5/install/install-emr/install-emr-test-troubleshoot.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Testing and Troubleshooting", 
"snippet" : "This is an excerpt of the User Guide Testing the Deployment Troubleshooting Common Issues Sending Diagnostics to Unravel Support Reconnecting to your EMR Instance Deleting the Unravel EC2 Instance Diagnosing and Using Oozie with Unravel Not seeing Sufficient Historical Data in Unravel Testing the De...", 
"body" : " This is an excerpt of the User Guide \n Testing the Deployment \n Troubleshooting Common Issues \n Sending Diagnostics to Unravel Support \n Reconnecting to your EMR Instance \n Deleting the Unravel EC2 Instance \n Diagnosing and Using Oozie with Unravel \n Not seeing Sufficient Historical Data in Unravel Testing the Deployment Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 node. For example: \n \n \n \n ssh -i ssh_key.pem centos@ Unravel_node_public_IP -L 3000:127.0.0.1:3000 Start your browser from your workstation and navigate to http:\/\/127.0.0.1:3000 Log in with username admin unraveldata The OPERATIONS Trial versions include a message in the top menu bar about the trial license and the number of days remaining until the trial expires. Please contact us Run some jobs from the EMR master node. The EMR master node has some sample MapReduce and Spark jobs on it. Run these jobs to verify that the Unravel EC2 node is collecting data from the EMR cluster. Your usage may vary depending on what applications you installed on your cluster. \n Sample MapReduce (MR) Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this MapReduce (MR) \"Pi\" job: sudo -u hdfs hadoop jar \/usr\/lib\/hadoop-mapreduce\/hadoop-mapreduce-examples.jar pi 100 100 When the MR job finishes, check Unravel UI: You should see one successful application labeled MR To see details about the MR job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. \n Sample Spark Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this Spark \"Pi\" job: sudo -u hdfs spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/lib\/spark\/examples\/jars\/spark-examples.jar 1000 When the Spark job finishes, check Unravel UI: You should see one successful application labeled SPARK To see details about the Spark job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. \n Sample Tez Job a. Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez b. Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Troubleshooting Common Issues Sending Diagnostics to Unravel Support In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. 1. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh 2. Email the bundle to the Unravel support team Reconnecting to your EMR Instance If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of this instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup saved in the S3 bucket. In this case, your existing EMR clusters just need to be reconnected to the newly created Unravel EC2 node as described in Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Deleting the Unravel EC2 Instance If you're done with the Unravel EC2 instance, you can delete it as follows. From your EC2 console ( https:\/\/console.aws.amazon.com\/cloudformation\/ In the Actions Delete Stack Click Yes, Delete Monitor the Status Diagnosing and Using Oozie with Unravel You may see this common error: org.apache.oozie.action.ActionExecutorException: JA010: Property [fs.default.name] not allowed in action [job-xml] configuration The reason behind can be older version of configuration where some properties being deprecated. The straight-forward solution is to comment out the <job-xml> element in the workflow.xml file. Not seeing Sufficient Historical Data in Unravel Settings are needed to adjust the time horizon. In this example, it is set to 2 years with recent data showing the max amount minus 2 weeks: In Manage page, Core section, under Retention heading, under TIME SERIES RETENTION DAYS, adjust the number of days to retain data. This corresponds to com.unraveldata.retention.max.days property. In \/usr\/local\/unravel\/etc\/unravel.properties set com.unraveldata.history.maxSize.weeks=104 After changing the settings,restart the servers \n sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Operational Guidance", 
"url" : "unravel-4-5/install/install-emr/install-emr-ops-guidance.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Operational Guidance", 
"snippet" : "Health Check Backup and Recovery Routine Maintenance Emergency Maintenance Support Support Costs Health Check For monitoring the Unravel server, CloudWatch alerts can be set up to monitor the specific EC2 instance that has the Unravel Server deployed. Unravel also has a monitoring service - a lightw...", 
"body" : " \n Health Check \n Backup and Recovery \n Routine Maintenance \n Emergency Maintenance \n Support \n Support Costs Health Check For monitoring the Unravel server, CloudWatch alerts can be set up to monitor the specific EC2 instance that has the Unravel Server deployed. Unravel also has a monitoring service - a lightweight daemon which allows you to monitor various Unravel components. Details here For Storage Capacity monitoring for RDS (if that’s the user chosen configuration for the Unravel DB), here (It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region disruption does not apply for Unravel. ) Backup and Recovery The best course of action to take in situations like instance failures is to have followed the process of backing-up and disaster recovery as described here It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region recovery does not apply for Unravel. Unravel's current design priority is to maintain business continuity and does not support complete\/true High Availability. True HA design for Unravel is work in progress. Routine Maintenance There are multiple means by which Unravel announces and documents details of availability of new versions e.g.: On the Unravel Product documentation main page here Unravel Solution Engineers and Account Management Team members engage with customers directly to tell them about the availability of upgrades and patches. \n Blogs Communicated via the Unravel Newsletter (sign up on the Unravel website Emergency Maintenance In the event of fault conditions, such as a transient failure of an AWS Service (e.g. availability of EC2 in a particular AZ is degraded), or a more permanent failure of an AWS service (e.g. EC2 instance has faulted, EC2 Scheduled Maintenance Event received): The best course of action to take in such situations is to have followed the process of backing-up and disaster recovery as described here Support Go here Details for support tiers, SLA etc. are available here Support Costs Currently, there are no additional costs for obtaining Unravel Support. " }, 
{ "title" : "Amazon Athena", 
"url" : "unravel-4-5/install/install-athena.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena", 
"snippet" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/...", 
"body" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/athena\/pricing\/?nc=sn&loc=3 You send Unravel information about your Athena queries through an AWS Lambda function which monitors your AWS CloudTrail trail for Athena events. Use Cases Amazon Athena is well suited to structured data such as logs. " }, 
{ "title" : "Integrating Athena with Unravel", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-IntegratingAthenawithUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Integrating Athena with Unravel", 
"snippet" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS...", 
"body" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS role to allow AWS Lambda functions to call AWS services on your behalf. Create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. In Unravel UI, look at Apps Athena " }, 
{ "title" : "Create a Trail in AWS CloudTrail", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaTrailinAWSCloudTrail", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create a Trail in AWS CloudTrail", 
"snippet" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into you...", 
"body" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select CloudTrail On the CloudTrail Trails Create trail In the Trail name In the Apply trail to all regions Yes In the Management events section, next to Read\/Write events All In the Data events In the Storage location You can create a new S3 bucket or use an existing S3 bucket. If you create a new bucket: Set the S3 bucket name to unravel-cloudtrail Expand the Advanced Leave the Log file prefix For Encrypt log files with SSE-KMS No For Enable log file validation Yes For Send SNS notification for every log file delivery No Click Create Configure CloudWatch permissions on unravel-cloudtrail Click your newly created trail, unravel-cloudtrail CloudWatch Logs Click Configure In the New or existing log group CloudTrail\/UnravelLogGroup Click Continue On the next page, expand View Details IAM Role Create a new IAM Role Role Name unravel-cloudtrail-role Click Allow " }, 
{ "title" : "Create a Role for Unravel's AWS Lambda Function", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaRoleforUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create a Role for Unravel's AWS Lambda Function", 
"snippet" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS L...", 
"body" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS Lambda with AWS CloudTrail Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select IAM On the IAM Roles Click Create role In the Select type of trusted entity AWS service In the Choose the service that will use this role Lambda Click Next: Permissions On the Attach permissions policies AmazonS3ReadOnlyAccess AWSLambdaVPCAccessExecutionRole Click Next: Tags (Optional) If you want to add tags to this role, add them here. Click Next: Review On the Review page, set Role name unravel-athena-lambda-role Click Create role The AWS console displays a message indicating that it created the role. Select the role in the list of roles. On the role summary page, select the Trust relationships " }, 
{ "title" : "Create Unravel's AWS Lambda Function", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create Unravel's AWS Lambda Function", 
"snippet" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/conso...", 
"body" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select Lambda On the Lambda Create function On the Create function Function name UnravelAthenaLambda Runtime Python 3.7 Execution role Use an existing role Existing role unravel-athena-lambda-role Click Create function AWS displays a banner indicating success, and displays your new Lambda function’s page. Add a Trigger to the Lambda Function On your new Lambda function’s page, select Amazon S3 From the list of triggers on the right, select S3 In the Configure triggers Bucket unravel-cloudtrail Event type All object create events Select the Enable trigger At the top of the page, click Test Click Add AWS shows the new S3 trigger at the bottom of the page. At the top of the page, click Save Add Code to Unravel’s AWS Lambda Function Select the new Lambda function: AWS displays configurable settings for this function. In the Function code Code entry type Upload a file Amazon S3 Amazon S3 link URL: s3:\/\/unraveldatarepo\/share\/lambda\/UnravelAthenaLambda.zip Runtime Python 3.7 In the Environment variables Key: unravel_lr_url Value: <Private IP of Unravel Node>:<Port>\/logs\/athena\/j-default\/athena\/athena <Private IP of Unravel Node> <Port> In the Execution role Select Use an existing role Existing role: unravel-athena-lambda-role In the Network Don’t select No VPC Select your VPC. Select at least two subnets from the pull-down list (hold CTRL to select multiple subnets). Select your private security group (SG). Review the inbound and outbound rules. At the top of the page, click Test At the top of the page, click Save AWS displays a banner indicating success. " }, 
{ "title" : "View Athena Queries in Unravel UI", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-ViewAthenaQueriesinUnravelUI", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ View Athena Queries in Unravel UI", 
"snippet" : "In Unravel UI, look at Apps Athena User Guide...", 
"body" : "In Unravel UI, look at Apps Athena User Guide " }, 
{ "title" : "Resources", 
"url" : "unravel-4-5/install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-Resources", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Resources", 
"snippet" : "Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs...", 
"body" : " Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-5/install/install-athena/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "MapR", 
"url" : "unravel-4-5/install/install-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MapR", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on MapR....", 
"body" : "This section explains how to deploy Unravel Server and Sensors on MapR. " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-5/install/install-mapr.html#UUID-d3b1deed-c8a0-bacc-6fd3-d9236d30ab81_id_MapR-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MapR \/ Ordered Steps", 
"snippet" : "MapR Pre-Installation Check Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, s...", 
"body" : " MapR Pre-Installation Check Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "MapR Pre-Installation Check", 
"url" : "unravel-4-5/install/install-mapr/install-mapr-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MapR \/ MapR Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network The following ports must be open on Unravel Server's host machine(s): Port(s) Direction Description 3000 or 4020 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on MapR", 
"url" : "unravel-4-5/install/install-mapr/install-mapr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR", 
"snippet" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you do not already have a gateway\/edge\/client host pr...", 
"body" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you do not already have a gateway\/edge\/client host provisioned for Unravel server, follow these steps which are needed to enable the hadoop fs For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands on Unravel Server as root NAME CLDB_LIST HISTORY_SERVER # sudo yum install mapr-client.x86_64\n# sudo \/opt\/mapr\/server\/configure.sh -N NAME CLDB_LIST HISTORY_SERVER Configure the host before installing the RPM: Run the following commands on Unravel Server as root # sudo useradd -g mapr unravel\n# hadoop fs -mkdir \/user\/unravel\n# hadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr MAPR_TICKETFILE_LOCATION \/srv\/unravel\/unravel_ctl Check available RAM to ensure availability: # free -g Adjust RAM if needed. Only change this setting on the Unravel gateway\/client machine. For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 For example, edit \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 Restart MapR-FS (mfs). Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. See Download Unravel Software Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv and symlink after installing). Do it before the first install if there is insufficient space on the target paths \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM. # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fizup.sh await_fizup.sh DONE The installation creates the following items: which contains the executables, scripts, the master configuration file ( \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ are scripts for controlling services. You can use \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh User unravel Initial internal database and other durable states in \/srv\/unravel\/ The initial installation includes a bundled database; you can switch to AWS RDS for production. For details, see externally managed MySQL During initial install, a bundled database is used. This can be switched to use an for production. Do host-specific post-installation actions. Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable additional daemons at this point, see Creating Multiple Workers for High Volume Data Modify unravel.properties Edit \/usr\/local\/unravel\/etc\/unravel.properties # Property Description Example Values com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/ LAN_DNS com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.tmpdir Optional. Location where Unravel's temp file will reside \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.hive.hook.topic.num.threads Optional. Defines the number of threads. Default is 1. Depending on job volume, increase this property to N N ThousandJobsPerDay 1 com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process \/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.spark.eventlog.location Where to find Spark event logs maprfs:\/\/\/apps\/spark yarn.resourcemanager.webapp.address Resource Manager web app address http:\/\/example.localdomain:8088 yarn.resourcemanager.webapp.username Resource Manager username to login yarn.resourcemanager.webapp.password Resource Manager password to login https.protocols Enable https access to Resource Manager TLSv1.2 javax.jdo.option.ConnectionURL A JDBC connection URL jdbc: mysql:\/\/example.localdomain:3306\/hive postgresql:\/\/example.localdomain:7432\/hive_zzzzzz javax.jdo.option.ConnectionDriverName JDBC driver or com.mysql.jdbc.Driver org.postgresql.Driver javax.jdo.option.ConnectionUserName Hive metastore user name hiveuser javax.jdo.option.ConnectionPassword Hive metastore password com.unraveldata.metastore.databasePattern Optional s*|t*|d* oozie.server.url Oozie URL http:\/\/example.localdomain:11000\/oozie If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties Substitute the correct filename and principal: com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab Find and verify the principal the keytab by running this command: # klist -kt KEYTAB_FILE Set the Linux file permissions of the keytab file to 500 ( chmod 500 unravel Run Unravel Daemons with Custom User If Sentry is Enabled, Add These Permissions: Define your own alt principal with narrow privileges. The alt principal can be admin unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory mapr or alt read Spark event log hdfs:\/\/usr\/history\/done mapr or alt read MapReduce logs hdfs:\/\/tmp\/logs mapr or alt read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse mapr or alt read Obtain table partition sizes Hive Metastore access hive read Hive table information Switch User Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where x y switch_to_user Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60 Run the echo # echo \"http:\/\/( UNRAVEL_HOST_IP This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP :3000\/ admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. Check Unravel Web UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel Web UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-5/install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Table of Contents Introduction 1. Enable Additional Instrumentation on Unravel Server's Host 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path 3. Confirm that Unravel Web UI Shows Additional Data 4. Confirm and Adjust the Settings in yarn-site.xml 5. Enable Additio...", 
"body" : "\n Table of Contents \n Introduction \n 1. Enable Additional Instrumentation on Unravel Server's Host \n 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path \n 3. Confirm that Unravel Web UI Shows Additional Data \n 4. Confirm and Adjust the Settings in yarn-site.xml \n 5. Enable Additional Instrumentation on Other Hosts in the Cluster \n 6. Enable Instrumentation Manually Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. \n HIGHLIGHTED UNRAVEL_HOST_IP \n SPARK_VERSION_X.Y.Z HIVE_VERSION 1. Enable Additional Instrumentation on Unravel Server's Host # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only \n For Sensor Upgrade only # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only \n Dry-Run (test\/check instrumentation, this does not change any configuration file): # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --dry-run Hive hook jar is installed under: \n \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \n \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2\/MapR 6.0) are made to the following files, (< SPARK_VERSION X.Y.Z> \n \/opt\/mapr\/spark\/spark-<SPARK VERSION X.Y.Z>\/conf\/spark-defaults.conf \n \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-site.xml \n \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-env.sh \n \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/yarn-site.xml \n \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/mapred-site.xml \n \/usr\/local\/unravel\/etc\/unravel.properties Copy of original configuration is saved in same directory named *.preunravel \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml.preunravel Once the files are present on edge host where Unravel rpm is installed, you can tar tar 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR in \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-agent\/ oozie.libpath 3. Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server. This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. \n someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 4. Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml \n yarn.resourcemanager.webapp.address \n \n \n <property> <name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value> <source>yarn-site.xml<\/source> <\/property> \n yarn.log-aggregation-enable \n \n \n <property> <name>yarn.log-aggregation-enable<\/name> <value>true<\/value> <description>For log aggregations<\/description> <\/property>> 5. Enable Additional Instrumentation on Other Hosts in the Cluster Run the shell script unravel_mapr_setup.sh Copy the newly edited yarn-site.xml Do a rolling-restart of HiveServer2 To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml 6. Enable Instrumentation Manually Enable instrumentation manually by updating the following files: \n hive-site.xml \n hive-env.sh \n spark-defaults.conf \n hadoop-env.sh \n mapred-site.xml Once the files are updated on edge host where Unravel rpm is installed, you can scp Update hive-site.xml Copy the content in \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-site.xml \n \n \n <property> <name>com.unraveldata.host<\/name> <value>{UNRAVEL_HOST_IP}<\/value> <description>Unravel hive-hook processing host<\/description> <\/property> <property> <name>com.unraveldata.hive.hook.tcp<\/name> <value>true<\/value> <\/property> <property> <name>com.unraveldata.hive.hdfs.dir<\/name> <value>\/user\/unravel\/HOOK_RESULT_DIR<\/value> <description>destination for hive-hook, Unravel log processing<\/description> <\/property> <property> <name>hive.exec.driver.run.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.pre.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.post.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.failure.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <\/configuration> Update hive-env.sh In \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-env.sh \n \n \n export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION _X.Y.Z} Update spark-defaults.conf In \/opt\/mapr\/spark\/spark-SPARK_VERSION_X.Y.Z\/conf\/spark-defaults.conf \n \n \n \n spark.unravel.server.hostport UNRAVEL_HOST_IP:4043 spark.eventLog.dir maprfs:\/\/\/apps\/spark \/\/ the following is one line spark.history.fs.logDirectory maprfs:\/\/\/apps\/spark -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=driver \/\/ the following is one line spark.executor.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-SPARK_VERSION_X.Y.Z,config=executort Update hadoop-env.sh In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh \n \n \n export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION_X.Y.Z.0-hook.jar Update mapred-site.xml In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/mapred-site.xml \n \n \n <property> <name>mapreduce.task.profile<\/name> <value>true<\/value> <\/property> <property> <name>mapreduce.task.profile.maps<\/name> <value>0-5<\/value> <\/property> <property> <name>mapreduce.task.profile.reduces<\/name> <value>0-5<\/value> <\/property> <property> <name>mapreduce.task.profile.params<\/name> <value> -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr UNRAVEL_HOST_IP <\/property> <property> <name> yarn.app.mapreduce.am <value> -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr <\/property> Make sure the original value of \n yarn.app.mapreduce.am " }, 
{ "title" : "Azure HDInsight", 
"url" : "unravel-4-5/install/install-hdi.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight", 
"snippet" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters....", 
"body" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters. " }, 
{ "title" : "Unravel VM", 
"url" : "unravel-4-5/install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelVM", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel VM", 
"snippet" : "Installing Unravel as a separate Azure VM...", 
"body" : " Installing Unravel as a separate Azure VM " }, 
{ "title" : "Unravel App", 
"url" : "unravel-4-5/install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelApp", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel App", 
"snippet" : "Installing Unravel as an Azure Marketplace app...", 
"body" : " Installing Unravel as an Azure Marketplace app " }, 
{ "title" : "Option 1: Install Unravel on a Separate Azure VM", 
"url" : "unravel-4-5/install/install-hdi/install-hdi-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM", 
"snippet" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel...", 
"body" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Create Azure Storage", 
"url" : "unravel-4-5/install/install-hdi/install-hdi-part1-option1/create-azure-storage.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Create Azure Storage", 
"snippet" : "Introduction This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: Windows Azure Storage Blob By def...", 
"body" : " Introduction This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: Windows Azure Storage Blob By default HDInsight 3.6 uses Blob storage, which is a general-purpose storage type for Big Data. Blob storage is a key-value store with a flat namespace. It has full support for: Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets Low-cost, tiered storage High availability\/disaster recovery Unravel doesn't support encryption (SSL) with Blob storage (WASB). Azure Data Lake Storage generation 1 The other major option for Hadoop clusters is ADLS v1. ADLS is a hierarchical file system. It has full support for Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets File system semantics File-level security Scalability Azure Data Lake Storage generation 2 Preview mode ADLS generation 2 combines the features of Blob storage andADLS generation 1. Unravel has not been tested with ADLS v2 since it is still in preview mode. For an in-depth comparison, see https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-comparison-with-blob-storage The rest of this document refers to these storage types as Blob ADLS Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network for your resource group. This virtual network will be shared by your Hadoop cluster and the Unravel VM. Steps Log into https:\/\/portal.azure.com Click Storage accounts + Add On the Basics Subscription Resource Group Storage Account Name Location Performance Standard Premium Standard Premium Account kind Blob Storage OR StorageV2 StorageV2 (ADLS v2) is still in preview mode and is not currently supported by Unravel. Replication Locally redundant storage (LRS) Zone-redundant storage (ZRS) Geo-redundant storage (GRS) Read-access geo-redundant storage (RA-GRS) FIXLINK: Handles failures in the data-center, zone, region, and allows read-access in another region. Durability guarantee is 16 9's. Access Tier Blob ADLS v2 hot Click the Advanced Set Secure transfer required Disabled Enabled Unravel doesn't support encryption (SSL) with Blob storage (WASB). For Virtual Networks, Click Review + create If your settings are correct, click Create Previous Related Resources Finding Unravel Properties' Values in Microsoft Azure Azure - creating a storage account Difference between Replication types " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"url" : "unravel-4-5/install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"snippet" : "Table of Contents Introduction Prerequisites Provision an Azure VM for Unravel Server Configure the VM at First Login Install the Unravel Server RPM on the VM Modify Properties and Start Unravel Daemons Log into Unravel UI Congratulations! Introduction This topic explains how to create a separate Az...", 
"body" : "\n Table of Contents \n Introduction \n Prerequisites \n Provision an Azure VM for Unravel Server \n Configure the VM at First Login \n Install the Unravel Server RPM on the VM \n Modify Properties and Start Unravel Daemons \n Log into Unravel UI \n Congratulations! Introduction This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network and network security group set up for your resource group. Your virtual network and subnet(s) must be big enough to be shared by the Unravel VM and the target HDInsight cluster(s). You must have root privilege You must already have created a storage system. For instructions, see Create Azure Storage You must have an SSH key pair. Your VM host must meet the requirements below. Support Chart and VM Requirements: \n \n \n Azure HDI cluster compatibility \n HDInsight 3.6 Storage type: Blob (WASB) or ADLS v1 Limitations Unravel currently only works with Blob (WASB) or ADLS v1. It does not support multiple Azure Data Lake Storage accounts or ADLS v2 (preview). HDP 2.6.5 Spark 1.6.3, 2.1.0, 2.2.0, 2.3.0 Limitations Spark relies on the yarn-site configuration property yarn.log-aggregation.file-formats <property>\n <name>yarn.log-aggregation.file-formats<\/name>\n <value>TFile<\/value>\n<\/property> Hive 2.1 Kafka 0.10.0, Kafka 1.0, Kafka 1.1 (preview) \n \n Image (underlying operating system for the VM) \n RHEL 7 or CentOS 7.2 - 7.6 Note that the actual HDInsight Kafka\/Spark cluster can run another OS. \n \n CPU and RAM minimum requirements \n Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 Cores: 8 min RAM: 64 GB min \n \n Disk requirement \n min 100GB for \/srv \/srv \n \n Network requirement \n Unravel VM should be located in the same VNET and VSNET as the HDInsight cluster Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server HDFS ports open from Hadoop cluster to Unravel Server Hive MetaStore DB port open to Unravel Server for partition reporting For Oozie, port 11000 open to Unravel Server \n \n Security requirement \n Allows inbound ssh to the unravel VM Allows outbound Internet access and all traffic within the subnet (VSNET). Allows TCP port 3000 and 4043 to Unravel VM from HDInsight cluster \n HIGHLIGHTED \n UNRAVEL_HOST_IP Provision an Azure VM for Unravel Server Log into https:\/\/portal.azure.com Select Virtual machines + Add On the Basics \n Subscription \n Resource Group \n Virtual machine name \n Region \n Availability options \n Image \n Size standard, memory optimized E8s_v3 Select your VM's Authentication type Best practice is to authenticate using an SSH public key, which you can generate using ssh-keygen Set Inbound Port Rules: If you plan on allowing external access to Unravel UI, then select Allow selected ports HTTPS SSH Click Next: Disks On the Disks \n OS disk type Premium SSD Standard SSD \n Advanced managed disks \n Data disks If you don't have a disk ready, click Create and attach new disk Otherwise, click Attach an existing disk Click Next: Networking On the Networking It is imperative that the VM, the Azure storage, and the cluster(s) you plan to monitor are all on the same virtual network and subnet(s). \n Virtual network \n Subnet https:\/\/www.aelius.com\/njh\/subnet_sheet.html \n NIC network security group: Basic Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the \" head node Add an inbound security policy to allow SSH access and 443 access to the Unravel node. The default security policy should allow all access within the VNET. Default rules start with a priority of 65000. Click Review + create Click Create It takes about 2 minutes to create your VM. When Azure completes the creation of your VM,click Go to resource Copy the VM's public IP address. Open an SSH session to your VM's public IP address. # ssh -i {ssh_key} {user}@{IP}\nVerify your IP as expected, example:\n Verify that eth0 on the new VM is bound to the private IP address shown in the Azure portal. # ifconfig\neth0 Link encap:Ethernet HWaddr 00:0d:3a:1b:c2:48\n inet addr:10.10.1.96 Configure the VM at First Login Install ntpd ntpd https:\/\/wiki.archlinux.org\/index.php\/Network_Time_Protocol_daemon # sudo su -\n# yum install ntp\n# ntpd -u ntp:ntp Disable Security Enhanced Linux (SELinux) permanently. This is important because HDFS maintains replication in different nodes\/racks, so setting firewall rules in SELinux will lead to performance degradation. # sudo setenforce Permissive Edit the file to make sure the setting persists after reboot, be sure to set, SELINUX=permissive # vi \/etc\/selinux\/config\nSELINUX=permissive\n\n:wq Install libaio.x86_64. # sudo yum -y install libaio.x86_64 Install lzop.x86_64. \n # sudo yum install lzop.x86_64 Disable the firewall and check your iptable rules. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L Prepare the second disk (for example, \/dev\/sdc fdisk -l # sudo su -\n\n\nList all disks and partitions\nYou should see one called \"sdc\" if you attached a 500-1000 GB disk.\n# fdisk -l\n# fdisk \/dev\/sdc\np (list current partitions)\nn (new partition)\np (primary)\nKeep accepting rest of default configs.\nw (save)\n\nFormat the disk\n# \/usr\/sbin\/mkfs -t ext4 \/dev\/sdc\n\n\n# mkdir -p \/srv\n\n# DISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\n# echo $DISKUUID\n\nMount the disk on \/srv\n# echo \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n# mount \/dev\/sdc1 \/srv\n\nVerify the disk space\n# df -hT \/srv\n\nFilesystem Type Size Used Avail Use% Mounted on\n\/dev\/sdc1 ext4 197G 61M 187G 1% \/srv\n\n\nSet permissions for Unravel and symlink Unravel's directories to the \/srv mount\n# mkdir -p \/srv\/local\/unravel\n# chmod -R 755 \/srv\/local\n# ln -s \/srv\/local\/unravel \/usr\/local\/unravel\n# chmod 755 \/usr\/local\/unravel Create the hdfs hadoop # sudo useradd hdfs\n# sudo groupadd hadoop\n# sudo usermod -a -G hadoop hdfs Install the Unravel Server RPM on the VM Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM. For instructions, see Download Unravel Software # cd \/tmp\n# Note that the same RPM is used for both EMR and HDInsight.\n# curl -u {USERNAME}:{PASSWORD} -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.3\/unravel-4.4.3.0-EMR-latest.rpm -o unravel-4.4.3.0-EMR-latest.rpm Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. The rpm rpm U # sudo rpm -U unravel-4.4.3.0-EMR-latest.rpm Run the specified await_fixups.sh If you're doing a routine upgrade, you can start all Unravel daemons, but don't stop or restart them until await_fixups.sh DONE # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\nDONE Useful Information \n Dirs \/usr\/local\/unravel\/ \n unravel.properties \n \/etc\/init.d\/unravel_* \n \/etc\/init.d\/unravel_all.sh \n Subsequent RPM upgrades don't change unravel.properties \n User: unravel \n DB \/srv\/unravel\/ \n Config \/usr\/local\/unravel\/etc\/unravel.properties \n Logs \/usr\/local\/unravel\/logs\/ Grant access to Unravel Server Security Reminder Do not make Unravel Server UI TCP port 3000 accessible on the public Internet because doing so would violate your licensing terms. By default, a Public IP should be assigned to the Unravel VM . Create a security policy that allows ssh It is recommended that you use an SSH key to access the Unravel node. Modify Properties and Start Unravel Daemons Open an SSH session to the Unravel VM. # ssh -i {ssh_private_key} {ssh_user}@{UNRAVEL_HOST_IP} Set correct permissions on the Unravel configuration directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh Check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions # Find the version of HDP that is installed by checking the HDP symlink. Take the first 2 digits, such as 2.6\n# You can also check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions \n# hdp-select status | grep hadoop\nhadoop-client - 2.6.5.3005-27\n\n# Append this classpath based on the version you found\n# echo \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Run the \"switch user\" script. # \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop In \/usr\/local\/unravel\/etc\/unravel.properties \n com.unraveldata.onprem This is optional at this time but is required later. echo \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties Modify other values in unravel.properties \n \n \n Property \n Description \n Required \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n Yes \n \n http:\/\/{LAN_DNS}:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n Yes \n \n Company_and_org \n \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n Yes; this is set by installation \n \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n optional \n \n 26 \n \n \n com.unraveldata.login.admins \n Unravel UI admin \n Yes; this is set by installation \n \n admin \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster \n Yes for Blob storage \n \n fs.azure.account.key.{STORAGE_NAME} \n \n \n com.unraveldat \n Primary storage account key \n Yes for Blob storage \n \n ABCDABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD \n \n \n com.unraveldat a.hdinsight \n Optional for Spark when HDInsight using blob storage Storage account name for the HDInsight cluster (same as account-name-1 \n Yes for Blob storage \n \n fs.azure.account.key. \n \n \n com.unraveldat a.hdinsight \n Secondary storage account key \n Yes for Blob storage \n \n ABCDEF\/ ABCDEFGHIJKLM \/ABCD ABCD ABCD ABCD ABCD ABCD AB \n \n \n com.unraveldat a.adl.accountFQDN \n The data lake fully qualified domain name. \n Yes for ADLS \n \n datalake0001.azuredatalakestore.net \n \n \n com.unraveldat a.adl.clientld \n An application ID. An application registration has to be created in the Azure Active Directory \n Yes for ADLS \n \n 12345678-1234-1234-1234-123456789ABC \n \n \n com.unraveldat a.adl.clientKey \n An application access key which can be created after registering an application \n Yes for ADLS \n \n ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABC= \n \n \n com.unraveldat a.adl.accessTokenEndpoint \n The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal \n Yes for ADLS \n https:\/\/login.microsoftonline.com\/ABCDABCD-ABCD-ABCD-ABCD-ABCDABCDABCD\/oauth2\/token \n \n \n com.unraveldat a.adl.clientRootPath \n It is the path in the Data lake store where the target cluster has been given access. \n Yes for ADLS \n \n \/clusters\/{CLUSTERNAME} \n \n \n com.unraveldat a.ext.kafka.clusters \n Name of Kafka cluster. The display name show on the Unravel UI to define Kafka cluster. Other Unravel Kafka properties depends on this name {CLUSTERNAME} \n mandatory for HDI Kafka \n \n udkafka \n \n \n com.unraveldat a.ext.kafka. \n Kafka cluster bootstrap server and port (usually are two worker nodes) \n mandatory for HDI Kafka \n \n wn0-UDKAFK:9092,wn1-UDKAFK:9092 \n \n \n com.unraveldat a.ext.kafka. \n Define Kafka cluster broker servers names \n mandatory for HDI Kafka \n \n broker-1,broker-2,broker-3 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 host \n mandatory for HDI Kafka \n \n wn0-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 port \n mandatory for HDI Kafka \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 host \n mandatory for HDI Kafka \n \n wn1-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 port \n mandatory for HDI Kafka \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 host \n mandatory for HDI Kafka \n \n wn2-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 port \n mandatory for HDI Kafka \n \n 9999 Update the following properties for an HDInsight cluster, depending on whether you're using Blob storage or ADLS Set these properties with values you obtain from Azure. For help in locating the right values, see Finding Unravel Properties' Values in Microsoft Azure For Blob storage, update: com.unraveldata.hdinsight.storage-account-name-1\ncom.unraveldata.hdinsight.primary-access-key\ncom.unraveldata.hdinsight.storage-account-name-2\ncom.unraveldata.hdinsight.secondary-access-key For ADLS, update: com.unraveldata.adl.accountFQDN\ncom.unraveldata.adl.clientld\ncom.unraveldata.adl.clientKey\ncom.unraveldata.adl.accessTokenEndpoint\ncom.unraveldata.adl.clientRootPath Restart Unravel Server Whenever making edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties The echo If you are using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: # sudo \/etc\/init.d\/unravel_all.sh restart\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" Log into Unravel UI Create an SSH # ssh -i {ssh_private_key} {ssh_user}@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"url" : "unravel-4-5/install/install-hdi/install-hdi-part1-option1/install-hdi-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"snippet" : "Table of Contents Introduction Prerequisites Option A: Deploy a New Cluster Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Option C: Configure an Existing Kafka Cluster Troubleshooting Tips Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and co...", 
"body" : "\n Table of Contents \n Introduction \n Prerequisites \n Option A: Deploy a New Cluster \n Option B: Configure an Existing Hadoop, Hive, or Spark Cluster \n Option C: Configure an Existing Kafka Cluster \n Troubleshooting Tips Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure \" Script action There are two kinds of Unravel \" script actions Note: For HDInsight cluster without Internet access, you can download these scripts and store them in your Azure blob storage and use the blob storage URI on the script action's \" Bash script URI \n \n \n Cluster Type \n Download path \n Supported HDI cluster(s) \n Apply to cluster node type(s) \n \n Hadoop, Hive, or Spark \n \n unravel_hdi_spark_bootstrap_3.0.sh \n Hadoop 2.7.3 Hive 2.1 Spark 2.0, 2.1, 2.3 \n Head Node, Worker Node, Edge node \n \n Kafka \n \n unravel_hdi_kafka_bootstrap.sh \n Kafka 0.10.0, Kafka 1.0.0, Kafka 1.1.0 \n Head Node Checks before running script action Read the latest documentation on the ports required by HDInsight: https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-hadoop-port-settings-for-services Ensure Unravel service is running on Unravel VM and ports 3000 and 4043 are reachable from the Azure HDInsight cluster master node before running the the Unravel \"script action\" script. E.g., # ssh -i {ssh_key} {ssh_user}@{UNRAVEL_HOST_IP}\n# sudo su -\n# netstat -anp | grep 3000\ntcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN 65072\/node\n# hostname\n\nOn one of the cluster's head nodes.\n# ping {UNRAVEL_HOST_IP} Depending on the type of cluster you are deploying, follow one of these options: \n Option A: Deploy a new cluster \n Option B: Configure an existing Hadoop, Hive, or Spark cluster \n Option C: Configure an existing Kafka cluster Prerequisites You must already have an Unravel VM on Azure HDInsight running and the Unravel UI available on port 3000. For instructions, see Step 1: Install Unravel Server for Azure HDInsight Cluster If you plan to create a cluster, you must have the following information ready: Virtual Network and subnet of the Unravel VM Your Azure Storage details. For storage setup, see Create Azure Storage Option A: Deploy a New Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight clusters Add In the Security + networking In the Storage In the \" Cluster size \n Optional Script action In the Summary - Confirm configurations Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, Click on Script actions Submit new Create \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh \n \n Node type(s) \n Head, Worker, Edge (only if you have deployed edge node) \n \n Parameters \n \n --unravel-server unravel_server_private_ip spark_version e.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0\n\nTo UNDO the changes use --uninstall parameter\ne.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 --uninstall \n \n Persist this script action \n Checked. Note that persistence only applies on new Head and Worker nodes Option C: Configure an Existing Kafka Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, If the Kafka cluster has no Internet access; then download the HDInsightUtilities-v01.sh script and scp\/download it to the Kafka \"head node\" \/tmp folder . wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click Script actions Submit new \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh \n \n Node type(s) \n Head \n \n Parameters \n \n --unravel-server unravel_server_private_ip e.g. --unravel-server 10.10.1.10:3000 \n \n Persist this script action \n Checked. Note that persistence only applies on new Head nodes Click Create After the Kafka script action script completed successfully, ssh to the Kafka cluster's \"head node\" and append the content of \/tmp\/unravel\/unravel.ext.properties nravel.ext.properties # Adding Kafka properties\ncom.unraveldata.ext.kafka.clusters=<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.bootstrap_servers=wn0-<cluster_name>:9092,wn1-<cluster_name>:9092\ncom.unraveldata.ext.kafka.<cluster_name>.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.host=wn0-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.host=wn1-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.port=9999 Unravel VM must have access to the Kafka worker nodes' broker port 9092 and Kafka JMX port 9999 After updating the Kafka properties, you will need to restart the Unravel server. sudo \/etc\/init.d\/unravel_all.sh restart Troubleshooting Tips From the Azure portal, you can check if a script action finished successfully by checking the SCRIPT ACTION HISTORY . If script action process fails, you can check the error messages from the HDInsight cluster's Ambari dashboard, which has a balloon next to the cluster name on the top menu bar with the recent operations. Click on the \"ops\" button and search for the most recent \"run_customscriptaction\" command and inspect the log messages. You may see multiple entries of \"run_customscriptaction\" which were created by previous runs. The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new \"script action\" script with a different name. " }, 
{ "title" : "Option 2: Install Unravel's Azure Marketplace App", 
"url" : "unravel-4-5/install/install-hdi/install-hdi-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 2: Install Unravel's Azure Marketplace App", 
"snippet" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure por...", 
"body" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure portal. Select the HDInsight service. Create a new cluster: For cluster type, select Spark For version 2.1 Enter the access credentials for the Spark cluster. Click Next. Set Up a Storage Account for the Cluster Create a new storage account or use existing one. Fill in the storage account information for the Spark cluster. Find the Unravel App on Azure Marketplace Enter UNRAVEL Available applications Click OK Click Create Accept the terms of use and privacy policy. Click Next Launch the Cluster and Unravel App Review the summary. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. Click Create Find the URL of the Unravel App After the Unravel app and the Spark2 cluster have launched successfully, go to the Azure portal and select the HDInsight service. Select the Spark2 cluster. Click Applications Click unravel-edgenode The Unravel HDInsight app's webpage URL is displayed. In most cases, the URL looks like https:\/\/clusterName-unr.apps.azurehdinsight.net\/ Log into Unravel UI In your web browser, navigate to the Unravel app's URL, https:\/\/clusterName-apps.azurehdinsight.net. The Unravel login screen appears. Log in with username admin The dashboard appears. (Optional) Start\/Stop Unravel Daemons Use ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Enter Your License By default Unravel app doesn't contains any license keys, and runs without any issue during the initial 30 days trial period. To continue using Unravel app and technical support, contact our sales team. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Get Started with Unravel UI See the Unravel User Guide Get Started with Unravel API Unravel provides REST API for some operations. To try the API, click the API An API page with available command options are displayed and explained. You can try the API by clicking \"Try it out\" ? Execute buttons as shown below. From the Unravel user interface, trying out the API results in an error \"TypeError: Failed to fetch\" because the generated curl Copy the generated curl Modify it to include default user credentials. For example: ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n Re-send it using the HTTPS protocol. The response body is in JSON format. The date field is in epoch time. {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "INTERNAL: Update the Azure Resource Manager if Needed", 
"url" : "unravel-4-5/install/install-hdi/internal--update-the-azure-resource-manager-if-needed.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed", 
"snippet" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters: INTERNAL: Step 3: Create Unravel VM Using ARM template INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions INTERNAL: Step 5: ARM Template for Kafka Clus...", 
"body" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters: INTERNAL: Step 3: Create Unravel VM Using ARM template INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions " }, 
{ "title" : "INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"url" : "unravel-4-5/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-3--create-unravel-vm-using-arm-template.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"snippet" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or cr...", 
"body" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or create your own template files. This example template creates an Azure \"standard E8s V3\" VM in the existing VNET and subnet, and it adds a data disk on the VM for \"\/ srv\" You will need to update VNET, subnetRef, vmName, adminPassword before running the ARM template to create VM Optionally change the size of data disk; it is currently set to 500G. If you change the disk size, update unravel-setup.sh For Centos 7.3 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.parameters.json For Redhat 7.4 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.parameters.json Both ARM template and parameter files have to be modified to fit your Azure environment This ARM template embedded with Azure Extension script to download and install Unravel RPM. The Azure Extension script for Unravel RPM installation Extension Script for CentOS 7.3 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/unravel-setup.sh Extension Script for CentOS 7.4 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/unravel-setup.sh The custom extension script will fix most of the basic unravel configuration; however you will have to manually edit \/usr\/local\/unravel\/etc\/unravel.properties file for blob storage account or data lake store access information. Please see Step 1 Below is the content of this extension script # Download unravel rpm\n\/usr\/bin\/wget http:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.2.7-Azure-latest.rpm\n\n\/usr\/bin\/sleep 5\n\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=1p6ed4s492012j5rb242rq3x3w702z1l455g501z2z4o2o4lo675555u3h\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start 1. Download the ARM template and parameter JSON files onto your configured Azure CLI workstation. 2. Use the Azure CLI to deploy Unravel VM using this template and parameters JSON file. To Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 2. Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually start unravel daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"url" : "unravel-4-5/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-4--arm-template-for-spark2-hdinsight-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"snippet" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. T...", 
"body" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. This ARM template uses the existing VNET, Subnet, and Storage Account on the same resource group. You need to update those values in the parameter variables to reflect your Azure environment. A Spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use Spark edge node for accessing the cluster and testing\/hosting your client applications. Substitute your local values for text in red, i.e., UNRAVEL_IP. You will need to deploy unravel VM and update the script action parameters --unravel-server UNRAVEL_IP Optionally you can change the VM size of header, worker and edge nodes; and currently they are all using VM size of \"Standard_D3_v2\" \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh After modifying this template please validate it before applying. HDInsight cluster creation takes about 15 - 25 minutes Install using CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 3. Use the Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Optionally, Install manually on an existing Spark2 cluster. 1. From Azure portal, click the resource of the target Spark2 cluster under your resource group and click \"Script actions\". In Script Actions dialog box: Click Submit new Select script type \"- Custom\" Enter a Name for this script, e.g., \" unravel-spark-setup\" Enter the script path from above e.g., https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Use input parameters: --unravel-server UNRAVEL_VM_IP_Address:30000 --spark-version 2.1.0 Check the box Persist this script action to rerun when .. Click Create The checkbox for Persist this script action to rerun when ... does not Script Action You can upload the unravel_hdi_bootstrap.sh 2. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed. 3. After completion, login to Ambari and check the Ambari task status. Install additional edge node on existing HDinsight cluster. The ARM template for install edge node with Unravel Script Action only \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json Use the ARM template to install the edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example). In this example, an edge node will be created first. Next, it runs emptynode-setup.sh unravel_hdi_bootstrap.sh \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires change in scriptActionUri path and application name in variables and also parameters for cluster name. Adjust the ARM templates for your setup and validate it before using. " }, 
{ "title" : "INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"url" : "unravel-4-5/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-5--arm-template-for-kafka-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"snippet" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. Th...", 
"body" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. The worker nodes in this Kafka cluster use two data disks per node. You need to deploy unravel VM and update the Script Actions UNRAVEL__IP \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Validate the modified ARM template before applying. The HDInsight cluster creation takes about 15 - 25 minutes Apply Script Actions via CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Modify parameter file. Change the parameter values to reflect your Azure environment, i.e., VNET, Subnet, StorageAccount, Cluster name, etc. You can change the VM size of header, worker nodes. By default they use \"Standard_D3_v2\" as the VM size. 3. Validate the template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 4. Use Azure CLI to deploy the Kafka cluster using the template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 5. After the Kafka cluster is successfully created, the unravel Script Actions \/usr\/local\/unravel\/etc\/unravel.properties The following is the example of the lines appended to unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 6. Once unravel.properties unravel_km # \/etc\/init.d\/unravel_km restart Alternatively, apply Unravel Kafka Script Actions scripts manually on an existing Kafka cluster. If you already have an existing Kafka cluster, you can apply Unravel's Kafka script via Azure portal. 1. From Azure portal, click the resource of the target Kafka cluster under your resource group and click Script Actions 2. In the Script Actions dialog box: Click Submit New Select script type, e.g., \"- Custom\" Enter a Name for this script, e.g. \" unravel-kafka-setup\" Enter the script path from above, e.g. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Input parameters: UNRAVEL_VM_IP_Address Check the box \" Persist this script action to rerun You can upload the unravel_hdi_kafka_bootstrop.sh 3. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed 4. After completion, login to Ambari of the cluster and check the Ambari task status. 4. Login to unravel VM and restart the unravel Kafka monitor daemon, unravel_km. # \/etc\/init.d\/unravel_km restart " }, 
{ "title" : "Upgrading the Unravel VM or App", 
"url" : "unravel-4-5/install/install-hdi/upgrading-the-unravel-vm-or-app.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Upgrading the Unravel VM or App", 
"snippet" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll a...", 
"body" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this requires you to re-submit the Unravel action scripts to head, worker, and edge nodes. " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-5/install/install-hdi/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Using Azure HDInsight APIs", 
"url" : "unravel-4-5/install/install-hdi/using-azure-hdinsight-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Using Azure HDInsight APIs", 
"snippet" : "Submit a Script Action: Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell $ az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run the script action. First, ref...", 
"body" : "Submit a Script Action: \n Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell $ az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run the script action. First, refer to the script you want to run and ensure you have its proper parameters. You may need to remove any \"--\" from the parameters. $ azure hdinsight script-action create $CLUSTER -g $RESOURCEGROUP -n $SCRIPTNAME -u $SHELLSCRIPT -p 'unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH' -t \"headnode;workernode;edgenode\" -g = Resource Group name -n = Name of this script action task -u = script path -p = paramaters -t = node types \nE.g., $ azure hdinsight script-action create DEVCLUSTER -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Create an Edge Node: An Edge Node 1. Determine which ARM template and parameter file to download to the workstation that contains Azure CLI. A. Edge node that also runs the Unravel script ( recommended \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json OR B. Edge node that only simple a emptynode-setup.sh script \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json 2. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. curl <file> -o <name.json> 3. Modify the VM type, parameters, Kafka\/Spark version, etc. For example, In the ARM Template file, edit these fields as appropriate. \"vmSize\": \"Standard_D3_v2\" \"parameters\": \"unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH\" \"applicationName1\": \"$NEW_EDGE_NODE_HOSTNAME\" In the Parameter file, modify the cluster name. E.g., \"clusterName\": {\n \"value\": \"$MY_CLUSTER_NAME\"\n} 4. Validate template before deployment. \n \n \n \n $ az group deployment validate --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json { \"error\": null, ... \"provisioningState\": \"Succeeded\", ... } 5. Create the edge node. This should take 10-15 minutes to run since it has to provision a VM and install the Hadoop binaries. \n \n \n \n $ az group deployment create --name deploymentname --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json 6. Verify it was added to Ambari. Auto-Scaling: HDInsight allows you to resize your cluster up\/down to meet your current demands. 1. From the Azure portal, navigate to HDInsight Clusters <Your Cluster> Cluster Size 2. Enter your desired number of workers and validate that you have enough resources for your resource group and region (based on any quotas). Click the Save button, and HDInsight will take the appropriate action. Down-size: Will run the \" Decommission Up-size: Will provision new VM, install the Hadoop bits, and add the worker components (DataNode, NodeManager, and potentially HBase RegionServer). Notice that if the Unravel script action was also \"persisted\" to run on \"worker nodes\", then new VMs will automatically run a custom command for the Unravel bootstrap script. " }, 
{ "title" : "MySQL", 
"url" : "unravel-4-5/install/install-mysql.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MySQL", 
"snippet" : "Install and Configure MySQL for Unravel Move MySQL to a Custom Location MySql Partitioning and Data Migration...", 
"body" : " Install and Configure MySQL for Unravel Move MySQL to a Custom Location MySql Partitioning and Data Migration " }, 
{ "title" : "Install and Configure MySQL for Unravel", 
"url" : "unravel-4-5/install/install-mysql/install-mysql-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MySQL \/ Install and Configure MySQL for Unravel", 
"snippet" : "Pre-install Steps Install MySQL Server 5.7 Configure and Start MySQL Server Install MySQL JDBC Driver Post-Install Steps Copy MySQL JDBC JAR Configure Unravel to Connect MySQL Server Pre-install Steps Do the following steps, before installing Unravel RPM. Install MySQL Server 5.7 Install MySQL datab...", 
"body" : " Pre-install Steps Install MySQL Server 5.7 Configure and Start MySQL Server Install MySQL JDBC Driver Post-Install Steps Copy MySQL JDBC JAR Configure Unravel to Connect MySQL Server Pre-install Steps Do the following steps, before installing Unravel RPM. Install MySQL Server 5.7 Install MySQL database. CentOS 6 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm\n# sudo yum install yum-utils\n# sudo rpm -ivh mysql80-community-release-el6-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n# sudo yum install mysql-community-server CentOS 7 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el7-1.noarch.rpm\n# sudo rpm -ivh mysql80-community-release-el7-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n# sudo yum install mysql-community-server If you are installing MySQL on a Unravel SELinux host and are not using the default datadir here Configure and Start MySQL Server Stop MySQL server if it is running. # sudo service mysqld stop Backup old InnoDB log files to a directory of your choosing, { Backup_Path \/var\/lib\/mysql\/ib_logfile # mv \/var\/lib\/mysql\/ib_logfile* {Backup_Path}\n\n\/\/ OR\n\n# rm -rf \/var\/lib\/mysql\/ib_logfile* Append the following properties at the end of [mysqld] \/etc\/my.cnf datadir \/srv\/unravel\/db_data key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M max_connections = 500 max_connect_errors = 2000000000 open_files_limit = 10000 port-open-timeout = 121 expire-logs-days = 1 character_set_server = utf8 collation_server = utf8_unicode_ci innodb_open_files = 2000 innodb_file_per_table = 1 innodb_data_file_path = ibdata1:100M:autoextend # The innodb_buffer_pool_size depends on load and cluster size. # On a dedicated machine, it can be 50% of the RAM size. # Using 1G is the absolute minimum. For a large cluster, we use 48G. innodb_buffer_pool_size = 4G innodb_flush_method = O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency = 20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format = mixed # if SSD disk is used uncomment the line below #innodb_io_capacity = 4000 Ensure MySQL server starts at boot. CentOS 6 # sudo chkconfig mysqld on CentOS 7 # sudo systemctl enable mysqld Start MySQL server. CentOS 6 # sudo service mysqld start CentOS 7 # sudo systemctl start mysqld Check disk space used by MySQL's datadir from the MySql configuration file (eg., ( \/etc\/my.cnf # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs du -sh Check available file system disk space for MySQL's datadir from the MySql configuration file (eg., \/etc\/my.cnf) # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs df -h Install MySQL JDBC Driver Download MySQL JDBC driver to \/tmp # wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz cd \/tmp # cd \/tmp\n# tar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz Post-Install Steps After installing the Unravel RPM, complete the following steps. Copy MySQL JDBC JAR Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ # mkdir -p \/usr\/local\/unravel\/share\/java\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Configure Unravel to Connect MySQL Server Run mysql and create a Database and user for Unravel. # mysql\nmysql> CREATE DATABASE unravel_mysql_prod;\nmysql> CREATE USER 'unravel'@'localhost' IDENTIFIED BY '<password>';\nmysql> GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'localhost'; Update the following properties in \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.username=unravel unravel.jdbc.password={password} # If MySQL JDBC driver is installed replace jdbc:mariadb with jdbc:mysql unravel.jdbc.url=jdbc: mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod Create schema for Unravel tables. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create default admin for Unravel UI. # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Once you have successfully configured Unravel to use the MySQL database, ensure that the unravel_pg service unravel_pg stop; chkconfig unravel_pg off " }, 
{ "title" : "Move MySQL to a Custom Location", 
"url" : "unravel-4-5/install/install-mysql/install-mysql-move.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MySQL \/ Move MySQL to a Custom Location", 
"snippet" : "Depending on your deployment, follow the steps in one of these sections below If you're using Unravel bundled MySQL see Move a Bundled MySQL If you're using an external MySQL see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another hos...", 
"body" : "Depending on your deployment, follow the steps in one of these sections below If you're using Unravel bundled MySQL see Move a Bundled MySQL If you're using an external MySQL see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another host please see here Move a Bundled MySQL Daemon user must Perform a slow shutdown ( Get DB root password from \/root\/unravel.install.include \/root\/unravel.install.include.prev The example below uses *.include # grep 'DB_ROOT_PASSWORD' { \/root\/unravel.install.include | \/root\/unravel.install.include.prev } Run the following commands to set MySQL clean shutdown. # \/usr\/local\/unravel\/mysql\/bin\/mysql -uroot --port=3316 --host=127.0.0.1 -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop unravel_db # \/etc\/init.d\/unravel_db stop Back up MySQL database folder \/srv\/unravel\/db_data # cd \/srv\/unravel # tar cvf unravel_db_data.tar db_data\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION # cp unravel_db_data.tar NEW_DB_LOCATION NEW_DB_LOCATION Update unravel.install.include .cnf In \/root\/unravel.install.include NEW_DB_LOCATION In \/usr\/local\/unravel\/mysql\/unravel_mysql.cnf innodb_data_home_dir innodb_log_group_home_dir NEW_DB_LOCATION Move an External MySQL MySQL user must Perform a slow shutdown ( Run the following commands to set MySQL clean shutdown. # mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon # service mysqld stop Backup MySQL database folder \/var\/lib\/mysql # cd \/var\/lib # tar cvf unravel_db_data.tar mysql\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION # cp unravel_db_data.tar NEW_DB_LOCATION Update MySQL cnf file In \/etc\/my.cnf NEW_DB_LOCATION " }, 
{ "title" : "MySql Partitioning and Data Migration", 
"url" : "unravel-4-5/install/install-mysql/install-mysql-partition.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MySQL \/ MySql Partitioning and Data Migration", 
"snippet" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upg...", 
"body" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upgrade Unravel Server. Stop all daemons. # \/etc\/init.d\/unravel_all.sh stop Make sure MySQL is running. Perform the data migration. For external MySQL servers you might need to change the chunk size to 100 or 1000. # \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh migration:migration_partitioning During migration process the status of the tables is written to stdout. The output is continually updated until the migration for the table is complete. Start migration: migration_partitioning\nStart table rows calculation...\nTable blackboards_old has 0 rows total\nTable impala_queries_old has 0 rows total\nTable jobs_old has 205437 rows total\nTable event_instances_old has 0 rows total\nTable oozie_workflow_jobs_old has 0 rows total\nTable hive_queries_old has 0 rows total\nTable rows calculation finished.\nMigrating records for table: blackboards, chunk size: 10000, chunk count: 10...\nMigrated records for table: blackboards, rows total: 0, chunk size: 10000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: impala_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: impala_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 5000, chunk size: 500, chunk count: 10, finished: false, time: 32 seconds, progress 2,43 %\nMigrating records for table: event_instances, chunk size: 50000, chunk count: 10...\nMigrated records for table: event_instances, rows total: 0, chunk size: 50000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: oozie_workflow_jobs, chunk size: 1500, chunk count: 10...\nMigrated records for table: oozie_workflow_jobs, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: hive_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: hive_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 10000, chunk size: 500, chunk count: 10, finished: false, time: 33 seconds, progress 4,87 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10... When all the tables have been successfully migrated you see: Migration: migration_partitioning is finished During the process a temporary migration_partitioning keeps track the tables' status. You can view the table using The chunks field increases until the migration for the table is complete. Upon completion the table's finished column is set to 1. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql> select * from migration_partitioning;\n+---------------------+--------------------+--------+----------+\n| migr_table_name | lowest_id_migrated | chunks | finished |\n+---------------------+--------------------+--------+----------+\n| blackboards | 11018258 | 119 | 0 |\n| event_instances | 0 | 1 | 1 |\n| hive_queries | 0 | 1 | 1 |\n| impala_queries | 0 | 1 | 1 |\n| jobs | 240884 | 100 | 0 |\n| oozie_workflow_jobs | 0 | 1 | 1 |\n+---------------------+--------------------+--------+----------+\n\n To see the oldest migrated records per each table you can use the following MySql query. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql>\nselect \"blackboards\" as \"table_name\", min(created_at) as \"oldest_migrated_record\" from blackboards\nunion select \"event_instances\", min(created_at) from event_instances\nunion select \"hive_queries\", min(created_at) from hive_queries\nunion select \"impala_queries\", min(created_at) from impala_queries\nunion select \"jobs\", min(created_at) from jobs\nunion select \"oozie_workflow_jobs\", min(created_at) from oozie_workflow_jobs;\n+---------------------+------------------------+\n| table_name | oldest_migrated_record |\n+---------------------+------------------------+\n| blackboards | 2018-08-30 00:57:45 |\n| event_instances | 2018-08-30 00:57:55 |\n| hive_queries | 2018-08-30 00:59:10 |\n| impala_queries | NULL |\n| jobs | 2018-08-30 00:57:50 |\n| oozie_workflow_jobs | 2018-09-07 10:25:22 |\n+---------------------+------------------------+ If the migration process is interrupted or killed, you can run shell script again. However, if the process has failed you must truncate and reset ID's During migration the original files were renamed to _old TableName migration_partitioning # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nDROP TABLE blackboards_old;\nDROP TABLE event_instances_old;\nDROP TABLE hive_queries_old;\nDROP TABLE impala_queries_old;\nDROP TABLE jobs_old;\nDROP TABLE oozie_workflow_jobs_old;\nDROP TABLE migration_partitioning;\nquit Restart daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Restarting a Failed Migration", 
"url" : "unravel-4-5/install/install-mysql/install-mysql-partition.html#UUID-aa105656-ef76-f548-7147-bcde7f4abe0e_RestartHowtorestartafailedmigration", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ MySQL \/ MySql Partitioning and Data Migration \/ Restarting a Failed Migration", 
"snippet" : "Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning # \/etc\/init.d\/unravel_all.sh stop # \/usr\/local\/unravel\/install_bin\/db_access.sh truncate blackboards; truncate event_instances; truncate hive_queries; truncate impala_queries; truncate jobs; truncate oozie...", 
"body" : " Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning # \/etc\/init.d\/unravel_all.sh stop\n\n# \/usr\/local\/unravel\/install_bin\/db_access.sh\n\ntruncate blackboards;\ntruncate event_instances;\ntruncate hive_queries;\ntruncate impala_queries;\ntruncate jobs;\ntruncate oozie_workflow_jobs;\n\ntruncate migration_partitioning;\n\ncall resetAutoIncrementId('blackboards_old', 'blackboards');\ncall resetAutoIncrementId('event_instances_old', 'event_instances');\ncall resetAutoIncrementId('hive_queries_old', 'hive_queries');\ncall resetAutoIncrementId('impala_queries_old', 'impala_queries');\ncall resetAutoIncrementId('jobs_old', 'jobs');\ncall resetAutoIncrementId('oozie_workflow_jobs_old', 'oozie_workflow_jobs'); Return to Step 3 and complete the remaining steps. " }, 
{ "title" : "OnDemand", 
"url" : "unravel-4-5/install/install-ondemand.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ OnDemand", 
"snippet" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on the Unravel Server host machine: Operational Insights | Queue Analysis Operational Insights | Cluster Optimization Cloud Reports Data Insights | Forecasting Data Insights | Small Files Data Insights | File...", 
"body" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on the Unravel Server host machine: Operational Insights | Queue Analysis Operational Insights | Cluster Optimization Cloud Reports Data Insights | Forecasting Data Insights | Small Files Data Insights | File Reports Data Insights | Top X Installation or Upgrade of OnDemand Library Versions and Licensing for OnDemand " }, 
{ "title" : "Installation or Upgrade of OnDemand", 
"url" : "unravel-4-5/install/install-ondemand/install-ondemand-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ OnDemand \/ Installation or Upgrade of OnDemand", 
"snippet" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before insta...", 
"body" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before installing OnDemand. (Optional) 1. Install External MySQL on the Unravel Host You can skip these steps if you plan to use Postgres with OnDemand. Install MySQL. See MySQL Verify MySql is installed correctly by running netstats # netstat -tunlp | grep :3306\ntcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld For fresh installations report_instances # cat \/usr\/local\/unravel\/sql\/mysql\/20180517031500.sql | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh Run db_access.sh ondemand_tasks ondemand_sessions report_instances # sudo \/usr\/local\/unravel\/install_bin\/db_access.sh\nmysql> show tables; Install the OnDemand Service on the Unravel Host Download the OnDemand package from https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/ \/tmp Contact Unravel Support for {USERNAME} {PASSWORD} For example, if your host operating system is Red Hat Enterprise Linux 7 (RHEL 7): # cd \/tmp\n# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel7.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel7.tar.gz -u {USERNAME}:{PASSWORD} For example, if your host operating system is Red Hat Enterprise Linux 6 (RHEL 6): # cd \/tmp\n# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel6.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel6.tar.gz -u {USERNAME}:{PASSWORD} Navigate to the the \/tmp ondemand # cd \/tmp\n# sudo rm -rf \/usr\/local\/unravel\/ondemand (Optional) If you want Unravel to generate Small Files reports and you've customized hive_properties.hive hive_properties.hive ondemand # cp \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive \\\nondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive Extract the contents of the tarball. # tar xvf Ondemand-4.5.0.0-GA-rhel<6|7>.tar.gz Run the installation script. # sudo mv ondemand\/ \/usr\/local\/unravel\/\n# cd \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\n# chmod +rwx install\/ondemand_quick_install.sh\n# sudo .\/install\/ondemand_quick_install.sh\n# sudo \/etc\/init.d\/unravel_all.sh restart In unravel.properties For details, see the General OnDemand Configurations for Reports Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart If your host operating system is SELinux, you might get alerts like these after restarting Unravel Server, depending on your environment. You can ignore them: # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Howver, if you encounter problems, contact Unravel Support. Start the OnDemand Daemon Execute the following four commands in the order shown, replacing { run_as_user run_as_group switch_to_user For RHEL 7.x: # sudo systemctl stop unravel_ondemand.service\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo systemctl start unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand stop\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo service unravel_ondemand start Confirm that the OnDemand service is running: You should see output similar to the example below. Process IDs will vary dynamically based on the number of processors in Unravel Server, number of current tasks, and so on. For RHEL 7.x: # sudo systemctl status unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand status You can also use ps # ps -ef | grep ondemand | grep -v grep root 11159 1 0 Sep21 ? 00:00:00 su - -c bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11163 11159 0 Sep21 ? 00:00:00 -bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11450 11176 0 Sep21 ? 00:00:42 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/flask run hdfs 11452 11176 0 Sep21 ? 00:27:19 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11670 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11671 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11672 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 Enable Various OnDemand-based Reports or Features Cluster Optimization Reports - See Enabling or Disabling Cluster Optimization Reports. Cloud Reports Enabling or Disabling Cloud Reports and Forecasting Reports Queue Analysis - is enabled by default. See Enabling or Disabling Queue Analysis Reports Small Files Reports and File Reports - are enabled by default. See Enabling or Disabling Small Files Reports and Files Reports Top X Report- is always enabled. There are no Top X specific properties. Sessions Enabling or Disabling Sessions " }, 
{ "title" : "Library Versions and Licensing for OnDemand", 
"url" : "unravel-4-5/install/install-ondemand/install-ondemand-libraries-licensing.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Installation Guides \/ OnDemand \/ Library Versions and Licensing for OnDemand", 
"snippet" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency ...", 
"body" : " Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency of FBprophet) fbprophet>=0.1.1 BSD scipy>=0.19.1 BSD seasonal>=0.3.1 MIT statsmodels>=0.8.0 BSD gatspy>=0.3 BSD 3-Clause numpy>=1.13.1 BSD PyAstronomy>=0.12.0 MIT python_dateutil>=2.6.1 Simplified BSD fastdtw>=0.3.2 MIT requests==2.20.1 Apache 2.0 seaborn>=0.8.1 BSD 3-Clause sqlalchemy==1.2.7 MIT License pymysql>=0.8.0 MIT psycopg2==2.7.6.1 LGPL, Version 3.0 cython 0.27.3 Apache License, Version 2.0 kombu 4.1.0 BSD 3-Clause \"New\" Thrift-sasl 0.3.0 Apache License, Version 2.0 " }, 
{ "title" : "Post Installation Steps", 
"url" : "unravel-4-5/post-installation-steps.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Post Installation Steps", 
"snippet" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication ldap properties If mode=saml see enable SAML authentication saml properties Configure the email properties Set com.unraveldata.customer.organiz...", 
"body" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication ldap properties If mode=saml see enable SAML authentication saml properties Configure the email properties Set com.unraveldata.customer.organization Configure Hive Metastore Permissions Set up HBASE Configuration Configure Kafka Monitoring Configure Oozie Configure the following Tez yarn.ats.webapp.username yarn.ats.webapp.password yarn.timeline-service.webapp.address yarn.timeline-service.port " }, 
{ "title" : "Other Configuration Options", 
"url" : "unravel-4-5/post-installation-steps.html#UUID-6d82f0d9-86d3-5fa2-de6c-9064960c0b6d_id_PostInstallationSteps-OtherConfigurationOptions", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Post Installation Steps \/ Other Configuration Options", 
"snippet" : "Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data...", 
"body" : " Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data " }, 
{ "title" : "Further Configuration Options", 
"url" : "unravel-4-5/post-installation-steps.html#UUID-6d82f0d9-86d3-5fa2-de6c-9064960c0b6d_id_PostInstallationSteps-FurtherConfigurationOptions", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Post Installation Steps \/ Further Configuration Options", 
"snippet" : "Custom Configurations Security Configurations...", 
"body" : " Custom Configurations Security Configurations " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-5/uguide.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page The Application Managers Events & Insights Auto Actions Use Cases Detecting Resource Contention in the C...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page The Application Managers Events & Insights Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-5/uguide/uguide-getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-5/uguide/uguide-getting-started.html#UUID-b3fb06e1-2e21-f098-7dd2-071d84f1cae6_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "The Use Case videos below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Pe...", 
"body" : " The Use Case videos below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Common UI Features", 
"url" : "unravel-4-5/uguide/ug-common-ui-features.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Common UI Features", 
"snippet" : "Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are end-user res...", 
"body" : " Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are end-user restricted by Role Based Access Control Applications About Logout If you are unrestricted end-user or an admin, you have all the pages available with possible read\/write restrictions. The pull-down menu has Manage About Logout If your admin has disabled Support If you can configure the date range time period cluster(s) When there are multiple tabs, click on the tab to display its contents. When detailed or further information is available open section ( To expand section to the width of the entire tile click on the double arrows displayed ( Clicking on the application name\/id\/workflow usually bring ups information on the appplication, fragment etc, i.e., the Spark Application Manager, table information, etc. Hovering over an auto action alert ( Lists\/Tables Can be sorted by a column, i.e., start time, in ascending or descending order. The sort column highlights the arrow indicating the sort order ( Clicking on a column being used reverses the sort order. If you can chose which columns to display a plus ( When applicable, the application status is color coded: Clicking on the app name\/id\/workflow usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. When applicable, the Notifications When relevant there is an Auto Actions\/Events column ( When an application has a parent a link to it will appear in the GoTo A block glyph ( Graphs (see Operations | Usage Details | Infrastructure Hovering over a line in a graph causes the information to be displayed in a text box ( When \" Applications running at mm\/dd\/yy hh:mm:ss\" Clicking Show More ( If graph can be displayed based upon Group By Tags Metric Click If you can zoom in\/out of a diagram\/execution graph the magnifiers ( " }, 
{ "title" : "The Operations Page", 
"url" : "unravel-4-5/uguide/ug-the-operations-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Operations Page", 
"snippet" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: and Dashboard Usage Detail Operations Dashboard Note Click here...", 
"body" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: and Dashboard Usage Detail Operations Dashboard Note Click here " }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-5/uguide/ug-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-Dashboard", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Operations Page \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applicat...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applications Tile The line graphs display the successful, failed, and killed jobs for the time period time incremen cluster Clicking on Open Section Applications Applications Finding Applications Running YARN Application Tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Clicking on Open Section Operations Usage Details Jobs here " }, 
{ "title" : "Resources Tile", 
"url" : "unravel-4-5/uguide/ug-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ResourcesTile", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Operations Page \/ Resources Tile", 
"snippet" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications Applicat...", 
"body" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications Applications Finding Applications Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. A separate entry appears for each time a particular Auto Action was triggered. In the image below, the same auto action triggered at 23:56 and 2:58. Clicking an event\/alert brings up a Cluster Resource view ( >Operations Usage Detail Infrastructure Add a New Auto Action Clear " }, 
{ "title" : "Usage Details", 
"url" : "unravel-4-5/uguide/ug-the-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ChartsUsageDetails", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Operations Page \/ Usage Details", 
"snippet" : "Usage Detail has four tabs: The Operations Page#Infrastructure The Operations Page#Jobs The Operations Page#Nodes Impala Usage Kafka The Operations Page#HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applicatio...", 
"body" : " Usage Detail has four tabs: The Operations Page#Infrastructure The Operations Page#Jobs The Operations Page#Nodes Impala Usage Kafka The Operations Page#HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details For example, Unravel can pinpoint the applications causing a sudden a spike in the total VCores or memory MB usage. This allows you to easily you drill down into these applications to understand their behavior. Whenever possible, Unravel provides recommendations and insights All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, e.g., change the date range, click on a point within in a graph. When disabled a Refresh Refresh By default the Usage Details tab opens showing the Infrastructure tab. For all charts, click on the menu bars ( Show more Reset Graph Infrastructure Infrastructure This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, i.e., Application Type User Queue Clicking within a chart (1) displays the applications running for that point in time. You can chose how to display the bottom two graphs by clicking on the View By Showing View Showing x Infrastructure Application Type show more To View by use the Business Tags Showing Jobs Graphs the running and accepted jobs as applicable. You can Group by Nodes This chart graphs the Total = Total Active Unhealthy Where: currently running and healthy nodes, and Active: currently running and unhealthy nodes. Unhealthy: You can toggle the display of an item by clicking on its name. Impala Usage Graphs memory MB consumption and Query Number. The # Queries Tags Group By Kafka Lists all the configured Kafka clusters. See Kafka Application Manager for more information. See Kafka Use Case for information on drilling down into a Cluster to locate lagging and stalled Topics\/Partitions. Clicking the cluster name brings detailed information about the Kafka Cluster HBase Please see HBase Configuration Clusters View Clusters page lists all the available HBase clusters Click on a cluster name to bring up the cluster's information the HBase Cluster view. Cluster View This view is divided into four (4) sections. When an component's health is noted, hovering over it's health glyph brings up details, Cluster Information A bar at shows what cluster you are displaying with a pull-down which allows you to switch between clusters. Listed immediately below are the cluster metrics. You can choose to tab between clusters by choosing all cluster Region Servers Lists the the cluster regional services, with their KPI's and health. You can search on the region server by name. Click on the server's name to bring up its details. Region Servers KPI Graphs the regional server metrics, the graphs are linked with the table list below them. Click within a graph to see up the tables associated servers that point in time. Hover over a point to bring up a popup displaying the information for that point in time. Click Show More Tables List all the tables associated with the cluster, their KPIs and the table's health. Click on the table name to bring up its information. You can search for a table by name; any table with a name matching or containing the string is displayed. Region Server View Server, Operational, and OS Metrics are displayed. Hover over the metric for its description. For more information on the metrics see here Table View Table has two tabs, Table Region Table Table Regions Lists all the regions with their KPIs and health. " }, 
{ "title" : "The Applications Page", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page", 
"snippet" : "Table of Contents Applications Tab Kill\/Move Applications Workflow Tab Sessions Tab Creating a Session Session The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, M...", 
"body" : " Table of Contents Applications Tab Kill\/Move Applications Workflow Tab Sessions Tab Creating a Session Session The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron Unravel currently supports the the following application frameworks: Cascading\/Pig Hive (on Map-Reduce) Hive (on Tez) Impala Kafka Map-Reduce Tez Spark Native Spark Streaming SparkSQL Athena (preview) Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, etc. It takes significant expertise and effort to get to the root cause(s) of an application's problems. Unravel's Intelligence Engine provides insights into your application's run to help resolve it's problems\/inefficiencies. These insights are called events Events & Insights The Applications Page has three tabs: Applications, Workflows, and Sessions Note Click here " }, 
{ "title" : "Applications Tab", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-ApplicationsTab", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page \/ Applications Tab", 
"snippet" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application...", 
"body" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application(s) in a variety of ways: The left sidebar allows you to filter you App Name App type Status Tags Queue User Cluster Duration Number of Events. By time period, If the job is part of a Hive query, Pig script, or a Workflow, a link to it is noted in the job's Go To Athena jobs are serverless and retrieved upon completion. Therefore, its Write Cluster ID Queue Go To " }, 
{ "title" : "Workflow Tab", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-WorkflowTab", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page \/ Workflow Tab", 
"snippet" : "The layout of this window mirrors the Applications...", 
"body" : "The layout of this window mirrors the Applications " }, 
{ "title" : "Sessions Tab", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-SessionsTab", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page \/ Sessions Tab", 
"snippet" : "This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliab...", 
"body" : " This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliability: in attempting to reduce resources Unravel prioritizes memory allocation to ensure the application doesn't fail due to \"out of memory\" exceptions. Why use sessions when Unravel already offers insights and recommendations on an application's run? You direct the tuning goal. You can provide multiple runs of an application providing a larger data pool for Unravel to analyze. You can have Unravel apply the recommendations for you and run the newly configured application. You can see the effects, both positive and negative, the tuning has on an applications run. You can compare runs configurations. You can repeatedly tune the application until Unravel has no more recommendations. Your session is saved and can be run again, e.g., new runs added, cluster configuration changed. You can tune: Spark Hive on MapReduce Sessions can serve simply as a tool to compare two runs of the same application. The Sessions tabs opens displaying all current sessions sorted on Sessions Name Start Time Number of Apps The four (4) KPI's Duration IO vCore Seconds Memory Seconds Duration Cluster ID You can search for a session by name. Enter the string in the search box; any session name matching or containing the string will be displayed. " }, 
{ "title" : "Creating a Session", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-CreatingaSession", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page \/ Creating a Session", 
"snippet" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Sessi...", 
"body" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Session. Application Type Tuning Goal App IDs +Add another App ID Add If you are tuning a Spark App you must supply the JAR path and Class Name. If you do not intend to use the \"Apply\" feature for sessions, you can enter \"none\" (without quotes) for JAR path and Class Name. Auto Tune Session You have the additional option to specify the maximum number of runs. If not specified, iterates continues until no recommendations are available. When specified, the iteration stops at the maximum number or lack of recommendations, whichever comes first. Via the Events Panel If an applications has events " }, 
{ "title" : "Session", 
"url" : "unravel-4-5/uguide/ug-the-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-Session", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Applications Page \/ Session", 
"snippet" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applicat...", 
"body" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applications Right Tabs - Keeps a log of all the activity. See example above. Progress Tab - Expanded graphs of Duration, IO, Resources Trends - Allows you to compare two of the runs. Compare " }, 
{ "title" : "The Reports Page", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page", 
"snippet" : "Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. Operational Insights Data Insights Cloud Reports Report Archives Scheduled Reports Scheduling Reports The Reports page opens displaying Operational Insights...", 
"body" : " Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. \n Operational Insights \n Data Insights \n Cloud Reports \n Report Archives \n Scheduled Reports \n Scheduling Reports The Reports page opens displaying \n Operational Insights " }, 
{ "title" : "Operational Insights", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-operational-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Operational Insights", 
"snippet" : "- generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performa...", 
"body" : " - generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performance and provide fine tuning insights\/recommendations. Cluster Optimization - Generates a report of active queues for time frame. The report analyzes queue activity by apps, vcores and memory. Queue Analysis - shows the aggregated workload for all clusters. Cluster Workload When you specify a date range, a pull down menu appears on the right hand side of the Operational Insights title bar. By default Operational Insights Chargeback Application Type , Click here Chargeback The Chargeback Yarn and Impala tabs are identical expect that the reports are limited to Yarn and Impala jobs respectively. You can generate ChargeBack Group By Application Type, User, Queue Tags. Application Type Donut graphs showing the top results for the Group by Charge back report showing costs, sorted by the Group By choice(s), and List of Yarn applications running. Generate Charge Back Report You can set the date range and clusters to use for the report in the Operational Insights Group By Group By User dept User dept r VCore\/Hou Memory MB\/Hour t Update Repor CSV A new charge back report is generated each time you change the Group By must Update Report Charge Back Yarn Cluster Summary The Cluster Summary Applications User Queue User Applications You can sort applications on vCore or memory seconds. User Queue Cluster Compare This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use y Group B User Queue e Time Rang Compare With Range Any deviation in metrics across the time ranges is highlighted (3). A green red Time Compare With Group By Cluster Optimization The OnDemand This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize your cluster throughput, resources, and performance. Currently this feature only supports Hive on MapReduce. You can these reports you can: Fine tune your cluster to maximize its performance and minimize your costs, and Compare your cluster's performance between two (2) time periods. Report are generated on an ad hoc or scheduled basis. All reports are archived and can accessed via the Reports Archive Download or Generate a Report Click Download JSON Reports Archive Click Generate New Report Date Range Run Running Run Generate New Report Click Schedule schedule your report Optimization Report The Report has three (3) sections. Contains the basic report information author, time run, and dates used to generate the report. Header KPIs Number of Jobs: per day average Number of vCore Hours: per day average Number of MapReduce Containers Percent used for Map Percent used for Reduce Amount of Memory from of MapReduce Containers Percent from Map containers Percent from Reduce containers The KPIs are a per-day average for the number of days in the report. In this case we generated a report for a two (2) day period. All the insights\/recommendations are based upon the analysis of all jobs, in this case113. Insights\/Recommendations This section contains a tab for each area, with the relevant properties under consideration for tuning. These are cluster wide properties and are the defaults for all applications. Applications, however, can override these properties on an application by application basis. MapReduce: mapreduce.map.memory.mb,mapreduce.reduce.memory.mb,mapreduce.input.fileinputformat.split.maxsize,mapreduce.job.reduce.slowstart.completedmaps Hive: hive.exec.reducers.bytes.per.reducer,hive.exec.parallel You can expand the insight tile to the full width of the window. Further below we go into greater detail on two of the insights to explain the contents. Insight\/Recommendations Tile Details Tune the size of the map containers Each tile is entitled with what's being tuned. Below is the expanded view of the first tile . Immediately below the title is the property to tune, in this case mapreduce.map.memory.mb. Click on on the Next ( 1 2 As expected 51% of the jobs (58) used the default, while 33% (37) used 512MB with the remaining jobs distributed across the remaining values. The graph shows Unravel's analysis of the property potential values. It shows each candidate % of memory saved for the input workload % of jobs from the workload that would still run with the candidate When there are tuning instructions it is noted above the graph ( 3 In this example, there is a recommendation but no tuning suggestions. Since there is only a low chance of improvement, Unravel does not recommend altering the property so provides no tuning suggestions. Tune the number of the map containers This analysis has additional information in addition a tuning suggestion and instruction. Click on it to see further information. Tune the number of reduce containers in Hive queries In this case, the information was simply informative. There can be cases where tuning suggestions for specific apps are offered. Queue Analysis The OnDemand You can generate a report of active queues for all your clusters or a particular cluster. The report analyzes queue activity by applications, vcores, memory., and disk. As with all reports. it can be generated on an ad hoc or scheduled basis.The tab opens displaying the last report, if any, generated. Reports are archived and can accessed via the Reports Archive Generate a Report Click t New Repor , Date Range . Run Running Run New Report Click Schedule schedule your report Report If the report was successfully generated a light green bar appears and a table listing all the queues in existence during the time range is displayed. The table lists each queue with its KPIs average ( Apps Running VCores Memory Disk Filter By Click Applications VCore Usage Memory Usage Disk Usage | Operations Usage Details Infrastructure Click on the expand arrows ( Cluster Workload Displays your cluster(s) yarn applications' workload across a date range using the following views: - by date, e.g., 10 October. Month - by hour regardless of date, e.g., 10.00 - 11.00. Hour - by weekday regardless of date, e.g., Tuesday. Day - by hour for a given weekday, e.g.,10.00 -11.00 on Tuesday. Hour\/Day You can filter each view by App Count Vcores Hour Memory Hour To measure the Vcores or Memory Hour usage is straightforward; at any given point the memory or vcore is being used or it's not. The App Count is not a count of unique app instances The App Count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore an app can be counted multiple times in a view. on multiple dates, e.g., on 11 & 12 October (2 days) in multiple hours, e.g., 10pm, 11pm & 12am hours on multiple days, Thursday & Friday, and in multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day App Count) Sum(App Counts for dates representing the day) App Count for Wednesdays (10, 17 & 24 October) = 2492, and App Count across Hour\/Day We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. By default the tab opens in the Month App Count Date Range t App Coun Vcores Hour Memory Hour y View B Hour Day Hour\/Day Average Sum See Drilling Down Month Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is Previous Next Hour, Day and Hour\/Day These graphs do not link jobs to any specific date at the graph level. For instance, the Hour at Day on Hour\/Day at a date Month above By default each view opens using the metric selected for the prior view. For instance, if Hour Vcores Month Day Vcores Hour When the Date Range - aggregated sum of job count, vcore or memory hour during the time range (default view), and Sum - Sum \/ (# of Days in Date Range) Average Hour Breaks out information by hour. The interval label indicates the start, i.e., 2AM is 2-3AM. Hover over an interval for its details. Click on the interval to drill down Day Displays the jobs run on a specific weekday. Hover over an interval for its details. Click on the interval to drill down Hour\/Day This views displays the intersection of Hour and Day graphs. The Hour Day Drilling Down in a Workload View Click on an interval to bring up its information. In our example, we selected 11 October in the Month App Count Click User Queue User Queue App Type: MR, User: HDFS, or User: ROOT, We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Applications | Applications App Count r Vcores Hou Memory Hour " }, 
{ "title" : "Data Insights", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-data-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Data Insights", 
"snippet" : "The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. -gives a quick view into the tables' and partitions' size Overview - Details - drills down into the tables. Details See Hive Metastore Configuration Hot Wa...", 
"body" : " The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. -gives a quick view into the tables' and partitions' size Overview - Details - drills down into the tables. Details See Hive Metastore Configuration Hot Warm Cold The last four provide disk management insights and help you manage your disk usage both in terms of capacity and cluster performance. In order to use Forecasting, Small Files, File Reports and Top X you must have the OnDemand - forecasts needed disk capacity based upon past performance Forecasting - generates a list of small files based upon user criteria Small Files - similar to Small Files, except canned reports for large, medium, tiny, and empty files. File Reports - top X Hive or Spark applications with respect to cluster usage, longest duration, and most data I\/O. Top X Click here Overview The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. Table KPI Partition The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Partition KPIs sections contain: : Number of Tables\/Partitions accessed, # Accessed : Number of Tables\/Partitions created, # Created : Size of Tables\/Partitions created, and Size Created : Total Number of Tables\/Partitions currently in the system. Total Number The Table KPI's also contains: : Total number of queries accessing the tables, and Accessed Queries : Total Read IO due to accessing the tables. Total Read IO Donut Chart These display the Current Label Distribution Details The details tab has two sections, a graph and a table list. By default the graph uses the Total Users Total Users Total Users, Total Apps, Total Size Graph In this example, the first through third table are selected and graphed. Use the Metric Total Users lTotal Apps Total Size Reset Graph Total Users Table List You can Search Show All Read IO More Info Table Detail Download CSV Table Detail This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. lists both Hive and Impala queries The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed: Users # Apps, Size There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Total Users Total Apps, Total Size Application Detail Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition Size MR jobs By default the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View Configuration This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access this modal pane from the Data Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save Forecasting (Disk Capacity) The OnDemand package must installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapolate capacity trends allowing you to more effectively plan for, and allocate your disk resources. The tab opens displaying the last forecasting report, if any, generated. The graph displays the trend from the historical range start date to the forecast range end date (x-axis). The trend line (in blue) shows the lower, middle, and upper bounds of Unravel's prediction. The y-axis is determined by your actual physical disk capacity. The report parameters are listed above the table headings. Click New Report History (Date Range) Forecasting (number of days) Run Schedule While Unravel prepares to generate the report Run Running New Report New Report You can download the report currently displayed by clicking the Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. Small Files The OnDemand package must be installed to be able to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here for properties which control this report. Each small file is accessed by a single mapper, therefore a large number of small files can lead to a large number of mappers. In turn, mappers are costly to run so applications using a large number of small files drive up your costs. This report helps you to identify users who create\/use an excessive amount of small files, allowing you to take corrective action such as: combine multiple files into large files, or notify, limit, or block\/ users who create\/use an excessive amount. in order to: correct and prevent future performance degradation, and lower your costs to run applications. The small file window by default opens with the last report that was generated, if any. The report parameters are listed above the table headings (1), and you can search the path list by string. The list is sorted in descending order of the total number of small files in the directory. Click Download CSV Click New Report : the average small file size in a directory. Uncheck the box to its right to enter a small file size to use instead. Average File Size (bytes) : which are in the directory. Minimum # of Small Files : is the maximum number of directories to display. # of Directories to Show : Advanced Options : minimum depth to start at, root + x descendants, i.e., 0=root, 1=root's children (\/one), etc. Min parent directory depth : maximum depth to end at, root + x descendants, i.e., 1=root's children (\/one), 2=root's grandchildren, (\/one\/two), etc. Max parent directory depth : determines how\/where the files are listed. Yes (default): lists the file in all it's ancestor's list. No: list file in it's directory list only. Drill down sub-directories Click Run Schedule While Unravel prepares to generate the report Run Running New Report New Report Click Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. File Reports The OnDemand package must be installed to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here and here for properties which control this report. This report is the same as Small Files except they are automatically generated using the File Reports properties. By default these reports are updated every 24 hours and are not archived. The default size for the files are: large file is any file with more than 100GB size, medium file is any file with 5GB - 10GB size, and tiny file is any file with less than 100KB size. Click on the size buttons ( Large Medium Tiny Empty Download CSV Top X TheOnDemand package must be installed to use this report. This report lists the top X Hive or Spark jobs which have consumed the most: duration, and data I\/O, and cluster usage. Click New Report History (Date Range) Top X Run Running Run New Report The report includes up to X apps (when available). If the report has been successfully completed a light green bar appears noting \"Top X Completed Successfully\". The report displays the Hive jobs by default and notes the reports parameters. Click the Filter By Download JSON The display is composed of three (3) tiles: Applications The Applications tile lists the app total along with successful and failed app count. Three tables list the top X apps by consumption type: Most Duration Most I\/O Most Cluster Usage Duration App Parent Query Snippet Note: The applications in each table are not necessarily the same. The top X apps with the most duration are not necessarily the top X apps using the most I\/O. Resources and Data These tiles display the cumulative totals for the queries. The Data Read Write I\/O Resources Map Reduce Total Spark Slot Time " }, 
{ "title" : "Cloud Reports", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports", 
"snippet" : "Table of Contents Overview Cluster Discovery Generating Reports for Cloud Mapping per Instance and per Host Tabs Cloud Mapping per Instance Cloud Mapping per Host...", 
"body" : " Table of Contents Overview Cluster Discovery Generating Reports for Cloud Mapping per Instance and per Host Tabs Cloud Mapping per Instance Cloud Mapping per Host " }, 
{ "title" : "Overview", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540149781", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Overview", 
"snippet" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -an...", 
"body" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -analyzes your on-prem cluster usage, workload patterns, and your hosts' hardware specs. Cluster Discovery -estimates the cost of moving to the cloud. Unravel analyzes three migration strategies by cloud provider (Azure, EC2, and EMR). Migration Analysis : one-to-one mapping of each existing host’s Lift and Shift capacity : one-to-one mapping of each existing host’s Cost Reduction actual usage : unlike the other methods this is not a one-to-one mapping. Unravel analyzes your workload for the time period and bases it recommendations on that workload. Unravel provides multiple recommendations based on the resources needed to meet X% of your workloads. It determines the optimal assignment of VM types to meet the requirements while minimizing cost. This method is typically the most cost effective method. Workload Fit Cloud Reports is generated base upon an analysis of your cluster workload over a specific date range. You can use it to: analyze on-prem clusters running Ambari running HDP 2.5 - 2.6, or Cloudera Manager running CDH 5 and examine the costs of running your clusters on three cloud providers: Azure HDInsight, Amazon EC2, and Amazon EMR. Limitations Cloud reports has not been thoroughly tested for clusters running Kerberos. Your cluster must have at least seven days of metrics for Unravel to generate useful reports. The pricing in reports use a static list of VM Instance type specs. Azure HDInsight: https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/hdinsight\/ Amazon EC2: https:\/\/aws.amazon.com\/ec2\/instance-types\/ Amazon EMR: https:\/\/aws.amazon.com\/emr\/pricing\/ Clusters running MapR Control System are not supported. Note In order to use Cloud Reports you must have OnDemand cluster manager UProp-Cluster See here - a dashboard containing information about your on-prem cluster Cluster Discovery - generates a report comparing costs for multiple strategies and aggregates the results by the VM type. Cloud Mapping per Instance - generates a report comparing costs for multiple strategies and shows the mapping for each individual host. Cloud Mapping per Host " }, 
{ "title" : "Cluster Discovery", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552539868963", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Cluster Discovery", 
"snippet" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report s...", 
"body" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report successfully generated, if any. Click New Report . History (Date Range) Run Running Run New Report If the report has been successfully generated NEW REPORT NEW REPORT DOWNLOAD JSON On-Prem Cluster Identity This tile contains information about your cluster. Click on Hosts Host Summary The table lists the hardware specs for each host and can be searched by name. Cluster overall usage of Applications grouped by App Type, User, and Queue. The donut graphs display the top ten (10) of each category. Cluster Resource Availability & Usage These graph your cluster's CPU and memory. The average usage is listed on the right hand side of the title bar. Hover over the the parenthetical text next to the resource to see Unravel's analysis of your usage. Below we see the allocated CPU resource is \"Very under-utilized and over-provisioned\". Cluster Heatmap The heatmap shows the actual CPU\/Memory usage and capacity by a weekday and hour, e.g., Monday between 5 and 6 pm.Each time slot in the heat map represents how relatively " }, 
{ "title" : "Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540242292", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"snippet" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type...", 
"body" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type picked. " }, 
{ "title" : "Cloud Mapping per Instance", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540343800", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Instance", 
"snippet" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction...", 
"body" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction " }, 
{ "title" : "Cloud Mapping per Host", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-cloud-reports.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540200429", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Host", 
"snippet" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on aver...", 
"body" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on average utilization. Cost Reduction -- a mapping based upon meeting SLA percentiles. Workload Fit The tab opens displaying the Lift and Shift Cost Reduction t Workload Fi Below we examine the three migration reports mapping the cluster to EMR with no specific instance picked. The Total Hourly Cost Lift and Shift Workload Fit Lift and Shift and Cost Reduction The report layout are exactly the same. The basis for the mapping recommendation is listed under the selection tabs, the hourly cost to map all hosts is aggregated and displayed in the upper right above the report table. The table has four columns: - in this case the on-prem host, Instance - the host's resources which were actually utilized, Actual Usage - the resources the host has available, Capacity - the cloud instance Unravel recommends mapping the host to, and Recommendation of the instance. Cost\/Hour Lift and Shift Lift and Shift is matching capacity, and therefore not attempting to minimize costs. You can see below the on-prem host is very underutilized. Since Unravel is matching capacity instance the on-prem host is mapped to will also be underutilized. This report mapped 20 hosts, only the first is shown. The hourly cost per instance is $5.69 for a Total Hourly Cost Cost Reduction Cost Reduction is again mapping each host to an instance (1-1), this time based on actual usage not capacity. Again we are only showing the first mapping. The EMR instance Unravel recommends is c3.8xlarge instead of m5d.24xlarge recommended above. The cost savings is significant compared to Lift and Shift; the Total Hourly Cost Workload Fit In this report, Unravel is not looking at individual host capacity\/usage but finding the optimal allocations of VM to meet a percentage of the workloads, specifically 80 - 100% in 5 point increments. For specifics details click on SLA x% mapping scroll through the window on the left size or click directly on the percentage. Below we see to meet 100% SLA – Unravel is recommends the same cloud server instance as in Lift and Shift, but instead of mapping to 1-1, it recommends using just six (6). The bar graph shows the SLA\/hour costs at a glance. You quickly can see there are costs saving from 100% to 95% (5%) and 95% to 90% (11%) but further reducing the SLA percentile results little to no savings. When comparing the total hourly cost of 100% SLA Workload Fit to Lift and Shift and Cost Reduction shows savings of 75% and 25% respectively. " }, 
{ "title" : "Scheduling Reports", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-scheduling-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Scheduling Reports", 
"snippet" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule...", 
"body" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule " }, 
{ "title" : "Reports Archive\/Scheduled Reports", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports", 
"snippet" : "Table of Contents Reports Archive Scheduled Reports Note Click here...", 
"body" : " Table of Contents Reports Archive Scheduled Reports Note Click here " }, 
{ "title" : "Reports Archive", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537890247", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports \/ Reports Archive", 
"snippet" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to vie...", 
"body" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to view and Example of Capacity Forecasting .csv file. Date Max Capacity Observed Capacity Trend Lower Trend Upper Trend 08\/23\/18 02:00 AM 3.172 kB 528.000 B 528.098 B 527.120 B 528.970 B 08\/29\/18 02:00 AM 3.172 kB 527.000 B 528.171 B 527.248 B 529.097 B 08\/28\/18 01:00 PM 3.172 kB 527.000 B 526.385 B 525.443 B 527.316 B 08\/27\/18 06:00 PM 3.172 kB 528.000 B 528.332 B 527.420 B 529.231 B 08\/29\/18 07:00 PM 3.172 kB 532.000 B 530.328 B 529.377 B 531.289 B 08\/26\/18 11:00 PM 3.172 kB 528.000 B 527.933 B 527.033 B 528.801 B 08\/26\/18 04:00 AM 3.172 kB 528.000 B 528.101 B 527.199 B 529.004 B 08\/22\/18 11:00 AM 3.172 kB 528.000 B 527.806 B 526.847 B 528.694 B 08\/24\/18 12:00 PM 3.172 kB 528.000 B 527.651 B 526.691 B 528.616 B 08\/26\/18 05:00 AM 3.172 kB 528.000 B 527.818 B 526.921 B 528.706 B 08\/29\/18 12:00 PM 3.172 kB 527.000 B 528.742 B 527.871 B 529.649 B 08\/28\/18 07:00 AM 3.172 kB 527.000 B 525.778 B 524.834 B 526.672 B Small File Report Small Files is a .csv file. The report is sorted on the number of file in descending order. Directory Number of Files Avg File Size Total File Size Min File Size Max File Size \/apps 26554 219.7 kB 5.6 GB 0 B 192.7 MB \/apps\/hbase 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data 14063 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data\/default 14044 378.0 B 5.1 MB 0 B 2.5 MB \/apps\/hive 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB \/apps\/hive\/warehouse 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB " }, 
{ "title" : "Scheduled Reports", 
"url" : "unravel-4-5/uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537990066", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports \/ Scheduled Reports", 
"snippet" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close...", 
"body" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close " }, 
{ "title" : "The Application Managers", 
"url" : "unravel-4-5/uguide/ug-apm-the-application-managers.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Application Managers", 
"snippet" : "Applications Managers Spark Application Manager...", 
"body" : " Applications Managers Spark Application Manager " }, 
{ "title" : "Applications Managers", 
"url" : "unravel-4-5/uguide/ug-apm-the-application-managers/ug-applications-managers.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers", 
"snippet" : "Table of Contents Typical Application Manager's Layout Athena Cascading and Pig Application Managers Hive Application Manager Impala Application Manager Kafka Application Manager MapReduce Application Manager Tez Application Manager Workflow Manager Note See here See here See here Typical Applicatio...", 
"body" : " Table of Contents Typical Application Manager's Layout Athena Cascading and Pig Application Managers Hive Application Manager Impala Application Manager Kafka Application Manager MapReduce Application Manager Tez Application Manager Workflow Manager Note See here See here See here Typical Application Manager's Layout A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the parent's type. Clicking on it brings up its APM. If it is a running yarn job (MR, Tez, or Spark) there is an action box ( kill or move Unravel's Intelligence Engine can provide insights into an application and may provide recommendations, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Example The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) : notes the number of events the job had. Event icon No Events Event Panel & Insights notes the job type and status. The box is colored code to indicate as the application's status. Job icon: Next to the job name will be an auto actions glyph ( Job Name: ) if the job has violated any actions. Hover over it to see a list of the violated actions. A fine tuning glyph ( ) appears when Unravel has tuning suggestions for the job. : job number, owner, queue, cluster and start\/stop time. Job Information these vary by job type. KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation ) column notes the number of events associated with the job\/stage. Common Tabs: The Hive, MapReduce and Workflow APMs contain this tab. It lists all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted for each job. The top line list the number of all jobs\/task. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Errors: Keywords : The MapReduce, Spark and Tez APMs contain this tab. It lists the configuration parameters for the task\/job being displayed and their values. The parameters vary according to task\/job. Conf\/Configuration : All APM's except Pig and Cascading contain this tab. It lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr Athena The Athena Application Manager is currently in beta (preview). The KPI bar contains information about your query. Since Athena jobs are retrieved upon completion the job status can only be success, killed, or failed. Next to the status box is your job information. App ID, User, user type, account ID, database, and start time. Output Key Performance Indicators : Currently Unravel does not have events for Athena jobs. Events : Total time taken by the query. Duration : The amount of data scanned by the query. Data I\/O : The cost is calculated based upon Cost Amazon's current pricing The Query Cascading and Pig Application Managers The only Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Tabs The left Tabs: : list of all the jobs, their status and type. Navigation : Exceptions, errors, and warnings associated with this application. See here for an example. Gantt chart : See Tags here The right Tabs : Displays Map and Reduce task attempts by success, failed, and killed status.The data displayed is for the Task Attempts entire : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time is noted in the upper left hand corner. The computer slot usage is noted below the graph. Attempts Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O : The number of YARN apps making up the query. Number of YARN apps Tabs By default the Hive APM opens displaying the Navigation Query The left Tabs are: : List all the MapReduce jobs associate with the query. Click on the job name to bring up job in the Navigation MapReduce Application Manager : Shows detailed information about the MapReduce jobs and their relationship with one another. Execution Graph The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other along some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). Click on a table name to bring up the table information Close Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. : Shows job sequencing using a gantt chart. Gantt Chart : Exceptions, errors, and warnings associated with this application. See Errors here : Lists defined tag keys and associated values. See Tags here The right Tabs are: : Shows the Hive Query. See the Hive Application Manager Query window Copy Query A list tables accessed by Application. Tables: : Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the Task Attempts entire : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time the job started is listed in the upper left hand corner. The total Map and Reduce slot duration time is noted below the graph. Attempts Impala Application Manager The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Operators Tabs The left Tabs are: : Displays a table with information about each fragment associated with this query. Click on Fragments More L ess This window shows the Fragment and it's KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking on the operator brings up the operator window. (See Operators Query Plan lists each instances with it's KPI's. Instance View: : Displays a list of all operators for all fragments. You can search the operators name. Click on the operator to display its details. Operators Scan HDFS details Aggregate Details Exchange Details : Charts the fragments and the time spent on each operation. Hover over a section see the operation and it's KPI's. Gannt Chart : Shows the query plan in fragment or operator view. Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. Query Plan : Lists defined tag keys and associated values. See Tags here The right Tabs are: : Shows the query plan code. Click on Query Query Copy window : Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. Mem Usage Kafka Application Manager The Kafka Application Manager provides Multi-Cluster support for monitoring: Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled | Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out\/sec Messages in\/sec Total Fetch Requests per \/sec Number of Active Controller Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafka Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators Number of Topics Number of Partitions The Topic lists displays the KPIs; when details are available a more info Partition Tab You can chose both the Partition Metric th offset Partition Details' Topic View The Kafka View has two tabs, Topic Detail Partition Detail Consumer Details' Kafka Topic Detail By default the Kafka Topic Detai Topic Detail Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is if: Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. if: Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: OK, the consumer is working, but falling behind, or Warning: : the consumer has stopped or stalled. Error MapReduce Application Manager The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O Tabs By default the MapReduce APM opens in the Graphs | Attempts : Has four (4) sub tabs. Graphs : Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in on the Map\/Reduce slot duration is noted below the graph. Attempts andMemory: Graphs utilization of slot containers, vcores, and memory over time. Containers, Vcores, : Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. Timeline The Timeline tab is divided into two sections: a Distribution Map Reduce a bottom table which lists either the tasks by stages on servers or teh list of tasks and their associated KPIs' The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map Reduce Timeline Selected : The metrics, their definitions and values. Metrics Lists the available logs by Map, Reduce and Application Master. Click on the tab to see the listing for that type (Map, Reduce, or Application Master). Click on an item to see the log. Logs: The defined parameters and their values. Configuration: : Resource Usage Initially all the executors are displayed using the Metric Metric nly Show All : Exceptions, errors, and warnings associated with this application. See Errors here : Lists defined tag keys and associated values. See Tags here Tez Application Manager The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O Tabs By default the Tez APM opens showing the Navigation and Program Tabs. The left Tabs are: : List the Dag jobs with KPIs, Duration and I\/O. See the Dag Detail information Navigation below : List the configuration parameters and their values. Configuration : Lists defined tag keys and associated values. See Tags here The right Tabs are: : Displays the query. Program : Has three (3) sub tabs. Graphs andMemory: Graphs utilization of slot containers, Vcores, and memory over time. Containers, Vcores, : Graphs the resources consumed. By default the Resources Resource systemCpuLoad Select series Metric Get Data DAG Detail The DAG detail has six tabs: Displays the query. Query: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Graph: : Lists all the relevant counters for the Tez-DAG and their values. Counter Vertex Timeline Wall Clock Total Run All Vertices : List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task Id and Vertex name; Tasks containing the string will be displayed. All Task : List all attempts, their status (failed, success, etc.), vertex name and other relevant information. The task attempts are searchable by Attempt Id, Task Id and Vertex name; Task attempts containing the string will be displayed. All Task Attempts : Lists all relevant parameters and their value. Changed Configuration Workflow Manager The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications | Applications Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow Number of Yarn Apps Tabs The APM opens showing the Navigation Compare The left Tabs : Provides an easy way to understand the breakdown of the workflow the applications which comprise the Workflow, i.e., Hive, Spark, MapReduce, Oozie. Click on Navigation More Below the second Oozienode is shown, it is comprised of one MapReduce job and three Hive jobs. The hive jobs comprise one or more tasks, so that too can be expanded. In the example below, the second Oozienode has been expanded along with the first hive job within it. You can click on any job to see the application manager for it. In the example, below you can click on the expanded hive job to bring up the hive application manager. Similarly you can click on the mapreduce job within the hive job to go directly to it. Click on Less : Displays the execution graph of the workflow. Click Execution : Exceptions, errors, and warnings associated with this application. See Errors here : See Tags here The right Tabs: : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events above : Displays charts for Map Task, Reduce, and Spark Tasks, broken down by success, failed, and killed as appropriate. Task Attempts : Graphs the attempts over the time interval in Wall Clock time and list the Map and Reduce Slot Duration in total computing time below. Attempts " }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-5/uguide/ug-apm-the-application-managers/uguide--apm-spark.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager", 
"snippet" : "Table of Contents Overview The Spark Application Manager's Basic Layout Actions Common Tabs Scala, Java, PySpark, and SQL-Query Navigation Tab Common Tiles Spark - Scala, Java, and PySpark Spark - SQL-Query Spark - Streaming Overview A Spark Application consists of one or more Jobs, which in turn ha...", 
"body" : " Table of Contents Overview The Spark Application Manager's Basic Layout Actions Common Tabs Scala, Java, PySpark, and SQL-Query Navigation Tab Common Tiles Spark - Scala, Java, and PySpark Spark - SQL-Query Spark - Streaming Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. : corresponds to a spark action, e.g., count, take, foreach. Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source code from the stage tile, spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an application's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark driver and executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. Unravel provides insights into Spark applications and potentially tuning recommendations; see Spark Event List There are multiple Spark application types and the Spark APM information can vary by the application type. Currently Unravel distinguishes between: , Scala, Java, and PySpark , and, SQL-Query . Streaming Regardless of the application type and how they are are submitted (e.g., from Notebooks, spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark Application Manager's Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). On the right side there are an actions Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions, or insights into how to improve the application's run. When there are insights, a bar appears immediately below the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples Spark Event List The next section contains the Key Performance Indicators (KPIs) and general application information. : notes the number of events the Spark application had. If there were none, Event icon No Events Event Panel notes the application's status and the window type (S-Spark, SJ-Spark Application and so on.). The box is colored code based upon its status. Application icon: notes the job type and status. The box is colored code to indicate as the application's status. Job icon: Next to the job name will be an auto actions glyph Job Name: : job number, owner, queue, cluster and start\/stop time. Job Information these vary by job type. KPIs: The last section, divided in half, has specific information related to the application. The sections for a specific Spark Application (e.g., Streaming) go into more detail. If the application is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Stream Actions Click on When a yarn application is running you can kill or move it. After the application has stopped, you can load logs and diagnostics. Clicking Load Diagnostics Load Logs Logs Common Tabs Except for the Spark streaming streaming Left Tabs - Lists all errors associated with the application. The errors are color coded (fatal Errors Keywords - Lists the critical logs that were collected for this Spark application. It is noted when no logs are available. Logs Click on the log name to see it, below is an excerpt of the executor-20 log. Lists the configuration parameters and their values. The parameters vary according to app\/task\/job. The tab opens listing all the properties. The number of properties displayed is noted above the list. Conf - You can narrow the list by choosing the configuration type to display; to see the spark version select metadata. Metadata and driver are selected in the example below and the list narrowed from 1042 to two (2) properties. Some properties appear in multiple categories, e.g., spark.executor.extraJavaOptions is listed under Memory, Driver and Executor. You can search by name; searching on yarn displays every property containing the word yarn. Click Reset Right Tabs - Displays the program's source code if available. Both the Spark Stage and Execution graph link to tab and display code associated with each. Note: Unravel allows you to upload Spark programs, see Program Uploading Spark Programs. - Graphically and textually notes the number of tasks and their status. The donut graphs show the percentage of successful (green), failed (orange), and killed (red) tasks. The legend on the right lists the number of each. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination. Hovering over the chart tells you the percentage of each. Task Attempts There are three (3) types of graphs. Graphs use \"wall clock time\" as opposed to computer usage time. Graphs - : The number of running containers. Running Containers : Allocated vcores. Vcores : Allocated memory. Memory - Graphs the resources the application\/job\/stage consumed. By default the graph displays all executors using the metric Resource availableMemory Get Data You chose one or more series to display using the Select X Hovering only resource highlights it, clicking on it toggles the display, i.e., if currently displayed it is removed from the graph. Conversely if you click the Only Show All Below is an example of the JSON when clicking on Get Data Scala, Java, PySpark, and SQL-Query Navigation Tab - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Tasks, Read Write Stages Start Time Tasks Stages In this example, 39,600 Tasks Stages Clicking on a job row brings up its details in a Job Block. You can open up the job even if it's running. The job block lists the KPIs Duration # of Stages , Stages Gannt Metadata ID Status Start Time Duration Tasks , Shuffle Read Shuffle Write Input Output Start Time Spark Stage This example is for Job 1 above, which is only 41% complete.Here the # of Stages Common Tiles Spark Job A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators The wall-clock time it took to complete the job. DURATION: Number of stages to the job. # of Stages: It has three tabs: his tab is the default view. It lists the stages with their KPIs and is initially sorted on - Stages start time - This tab again shows all the stages, graphically displaying the time spent in each. The stages are initially displayed in order of execution, i.e., first, second, ..., nth. Gannt Chart Lists all the attributes and values for the jobs. Metadata - Spark - Scala, Java, and PySpark Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events Event Panel Examples Spark Event List : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Paritions\/Tasks Read Write # Stages. above. - The execution graph shows a RDD DAG of the application. The application's Execution RDDs Operations Program If the program tab displays the code it is linked with the DAG. If you display the execution and program tab simultaneously, as shown below. Click on the vertex to highlight relevant code. Below we see the corresponding code for vertex 18. Below we expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call and finally the number of stages which were involved. Below RDD represented by vertices 17-16 involved two (2) stages, while 15-16 had five (5). Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. Gantt Chart Displays the stages using a Gantt Chart. The table is sorted on Start Time - For an explanation of these tabs see Errors, Log and Conf Tabs Errors Logs Conf Four Right Tabs - When available it displays the program associated with the application. See Program above - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource Spark - SQL-Query Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events Event Panel Examples Spark Event List : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Paritions\/Tasks Read Write # Stages. above. - A execution graph of the query. There are times when the DAG is too large to display and it will be noted. See Execution above - Displays the stages using a Gantt Chart. For more details see Gantt Chart above Tabs For an explanation of these tabs see Errors, Log and Conf Errors Logs Conf Four Right Tabs - This tab connects all the pieces of a SQL query. The table lists all queries with significant KPI's and the top five stages, i.e., the stages with the longest duration. The lower section contains two tabs, Program SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. Click on query - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource " }, 
{ "title" : "Spark Stage", 
"url" : "unravel-4-5/uguide/ug-apm-the-application-managers/uguide--apm-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611722669", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark Stage", 
"snippet" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators The computer time it took to complete the job. Duration: Date IO It has two tabs, by default it opens in the graph Graph Displays the number of t...", 
"body" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators The computer time it took to complete the job. Duration: Date IO It has two tabs, by default it opens in the graph Graph Displays the number of total tasks for the stage and number of total attempts made to run these tasks. The number of tasks is noted on the left side of the bar with the number of attempts on the right. The donut chart graphically displays the successful ( - Task Attempt Displays the stage's program details. Unravel extracts Source file name and call line. Click on the source file or line number to see program code. The Description shows the stage's Stack trace. The first line is always a call to the Spark library. The source file name and line number are extracted from the second line. - Program Details Time Line Tab The Time Line tab has two sections, Distribution Charts Three tabs below the chart: , Time Line and Timeline Breakdown . Selected Tasks Time Line The Time Line tab has two sections, Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown Selected Tasks Be default the Time Line Distribution Charts ShuffleMap Disk Bytes Spilled Memory Bytes Spilled Records Read. The lower section opens displaying the Time Line , Timeline Breakdown This is useful to identify bottlenecks.For each executor used in the current stage, multiple metrics are graphed: Scheduler Delay, Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Using the ratio of Executor Computing Time performing actual work thrashing, or waiting for scheduling. A list of tasks, if any, for the stage. Selected Tasks " }, 
{ "title" : "Spark - Streaming", 
"url" : "unravel-4-5/uguide/ug-apm-the-application-managers/uguide--apm-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611588233", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark - Streaming", 
"snippet" : "Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events Event Panel Examples Spark Event List : Wall clock time time by the Spark job (the total time to process all stage...", 
"body" : " Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events Event Panel Examples Spark Event List : Wall clock time time by the Spark job (the total time to process all stages – Duration : Total data read and written by the query. Data I\/O : The number of jobs that make up the streaming. Number of Jobs : The number of stages that make up the streaming. Number of Stages Unlike other Spark Application Managers this has a Stream tab Stream Program Left Four Tabs - Displays the core of an Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Stream Scheduling Delay Metric Scheduling Delay, Processing Time Total Delay. It has two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven (7) pages. In the example below, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one (1) page, versus the seven (7) above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time Click on a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. The KPI's, Duration Processing Delay Scheduling Delay Total Delay Output Operation Input The example below is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two (2) jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click on the Job ID Spark Job The Input Tab -For an explanation of these tabs see Errors, Log and Conf Errors Logs Conf Right Four Tabs - The program (if uploaded by the user) is shown in this tab. Program - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource . " }, 
{ "title" : "Events & Insights", 
"url" : "unravel-4-5/uguide/uguide-events-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Events & Insights", 
"snippet" : "Event Panel & Insights Impala Events...", 
"body" : " Event Panel & Insights Impala Events " }, 
{ "title" : "Event Panel & Insights", 
"url" : "unravel-4-5/uguide/uguide-events-insights/ug-ev-event-panel---insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Events & Insights \/ Event Panel & Insights", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. For detail information Impala Insights see here For a list of all events see here Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job Example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. \n Recommendations \n Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). \n Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . \n Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). \n Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG Example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. \n Recommendations \n Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. \n Efficiency 2: Tez DAG Resulted in one recommendation (#1). \n Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Impala Events", 
"url" : "unravel-4-5/uguide/uguide-events-insights/ug-ev-impala-events.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Events & Insights \/ Impala Events", 
"snippet" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the ev...", 
"body" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the event box brings up the event panel where you can scroll through the multiple insights. Each insight has two sections, the analysis (brown bar on the left), and actions you can take (green bar on the left). When Fragments and Operators are referred to, their ID's are always noted so you can refer back to their views\/tiles in the Impala APM Time Breakdown Analysis The analysis breaks down where the time is spent during the query execution across the following phases: Query planning, i.e., time spent on parsing the SQL and optimizing the query plan, Admission control, i.e., time spent waiting on a queue\/resource pool, Query execution time until the first row becomes available, and Results fetching, which is a combination of row fetching, execution, and idle time. Next, it displays the longest phase and provides additional analysis of where time spent as part of that phase. In the example below, the longest phase is query execution (56.829s). The details on the operator taking the longest time are noted; in this case the Hash Join took 41 out of the 57 seconds. When possible, the event has insights into why this operator took long time to run, and makes tuning suggestions. (See Impala Slow Operator Even Missing Optimizer Statistics Displays a list of tables accessed by the query that had missing statistics and recommends running \"COMPUTE STATS\" to collect statistics for these tables. Underestimated Count of Rows In this case, the query scanned a table for which the optimizer statistics were outdated resulting in the optimizer underestimating the number of rows to be returned. This, potentially, led to a bad execution plan and the query to run slower. Unravel recommends refreshing the statistics by running \"COMPUTE STATS\". Time Skew Analysis This insight detects whether one or more operator instances took substantially longer than other instances and provides a potential root cause for the time skew. It notes whether the time skew is correlated with either: a bottleneck node, e.g., due to slow disk I\/O on that particular node, or data skew, e.g., if the bottleneck node processed much larger volumes of data compared to the rest of the nodes. Slow Operator Analysis This insight indicates that a query operator took significant amount of time to execute. It shows details such as the operator execution time and the time spent in different phases of the operator's execution. Depending on the operator type, this insight provides a root cause analysis that can explain the operator's poor performance. For example, for a SCAN operator the following potential causes of poor performance are examined: Missing or stale statistics on the table the operator scanned, Less efficient storage format of the scanned table, Inefficient partition pruning, and Remote or non-circuit byte reads For a JOIN operator the following issues are examined: Inefficient join algorithm used, e.g., a broadcast join was executed when a partitioned join would be significantly faster, Inefficient order of joined tables in a hash join, e.g., the right hand side table contains more rows than the left hand side, and Missing or stale statistics on one or both joined tables. Based on the issues identified this insight provides corresponding recommendations to improve the performance of the operator. For example, if a partitioned (shuffle) join would be faster, we recommend you use the \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join. Non-Columnar Storage Formats This lists the tables that were accessed which use a less efficient storage format, such as TEXT or (non-splittable) GZIP. This insight is generated when scans on these tables take long time to execute. In order to address this, we recommend modifying the storage format to a columnar one such as PARQUET or ORC. Inefficient Partition Pruning This insight shows the tables the query accessed which: have table partitions, and all the partitions were used during the query execution. This could lead to poor query performance for all scan operators accessing the tables, especially when the number of partitions is large. Often older partitions, e.g., partitions containing \"cold\" or outdated data, are not useful. In such cases we recommend rewriting the SQL query by adding a predicate on the partition key, such that the older\/cold partitions will get pruned. " }, 
{ "title" : "Event List", 
"url" : "unravel-4-5/uguide/uguide-events-insights/ug-ev-event-list.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Events & Insights \/ Event List", 
"snippet" : "Hive IMPALA MR Spark Tez Workflow Hive Event Category Event Name Parameters Description of When Event Occurs Application failure HiveFailureBrokenPipeEvent N\/A Hive query fails with \"broken pipe\" exception. Application failure HIveFailureIncorrectHeaderEvent N\/A Hive query fails with \"incorrect head...", 
"body" : " Hive IMPALA MR Spark Tez Workflow Hive Event Category Event Name Parameters Description of When Event Occurs Application failure HiveFailureBrokenPipeEvent N\/A Hive query fails with \"broken pipe\" exception. Application failure HIveFailureIncorrectHeaderEvent N\/A Hive query fails with \"incorrect header check\" exception. Application failure HiveFailureReturnCodeEvent N\/A Hive query fails and shows return code. Application failure HiveMapJoinMemoryExhaustionEvent hive.auto.convert.join Hive query fails because it is out of memory in map join, and recommends turning off mapjoin. Application failure HiveOutOfMemoryErrorEvent N\/A Hive query fails because it is out of memory. Informational HiveKillFailEvent N\/A Hive query is killed or failed with lots of wasted work. Informational HiveSuccWithKillFailEvent N\/A Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. Informational HiveShuffleBytesEvent N\/A Hive query has lots of data shuffle from map to reduce side. Informational HiveTimeBreakdownEvent N\/A Identifies where time is spent on for the query and points out significant events, including MR-level skew events. Speedup HiveExecuteParallelEvent hive.exec.parallel Detects that hive.exec.parallel is set to false and that the jobs in the query could be run in parallel if set to true. Speedup HiveSingleReduceCountDistinct N\/A Hive query has a job with a long single reducer because the query has \"count distinct\". Speedup HiveSingleReduceLongWait N\/A Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side. Speedup HiveSingleReduceOrderBy N\/A IHive query has a job with a long single reducer because the query has \"order by\". Speedup HiveTooFewReduceEvent hive.exec.reducers.bytes.per.reducer Hive query is using too few reducers. Speedup\/ Resource Utilization HiveTooLargeMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb Mappers in the Hive query are requesting too much memory. Speedup\/ Resource Utilization HiveTooLargeReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb Reducers in the Hive query are requesting too much memory. Speedup\/ Resource Utilization HiveTooManyMapEvent mapreduce.input.fileinputformat.split.maxsize mapreduce.input.fileinputformat.split.minsize Hive query is using too many mappers. Speedup\/ Resource Utilization HiveTooManyReduceEvent hive.exec.reducers.bytes.per.reducer Hive query is using too many reducers. IMPALA Event Category Event Name Parameters Description of When Event Occurs Application failure ImpalaFailureEvent N\/A Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible). Informational\/ Resource Utilization ImpalaNoFilterEvent N\/A An Impala query does not contain any filter conditions. Informational\/ Resource Utilization\/ Speedup\/ ImpalaTimeBreakdownEvent N\/A Displays the longest phase in the Impala query. If the longest phase is query execution then it displays the longest operator in the Impala query. If applicable, this event shows insights as to why this operator took the most time, and makes tuning suggestions. Resource Utilization ImpalaTooManyPartitionsEvent N\/A A table has too many (>30K) partitions and recommends using a different partitioning scheme based on frequently used filter conditions. Speedup ImpalaNonColumnarTablesEvent N\/A A table is in non-columnar format and a scan on this table represents significant percentage of the total query execution time. Speedup ImpalaNonPrunedPartitionsEvent N\/A A scan accessed all partitions of a table (i.e. no partition pruning occurred). Speedup ImpalaNonPartitionedTableEvent N\/A A table does not have any partitions and recommends creating partitions on this table based on frequently used filter conditions. Speedup ImpalaSlowAggregateOperatorEvent N\/A An aggregate operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaSlowExchangeOperatorEvent N\/A An exchange operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaSlowScanOperatorEvent N\/A Ascan operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if statistics for the scanned table are missing or stale, this insight recommends refreshing the statistics.. Speedup ImpalaSlowJoinOperatorEvent N\/A A join operator took significant amount of time to execute and shows details such as the size of the left and right hand side inputs, the join algorithm and type, and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if a broadcast join was used but a partitioned (shuffle) join would be faster, this insight recommends using \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join instead. Speedup ImpalaSlowSortOperatorEvent N\/A A sort operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaTablesMissingStatsEvent N\/A The Impala query accessed tables with missing optimizer statistics and recommends collecting these statistics. Speedup ImpalaTimeSkewEvent N\/A Detects whether one of the operator instances took much longer than the other instances and indicates the bottleneck node. If the skewness is significant enough, this event indicates whether the skewness is correlated with data skew (i.e. the bottleneck node had to process much larger volumes of data) or because of slow disk I\/O on the bottleneck node. Speedup ImpalaTooManyJoinsEvent N\/A An Impala query contains too many joins and recommends denormalizing some tables to reduce the number of joins. Speedup ImpalaUnderestimatedCounfOfRowsEvent N\/A An estimate is outdated and that statistics for the corresponding table should be refreshed. MR Event Category Event Name Parameters Description of When Event Occurs Application Failure MRClassNotFoundEvent N\/A A MR job failed due to a \"class not found\" exception. Application Failure MRFailureCompressLibNotAvailable N\/A A MR job failed due to a \"compression library not available\" exception. Application Failure MRFailureFileNotFoundEvent N\/A A MR job failed due to a \"file not found\" exception. Application Failure MRFailureIllegalArgumentEvent N\/A A MR job failed due to an \"illegal argument\" exception. Application Failure MRFailureNumberFormatEvent N\/A A MR job failed due to a \"number format\" exception. Application Failure MRFailureTimeOutEvent N\/A A MR job failed due to a \"time out\" exception . Application Failure MRGcOverheadLimitExceededMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb hive.auto.convert.join The MR job failed because the GC overhead limit is exceeded on map side. Application Failure MRGcOverheadLimitExceededReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb hive.auto.convert.join A MR job failed because the GC overhead limit is exceeded on reduce side. Application Failure MRJavaOutOfMemoryMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb A MR job failed because it is out of memory on map side. Application Failure MRJavaOutOfMemoryReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb A MR job failed because it is out of memory on reduce side. Informational MRKillFailEvent N\/A MR job was killed or failed with lots of wasted work. Informational MRSuccWithKillFailEvent N\/A MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. Informational MRShuffleBytesEvent N\/A MR job has lots of data shuffle from map to reduce side. Informational MRTimeBreakdownEvent N\/A Identifies where time is spent on for the job and points out significant events. Speedup MRLongTasksFromSlowNodeEvent N\/A MR job has long-running map\/reduce tasks from slow nodes. Speedup MRMapSkewDataIOEvent N\/A The map phase of the MR job has a time skew with strong correlation with IO. Speedup MRReduceSkewDataIOEvent N\/A The reduce phase of the MR job has a time skew with strong correlation with IO. Speedup MRTooFewReduceEvent hive.exec.reducers.bytes.per.reducer The MR job is using too few reducers. Speedup\/ Resource Utilization MRTooLargeMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb Mappers in the MR job are requesting too much memory. Speedup\/ Resource Utilization MRTooLargeReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb Reducers in the MR job are requesting too much memory. Speedup\/ Resource Utilization MRTooManyMapEvent mapreduce.input.fileinputformat.split.maxsize mapreduce.input.fileinputformat.split.minsize The MR job is using too many mappers. Speedup\/ Resource Utilization MRTooManyReduceEvent hive.exec.reducers.bytes.per.reducer The MR job is using too many reducers. Spark Event Category Event Name Parameters Description of When Event Occurs Application Failure DriverOomeEvent spark.driver.memory (As a suggestion) A driver failed with \"OutOfMemory\" error. Application Failure ExecutorOomeEvent spark.executor.memory (As a suggestion) An executor failed with \"OutOfMemory\" error. Application Failure YarnContainerKilledEvent (As a suggestion) spark.driver.memory spark.executor.memory spark.yarn.executor.memoryOverhead There are containers killed by YARN. Resource Utilization ContainerSizingUnderutilizationEvent spark.driver.memory spark.executor.memory spark.executor.cores spark.yarn.executor.memoryOverhead spark.executor.instance spark.default.parallelism spark.sql.shuffle.partitions Container resources are underutilized. Resource Utilization InefficientInputSplitSizeEvent spark.default.parallelism mapreduce.input.fileinputformat.split.minsize An inefficient input split size. Resource Utilization TooFewPartitionsEvent spark.executor.instance spark.default.parallelism spark.sql.shuffle.partitions Indicates that there are too few partitions with respect to available parallelism. Resource Utilization UnderutilizedCpuEvent spark.executor.cores (As a suggestion) There is low utilization of CPU resources. Resource Utilization UnderutilizedNodeMemoryEvent NA There is low utilization of memory resources. Resource Utilization UnderutilizedStorageMemoryEvent NA The Spark storage memory has low utilization. More RDDs can be cached in memory. Speedup CachingOpportunityEvent NA There is an (unused) opportunity for RDD caching. Speedup ContendedCpuEvent NA There is contention for CPU resources. Speedup ExcessiveGcEvent spark.executor.memory (As a suggestion) There is high garbage collection overhead. Speedup ExecutorImbalanceEvent NA There is load imbalance among executors. Speedup ExhaustedStorageEvent NA The Spark storage memory is getting exhausted. Speedup LightExecutorEvent NA There is large idle time for one or several executors. Speedup LongStageEvent NA There is load imbalance among tasks for the longest stage of the application. Speedup ContendedDriverEvent NA The driver is a bottleneck in the application. It monitors data transfers to the driver (i.e., the time spent in fetching the data from executors). Tez In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook Event Category Event Name Parameters Description of When Event Occurs Application Failure TezIllegalArgumentEvent (As a suggestion) tez.runtime.io.sort.mb hive.tez.java.opts Application failed because tez.runtime.io.sort.mb less was set outside the allowed memory limit. Application Failure TezBlockMissingEvent N\/A Indicates that an HDFS disk is missing or corrupted Application Failure TezOutOfMemoryErrorEvent N\/A Application ran out of memory. Application Failure TezUncheckedExceptionEvent N\/A Application failed with some internal error. Resource Utilization\/ Inefficiency TezNoDAGEvent (As a suggestion) tez.am.container.idle.release-timeout-min.millis tez.am.container.idle.release-timeout-max.millis Session was created but no DAG was not submitted. Speedup HiveExecuteParallelEvent hive.exec.parallel Detects that hive.exec.parallel is set to false and that stages in the query could be run in parallel if set to true. Speedup TezNoColStatsEvent N\/A The Hive query accessed tables with missing column statistics and recommends collecting these statistics. Speedup\/ Resource Utilization TezMapVertexTooFewTaskEvent tez.grouping.min-size tez.grouping.max-size TDAG is using too few map tasks. Speedup\/ Resource Utilization TezMapVertexTooManyTaskEvent tez.grouping.min-size tez.grouping.max-size DAG is using too many map tasks. Speedup\/ Resource Utilization TezReduceVertexTooFewTaskEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor DAG is using too few reducer tasks. Speedup\/ Resource Utilization TezReduceVertexTooManyTaskEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor DAG is using too many reducer tasks. Speedup\/ Resource Utilization TezAutoParallelismEvent hive.tez.auto.reducer.parallelism Detects that auto reducer parallelism is not enabled and recommends setting hive.tez.auto.reducer.parallelism to true Speedup\/ Resource Utilization TezJobReducesEvent (As a suggestion) mapreduce.job.reduces mapred.reduce.tasks Detects that a fixed number of tasks are generated for every reducer because mapreduce.job.reduces or mapred.reduce.tasks are manually set, and recommends removing these properties. Speedup\/ Resource Utilization TezReduceVertexTasksReducedEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor A reducer vertex was initiated with too many tasks, and that after sampling the parallelism was auto-reduced. Recommends increasing the value of hive.exec.reducers.bytes.per.reducer. Informational TezHeavyWeightVerticesEvent N\/A The execution of a Tez DAG was dominated by a few vertices. Informational TezVertexTaskTimeSkewEvent N\/A A large variation in task run times occurred for some vertices during the execution of a Tez DAG. Informational TezBreakdownEvent hive.exec.parallel hive.tez.auto.reducer.parallelism Provides a summary of the DAGs that were run inside a Tez session, along with their status and recommendations. Workflow Catogory Name Parameters Description of When Even Occurs Informational WorkflowTimeBreakdownEvent N\/A Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. Informational\/ Application Failure WorkflowGeneralFailureEvent N\/A For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. Informational\/ Resource Utilization WorkflowIRSummaryEvent N\/A Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources. Informational\/ SLA analysis WorkflowDurationAnomalousEvent com.unraveldata.analytics.max.past.samples com.unraveldata.analytics.min.past.samples com.unraveldata.analytics.anomaly.threshold If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. Informational\/ Speedup WorkflowIASummaryEvent N\/A Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time. " }, 
{ "title" : "Auto Actions", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions", 
"snippet" : "Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Running Auto Action Demos Supported cluster metrics...", 
"body" : " Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Running Auto Action Demos Supported cluster metrics " }, 
{ "title" : "Auto Actions Overview", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-auto-actions-overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Auto Actions Overview", 
"snippet" : "Table of Contents Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Addi...", 
"body" : " Table of Contents Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can be set to automatically kill an application or move it to a different queue. The Unravel Server processes auto actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: The conditions which cause a violation, e.g., the number of jobs running, memory used. Prerequisite conditions: Who\/what\/when can cause the violation, e.g., user, applications Defining conditions: for Unravel Server to execute whenever it detects a rule violation. Actions Manage | Auto Actions The auto actions tab provides a quick way to view auto actions and quickly see their status, along with its defined actions and scope. The tab displays all defined auto actions separated into an Active and Inactive list. You enable\/disable by clicking the check box on the left. You can edit ( define new auto actions Hovering over the auto action's name gives you the description which was entered when defining the auto action. Hovering over action or scope glyph brings up its detail. For example, for the active auto action above: rule description: email action: an email is sent to only one (1) person, queue scope: is three queues: The Actions Scope quicktest must Expert Rule quicktest in MR History of Runs By default all actions are off. Possible actions are: Send an Email ( Kill the App ( Move the app to another queue ( Send a Http post ( By default the various scopes apply to all, i.e., all applications and constantly on. The scopes are: User ( Queue ( Cluster ( Application ( Time ( Sustained Violation: This is not shown in the auto actions list. If you have not defined a particular action or scope, i.e., it's using the default, the glyph is grey ( Click on the History of Runs Click on a run's Link Operations Usage Details | Infrastructure Applications | Applications Notifications 'Snoozing' Auto Actions The snooze function prevents automatic actions from repeating during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., the new violation is essentially noise. See Snooze Feature Property Definition Possible Value Default com.unraveldata.auto.action.snooze.period.sec The time repeated violations are be ignored for the violator, i.e., app or user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app Value is in milliseconds. 0: snooze is turned off. > 0: snooze is on, there is no no upper bound. 3,600,000 (1 hour) When you change the snooze time period all applications currently snoozed are reset. Upon next violation the application is \"snoozed\" using new snooze value. To change the snooze time On Unravel Server open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart Limitations Alerting on Running Apps Applications of the following types do not provide any means for real-time alerts, i.e. when in the running state. Once the application has finished the alerts are generated notifying users about policy violations that already occurred. Hive Impala Kill and Move to Queue not Supported The following applications types do not support Kill Move to Queue Hive Impala Workflow Running Duration versus Final Duration Inconsistency Unravel calculates and publishes internally the current duration for applications of the following types in real-time, i.e., when in the running state. Upon the application completion Unravel receives the actual end time and performs the final duration calculation. This can lead to an inconsistency where the duration aggregated and published during the running state is greater than the duration published upon the application'scompletion. Workflow Missing Auto Action Violation Badge () The badge is not displayed for following application types. For example, in the cluster view above Hive Workflow \"Cloud\" Type Setups are not Supported Currently, auto actions are not triggered for applications on a “cloud” type setup (EMR, HDI, Qubole, etc.) of Unravel; therefore the Kill Move to Queue etl Auto Actions' Properties See here " }, 
{ "title" : "Creating Auto Actions", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-creating-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Creating Auto Actions", 
"snippet" : "Table of Contents 1. Select Manage | Auto Actions. 2. Using the Create from Template or Build Rule. 3. Create an auto action. 4. Click on Save Auto Action See here 1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/di...", 
"body" : " Table of Contents 1. Select Manage | Auto Actions. 2. Using the Create from Template or Build Rule. 3. Create an auto action. 4. Click on Save Auto Action See here 1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edit ( To create a new auto action, click either Create from Template, Build Rule, Expert Rule - provides a partially completed template designed for the task. The only ruleset options available are relevant to the task being defined. Fields which you need to fill in are highlighted. Create from Template - provides an empty template, with all the options available. Build Rule - provides only a text box for defining your auto action using JSON. Expert Rule 2. Using the Create from Template or Build Rule. Whether using Create from Template Build Rule The sections are: and Name Description Create from Template Build Rule Expert Rule - The name and description of the auto action. Auto Action Name and Description - The rules for the auto action to alert. Ruleset - The scope of the rule and when\/how it is activated. Options - The actions to be taken when the auto action is triggered. Actions Name and Description The name is mandatory; the description is optional. The name is used by the UI for all auto actions' displays; we recommend using a name which reflects the auto action's purpose. The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name the description is displayed. This example is from Create from Template Build Rule Ruleset At least one rule type ( User Queue, Apps Expert Rule metric type state Expert Rule Build Rule defines a rule \"metric\" \"comparision operator\" \"value\". Metric See Supported Cluster Metrics metrics The comparison value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type mapreduce, yarn, tez, spark, impala, workflow and hive The State new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: Or, And Same and Or work And Same Logical Operator Same Create From Template This template has the Ruleset metric comparison type state Metric Type or State metric Same'd Same Logical Operator Build Rule The Ruleset User Queue, Cluster App Expert Rule Add Queue metric type state Expert Rule must In the example below, Metric Type Queue metric comparison operato type state above , Apps , Same Or And Same Logical Operator Same Or And . Click Close trash Options Define the scope ( User Queue Cluster Application Name Time Sustained Selected options default to All, Time Sustained Violation For Build Rule When using Create from Template Queue . Queue Cluster Application Name the box next to the option name to select it. You can narrow the scope of Check User Queue Cluster Application Name Only Except Only rule Except all but those specified Transform Application Name t Except Add Application except Create from Template All The Time start end click specifies a length of time violation must occur before the auto action is triggered. This allows time for specified by the option to \"self correct\" and lowers the number of false positives The default is zero, i.e., all auto actions are immediately triggered upon violation and the specified action is carried out. You can select Sustained violation minimum maximum sustained mode triggers the action(s) only if this violation was continuously detected for at least the specified period of time. This allows you to suppress triggering of violation actions for “on-offs” and metric spikes. These are normal in multi-tenant cluster environments can retun back to normal operation on their own. If a violation stops before the minimum time period, the clock is reset for that application. For instance, if the minimum time is one (1) hour and the application violates the auto action for 58 minutes and then returns to normal – no action is taken and the time period for that application resets to 0. Minimum sustained mode triggers the action(s) only if this violation is continuously detected for less than the specified period of time. This suppresses the triggering of violations for long running applications and triggers on auto action rule scope on ad-hoc short lived user applications. Maximum Actions Defines the actions to take when the auto action is triggered. and Build Rule Create from Template, Send an email Http Post Post to Slack Move App to Queue Kill App Move Kill App Build Rule Expert Action. Expert Mode Auto Actions and Pagerduty can not You can chose one of more actions. Check the text box to chose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. For Send Email Add Recipient Include Owner For HTTP post Add URL uses the official Slack API available at Post to Slack https:\/\/slack.com\/api\/chat.postMessage public Slack channel, private channel, or direct message\/IM channel, It provides a better integration with Slack Service and allows you to send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: - generic user icon and \"bot\" username test tokens - generic bot icon, with generic \"bot\" username custom bot user token with Slack App user token chat:write:bot - inherits Slack App's icon, with generic \"bot\" username Slack App bot user token or Move app to queue Kill App. The Move App Kill App Kill App Move App Have directly caused the rule violation, and Have allocated resources, i.e. in allocated or running states. is a non-destructive action that should not affect the cluster performance and its availability to the user; however we suggest using it with caution. Move App is a destructive action. It can affect the cluster performance and its availability to the users. This option is primarily to kill rogue applications that are causing contention of a cluster resources. Kill App The Build Rule Expert Rules Both the Email Http Post Cluster View auto action history Sample Email Auto Action policy \"ROGUE APPLICATION #1\" violation detected.\n\nPolicy description: \"Identify applications that are using too much of the cluster resources\"\n\nApplication \"application_1498514199803_2411\" has 1 violation:\n\n1. Sum of memory in MB allocated to containers is 1GB >= 1MB\n\nTimestamp: 07\/19\/2017 08:41:35 +0000\n\nReach Unravel server at http:\/\/localhost:3000\/\n\nSee cluster history at http:\/\/localhost:3000\/ops_dashboard\/charts\/resources?from=1499817549571&to=1499817669570&at=1499817609571&interval=1m Sample Slack Post 3. Create an auto action. Create From Template Click on Create a Template above Resource contention - monitors overall memory or vcore usage on a cluster and number of pending\/running jobs in order to detect when the cluster is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in cluster e - monitors overall memory or vcore usage in a queue and number of pending\/running jobs in order to detect a state when the queue is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in queu Rogue Identification - identifies so-called “rogue” users on a cluster who can potentially affect other users and the cluster as a whole, i.e. users who are submitting jobs that are using too much of the cluster resources (memory or vcores). Once the rogue user is detected the specified auto action(s) are taken. Rogue user - identifies so-called “rogue” applications on a cluster which can potentially affect other applications and the cluster as a whole, i.e. applications that are using too much of the cluster resources (memory or vcores). Once the rogue application the specified auto action(s) are taken. Rogue application - identifies so-called “rogue” impala queries on a cluster which can potentially affect other applications and the cluster as a whole, i.e. queries that are using too much of the cluster resources (memory or vcores). Once the rogue application the specified auto action(s) are taken. Rogue Impala query (HDFS Read\/Write) Long Running Jobs - monitors elapsed time of a running YARN application (i.e., MapReduce, Spark, Hive, etc) and executes the action(s) if the job runs longer than desired. Long running YARN application - tracks Hive jobs by monitoring the total elapsed time of a running Hive query and executes the action(s) if the job runs longer than desired. Long running Hive query - tracks workflow jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long running workflow - tracks impala jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long Impala query Build Rule Click on Build Rule above Expert Rule The Auto Actions engine is capable of much more than is available through the templates. Expert Rule is a very powerful mode giving you total access to all of the Auto Actions engine capabilities. Using the Expert Rule allows you to create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts. JSON is used to define the rules and actions. See the Expert Rule Sample Auto Actions 4. Click on Save Auto Action Your auto action is now listed in the Manage Auto " }, 
{ "title" : "Expert Rule", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-expert-rule.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Expert Rule", 
"snippet" : "Table of Contents Overview Defining the Rule Header Rules: Defining conditions Logical Operators for Evaluating Multiple Rules A Single Rule A Rule Array Options - Policy\/Scope: Prerequiste conditions Options - Policy\/Scope Rule Actions: action(s) to implement upon violation Single Action Multiple A...", 
"body" : " Table of Contents Overview Defining the Rule Header Rules: Defining conditions Logical Operators for Evaluating Multiple Rules A Single Rule A Rule Array Options - Policy\/Scope: Prerequiste conditions Options - Policy\/Scope Rule Actions: action(s) to implement upon violation Single Action Multiple Actions Actions can be Ignored When in Conflict Action(s) Fail if the Required Information is Invalid or not Specified. Send_email An Expert Rule Example Auto Actions Examples Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts and capabilities, along with JSON, which is used to define the auto action. This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule See here In this mode you must specify : boolean conditions that must be met for the Unravel Server to evaluate the Auto Action's defining conditions, e.g., Auto Action should run from 8.00 and 14.00. Prerequisite condition s : boolean conditions that must be met for the Unravel Server to execute the corresponding action, e.g., the application can't run more than 50 mapper tasks. Defining conditions : steps to be taken when the prerequisite and defining conditions evaluate to true, e.g., send an email to admin. Actions When using Expert Rule must rules actions {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} basic auto action information, name, etc., including status (in\/active). Header: : the rules for the scope. You must define at least one rule. Rules who, what, where causes a violation and when. You must specify at least one. Policy\/Scope: Options - actions to perform when a violation triggers the auto action. If none are defined the UI still implements and tracks auto actions. Actions: Defining the Rule Header You must define a header. Attributes Name Definition Possible Value Required Default Value enabled Whether the auto action is active or not. True: active\/enabled. False: inactive\/disabled True | False √ - policy_name Value defined by Unravel. AutoActions2 √ AutoActions2 policy_id Value defined by Unravel. 10 √ 10 instance_id Any unique value √ - name_by_user Any unique string. The name is used when the auto action is displayed in the UI. √ - description_by_user Description of the auto action. - created_by Value defined by Unravel. admin √ admin last_edited_by Value defined by Unravel. admin √ admin created_at Time created. Date and time is in the form of a Epoch\/Unix timestamp. √ - updated_at Time updated. Date and time is in the form of a Epoch\/Unix timestamp. √ - \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: Defining conditions You must define at least one rule. Field Name Definition Possible Values Required\/ Required by Default Value scope The rule scope. app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container ,containers, multi_ containers queue, queues, multi_queue, user, users, multi_user : apps==multi_app, users==multi-user, etc Note √ - target Application name any valid application name when scope is by_name - metric Metric used for comparison. see supported metrics per type - comparison Comparison operator >, >=, ==, <=, < metric - value Value for comparison. The value form varies by metric. number metric - state Scope state new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * - type Job type mapreduce, yarn, tez, spark, workflow, hive - Logical Operators for Evaluating Multiple Rules Operator Condition for a Violation OR At least one rule evaluates to true. AND All rules evaluate to true. SAME All the rules evaluate to true and See Same Logical Operator A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n\n \/\/ at least one of the following\n\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \n \"state\":\"\",\n\n \"type\":\"\"\n }\n] Violation occurs when the application is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] application is a workflow with a duration > 10. Removing Violation occurs when the state \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] application has a duration > 10. Removing Violation occurs when the state type \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n Note is equivalent to the plural of Multi_X X. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n OR Example When they are OR'ed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] AND Example When AND'ed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] SAME Example When SAME'd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the auto action is not triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However if My_App violates both rules (allocatedMB andallocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the auto action is triggered for My_App but not Your_App. OR, AND and SAME Given the same ruleset, evaluation becomes more restrictive. OR: the auto action is triggered if one or more of conditions is true. AND: the auto action is triggered if all of conditions are true. SAME: the auto action is triggered if all of the conditions are true within Options - Policy\/Scope:Prerequiste conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. Field Name Definition Conditions Required\/ Required by Possible Values Default Value X_mode where X The mode defines how the rules are applied to type X 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type X. 2 - the rules are evaluated for only X X 3 - the rules are evaluated for everything but X X By Definition. You must define at least one option\/policy. 0, 1, 2, 3, 0 X_list A list of X type Only applicable if mode is set to 2 (only) or 3 (except). if X X_ empty, single item or comma separated list. - X_transform A list of regex used to generate a list of X Only applicable if mode is set to 2 (only) or 3 (except). if X X empty, single regex or comma separated regex list - Time The daily time the Auto Action is trigger. any time period spanning less than 24 hours. - Sustained Violation Set a minimum or maximum time period for the auto action to be triggered. See here any time period less than 24 hours. - Options - Policy\/Scope Rule where X \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , - does not apply to any clusters. Cluster \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", - applies all queues. Queue \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", - applies to all applications Application Name except \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action(s) to implement upon violation You do not have to define any actions, but it defeats to purpose not to. If no actions are defined, the UI keeps track of the auto action and when triggered, who triggers it. Both the prerequisite defining Field Name Definition Required\/ Required by Possible Value Default Value action The action to be taken. at least one send_email, http_post, post_in_slack, move_to_queue, kill_app - to Email recipients. send_mail if to_owner One or more recipients in a comma delimited list. - to_owner Send email to owner. send_mail if to false: do not send email true: send email false urls URLs for Http post. http_post One or more URLs in a comma delimited list. - token Token generated by slack post_in_slack Slack token - channels Slack channel. post_in_slack One or more channels in a comma delimited list - queue Queue name. move_to_queue The name of a valid queue to move the app to. - Single Action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple Actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] Actions can be Ignored When in Conflict Below we specified two actions, move_to_queue kill_app kill_app move_to_queue actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n] Action(s) Fail if the Required Information is Invalid or not Specified. Below are two actions with invalid information. In send_mail http_post Operations Dashboard , history of runs cluster view \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] Example Actions There are five (5) main actions : send_email http_post post_in_slac , move_to_queue kill_app and send_email, http_post, post_in_slack Creating Auto Actions send_email http_post post_in_slack Note You must take care when entering information. A specified action fails if you enter the incorrect information, i.e., bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template Build Rule to_owner \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] http_post Just like send_email \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma delimited list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] kill_app This is straight forward, but kill_app is a destructive action \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] An Expert Rule Example This auto action triggers on applications using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the applications, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the application is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} Auto Actions Examples See Sample Auto Actions Build Rule " }, 
{ "title" : "Same Logical Operator", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-samerule.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Same Logical Operator", 
"snippet" : "SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[ “SAME”:[ { “scope”:”users”, “metric”:”appCount”, “operator”:”>”, “v...", 
"body" : " SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[\n “SAME”:[\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:10,\n state”:”running”\n },\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:5,\n “state”:”pending”\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and 3 pending apps user B has 7 running and 1 pending apps user C has 21 running and 11 pending apps First, the two simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has 7 → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has 3 → FALSE User B has 1 → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called “linking” of rules (see Ruleset.java). Cluster “User A”, link count = 1. User A > 10 running apps? → TRUE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Cluster “User C”, link count = 2. User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule reset for the group. Cluster “User A” has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE   User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster “User C”, link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User Auto Action, therefore User C triggers the Auto Action and the alert is sent and\/or the actions performed. Comparison to AND Both User A and User C would have triggered the above rule were AND SAME any AND any To achieve the same result as the above example using AND SAME each and every user ( Username AND Username " }, 
{ "title" : "Snooze Feature", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-snooze-feature.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App ...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App An auto action specifying a Kill App or Move App action can not be snoozed. Snooze is set the first time the app violates the rule. The auto action itself continues to run uninterrupted whether zero (0) or all apps currently covered by the auto action are snoozed. The auto action takes action for any app not snoozing If an app is still violating upon awaking, snoozed Example Rule\/Action Two apps Snooze time at 20:00 A > 1GB → email is sent + snooze set (snoozed until 20:30). B < 1GB → application is not violating so nothing is done. at 20:10 A > 1GB → snoozing B > 1GB → email is sent + snooze set (snoozed until 20:40). at 20:20 A > 1GB → snoozing B > 1GB → snoozing at 20:30 A > 1GB → application wakes >B > 1GB → snoozing at 20:40 A > 1GB → snoozing, >B < 1GB → app wakes To change the snooze time On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms. com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Sample Auto Actions", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-sample-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Sample Auto Actions", 
"snippet" : "Table of Contents Sample JSON rules Alert Examples Kill App Example Auto Actions Rules, Predefined Templates v Expert Mode Map Reduce Spark Hive Tez Workflow User Queue Applications Related articles Limitations on auto actions can be found FIX_LINK here FIX_LINK. Information on auto action demos can...", 
"body" : " Table of Contents Sample JSON rules Alert Examples Kill App Example Auto Actions Rules, Predefined Templates v Expert Mode Map Reduce Spark Hive Tez Workflow User Queue Applications Related articles Limitations on auto actions can be found FIX_LINK here FIX_LINK. Information on auto action demos can be found here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Examples When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n >Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. a. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Tez Alert if a Tez query duration is running longer than expected. Check if a Tez query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. a. Check if a Tez query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Tez query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Tez query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Tez query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Tez query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Tez queries, i.e. for example alert if a Tez query has not read lots of data but running for a longer time. Check if any Tez query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n >Workflow Alert if a workflow is taking longer than expected. a. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} b. Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} USER UserAlert for Rogue User - Any user consuming a major portion of cluster resources. a. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Alert for Rogue application - any application which is consuming a major portion of cluster resources. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue. You can use the Rogue Application or memory Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Set Time rule condition as: Set Move app rule as: Any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application or memory Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n }\n or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\":\"allocated_mb\",\n \"compare\": \">\",\n \"value\": [X]\n } Set Queue rule conditions as: Set Move app action as: Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/ug-aa-running-auto-action-demos.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED D...", 
"body" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED DEMO_PATH >Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos The directory should contain the following files. # ls auto-actions-demos\ndemos\/ setup\/ Go to DEMO_PATH setup >Open .\/settings Execute the .\/setup-all # .\/setup-all The auto action rules that include time specification are automatically adjusted to the current time period, e.g., from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Manage Auto Actions You should see all the auto action demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag AA-Spark-1c Map-1b Executing the demos Go to DEMO_PATH demos Manage Auto Actions For example, in the UI the auto action named AA-Spark-1c demo-Spark-1c # cd {DEMO_PATH}\/setup \n# ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute .\/demo-tag AA-tag AA-Spark-1c demo-Spark-1c Some auto action's demo scripts trigger multiple auto actions. This side effect can happen when running your own defined auto actions due to auto actions having overlapping definitions. Cleaning up demos Run .\/clean-all script. # cd {DEMO_PATH}\/setup \n# .\/clean-all This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 Auto Actions demos list Application type Use case Auto Action Triggering Script Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vcores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Will also trigger MR-1a as well. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. AA-MR-2b Demo-MR-2b Will also trigger MR-2a as well. Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. Spark Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark application has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark application allocated vcores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark application is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Alert if a workflow is reading more data than expected. Related articles Page: Auto Actions Overview Page: Running Auto Action Demos Page: Snooze Feature Page: Sample Auto Actions Page: Setting Up Email for Auto Actions and Collaboration " }, 
{ "title" : "Supported cluster metrics", 
"url" : "unravel-4-5/uguide/ug-aa-auto-actions/supported-cluster-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Auto Actions \/ Supported cluster metrics", 
"snippet" : "Table of Contents Hive\/Workflow Metrics MapReduce Application Master Metrics MapReduce Metrics MapReduce Counters File System Counters Job Counters File Input\/Output Format Counters Map-Reduce Framework Counters Shuffle Errors Spark Metrics YARN Resource Manager metrics Auto Actions by default colle...", 
"body" : " Table of Contents Hive\/Workflow Metrics MapReduce Application Master Metrics MapReduce Metrics MapReduce Counters File System Counters Job Counters File Input\/Output Format Counters Map-Reduce Framework Counters Shuffle Errors Spark Metrics YARN Resource Manager metrics Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on all live running Monitoring is only Hive\/Workflow Metrics Metric Definition duration total time taken by the application totalDfsBytesRead total hdfs bytes read totalDfsBytesWritten total hdfs bytes written stevev mun MapReduceApplication Master Metrics MapReduce Metrics Type Metric Definition elapsedAppTime time since the application was started Map mapsCompleted number of completed maps mapsPending number of maps still to be run mapsRunning number of running maps mapsTotal total number of maps Map Attempts failedMapAttempts number of failed map attempts killedMapAttempts number of killed map attempts newMapAttempts number of new map attempts runningMapAttempts number of running map attempts Reduce reducesCompleted number of completed reduces reducesPending number of reduces still to be run reducesRunning number of running reduces reducesTotal total number of reduces Reduce Attempts failedReduceAttempts number of failed reduce attempts killedReduceAttempts number of killed reduce attempts newReduceAttempts number of new reduce attempts runningReduceAttempts number of running reduce attempts successfulReduceAttempts number of successful reduce attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduceCounters File System Counters Metric Definitions fileBytesRead mount of data read from local file system fileBytesWritten amount of data written to local file system fileReadOps number of read operations from local file system fileLargeReadOps number of read operations of large files from local file system fileWriteOps number of write operations from local file system hdfsBytesRead amount of data read from HDFS hdfsBytesWritten amount of data written to HDFS hdfsReadOps number of read operations from HDFS hdfsLargeReadOps number of read operations of large files from HDFS hdfsWriteOps number of write operations to HDFS Job Counters Type Metric Definition Map dataLocalMaps number of map tasks which were launched on the nodes containing required data mbMillisMaps total megabyte-seconds taken by all map tasks millisMaps total time spent by all map tasks slotsMillisMaps total time spent by all executing maps in occupied slots vcoresMillisMaps total vcore-seconds taken by all map tasks Reduce mbMillisReduces total megabyte-seconds taken by all reduce tasks millisReduces total time spent by all reduce tasks slotsMillisReduces total time spent by all executing reduces in occupied slots totalLaunchedReduces total number of launched reduce tasks vcoresMillisReduces total vcore-seconds taken by all reduce tasks File Input\/Output Format Counters Metric Definition bytesRead amount of data read by every tasks for every filesystem bytesWritten amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Map-Reduce Framework Counters Type Metric Definition Map failedShuffle total number of mappers which failed to undergo through shuffle phase mapInputRecords total number of records processed by all of the mappers mapOutputBytes total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes amount of (compressed) data which was actually written to disk mapOutputRecords total number of records produced by by all of the mappers mergedMapOutputs total number of mapper output files undergone through shuffle phase shuffledMaps total number of mappers which undergone through shuffle phase Reduce reduceInputGroups total number of unique keys reduceInputRecords total number of records processed by all reducers reduceOutputRecords total number of records produced by all reducers reduceShuffleBytes amount of data processed in shuffle and reduce phase Records combineInputRecords total number of records processed by combiners combineOutputRecords total number of records produced by combiners spilledRecords total number of map and reduce records that were spilled to disk Time gcTimeMillis wall time spent in Java Garbage Collection cpuMilliseconds cumulative CPU time for all tasks Memory committedHeapBytes total amount of memory available for JVM physicalMemoryBytes total physical memory used by all tasks including spilled data splitRawBytes amount of data consumed for metadata representation during splits virtualMemoryBytes total virtual memory used by all tasks Shuffle Errors Metric Definition badId total number of errors related with the interpretations of IDs from shuffle headers connection total number of established network connections ioError total number of errors related with reading and writing intermediate data wrongLength total number of errors related to compression and decompression of intermediate data wrongMap total number of errors related to duplication of the mapper output data wrongReduce total number of errors related to the attempts of shuffling data for wrong reducer Spark Metrics In addition to the metric set supported by MapReduce applications, Spark applications can be polled on: Join joinInputRowCount the total input rows of the first join of the SQL query, aggregated for all the queries that are part of the application totalJoinInputRowCount total number of input rows count for all join operators of all SQL queries that are part of the application totalJoinOutputRowCount total number of output rows count for all join operators of all SQL queries that are part of the application joinOutputRowCount the total output rows of the first join of the SQL query, aggregated for all the queries that are part of the application Partitions inputPartitions total number of input partitions for all SQL queries that are part of the application outputPartitions total number of output partitions for all SQL queries that are part of the application Records inputRecords cumulative number of input records for all SQL queries that are part of the application (collected at stage level) outputRecords cumulative number of output records for all SQL queries that are part of the application (collected at stage level) outputToInputRecordsRatio outputRecords \/ inputRecords if inputRecords > 0, else 0 YARN Resource Manager metrics allocatedMB The sum of memory in MB allocated to the application’s running containers allocatedVCores The sum of virtual cores allocated to the application’s running containers appCount total number of applications elapsedTime The elapsed time since the application started (in ms) runningContainers The number of containers currently running for the application memorySeconds The amount of memory the application has allocated (megabyte-seconds) vcoreSeconds The amount of CPU resources the application has allocated (virtual core-seconds) For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-5/uguide/ug-uc-use-cases.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-5/uguide/ug-uc-use-cases/ug-uc-identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-5/uguide/ug-uc-use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "unravel-4-5/uguide/ug-uc-use-cases/ug-uc-optimizing-the-performance-of-spark-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web...", 
"body" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Kafka Insights", 
"url" : "unravel-4-5/uguide/ug-uc-use-cases/ug-uc-kafka-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, : commit offset is in pace with the log end offset. OK : the Consumer is lagging behind the Producer. Both the consumer's and log commit offsets are increasing but the Producer’s is increasing faster. When graphed over time, the slope of the Producer is greater than the slope of the consumer. Lagging :the Consumer has essentially stopped while the Producer is still active. The consumer's offset is not increasing while the log commit offset is increasing. Graphically, the slope of the consumer is essentially zero. Stalled A Topic's status is set to the lowest status among its Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Example 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Note: Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric , offset test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI’s. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-5/uguide/resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory avail...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory available for launching new processes avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations committedHeap bytes Committed heap size committedNonHeap bytes Committed non-heap size committedVirtualMemory bytes The committed virtual memory in the operating system currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems edenPeakUsage bytes Maximum memory usage in the eden space freePhysicalMemory bytes The free physical memory in the operating system freeSwap bytes The free swap size fullGcCount COUNT Number of full GC runs fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms initHeap bytes Initial heap size initNonHeap bytes Initial non-heap size maxHeap bytes Maximum heap size maxNonHeap bytes Maximum non-heap size minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC oldPeakUsage bytes Maximum memory usage in the old space processCpuLoad PERCENT Average process CPU load for the last minute (all cores) snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started survivorPeakUsage bytes Maximum memory usage in the survivor space systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) totalPhysicalMemory bytes The total physical memory in the operating system totalSwap bytes The total swap size usedHeap bytes Used heap size usedNonHeap bytes Used non-heap size vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "unravel-4-5/uguide/resource-metrics/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ User Guide \/ Resource Metrics \/ Some Keywords and Error Messages", 
"snippet" : "Table of Contents Spark Keywords Spark Error Messages MapReduce\/Hive Keywords Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client...", 
"body" : " Table of Contents Spark Keywords Spark Error Messages MapReduce\/Hive Keywords Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application on a worker node Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset spark.default.parallelism Default number of partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.executor.memory Related to executor memory spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.sql.shuffle.partitions Number of SparkSQL partitions spark.yarn.executor.memoryOverhead YARN memory overhead SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SparkConf Spark configuration object SQLContext Main Spark SQL entry point StreamingContext Main Spark Streaming entry point Spark Error Messages Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. MapReduce\/Hive Keywords Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-5/advanced-topics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics", 
"snippet" : "Backing-up, Disaster Recovery and Reverting to Prior Version Backing-up, Disaster Recovery, and Reverting to Prior Version copy 1 Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Ser...", 
"body" : " Backing-up, Disaster Recovery and Reverting to Prior Version Backing-up, Disaster Recovery, and Reverting to Prior Version copy 1 Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Enabling or Disabling Cluster Optimization Reports Enabling or Disabling Small Files Reports and Files Reports Enabling or Disabling Cloud Reports and Forecasting Reports Enabling or Disabling Queue Analysis Reports Enabling or Disabling Sessions Triggering an import of FSImage Configure JVM Sensor CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) Connectivity Connecting to\/Configuration of a Kafka Stream Hive Metastore Access Connecting to a Hive Metastore Hive Metastore Configuration How to Write Jolokia JMX MBean Deploying Unravel over SELinux Monitoring Workflows Monitoring Oozie Workflows Monitoring Airflow Workflows Roles and Role Based Access Control RBAC Roles Configure RBAC Configure LDAP or SAML RBAC Properties Creating Read-only Admins Example RBAC Configurations RBAC UI Manage Page Running Verification Scripts and Benchmarks Tagging What is Tagging? Tagging Workflows Tagging a Hive on Tez Query Tagging Applications APIs REST API Sign In Apps APIs Auto Actions APIs Applications APIs Data APIs Operational APIs Reports APIs Workflow APIs Use Case - Auto Actions and Pagerduty Unravel Monitoring Service Configuration Disk Monitoring JMX Client Monitors REST API Unravel Properties Configurations for OnDemand Reports Sensors Unravel Servers and Sensors Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-5/advanced-topics/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster Wide Report", 
"url" : "unravel-4-5/advanced-topics/cluster-wide-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's curren...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of these recommendations revolve around these parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "unravel-4-5/advanced-topics/cluster-wide-report.html#UUID-3b31a398-1e68-ff7b-3ff4-39555bca00d8_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSe...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/ usr\/local\/unravel\/install_bin\/ClusterReport. cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz cd dbin Input.txt. # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report # su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "unravel-4-5/advanced-topics/configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations", 
"snippet" : "Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User ...", 
"body" : " Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Enabling or Disabling Cluster Optimization Reports Enabling or Disabling Small Files Reports and Files Reports Enabling or Disabling Cloud Reports and Forecasting Reports Enabling or Disabling Queue Analysis Reports Enabling or Disabling Sessions Triggering an import of FSImage " }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time...", 
"body" : " Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source " }, 
{ "title" : "Configure Permission for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/configure-permission-for-unravel-daemons-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configure Permission for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"snippet" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user access Purpose hdfs:\/\/user\/un...", 
"body" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt command login as admin user psql cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/configuring-multiple-hosts-for-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED must be a fully qualified DNS or IP address UNRAVEL_HOST_IP Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc Port 4043 unravel_lr If you do not use an external database (db), unravel_db unravel_db is identical on all Unravel hosts in the ensemble. \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig Stop Unravel Server On each Unravel host, run this command: # sudo \/etc\/init.d\/unravel_all.sh stop Modify unravel.properties on host1. Pick a machine to be host1, where the Unravel Web UI will run. If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties must be a fully qualified DNS or IP address. Replace UNRAVEL_HOST_IP 3316 unravel_mysql_prod To find your fully qualified hostname # {hostname} -I unravel.jdbc.url=jdbc:mysql:\/\/{UNRAVEL_HOST_IP}:{3316}\/{unravel_mysql_prod} Copy host1's unravel.properties to other hosts. Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh \/etc\/unravel_ctl # scp \/usr\/local\/unravel\/etc\/unravel.properties {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/usr\/local\/unravel\/etc\/unravel.ext.sh {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/etc\/unravel_ctl {host2}:\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel \/etc\/unravel_ctl root:root. The scripts invoked below will make an identical change to the unravel.properties Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id.properties For a 2-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh host1 host2 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh host1 host2 For a 3-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh host1 host2 host3 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh {host1} {host2} {host3} on host3 #sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh {host1} {host2} {host3} Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host2 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host3 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host2 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host3 # sudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15 seconds for Zookeeper quorum to settle: # sleep 15 Start up Kafka ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-k on host2 # sudo \/etc\/init.d\/unravel_all.sh start-k on host3 # sudo \/etc\/init.d\/unravel_all.sh start-k Wait 10 seconds for Kafka coordination: # sleep 10 Create the Kafka topics (only on one host): on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh Start Unravel Server Finish multi-host installation by starting up Unravel Server: on host1 # sudo \/etc\/init.d\/unravel_all.sh start \necho \"http:\/\/{UNRAVEL_HOST_IP}:3000\/\" on host2 # sudo \/etc\/init.d\/unravel_all.sh start Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 Snapshot unravel.properties as new golden file " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # sudo chkconfig ...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 \n# sudo chkconfig --add unravel_ma_2 20000-30000 jobs per day # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3\n# sudo chkconfig --add unravel_ma_3 Greater than than 30000 jobs per day # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4\n# sudo chkconfig --add unravel_ma_4 Start Unravel Server. Start the workers' daemons. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Custom Banner", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/custom-banner.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Custom Banner", 
"snippet" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner. Property Description Set By User Unit Default prepend display Displays a ...", 
"body" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner. \n \n \n Property \n Description \n Set By User \n Unit \n Default \n \n \n prepend \n \n display \n Displays a banner at the top of the Unravel UI. \n true text end.date \n false \n bool \n false \n \n text \n Text to display when display true The text end.date \n Opt \n string \n \n - \n \n end.date \n Date and Time to stop displaying the custom banner. There is no date\/time limit. Format: YYYYMMDD T Z The text end.date \n Opt \n string \n - " }, 
{ "title" : "Defining a Custom Web UI Port", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/defining-a-custom-web-ui-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom Web UI Port", 
"snippet" : "These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT=18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart...", 
"body" : " These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT=18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Email Alerts", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/email-alerts.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Email Alerts", 
"snippet" : "Configure the following properties to set up email alerts. For a complete listing of email properties see here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.do...", 
"body" : "Configure the following properties to set up email alerts. For a complete listing of email properties see here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.domain Default email domain used for email alerts. Note: This is required for Auto Action email alerts. - com.unraveldata.login.admins Comma separated list of email recipients. A Recipient can be defined by complete email address or by email local-part, e.g., admin,support@ unraveldata.com When no email domain is specified then default domain is used. admin mail.smtp2.from Used for email \"from\" and \"reply-to\" headers when at least one email recipient has unraveldata.com unravel.noreply@unraveldata.com mail.smtp.from Used for email \"from\" and \"reply-to\" headers when no email recipienthas unraveldata.com Note: This is required for Auto Action email alerts unravel.noreply@unraveldata.com " }, 
{ "title" : "HBASE Configuration", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/hbase-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ HBASE Configuration", 
"snippet" : "Please see here Edit \/usr\/local\/unravel\/etc\/unravel.propertiesand define the following properties. If a property is not found, add it, be sure to substitute your local values for the bracketed text red here AMBARI and CDH must com.unraveldata.hbase.source.type=hbase_source_type com.unraveldata.hbase...", 
"body" : "Please see here Edit \/usr\/local\/unravel\/etc\/unravel.propertiesand define the following properties. If a property is not found, add it, be sure to substitute your local values for the bracketed text red here AMBARI and CDH must com.unraveldata.hbase.source.type=hbase_source_type com.unraveldata.hbase.rest.url=Ambari_or_Cloudera_base_urlcom.unraveldata.hbase.rest.user=hbase_rest_usernamecom.unraveldata.hbase.rest.pwd=hbase_rest_password com.unraveldata.hbase.clusters=comma_separated_cluster_names You can optionally com.unraveldata.hbase.service.name=comma_separated_clustername_and_servicename com.unraveldata.hbase.rest.ssl.enabled=true_or_false com.unraveldata.hbase.master.port=Port# com.unraveldata.hbase.regionserver.port=Port# com.unraveldata.hbase.metric.poll.interval=Seconds com.unraveldata.hbase.http.conn.timeout=Seconds com.unraveldata.hbase.http.read.timeout=Seconds com.unraveldata.hbase.http.poll.parallelism=Number com.unraveldata.hbase.alert.average.threshold=Number JMX must com.unraveldata.hbase.source.type=JMX com.unraveldata.hbase.clusters=comma_separated_cluster_names com.unraveldata.hbase.cluster_name.node.http.apis=comma_separated_base_node_http_api You can optionally com.unraveldata.hbase.metric.poll.interval=Seconds com.unraveldata.hbase.http.conn.timeout=Seconds com.unraveldata.hbase.http.read.timeout=Seconds com.unraveldata.hbase.http.poll.parallelism=Number com.unraveldata.hbase.alert.average.threshold=Number Restart the HBase service unravel_us_1. # sudo service unravel_us_1 restart Verifyunravel_us_1is running. # sudo service unravel_us_1 status unravel_us_1 is running Verify themetrics are collected in Elasticsearch. A separate index file is created for each week with an alias hb-search,e.g., hb-20180813_19. Troubleshooting Ifunravel_us_1is not running check the logs for any errors. \/usr\/local\/unravel\/logs\/unravel_us_1.out \/usr\/local\/unravel\/logs\/unravel_us_1.log If hb-* index is not created or no data in Elasticsearch, verify the following daemons are running. unravel_us_1 unravel_s_1 unravel_hl unravel_k_1 Resources Service name unravel_us_1 Service logs \/usr\/local\/unravel\/logs\/unravel_us_1.log, \/usr\/local\/unravel\/logs\/unravel_us_1.out Configuration file \/usr\/local\/unravel\/etc\/unravel.properties Elasticsearch: Template File \/usr\/local\/unravel\/etc\/template_hbase_metrics.json Index Name hb-* " }, 
{ "title" : "Run Unravel Daemons with Custom User", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/run-unravel-daemons-with-custom-user.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Run Unravel Daemons with Custom User", 
"snippet" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to...", 
"body" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to Switch User Run the following command to switch running Unravel daemons to user {USER} and with group {GROUP}. Replace both with valid names, without the curly braces; see the scenarios below. # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh {USER} {GROUP} Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\" foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\" hdfs hadoop Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh \/etc\/unravel_ctl RUN_AS USE_GROUP HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL env vars are removed from \/usr\/local\/unravel\/etc\/unravel.ext.sh \/usr\/local\/unravel\/ \/srv\/unravel\/* RUN_AS: \/srv\/unravel\/tmp_hdfs\/ logs in \/srv\/unravel\/log_hdfs \/usr\/local\/unravel\/logs \/srv\/unravel\/log_hdfs the umask of the run-as the chmod bits of \/usr\/local\/unravel \/srv\/unravel " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job ...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space; you can store approximately 1000 jobs per 1GB of disk. Open \/usr\/local\/unravel\/etc\/unravel.properties file. # vi \/usr\/local\/unravel\/etc\/unravel.properties Search for and set the following properties. If not found, add the properties. com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks com.unraveldata.retention.max.days After changing any of the properties, restart Unravel for the change to take effect. # sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Spark Properties for Spark Worker daemon @ Unravel", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/spark-properties-for-spark-worker-daemon---unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel", 
"snippet" : "Table of Contents Live Pipeline Event log processing Executor log processing Tagging Events Related Other Properties S3 specific properties EMR\/HDInsight specific properties Block storage specific properties (for HDInsight) Data Lake (ADL) specific data properties Live Pipeline Property Definition D...", 
"body" : " Table of Contents Live Pipeline Event log processing Executor log processing Tagging Events Related Other Properties S3 specific properties EMR\/HDInsight specific properties Block storage specific properties (for HDInsight) Data Lake (ADL) specific data properties Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:,,.lz4,.snappy,.inprogress (This makes sure that uncompressed, lz4 and snappy compressed and inprogress event log files are processed) com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. 1000 Executor log processing Property Definition Default com.unraveldata.job.collector.log.aggregation.base Base path to look for aggregated executor logs Note { yarn.nodemanager.remote-app-log-dir}\/*\/{ yarn.nodemanager.remote-app-log-dir-suffix \/tmp\/logs\/*\/logs\/ \"*\" is replaced by the user running the application com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) 60000 (10 mins) com.unraveldata.min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds) 6000 (1 min) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs) Valid Options yarn Tagging Property Definition Default com.unraveldata.tagging.enabled Enables tagging functionality. True com.unraveldata.tagging.script.enabled Enables tagging. False com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g. , \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_ke aws_secret_access_key access_key\/secretKey - EMR\/HDInsight specific properties Property Definition Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False True The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel Properties' Values in Microsoft Azure Block storage specific properties (for HDInsight) For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property Definition Default com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Data Lake (ADL) specific data properties Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster “spk21utj02” has been given access to “\/clusters\/spk21utj02” on Data Lake store. retrieve from Microsoft Azure " }, 
{ "title" : "Using the Impala Daemon (impalad) as a Data Source", 
"url" : "unravel-4-5/advanced-topics/configurations/custom-configurations/using-the-impala-daemon--impalad--as-a-data-source.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Using the Impala Daemon (impalad) as a Data Source", 
"snippet" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics...", 
"body" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html CM as the data source Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM On Unravel Server, add the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port For example: com.unraveldata.data.source=impalad com.unraveldata.impalad.nodes=IP:port,IP:port,IP:port Change the Impala Lookback Window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us References https:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Security Configurations", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabl...", 
"body" : " Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel " }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi. # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins. For example, com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui service. # ...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi. # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins. For example, com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui service. # sudo service unravel_ngui restart " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/adding-ssl-and-tls-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure ...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure cookies are NOT supported when using this Apache2 reverse-proxy method, see instead Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4 and support listening on port 443. Install needed packages. # sudo yum install httpd mod_ssl Note: There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf. Use the following as a model (replace unravelhost_FQDN and settings for SSLCertificate* with values appropriate for your installation). <VirtualHost *:80> \n ServerName unravelhost_FQDN \n Redirect permanent \/ https:\/\/ unravelhost_FQDN \n<\/VirtualHost> <VirtualHost *:443>\n\n DocumentRoot \/var\/www\/html\n ServerName unravelhost_FQDN\n # use this if http to https errors #RequestHeader set X-FORWARDED-PROTO 'https'\n\n SSLEngine on \n SSLCertificateFile \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt \n SSLCertificateKeyFile \/etc\/certs\/wildcard_unravelhost_RSA_private.key \n SSLCertificateChainFile \/etc\/certs\/IntermediateCA.crt\n\n # set this off for reverse proxy security \n ProxyRequests Off \n # might be helpful in logs \n ProxyPreserveHost On \n ProxyPass \/ http:\/\/localhost:3000\/ connectiontimeout=180 timeout=180 \n ProxyPassReverse \/ http:\/\/localhost:3000\/ \/ <Location \/> \n Order deny,allow \n Deny from all \n Allow from all \n <\/Location> <\/VirtualHost> Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties. (No trailing slash. :port is optional). com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart theunravel_tcdaemon. # sudo service unravel_ngui restart Start thehttp daemon. # sudo service httpd start Visithttps:\/\/unravelhost_FQDN(using value appropriate for your site) to test access. Troubleshooting To enable verbose logging in Apache2, addLogLevel where LogLevel can be set to debug, trace1,..., trace8. LogLevel debug Note: Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ Add the following line after the ServerName RequestHeader set X-FORWARDED-PROTO 'https' Restart apache2. " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/alternate-kerberos-principal-for-cluster-access-on-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in part 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in part 2 of the installation guide. HIGHLIGHTED Check HDFS Default umask. For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see sub-directories and read files. In Cloudera Manager check the value of dfs.umaskmode hdfs-site.xml fs.permissions.umask-mode Enable ACL Inheritance. In Cloudera Manager, HDFS Configuration, search for \"namenode advanced configuration snippet\", and set dfs.namenode.posix.acl.inheritance.enabled https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 Restart cluster. When you are ready, restart the cluster to effect the change of dfs.namenode.posix.acl.inheritance.enabled Change ACL of Target HDFS directories. Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented): Set ACL for future directories. The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse Please make sure you set the permissions at the \/user\/history \/user\/history \/user\/history\/done Set ACL for existing directories # hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target user ( unravel hdfs mapr KEYTAB_FILE PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n " }, 
{ "title" : "Configure HDFS Permission for Unravel on CDH Sentry-Secured Cluster", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/configure-hdfs-permission-for-unravel-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure HDFS Permission for Unravel on CDH Sentry-Secured Cluster", 
"snippet" : "The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_...", 
"body" : " The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) To configure HDFS ACL permission for the folders above, see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH, see Cloudera Documentation here " }, 
{ "title" : "Configure Hive Metastore Permissions", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/configure-hive-metastore-permissions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Hive Metastore Permissions", 
"snippet" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metast...", 
"body" : " HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. For TLS-secured CM # curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/ $ClusterName For non-TLS secured CM # curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/ $ClusterName SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE Metastore Access Information See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Disabling Browser Telemetry", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/disabling-browser-telemetry.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Browser Telemetry", 
"snippet" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/u...", 
"body" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Set com.unraveldata.do.not.track false If this property isn't in the file, add it and set it to false com.unraveldata.do.not.track=false Restart the Unravel UI. # sudo service unravel_ngui restart Re-Enable Mixpanel Follow the above steps but in step 2 set com.unraveldata.do.not.track true " }, 
{ "title" : "Disabling Support\/Comments Panel", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/disabling-support-comments-panel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Support\/Comments Panel", 
"snippet" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel...", 
"body" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi. # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled. If not found add the property. Set it to false. com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. # sudo service unravel_ngui restart Your title bar should be missing the support button like below. . Show\/Re-enable Support Button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled true unravel.properties Pop-up Support Box " }, 
{ "title" : "Enabling LDAP Authentication for Unravel UI", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/enabling-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling LDAP Authentication for Unravel UI", 
"snippet" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3...", 
"body" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3.1.2, the property com.unraveldata.ldap.use_jndi true If you used com.unraveldata.ldap.search_bind_authentication If you set the bind_dn bind_pw Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties For MYDOMAIN QA.EXAMPLE.COM If SSL is in use: Use ldaps:\/\/ LDAP_HOST ldap:\/\/ LDAP_HOST Put a truststore unravel\/jre\/ unravel\/etc\/unravel.ext.sh You can append a port number, if needed; for example, ldap:\/\/ LDAP_HOST Knowing your precise DN is essential for some configuration examples below. If you are uncertain about what a normal DN is, use ldapsearch If you use AD, you can find the Windows domain using ldapsearch @ userPrincipalName To debug, use the property com.unraveldata.ldap.verbose null= Active Directory Domain Setup This is the simplest to implement and enables the widest access to Unravel Server. On the login page, if the user enters a login name with has no domain name appended --like thisUser @ MYDOMAIN @ MYDOMAIN The login name can appear in AD or be seen in an ldapsearch userPrincipalName sAMAccountName @ MYDOMAIN For MYDOMAIN com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST You can combine the optional property hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern Example 1: Active Directory with Base DN Defined This example works by composing a DN from the entered user account name in the web UI. For a user login foo uid=foo,ou=myou,dc=domain,dc=com guidKey uid sAMAccountName com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldaps:\/\/ LDAP_HOST An optional hive.server2.authentication.ldap.userFilter Example 2: Active Directory with User Pattern Here we match users with one or more colon-separated patterns, combined with an optional inclusion filter ( userFilter name@MYDOMAIN %s Multiple userDNPatterns can be defined by using a colon ':' separator. The userFilter userDNPattern userDNPattern LDAP_HOST CN=%s,CN=Users,DC=domain DC=com =%s uid com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST Example 3: Active Directory with Group Pattern Here we match users with one or more patterns, and verify group membership in order to approve login. In this example, if the user enters name@MYDOMAIN %s Multiple groupDNPattern : groupDNPattern groupFilter :memberOf=CN= mygroup mygroup myou subdomain domain com Spaces (' ') are significant in the Unravel web login form. Use values relevant to your installation for LDAP_HOST %s myou subdomain com %s uid UNRAVEL_GROUP com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST For Open LDAP LDAP Example 1 Use values relevant to your installation for LDAP_HOST myunit example com uid An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST LDAP Example 2 Below is a typical DN to be uid=%s,ou=myunit,dc=example,dc=com %s cn uid You can specify multiple userDNPattern : hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST Optional Master Bind Account When user accounts cannot do an LDAP search, you can add the properties below to specify a search account. This can be combined with any of the examples above. The password can be encrypted com.unraveldata.ldap.bind_dn=CN=unravel,ou=myunit,dc=example,dc=com \ncom.unraveldata.ldap.bind_pw= bigsecret Web UI Login Syntax In most cases, people want to enter a simple login account name on the Unravel login page, and the instructions here reflect that. However, if @ name windowsDomain The name field of the login page allows spaces to be entered. That way a name like john smith Restart unravel_ngui. # sudo \/etc\/init.d\/unravel_ngui restart Advanced Properties and Details These properties narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The Unravel login process is described next. Property names are shortened for readability, but full names should be used. The SIMPLE LDAP authentication mechanism is used. Authentication Process If a bind_dn and bind_pw are set: Authentication starts out by binding with the given account. If the bind works, the verbose log contains Connected using bindDN inetOrgPerson sAMAccountName guidKey guidKey guid userPassword pw is a match inetOrgPerson userDNPattern guidKey If Windows domain is set: Bind as username + at sign + Windows domain, using the given password. The verbose log contains Connecting Connected using principal unravel_ngui.log userDNPattern guidKey Connected using DN Authorization Process These extra authorization steps are optional to further narrow who can access Unravel Server. The searches will be done using a bindDN account, if specified, otherwise the user account bind is used. If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful. If there are no results from the custom query, then login fails. Each search result will be matched with the simple name entered on the login page U. ser or group filters are ignored if a custom query is specified. If a user filter is specified, it is matched with the simple name entered on the login page. No additional LDAP search is done in this case. This allows access to Unravel to be controlled with an explicit list in Unravel. If a group pattern or filter is specified, it is checked. A query is made to find the groups to which a user belongs. The user membership list is scanned and if one of the groups is in the specified list of allowed groups, then this authorization step succeeds. The verbose log contains the resulting list and the match arguments in effect under Checking group Property Description Example Value or hive.server2.authentication.ldap.baseDN com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) or hive.server2.authentication.ldap.guidKey com.unraveldata.ldap.user_name_attr LDAP attribute name whose values are unique in this LDAP server. Default is uid uid or CN or sAMAccountName hive.server2.authentication.ldap.groupDNPattern Colon-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter Comma-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern Colon-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter Comma-separated list of LDAP usernames (just short names, not full DNs). , hiveuser impalauser hiveadmin hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. , member uniqueMember memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. , group groupOfNames groupOfUniqueNames or hive.server2.authentication.ldap.url com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST1 ldaps:\/\/ LDAP_HOST2 com.unraveldata.ldap.verbose Enables verbose logging. Search for Ldap \/usr\/local\/unravel\/logs\/unravel_ngui.log Can be true or false or not set; default is false. " }, 
{ "title" : "Enabling SAML Authentication for Unravel Web UI", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/enabling-saml-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling SAML Authentication for Unravel Web UI", 
"snippet" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configu...", 
"body" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configure LDAP or SAML RBAC Properties Edit saml.config.json file Property Description Req Example Values entryPoint Identity provider entrypoint, Ping IdP address (SSO URL) Note: Identity provider entrypoint is required to be spec-compliant when the request is signed. Yes \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SSOService.php\" issuer Name of app that will connect to the saml server. Issuer string to supply to identity provider(Environment name). Should match the name configured in Idp. Yes \"unravel-myHost” cert IDP's public cert to validate auth response signature Note: You retrieve this from saml host. Yes Idp Cert String logoutUrl Base address to call with logout requests Default:entryPoint No \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\" logoutEnabled If true logs you out from every app. No false unravel_mapping Mapping saml auth response attributes to Unravel attributes. Yes { \"username\":\"userid\", \"groups\":\"ds_groups\" } privateCert Unravel private cert string to sign Auth requests. No Unravel cert string Click here to see a saml.json example. { \"entryPoint\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\", \"issuer\":\"localhost\", \"logoutUrl\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\", \/\/ generate by saml host \"cert\":\"MIIDXTCCAkWgAwIBAgIJALmVVuDWu4NYMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEwHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdAcQf2CGAaVfwTTfSlzNLsF2lW\/ly7yapFzlYSJLGoVE+OHEu8g5SlNACUEfkXw+5Eghh+KzlIN7R6Q7r2ixWNFBC\/jWf7NKUfJyX8qIG5md1YUeT6GBW9Bm2\/1\/RiO24JTaYlfLdKK9TYb8sG5B+OLab2DImG99CJ25RkAcSobWNF5zD0O6lgOo3cEdB\/ksCq3hmtlC\/DlLZ\/D8CJ+7VuZnS1rR2naQ==\", \"privateCert\":\"-----BEGIN PRIVATE \/\/ generated by unravel node KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDEt4Ma2k4DUkoW\\nG9QDHUBnY7S\/iS\/+u2BjPZqUG2JktzYZl30J05zA6i642i2VDn8eUIPHqt2Hw249\\nZ3nHKL4YnBVqa3yTfEkdMB\/6GSAkoCbnufaD3IsGcFJnlW5raDiT\/GZMy+1WnDfJ\\npB0\/.......vD8kRkcmEi9t3KLmKVy3SO15\/YHAhLxP9oTnTFGkPnIqZLRM0Y55UfwbRSZDlgH\/\\ny9GGmsV5IaIwhepuALJMdkHp\\n-----END PRIVATE KEY-----\\n\", \"unravel_mapping\": { \"username\":\"userid\", \"groups\":\"ds_groups\" } } For Ping, the IdP certificate can be obtained as follows: In the Server Configuration Certificate Management and Digital Signing & XML Decryption Keys & Certificates. Click Export Select Certificate Only Next Click Export Configure SAML Server Configure the following properties on the SAML server. Replace UNRAVEL_HOST Property Description Req PingFederate Specific configuration AssertionConsumerService \/ ACS Url http(s):\/\/ UNRAVEL_HOST Yes https:\/\/docs.pingidentity.com\/bundle\/p1_enterpriseEditAnApplication_cas\/page\/p1_t_EditASAMLApplication.html Entity Identifier unravel-Congo24 Yes Should be same as the issuer in saml.json Single Logout Endpoint http:\/\/ UNRAVEL_HOST Single Logout Response Endpoint http:\/\/ UNRAVEL_HOST No " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/enabling-tls-to-unravel-web-ui-directly.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port ...", 
"body" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties OPTION 1: Simple ssl config Update or add the following properties. For example, to enable ssl with minimal configuration #ENABLE\/DISABLE SSL com.unraveldata.ngui.ssl.enabled=true \n #PATH TO CERT FILE com.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate \n #PATH TO KEY FILE com.unraveldata.ngui.ssl.key.file=\/etc\/certs \/wildcard_unravelhost_RSA_private.key \n #OPTIONAL - COMMA SEPARATED LIST OF CA FILES com.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt \n #OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE com.unraveldata.ngui.ssl.passphrase=testp OPTION 2: Advanced ssl config: Update or add the following properties. For example, to enable SSL with advance configuration, update\/add these properties: #ENABLE\/DISABLE SSL com.unraveldata.ngui.ssl.enabled=true \n#PROVIDE SSL CONFIG THROUGH JS FILE FOR ADVANCE CONFIG\ncom.unraveldata.ngui.ssl.advance.config=\/usr\/local\/unrave\/etc\/advanced_unravel_ssl.js Content of advanced_unravel_ssl.js \/* advanced_unravel_ssl.js \n update below config variables \n SSL_KEY_FILE_PATH \n CA_CERT_FILE_PATH \n comment and uncomment the needed blocks \n *\/ \nconst fs = require('fs');\nconst constants = require('constants');\n\/* absolute path for ssl key file *\/\nconst SSL_KEY_FILE_PATH= '\/cert\/unravel_ssl.key'\n\/* absolute path for ssl cert file *\/\nconst SSL_CERT_FILE_PATH= '\/certunravel_ssl.crt'\n\/* absolute path for CA certs *\/\n\/* const CA_CERT_FILE_PATH=''*\/\nmodule.exports = {\nkey: fs.readFileSync(SSL_KEY_FILE_PATH),\npassphrase:'The password you gave when you created the key',\ncert: fs.readFileSync(SSL_CERT_FILE_PATH),\n\/\/ un comment below if using custom ca certs\n\/\/ ca : fs.readFileSync(CA_CERT_FILE_PATH),\n\/\/ uncomment below to enable disable TLS version.\n\/\/ secureOptions: constants.SSL_OP_NO_TLSv1 | constants.SSL_OP_NO_TLSv1_1,\n\/* LIST OF RECOMMENDED CIPHERS *\/\n\/* note OpenSSL-style format *\/\nciphers: ['TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA',\n'ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_RSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_RSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA'].join(':')\n} Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties. This prefix will be used by Unravel server right after login or logout. com.unraveldata.advertised.url=https:\/\/unravel.example.com:3000 Restart Unravel web UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/encrypting-passwords-in-unravel-properties-and-settings.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, incl...", 
"body" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Kafka Security", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/kafka-security.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Kafka Security", 
"snippet" : "Table of Contents SSL+Kerberos for Kafka clients For Single Kafka clients For Multiple Kafka clients Kafka Authorizations References You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prer...", 
"body" : " Table of Contents SSL+Kerberos for Kafka clients For Single Kafka clients For Multiple Kafka clients Kafka Authorizations References You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For Single Kafka clients Create a file named consumerConfig.properties. Add the following properties and copy\/move the file \/usr\/local\/unravel\/etc. You can locate youSLL + Kerbero configuration. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Kerberos configuration sasl.mechanism = GSSAPI security.protocol = SASL_PLAINTEXT sasl.kerberos.service.name = kafka sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Copy\/move consumerConfig.properties to \/usr\/local\/unravel\/etc. Edit \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. com.unraveldata.ext.kafka.clusters=ClusterName Add the following property using the ClusterName from above, com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties Restart the kafka monitor daemon unravel_km. # service unravel_km restart For Multiple Kafka clients Each cluster must have a separate consumerConfig.properties Open \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. The property should be defined with a comma separated list. If there is only one cluster name see above com.unraveldata.ext.kafka.clusters=ClusterName1,ClusterName2,ClusterName3 Create a file named consumerConfigClusterName.properties for each cluster. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Copy\/move each file to \/usr\/local\/unravel\/etc. Edit \/usr\/local\/unravel\/unravel.properties. For each cluster add the following property. com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties Restart the kafka monitor daemon unravel_km. # service unravel_km restart KafkaAuthorizations Unravel consumes message to topic __consumer_offsets UnravelOffsetConsumer SentryAuthorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe HOST=*->TOPIC=__consumer_offsets→action=read HOST=*->TOPIC=__consumer_offsets→action=write HOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization Kafka with Ranger Authorization The following privilege must be granted using Ranger for the topic __consumer_offsets Publish Consume Describe For further details, see Security - Create a Kafka Polic References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "Restricting Direct Access to Unravel UI", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/restricting-direct-access-to-unravel-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Restricting Direct Access to Unravel UI", 
"snippet" : "On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh. # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. #SET NGUI_HOSTNAME export NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart...", 
"body" : " On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh. # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. #SET NGUI_HOSTNAME \nexport NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "unravel-4-5/advanced-topics/configurations/security-configurations/using-a-private-certificate-authority-with-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with \"sudo \/etc\/init.d\/unravel_all.sh restart\" after making the change. HIGHLIGHTED \/path\/to\/jks_keystore Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts \/usr\/local\/unravel\/etc\/unravel.ext.sh bin\/java \/usr\/java\/jdkl1.8 For example: export JAVA_HOME {\/usr\/java\/jdk1.8} Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts jssecacerts # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for {mycompanyca} {something.cer} # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias {mycompanyca} -file {something.cer}\n# sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts " }, 
{ "title" : "Autoscaling HDInsight Spark Cluster using Unravel API", 
"url" : "unravel-4-5/advanced-topics/configurations/autoscaling-hdinsight-spark-cluster-using-unravel-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Autoscaling HDInsight Spark Cluster using Unravel API", 
"snippet" : "Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using this ...", 
"body" : "Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using this command. # azure hdinsight cluster list Download the customizable script from https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/unravel-autoscaling\/unravel_HDInsight_autoscaling.py Open unravel_HDInsight_autoscaling.py Property Notes Example Value unravel_base_url http:\/\/localhost:3000\/ memory_threshold scale up\/down when memory_usage higher\/lower 80% 80 cpu_threshold scale up when cpu_usage higher\/lower 10% 10 min_nodes min worker nodes 4 max_nodes max worker nodes can scale up to 10 resource_group UNRAVEL01 cluster_name estspk2rh75` Run auto scaling script # python unravel_HDInsight_autoscaling.py Below is a screenshot ( Operations Dashboard " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-5/advanced-topics/configurations/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replac...", 
"body" : " Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel andhdfs: Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires UN Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: Run these commands in a cmd or powershell console: setspn -A unravel\/HOSTunravel \nsetspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass utility in Active Directory: ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 \n ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/*\n sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab\n sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"url" : "unravel-4-5/advanced-topics/configurations/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"snippet" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdent...", 
"body" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier Next In Alarm Threshold Name Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 Under Actions Send notifications to This SNS topic should already be set up before you add it. Click Create Alarm The UI displays the alarm you just created in Alarms INSUFFICIENT DATA ALARM Alarms Click Create Alarm " }, 
{ "title" : "Enabling or Disabling Cluster Optimization Reports", 
"url" : "unravel-4-5/advanced-topics/configurations/enabling-or-disabling-cluster-optimization-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Cluster Optimization Reports", 
"snippet" : "This topic explains how to enable or disable Cluster Optimizationreports in Unravel UI. Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.pro...", 
"body" : "This topic explains how to enable or disable Cluster Optimizationreports in Unravel UI. Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Sentry-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Ranger-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Kerberos-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Optional Settings   If Unravel Server Has SSL Enabled   Disable Cluster Optimization Reports   " }, 
{ "title" : "Enabling or Disabling Small Files Reports and Files Reports", 
"url" : "unravel-4-5/advanced-topics/configurations/enabling-or-disabling-small-files-reports-and-files-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Small Files Reports and Files Reports", 
"snippet" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties and Property Description Set By User Unit...", 
"body" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties and Property Description Set By User Unit Default unravel.python.reporting.files.disable This flag disables the the the small files and files report feature. false: small files and files report are generated true: small files and files report features is disabled. Optional boolean false Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default unravel.hive.server2.host Hive Server2 fully qualified hostname Optional string localhost unravel.hive.server2.port Hive Server2 port Optional integer 10000 Security-Based Settings Settings for Kerberos-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default unravel.hive.server2.authentication Authentication mechanism to connect to Hive Server2 Required string - unravel.hive.server2.kerberos.service.name Hive Server 2 Kerberos service name Required string - Settings for Sentry-Secured CDH Clusters (With or Without Kerberos) If your CDH cluster is secured with Sentry, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative To grant this privilege, set\/update the HDFS configuration property dfs.cluster.administrators If you can't grant dfsadmin privilege to the Unravel user, follow the steps in the Troubleshooting Triggering an import of FSImage Remove the following line from \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive ADD JAR {UDF_JAR_LOC}\/unravel-udf-0.1.jar This line is no longer required because UDFs jars are stored locally on the HiveServer2 node in Sentry-secured CDH clusters. Allow the Unravel user to submit Hive queries to a YARN queue. If you can't allow this on your default YARN queue, you can grant this permission on a different YARN queue: Create a different YARN queue for the Unravel user. Give the Unravel user permission to submit Hive queries to the new YARN queue. In unravel.properties unravel.python.reporting.files.hive_mr_queue Create a new Sentry role, unravel_role beeline create role unravel_role Map the unravel unravel_role grant role unravel_role to group unravel Set HDFS access privileges for the Unravel user: The Unravel user needs to copy FSImage to \/tmp\/fsimage grant all on uri 'hdfs:\/\/\/tmp\/fsimage' to role unravel_role; Grant the Unravel user the following privileges on the Hive tables under the default Create\/drop\/truncate\/alter Hive tables Run\/select\/insert queries on Hive tables Alternatively, you can Use a different database for the Unravel user, such as unravel_db Give the Unravel user the above permissions on that database: grant all on database unravel_db to role unravel_role; In unravel.properties unravel.python.reporting.files.hive_database For example, unravel.python.reporting.files.hive_database=unravel_db Add a JAR and create temporary UDFs: As the Unravel user, copy the Unravel JAR, \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/jars\/small_files\/unravel-udf-0.1.jar The HiveServer2 aux JAR path is specified by the Hive Auxillary Jars directory (in a Hive Service wide configuration) OR by hive.reloadable.aux.jars.path Hive user and group should own this JAR. For example, if the HiveServer2 aux JAR path is \/tmp\/hive_jars, this directory and the copied jar must be owned by hive admin user ( hive chown -R hive:hive \/tmp\/hive_jars Grant the Unravel user access to this JAR: grant all on uri 'file:\/\/\/tmp\/hive_jars\/' to role unravel_role; In Cloudera Manager, restart HiveServer2. Use the show grant role unravel_role +--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| file:\/\/\/tmp\/hive_jars\/unravel-udf-0.1.jar | | | | unravel_role | ROLE | * | false | 1550829915318000 | -- |\n\n| unravel_db | | | | unravel_role | ROLE | * | false | 1550829820331000 | -- |\n\n| hdfs:\/\/\/tmp\/fsimage | | | | unravel_role | ROLE | * | false | 1550830532328000 | -- |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n Settings for Ranger-Secured HDP Clusters (With or Without Kerberos) If your HDP cluster is secured with Ranger, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative For example: Allow the Unravel user to connect to HiveServer2. Allow the Unravel user to CREATE, TRUNCATE, ALTER, DROP, INSERT, and SELECT Hive tables. For example: Allow the Unravel user to change or switch the Hive database. For example: Allow the Unravel user to submit Hive queries to a particular YARN queue. For example: Allow the Unravel user to use concurrent hive queries: Set hive.txn.manager=DbTxnManager hive.support.concurrency=true Allow the Unravel user to do the following actions dynamically: Set the following parameters: hive.auto.convert.join\nhive.support.concurrency\nhive.support.sql11.reserved.keywords\nhive.txn.manager\nhive.variable.substitute\nmapred.job.queue.name\nmapreduce.map.java.opts\nmapreduce.map.memory.mb Add JAR and create temporary functions from UDFs in this JAR. Settings for SSL-Enabled Systems Currently not supported. " }, 
{ "title" : "Enabling or Disabling Cloud Reports and Forecasting Reports", 
"url" : "unravel-4-5/advanced-topics/configurations/enabling-or-disabling-cloud-reports-and-forecasting-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Cloud Reports and Forecasting Reports", 
"snippet" : "Cloud Reports and Forecasting Reports share the same configuration; therefore, changing any property affects both reports. There is no enable\/disable flag for these reports; to enable them you must configure the properties below. Un-setting (setting to nil, removing, or commenting out) the propertie...", 
"body" : " Cloud Reports and Forecasting Reports share the same configuration; therefore, changing any property affects both reports. There is no enable\/disable flag for these reports; to enable them you must configure the properties below. Un-setting (setting to nil, removing, or commenting out) the properties disables the report. On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.cluster.type Possible values are HDP, CDH, MAPR Required String - com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint. E.g,. http:\/\/HOST:8080\/api\/v1\/clusters\/ For Cloudera Manager, the cluster id is the \"name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:7180\/api\/v17\/clusters\/ Required String - The following properties are defined by Cluster tool ManagerName Be sure to only set the properties for your cluster manager tool, which is either \"ambari\" or \"cloudera\" Property Description Set by User Unit Default com.unraveldata. ManagerName URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8443 If the Cloudera URL does not contain a port you must define port below. Required URL - string - com.unraveldata. ManagerName user name for the manger Required string - com.unraveldata. ManagerName password Required string - com.unraveldata.cloudera.manager.port This is required only Required number - com.unraveldata.cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Optional number - Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata. ManagerName If SSL is enabled, then make sure to change the URL to use HTTPS and that the port is correct. Cloudera Manager uses SSL port 7183 Ambari uses SSL port 8443 Required URL - Settings for Sentry-Secured Clusters None Settings for Ranger-Secured Clusters None Settings for Kerberos-Secured Clusters Currently not Supported " }, 
{ "title" : "Enabling or Disabling Queue Analysis Reports", 
"url" : "unravel-4-5/advanced-topics/configurations/enabling-or-disabling-queue-analysis-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Queue Analysis Reports", 
"snippet" : "This topic explains how to enable or disable Queue Analysisreports in Unravel UI. The Queue Analysis Reports feature is enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report...", 
"body" : "This topic explains how to enable or disable Queue Analysisreports in Unravel UI. The Queue Analysis Reports feature is enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. bool true Security-Based Settings None. Settings for Sentry-Secured Clusters None. Settings for Ranger-Secured Clusters None. Settings for Kerberos-Secured Clusters None. If Unravel Server Has SSL Enabled None. Optional Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 com.unraveldata.report.queue.http.timeout.msec* YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.http.retries* YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec* YARN Resource Manager HTTP connection retry wait period. ms 0 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. number 1000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 * - obsolete since 4.5.0.4. " }, 
{ "title" : "Enabling or Disabling Sessions", 
"url" : "unravel-4-5/advanced-topics/configurations/enabling-or-disabling-sessions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Sessions", 
"snippet" : "This topic explains how to enable or disable Sessionsin Unravel UI. Sessions are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.session.enabled Switch to enable\/disable sess...", 
"body" : "This topic explains how to enable or disable Sessionsin Unravel UI. Sessions are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.session.enabled Switch to enable\/disable session functionality from Unravel false: disables Sessions true: disabled No Boolean true Security-Based Settings N\/A Settings for Sentry-Secured Clusters N\/A Settings for Ranger-Secured Clusters N\/A Settings for Kerberos-Secured Clusters N\/A If Unravel Server Has SSL Enabled N\/A Optional Settings Property Description Set By User Unit Default com.unraveldata.session.max.autotune.runs Maximum runs session is allowed. Users can't exceed this value Yes Integer 8 " }, 
{ "title" : "Triggering an import of FSImage", 
"url" : "unravel-4-5/advanced-topics/configurations/triggering-an-import-of-fsimage.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Triggering an import of FSImage", 
"snippet" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage curl -v http:\/\/localhost:5000\/small-files-etl This script ensure...", 
"body" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage curl -v http:\/\/localhost:5000\/small-files-etl This script ensures that the latest FSImage is incorporated in Unravel's Small Files\/File Reports. The etl_fsimage etl_fsimage FSImage Size etl_fsimage 19 GB 24 hours 9 GB 14 hours 4 GB 7 hours Troubleshooting If etl_fsimage [2018-09-10 23:11:57,357: WARNING\/ForkPoolWorker-1]* stderr: sudo: hdfs: command not found* In this case, do the following: Fetch the FSImage as a user with dfadmin privileges using the commands rm -rf unravel_node_fsimage_dir\/* hdfs dfsadmin -fetchImage unravel_node_fsimage_dir These commands delete all existing FSImage files and then copy the latest FSImage into the directory you specify ( unravel_node_fsimage_dir The directory unravel_node_fsimage_dir must \/srv\/unravel\/tmp\/reports\/fsimage and it should be readable by unravel user. Best practice is to run these commands in a cron job that completes before Unravel's etl_fsimage task is triggered every day at 00:00 UTC. Configure Unravel OnDemand to access FSImage from unravel_node_fsimage_dir unravel.properties unravel.python.reporting.files.skip_fetch_fsimage=true unravel.python.reporting.files.external_fsimage_dir=unravel_node_fsimage_dir Note: unravel_node_fsimage_dir. Restart the Unravel OnDemand daemon. rm -rf unravel_node_fsimage_dir\/* hdfs dfsadmin -fetchImage unravel_node_fsimage_dir Unravel OnDemand assumes the FSImage filename starts with fsimage not .txt " }, 
{ "title" : "Configure JVM Sensor", 
"url" : "unravel-4-5/advanced-topics/configure-jvm-sensor.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor", 
"snippet" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)...", 
"body" : " CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) " }, 
{ "title" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"url" : "unravel-4-5/advanced-topics/configure-jvm-sensor/cdh-enable-jvm-sensor-cluster-wide-for-mapreduce--mr-.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor \/ CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"snippet" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_...", 
"body" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property> \n\/\/ this is one line \n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> Save changes. Deploy client configuration by clicking the deploy glyph () or by using the Actions Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services Unravel UI , monitor the situation. You should see in a Resource Usage Application " }, 
{ "title" : "HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"url" : "unravel-4-5/advanced-topics/configure-jvm-sensor/hdp-enable-jvm-sensor-cluster-wide-for-mapreduce2--mr-.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor \/ HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"snippet" : "must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/bt...", 
"body" : " must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP On the top notification banner, click Save In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site Inside Custom mapred-site Add Property On the top notification banner, click Save You can manually edit \/etc\/hadoop\/conf\/mapred-site.xml -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Propagate the Unravel resource metrics sensor JAR onto all the servers in the cluster. If you have already run the unravel_hdp_setup.sh Open an SSH session to the Unravel gateway host server and use guided steps to unzip the Unravel MR JARs. With root or sudo access, change directory to \/usr\/local\/unravel-agent # cd \/usr\/local\/unravel-agent\n# curl http:\/\/localhost:3000\/hh\/unravel-agent-pack-bin.zip -o unravel-agent-pack-bin.zip\n# unzip -d jars unravel-agent-pack-bin.zip Ensure you have already installed unzip curl Create a .tar file of the \/usr\/local\/unravel-agent # cd \/usr\/local\/\n# tar -cvf unravel-agent.tar .\/unravel-agent Copy the unravel-agent.tar untar \/usr\/local untar unravel-agent Restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. " }, 
{ "title" : "Connectivity", 
"url" : "unravel-4-5/advanced-topics/connectivity.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Connectivity", 
"snippet" : "Connecting to\/Configuration of a Kafka Stream Hive Metastore Access...", 
"body" : " Connecting to\/Configuration of a Kafka Stream Hive Metastore Access " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "unravel-4-5/advanced-topics/connectivity/connecting-to-configuration-of-a-kafka-stream.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= port_num The default JMX port for kafka...", 
"body" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= port_num The default JMX port for kafka in CDH is 9393. In HDP you would export this parameter under Advanced kafka-env kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS in kafka_run_class.sh: -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Djava.rmi.server.hostname=127.0.0.1\n-Djava.net.preferIPv4Stack=true Not required for HDP. Verify the configuration changes on the Kafka cluster. Restart the Kafka broker. Configure Unravel Server to monitor the Kafka cluster. The unravel daemon, unravel_km, Kafka Monitoring Properties In \/usr\/local\/unravel\/etc\/unravel.properties RED com.unraveldata.ext.kafka.clusters \ncom.unraveldata.ext.kafka. ClusterID ClusterID ClusterID JMX_ServerID ClusterID JMX_ServerID Example: com.unraveldata.ext.kafka.clusters=c1,c2 \ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093 \ncom.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193 \ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Hive Metastore Access", 
"url" : "unravel-4-5/advanced-topics/connectivity/hive-metastore-access.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access", 
"snippet" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore details Step 2: Configuring Unravel to access Hive Metastore...", 
"body" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore details Step 2: Configuring Unravel to access Hive Metastore " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-5/advanced-topics/connectivity/hive-metastore-access/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from th...", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API. From CDH version 5.5 onward, use the REST API http:\/\/CMGR_HOSTNAME_IP:7180\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for metastore Edit \/usr\/local\/unravel\/etc\/unravel.properties See Hive Metastore Configuration for information. FIXLINK. Locate and edit the following properties. If necessary, add the properties. Substitute your local values. javax.jdo.option.ConnectionURL=hive_metastore_database_host ;javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver \njavax.jdo.option.ConnectionPassword=hive_metastore_database_password ; \njavax.jdo.option.ConnectionUserName=hive_metastore_database_user ; Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in Applications Applications For HDP and MapR Please contact your cluster administrator. " }, 
{ "title" : "Hive Metastore Configuration", 
"url" : "unravel-4-5/advanced-topics/connectivity/hive-metastore-access/hive-metastore-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration", 
"snippet" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/mybat...", 
"body" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/mybatis\/ All parameters are defined in \/usr\/local\/unravel\/etc\/unravel.properties Hive Metastore Access You must configure the following properties for the Data Insights tab to populate its information correctly. Property Definition Example Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. postgresql org.postgresql.Driver - mariadb org.mariadb.jdbc.Driver MySql org.mysql.jdbc.Driver javax.jdo.option.ConnectionPassword Password used to access the data store. OIhwSFl - javax.jdo.option.ConnectionUserName Username used to access the data store. hive - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT postgresql jdbc:postgresql:\/\/congo21.unraveldata.com:7432\/hive - JDBC Configurations You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false False com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. 5 Reference [1] c3p0 project page: http:\/\/www.mchange.com\/projects\/c3p0 " }, 
{ "title" : "How to Write Jolokia JMX MBean", 
"url" : "unravel-4-5/advanced-topics/how-to-write-jolokia-jmx-mbean.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean", 
"snippet" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean While not required, it is good practice for the name to have the suffix MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class...", 
"body" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean While not required, it is good practice for the name to have the suffix MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class MBean can be injected by google Guice and used by Unravel daemon (or by other java classes). MBean must be thread safe The Unravel daemon (typically for write access) The Jolokia agent (typically for read access) The Jolokia MBean is singleton The Jolokia MBean interface has one mandatory method: which must return an unique MBean ObjectName getName() Jolokia MBean can contain also operations. Operations are methods that can be called remotely on a MBean. They may: Trigger some action on an unravel daemon Have any number of parameters Return any supported type Jolokia supports following MBean attribute types: Primitive types and their object equivalents List, Set, and Map types POJO type composed of types mentioned above and which can be nested Objects of other types have to be converted to supported types, for example, to string Use the helper class com.unraveldata.jmx.Converter " }, 
{ "title" : "Example", 
"url" : "unravel-4-5/advanced-topics/how-to-write-jolokia-jmx-mbean.html#UUID-95179eee-0e3f-fc1a-9f38-47f5239772a5_id_HowtoWriteJolokiaJMXMBean-Example", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Example", 
"snippet" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean public interface DbStatusMXBean extends JolokiaMBean { boolean isConnectionOk(); } Class DbStatusMBean @JsonMBean @Singleton @AutoService(JolokiaMBean.class) public class DbStatusMBean implements DbS...", 
"body" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean\npublic interface DbStatusMXBean extends JolokiaMBean {\n\n boolean isConnectionOk();\n} Class DbStatusMBean @JsonMBean\n@Singleton\n@AutoService(JolokiaMBean.class)\npublic class DbStatusMBean implements DbStatusMXBean {\n private static final String JMX_NAME = \"com.unraveldata:type=Monitoring,group=Database,name=DbStatus\";\n\n private boolean isConnectionOk;\n private MBeanStatus mBeanStatus;\n\n public synchronized void setConnectionOk(boolean connectionOk) {\n this.isConnectionOk = connectionOk;\n this.mBeanStatus = new MBeanStatus(LocalDateTime.now());\n }\n\n @Override\n public synchronized boolean isConnectionOk() {\n return isConnectionOk;\n }\n\n @Override\n public synchronized MBeanStatus getMBeanStatus() {\n return mBeanStatus;\n }\n\n @Override\n public String getName() {\n return JMX_NAME;\n }\n} Usage DbStatusMonitor @Singleton\npublic class DbStatusMonitor {\n\n private final DbStatusMBean mBean;\n\n @Inject\n public DbStatusMonitor(DbStatusMBean mBean) {\n this.mBean = mBean;\n }\n\n void execute(Connection sqlConnection) {\n boolean isConnectionOk = testConnection();\n mBean.setConnectionOk(isConnectionOk);\n }\n} " }, 
{ "title" : "Deploying Unravel over SELinux", 
"url" : "unravel-4-5/advanced-topics/how-to-write-jolokia-jmx-mbean/deploying-unravel-over-selinux.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Deploying Unravel over SELinux", 
"snippet" : "Security Enhanced Linux (SELinux) allows you to set access control through SELinux policies. SELinux Modes : In this mode the SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enfo...", 
"body" : "Security Enhanced Linux (SELinux) allows you to set access control through SELinux policies. SELinux Modes : In this mode the SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enforced whenever any violations or errors are detected and the violations\/errors are logged. Enforcing : The policies and rules of SELinux are applied over the subjects and objects but are not enforced. All violations and errors based on the SELinux policy are ignored and logged into the log files. If the SELinux policy prevents a specific service from accessing a specific folder, this mode allows access but logs a denial message. This mode provides enough debugging information to fine tune the SELinux Policy so it runs smoothly in enforcing mode. Permissive : No policies are enforced. Disabled SELinux Policy Unravel currently only supports the targeted Prerequisites Enable SELinux on Unravel Node running Linux. Open \/etc\/sysconfig\/selinux Set the SELinux mode. This is SELinux's default; whenever the system reboots it starts SELinux in this mode. See Switching Modesfor how to change the mode while running. SELINUX=enforcing Use the default policy, targeted SELINUXTYPE=targeted Reboot the system to effect the changes. # getenforce\nenforcing Verify the mode setting after reboot. Installing Unravel Core RPMs on a Node with SELinux Install Unravel in permissive mode or enforcing mode. You can install Unravel in either mode. However, installing Unravel in enforcing mode is highly discouraged since SELinux issues a warning regarding uncertainty of functionality. Installing in Permissive Mode (Recommended) Set mode to permissive # setenforce 0\n# getenforce\npermissive Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n SELinux may generate similar alerts during the installation process depending on the environment. But this should not hinder with the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Installing in Enforcing Mode (Highly Discouraged) When Unravel is installed in enforcing Execute getenforce enforcing Step b # getenforce\nenforcing Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n The rpm installation sets SELINUX permissive -----RPM installation log\n+ setenforce Permissive\n+ echo\n+ tee_echo '[CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.'\n+ tee -a \/tmp\/rpm_upgrade.log\n++ date '+%Y-%m-%d %H:%M:%S'\n + echo '[2019-01-28 06:33:17] [CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.' \n+ echo\n+ FILE_CACHE_HEADROOM_MB=2000\n----- # getenforce\npermissive SELinux generates two alerts like the ones below. Similiar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Switch to user. There should be no alerts at this stage. Set SELINUX enforcing # setenforce 1\n# getenforce\nenforcing Run the script switch_to_user.sh X Y switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y Start Unravel services after RPM installation. Run the following command to make sure all services start up successfully. # sudo \/etc\/init.d\/unravel_all.sh start SELinux generates two alerts. Similar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Verify SELINUX is set to enforcing. # getenforce\nenforcing If getenforce permissive # sudo \/etc\/init.d\/unravel_all.sh stop\n# setenforce 0\n# sudo \/etc\/init.d\/unravel_all.sh start Configure Unravel Server and install sensors. Substitute your fully qualified domain name or your host's IP for UNRAVEL_HOST # python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_cdh_setup.py --spark-version 2.3.0 --unravel-server UNRAVEL_HOST Checking for Any Alerts, Denials, or Policy Violations Run these commands to check for any violations which might have after an installation or an operation\/job submission to see if any violations have occurred. To view any Unravel specific alerts: # sealert -a \/var\/log\/audit\/audit.log | grep unravel To view any system specific alerts: # sealert -a \/var\/log\/audit\/audit.log Installing and Using SELinux Tools # yum install setroubleshoot setools\n# yum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setroubleshoot setools setools-console These tools help you get more information about the policy and analyze the avc Use seinfo # seinfo\nStatistics for policy file: \/sys\/fs\/selinux\/policy\nPolicy Version & Type: v.28 (binary, mls)\n\nClasses: 94 Permissions: 262 \nSensitivities 1 Categories: 1024\nTypes: 4747 Attributes: 251\nUsers: 8 Roles: 14\nBooleans: 307 Cond. Expr.: 56\nAllow: 101746 Neverallow: 0\nAuditallow: 155 Dontaudit: 8846\nType_trans: 17759 Type_change: 74\nType_member: 35 Role allow: 39\nRole_trans: 416 Range_trans: 5697\nConstraints: 109 Validatetrans: 0\nInitial SIDs: 27 Fs_use: 29\nGenfscon: 105 Portcon: 602\nNetifcon: 0 Nodecon: 0\nPermissives: 6 Polcap: 2 Use semodule # semodule -DB Use sealert All alerts generated by SELinux # sealert -a \/var\/log\/audit\/audit.log Unravel specific alerts # sealert -a \/var\/log\/audit\/audit.log | grep unravel For debugging (in other words, if you're testing in enforcement mode), run the following commands: Log all trivial violations logged by SELinux. # semodule -DB Set the audit log file to 0 so you get to know of access violations happening during the testing of enforcement mode. # > \/var\/log\/audit\/audit.log Working with Modes Checking Which Mode SELinux is Running Retrieve the current SELinux mode. Output is permissive nforcing # getenforce Switching Modes You can switch modes on the fly using the setenforce When Unravel is restarted SELinux returns to the default mode set in \/etc\/sysconfig\/selinux To set permissive mode # setenforce 0 To set enforcement mode # setenforce 1 Installing MySQL in Enforcing Mode Instructions for installing and configuring MySQL are available here If the datadir \/srv\/unravel\/db_data MySQL installation page Configure and Start MySQL Server An alert is generated in Step 5 . when starting mysqld datadir=\/srv\/unravel\/db_data : If you believe that mysqld should be allowed read access on the plugin.frm file by default Alert Configure Unravel to Connect My SQL Server An alert is generated in Step 1while creating the database. : If you believe that mysqld should be allowed create access on the ibdata1 file by default. Alert An alert is generated in Step 3when creating the schema for Unravel. : If you believe that mysqld should be allowed remove_name access on the Alert edge-4.lower-test Sample policy module my-mysqld 1.0;\nrequire {\ntype mysqld_safe_t;\ntype var_t;\ntype mysqld_t;\nclass process siginh;\nclass dir { add_name create remove_name write };\nclass file { create getattr lock open read rename unlink write };\n}\n#============= mysqld_safe_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_safe_t mysqld_t:process siginh;\n#============= mysqld_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:dir { add_name create remove_name write };\nallow mysqld_t var_t:file rename;\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:file { create getattr lock open read unlink write }; " }, 
{ "title" : "Monitoring Workflows", 
"url" : "unravel-4-5/advanced-topics/monitoring-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows", 
"snippet" : "Monitoring Oozie Workflows Monitoring Airflow Workflows...", 
"body" : " Monitoring Oozie Workflows Monitoring Airflow Workflows " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-5/advanced-topics/monitoring-workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows....", 
"body" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows. " }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-5/advanced-topics/monitoring-workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Airflow Web UI Access Changing the Monitoring Range Enabling AirFlow Configuration Properties Note Before you start, ensure the Unravel Server host and the server...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Airflow Web UI Access Changing the Monitoring Range Enabling AirFlow Configuration Properties Note Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 HIGHLIGHTED Airflow Web UIAccess Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following properties. If you can't find them, add them. Http For Airflow Web UIAccess Set the following three (3) properties. Replace AIRFLOW_WEB_URL http:\/\/ com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true Https For Airflow Web UIAccess Set the following four (4) properties. Replace AIRFLOW_WEB_URL https:\/\/ Replace AIRFLOW_WEB_UI_username AIRFLOW_WEB_UI_password com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true com.unraveldata.airflow.login.name=AIRFLOW_WEB_UI_username com.unraveldata.airflow.login.password= AIRFLOW_WEB_UI_password Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Range By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following property. If you can't find it, add it.Note there’s a “-” (minus sign) in the value. airflow.look.back.num.days=- X Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Below is a sample script, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add Hooks for Unravel Instrumentation. This script below, example-hdp-client.sh spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application only spark-defaults.conf This script can be invoked to submit an Airflow Spark application via spark-submit PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call Configuration Properties See here Note Due to this Airflow bug in v1.10.0: https:\/\/issues.apache.org\/jira\/browse\/AIRFLOW-2799 " }, 
{ "title" : "Roles and Role Based Access Control", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control", 
"snippet" : "Unravel supports three roles. Admin Read-Only Admin End-User...", 
"body" : "Unravel supports three roles. Admin Read-Only Admin End-User " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control.html#UUID-703b7757-54af-b296-3e5a-078126a6f880_id_RolesandRoleBasedAccessControl-RoleBasedAccessControlRBAC", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Role Based Access Control (RBAC)", 
"snippet" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected If you are not familiar with the concept of tagging, please What is Tagging? RBAC Roles Configure RBAC RBAC UI Configure LDAP or SAML RBAC Properties Creating R...", 
"body" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected If you are not familiar with the concept of tagging, please What is Tagging? RBAC Roles Configure RBAC RBAC UI Configure LDAP or SAML RBAC Properties Creating Read-only Admins Manage Page " }, 
{ "title" : "RBAC Roles", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/rbac-rbac-roles.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ RBAC Roles", 
"snippet" : "RBAC allows admins to restrict the pages a specific end-user can view and those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restricted ...", 
"body" : "RBAC allows admins to restrict the pages a specific end-user can view and those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restricted Based Upon Three Factors Tags for the End-user You can create tags for Applications - See Tagging Applications Workflows - See Tagging Workflows End-users are then associated with the tags via LDAP or SAML. See com.unraveldata.login.mode When RBAC is turned on an end-user view is filtered based upon their tags. For instance, if a user only has the defined tag dept:marketing dept:marketing. Unravel Default Tag com.unraveldata.rbac.default Username Mode com.unraveldata.ngui.user.mode extended In extended Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage, Reports | Operational Insights | Chargeback In restricted Application | Applications What the End-user Sees When RBAC is Turned On The available pages (as defined by mode) display applications contained in (filtered end-user tags) ∪ c om.unraveldata.rbac.default If c om.unraveldata.rbac.default and How to Exempt an End-User from RBAC To exempt a end-user from RBAC, you can make the end-user an read-only admin. " }, 
{ "title" : "Configure RBAC General Properties", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/rbac-configure-rbac.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Configure RBAC General Properties", 
"snippet" : "Open \/usr\/local\/unravel\/etc\/unravel.properties \/\/ set login mode to open, ldap or saml. See here for definition. com.unraveldata.login.mode=mode com.unraveldata.rbac.enabled=true com.unraveldata.rbac.default=Username com.unraveldata.ngui.user.mode=extended com.unraveldata.rbac.tagcmd=path_of_tag_fil...", 
"body" : " Open \/usr\/local\/unravel\/etc\/unravel.properties \/\/ set login mode to open, ldap or saml. See here for definition. \ncom.unraveldata.login.mode=mode \ncom.unraveldata.rbac.enabled=true \ncom.unraveldata.rbac.default=Username \ncom.unraveldata.ngui.user.mode=extended \ncom.unraveldata.rbac.tagcmd=path_of_tag_file If you are upgrading from 4.3 you must replace\/redefine the following properties. Below we are assuming login.mode ldap 4.3 property Replacement com.unraveldata.rbac.mode=ldap com.unraveldata.login.mode=ldap com.unraveldata.rbac.prefix=dept- com.unraveldata.rbac.ldap.tag.dept.regex.find=dept-(.*) com.unraveldata.rbac.tag=dept com.unraveldata.rbac.ldap.tags.find=dept com.unraveldata.rbac.user.operations.enabled=true com.unraveldata.ngui.user.mode=(extended|restricted) You can exempt specific end-users from RBAC effects by adding them to com.unraveldata.login.admins.readonly property. If you are using LDAP or SAML see Configure LDAP or SAML RBAC Properties. com.unraveldata.login.admins.readonly=user1,user2,user3 " }, 
{ "title" : "Configure LDAP or SAML RBAC Properties", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/configure-ldap-or-saml-rbac-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Configure LDAP or SAML RBAC Properties", 
"snippet" : "Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode com.unraveldata.login.admins.readonly.[saml|ldap].groups here LDAP \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 com.unraveldata.rbac.ldap.tags.find=proj,dept co...", 
"body" : "Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode com.unraveldata.login.admins.readonly.[saml|ldap].groups here LDAP \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.ldap.tags.find=proj,dept \ncom.unraveldata.rbac.ldap.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.ldap.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.ldap.groups=RO-admin4,RO-admin5,RO=admin6 SAML \/\/ Required com.unraveldata.login.admins.saml.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.saml.tags.find=proj,dept \ncom.unraveldata.rbac.saml.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.saml.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.saml.groups=RO-admin4,RO-admin5,RO=admin6 Examples Implementation When a user logs on, their LDAP or SAML groups User LDAP Groups Tags Project Tenant user1 [â€œdept-hrâ€?,â€?dept-salesâ€?,â€?dept-financeâ€?] { â€œdeptâ€?:[â€œhrâ€?,â€?salesâ€?,â€?financeâ€?]} dept hr, sales, finance user [â€œproj-group01â€?,â€?proj-group02â€?,â€?proj-group03â€?] { â€œprojâ€?:[â€œgroup01â€?,â€?group02â€?,â€?group03â€?]} proj group01, group02, group03 user [â€œproj-group01â€?,â€?proj-group02â€?,â€?proj-group03â€?, â€œdept-hrâ€?,â€?dept-salesâ€?,â€?dept-financeâ€?] { â€œprojâ€?:[â€œgroup01â€?,â€?group02â€?,â€?group03â€?]} proj group01, group02, group03 user [â€œdiv-div01â€?,â€?div-div02â€?,â€?div-div03â€?] n\/a n\/a n\/a and User 1 User 2 com.unraveldata.rbac.LDAP.tags.find LDAP groups has two tags matching User 3 com.unraveldata.rbac.LDAP.tags.find proj LDAP groups has one tag, User 4 div, " }, 
{ "title" : "Creating Read-only Admins", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/creating-read-only-admins.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Creating Read-only Admins", 
"snippet" : "A read-only admin has access to all the UI's page including the Manage page Open \/usr\/local\/unravel\/etc\/unravel.properties. Locate com.unraveldata.login.admins.readonly. If you can not find the property add it. Using a comma separated list, add\/append the users you wish to grant read-only admin stat...", 
"body" : "A read-only admin has access to all the UI's page including the Manage page Open \/usr\/local\/unravel\/etc\/unravel.properties. Locate com.unraveldata.login.admins.readonly. If you can not find the property add it. Using a comma separated list, add\/append the users you wish to grant read-only admin status. com.unraveldata.login.admins.readonly= user1 user2 user3 If you are using LDAP or SAML, you must configure the read-only admins using com.unraveldata.login.admins. MODE MODE com.unraveldata.login.mode com.unraveldata.login.admins.MODE.groups= admin1 admin2 admin3 user1 user2 user3 " }, 
{ "title" : "Example RBAC Configurations", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/rbac-example-rbac-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Example RBAC Configurations", 
"snippet" : "Admins and read-only admins are always exempt for RBAC restrictions. To use RBAC, set these properties: For a description of this property, see com.unraveldata.rbac.enabled=true here com.unraveldata.ngui.user.mode=[extended | restricted] com.unraveldata.login.admins[.readyonly] In all the examples b...", 
"body" : "Admins and read-only admins are always exempt for RBAC restrictions. To use RBAC, set these properties: For a description of this property, see com.unraveldata.rbac.enabled=true here com.unraveldata.ngui.user.mode=[extended | restricted] com.unraveldata.login.admins[.readyonly] In all the examples below we are using LDAP mode. Set Admin Access To set admin and not read-only admin access, set these properties: com.unraveldata.login.admins=L772417,K228680\n\/\/ comment out or remove\n#com.unraveldata.login.admins.readonly= For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP\ncom.unraveldata.login.admins.ldap.groups=LDAP_Users,,,,\n\/\/ comment out or remove\n#com.unraveldata.login.admins.readonly.ldap.groups=LDAP_Users,,,, Set Only Read-Only Admin Access To set only read-only admin access, set these properties: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680\n \/\/comment out or remove #com.unraveldata.login.admins=L772417,K22868 For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Set Admin and Read-Only Admin Access To set admin and read-only admin access, set these properties: com.unraveldata.login.admins=L772417,K228680\n com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Exempt Select End-Users from RBAC To exempt select end-users from RBAC – add them to the read-only admin property: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set these property: com.unraveldata.login.admins.ldap.groups= LDAP_Users " }, 
{ "title" : "RBAC UI", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/rbac-rbac-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ RBAC UI", 
"snippet" : "If c om.unraveldata.rbac.mode Configure LDAP or SAML RBAC Properties The tags used for RBAC end-users must also be loaded as Application Workflows c om.unraveldata.rbac.default Go to Manage | Role Manager to access the Role Manager. The RBAC default is set via com.unraveldata.rbac.enabled . If you a...", 
"body" : " If \n \n c \n om.unraveldata.rbac.mode Configure LDAP or SAML RBAC Properties The tags used for RBAC end-users must also be loaded as Application Workflows \n \n c \n om.unraveldata.rbac.default Go to Manage | Role Manager to access the Role Manager. The RBAC default is set via \n com.unraveldata.rbac.enabled . If you are not using LDAP\/SAML login mode, you can add filters for specific end-users. Any end-user roles you have previously set are displayed. If the Unravel daemon was restarted after you added end-user roles the entries are lost. You can add end-users one at at time via Add New Role. csv Adding Roles You limit end-user access through tags. In the example below only two tags are available, project tenant department csv Clicking on Add New Role Adding one or more roles via a role file Click on the Select role file csv csv \n first row is a header row defining the columns tags user, tagKey[,tagKey]* \n \n tagKey \n one or more rows defining user and tag values user, tagValue[,tagValue]* \n \n tagValue tagKey tagString \n \n tagString tagValue \n * Note: The file must tagKey tagValue user After you add your last tagValue .csv \n tagValue No special characters or spaces are allowed in file. The . csv \n \n \n \n user,project,tenant user72,\"group1,group2\",mm user25,,\"3n,3m\" userNew,groupNew user33,\"group3,group2\",\"3m,mm\" Editing or Deleting Roles To edit a role, click the edit glyph ( To delete a role, click the delete glyph. Effect of Role Access Control End-user's Access with RBAC turned off The user has access to all the Unravel UI features and all applications. End-user's Access with RBAC turned on. The user only has access to their applications or those matching their tags. " }, 
{ "title" : "Manage Page", 
"url" : "unravel-4-5/advanced-topics/rbac-roles-and-role-based-access-control/rbac-manage-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Manage Page", 
"snippet" : "This page is only available to Admins and read-only Admins....", 
"body" : "This page is only available to Admins and read-only Admins. " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-5/advanced-topics/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "Table of Contents Why Run Verification Tests or Benchmarks? Running Verification Tests (“Smoke Tests”) CDH HDP MapR Running Benchmarks Spark Available Benchmark Packages Executing the benchmarks Benchmarks for 1.6.x Benchmarks 2.0.x This topic explains how to run verification tests and benchmarks af...", 
"body" : " Table of Contents Why Run Verification Tests or Benchmarks? Running Verification Tests (“Smoke Tests”) CDH HDP MapR Running Benchmarks Spark Available Benchmark Packages Executing the benchmarks Benchmarks for 1.6.x Benchmarks 2.0.x This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "unravel-4-5/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "unravel-4-5/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel ...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "unravel-4-5/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where { LOCATION FNAME # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz 71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ lib: scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh src: tpch-query-instances: cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x Example Description Demonstrates example1 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. example2 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. example3 A Scala program containing a SparkSQL How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. example4 A Scala-based application. This application generates its input and applies multiple transformations to the generated data. How Unravel helps to root-cause a failed example5 A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x Example Description Demonstrates example1 see example1 in Benchmarks for Spark 1.6.x example2 A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. example3 example4 example5 see example3 - example5 in Benchmarks for Spark 1.6.x example6 A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. " }, 
{ "title" : "Tagging", 
"url" : "unravel-4-5/advanced-topics/tagging.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Tagging", 
"snippet" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control What ...", 
"body" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control What is Tagging? Tagging Workflows Tagging a Hive on Tez Query Tagging Applications " }, 
{ "title" : "What is Tagging?", 
"url" : "unravel-4-5/advanced-topics/tagging/what-is-tagging-.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Tagging \/ What is Tagging?", 
"snippet" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. Application Workflow Application Tagging By the time your application reaches the clus...", 
"body" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. Application Workflow Application Tagging By the time your application reaches the cluster much of its context Available Metadata by Default All applications have the following metadata. (See here App type App Id App Name Username Cluster (empty for server-less applications) Queue (empty for server-less applications) Configuration (accessible for some applications) As an admin you need to be able to organize\/view the applications running on the cluster with more granularity than this limited set allows. You will want to “slice and dice\" your applications a myriad of ways for a variety of reasons. Unravel can not “magically” deduce which applications belong to what tenants, projects, teams, etc.; to gain such granularity you must provide Unravel with this information. Tagging an app is how you tell Unravel about its context. What is a tag At the basic level a tag is simply a < key value> A simple way to conceptualize tagging is to think of a spreadsheet. The key value App Type App ID App Name User Cluster Queue Tenant Project MR j_134 distc john def root.UM-proj1.print marketing proj1 Spark s_345 rate.spark.sim jane def root.UM-proj2.print sales proj2 Spark s_456 over.spark.sim jane def root.UM-proj3.print sales proj3 See below Effective Use of Tags You can use tags to, among other things, to Generate chargeback reports based upon specific criteria, e.g., project, dept, team, etc. Decide which apps a specific user can see, e.g., the marketing head can see all marketing apps while user2 can only see specific marketing project apps. Group applications together (see Tagging Workflows Tagging Workflows In order to effectively use tags you need to understand your requirements for displaying or grouping your apps. You might want to consider, among other things Do you need chargebacks reports for each tenant in a multi-tenant cluster? Apps must be tagged with the tenant it belongs to. Do applications need to be billed back to departments and teams? Apps need department and team tags. Do you want to allow some users to see all projects and others just a subset (see Role Based Access Control A specific tag can be used for different purposes. You can use the project tag to generate chargeback reports and to filter views specific users. Assigning Tags You generate a tagging dictionary But like Unravel, the only information you have about the application running on the cluster is its metadata. So how can you develop a script to tag specific applications; how can you determine and generate your < key value Methods to generate\/create tags for an app Naming Conventions By creating naming conventions for app, queues, and cluster names you can embed information to use for your tags, e.g., placing all apps belonging to project-1 root.UM-proj1.prin Using the Metadata You can use the app’s metadata to create the tag values, some examples directly, e.g., <team, username parse it to extract information, e.g., <project, {extracted from queue name concatenate metadata with other metadata or strings, e.g., <dept, { username app name External Mapping Information A tagging script can access files which contain further mapping information. e.g., maps projects to tenants. You can find example tagging scripts at https:\/\/github.com\/unraveldata-org\/tagging Example The above table is fairly simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. Below we explain how the tags were developed Determining tags The cluster is multi-tenant; we created the tags [tenant: marketing, sales]. <tenant, marketing> <tenant, sales> There are three projects; we created the tags [project: proj1, proj2, proj3]. <project, proj1> <project, proj2> <project, proj3> Finally, we have a file which maps the projects to the tenants proj1 to marketing proj2 and proj3 to sales Assigning Tags Then we told Unravel about the tags and how to assign them, i.e., developed the tagging dictionary. First, an app’s queue is parsed to extract the project it belongs to. The project name is encoded between \"UM-\" and \".\". Once extracted, the name was used as the project value value key The script results in the three applications being assigned tenant and project tags. Workflow Tagging Workflow tags are much simpler than application tags. You use preexisting Unravel tags to create workflows, specifically unravel.workflow.name unravel.workflow.utctimestamp Tagging Workflows " }, 
{ "title" : "Tagging Workflows", 
"url" : "unravel-4-5/advanced-topics/tagging/tagging-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows", 
"snippet" : "About Unravel Workflow Tags Hive Query Example Easy Recipes for Tagging Workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Follow the instructions for your job type. Examples by Job Type Hive on MR Query Using SET Commands in Hive...", 
"body" : " \n About Unravel Workflow Tags \n Hive Query Example \n Easy Recipes for Tagging Workflows \n Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. \n Follow the instructions for your job type. \n Examples by Job Type \n Hive on MR Query Using SET Commands in Hive \n Sqoop Job Using –D Command Line Parameters \n Direct MapReduce Job Using –D Command Line Parameters \n Spark Job Using --conf Command Line Parameters \n Pig Job Using –param and SET Commands \n Impala Job Using SET Commands \n Finding Workflows in Unravel Web UI About Unravel Workflow Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: \n \n unravel.workflow.name TenantName-ProjectName-WorkflowName \n \n unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: \n SET unravel.workflow.name=\"ETL-Workflow; \n SET unravel.workflow.name=ETL-Workflow; Different runs of the same the same unravel.workflow.name \n different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name \n HIGHLIGHTED Hive Query Example This is a Hive query that was marked as part of the Financial-Tenant-ETL-Workflow \n \n \n \n SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; SET unravel.workflow.utctimestamp=20160201T000000Z; SELECT foo FROM table WHERE … Your Hive Query text goes here Easy Recipes for Tagging Workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Here, we use bash date \n \n \n \n export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Follow the instructions for your job type. \n Hive on MR query \n Hive on Tez query \n Sqoop job \n Direct MapReduce job \n Spark job \n Pig job \n Impala Job Examples by Job Type Hive on MR Query Using SET Commands in Hive # hive -f hive\/simple_wf.hql In hive\/simple_wf.hql \n \n \n \n SET unravel.workflow.name=${env:WORKFLOW_NAME}; \n SET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP}; select count(1) from lineitem; Sqoop Job Using –D Command Line Parameters # sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes: https:\/\/issues.apache.org\/jira\/browse\/SQOOP-3061 Direct MapReduce Job Using –D Command Line Parameters Substitute your file name for \/tmp\/data\/small \/tmp\/outsmoke # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input {\/tmp\/data\/small} -output {\/tmp\/outsmoke} Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name # spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster Pig Job Using –param and SET Commands # pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig \n \n \n \n SET unravel.workflow.name $WORKFLOW_NAME; SET unravel.workflow.utctimestamp $UTC_TIME_STAMP; lines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word; grouped = GROUP words BY word; wordcount = FOREACH grouped GENERATE group, COUNT(words); DUMP wordcount; Impala Job Using SET Commands # impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql \n \n \n \n SET DEBUG_ACTION=\":::: unravel.workflow.name Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications | Workflows " }, 
{ "title" : "Tagging a Hive on Tez Query", 
"url" : "unravel-4-5/advanced-topics/tagging/tagging-workflows/tagging-a-hive-on-tez-query.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Tagging a Hive on Tez Query", 
"snippet" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path={\/usr\/local\/unravel\/etc\/tag_app.py} com.unraveldata.app.tagging.script.method.name= {get...", 
"body" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path={\/usr\/local\/unravel\/etc\/tag_app.py} com.unraveldata.app.tagging.script.method.name= {get_tags} You can create tagged workflows for tez applications in four (4) ways. 1. Use --hiveconf via hive command. Enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql. set hive.execution.engine=tez; select count(*) from my_test_table; 2.Use theglobal python script for application tagging. Assuming the global script is \/tmp\/tag_app.py, 3.Use--hiveconf via beeline command. Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000' --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n 4.Use thetez.sql script, then run beeline. You must define these the two (2) workflow tags in tez.sql: set hive.execution.engine=tez; set unravel.workflow.name=my_tez_workflow; set unravel.workflow.utctimestamp=20180801T000001Z; select count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000'-f tez.sql\n " }, 
{ "title" : "Tagging Applications", 
"url" : "unravel-4-5/advanced-topics/tagging/tagging-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications", 
"snippet" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script of creating a database comprised of a list tag_keys tag_values <tag_key, tag_value> For example, You ...", 
"body" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script of creating a database comprised of a list tag_keys tag_values <tag_key, tag_value> For example, You have three departments: finance, hr, and marketing. You would create the tag_key department give it three tag_values finance hr marketing You would then associate applications with one of more of <tag_key, tag_value> . one hive query might be associated with dept:marketing dept:market dept:finance See What is Tagging? Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. Application tags are immutable and once created they cannot be changed. To Use Python Script See Writing a Python Script example script Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties \/\/ turns tagging on com.unraveldata.tagging.script.enabled=true \/\/ set to the fully qualified path + script name, e.g., \/usr\/local\/scripts\/myTest.py com.unraveldata.app.tagging.script.path=python_script \/\/ identifies the method within the script for Unravel to use. com.unraveldata.app.tagging.script.method.name=method_name Restart the following daemons. You must # \/etc\/init.d\/unravel_all.sh stop-etl\n# \/etc\/init.d\/unravel_all.sh start Writing a Python Script You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags Format In the Python script, you set a tag_key tag_value Your tag_value tag[\"auth\"]=\"admin\" tag[\"scope\"]=app_obj.getAppQueue() tags[\"dept\"]=app_obj.getAppName() + \"_\" + app_obj.getQueue() Field Where generated Description Method app_id Hadoop Application ID app_obj.getAppId() app_type Unravel Application type: mr, spark, hive, impala, or tez app_obj.getAppType() app_name Hadoop Application name app_obj.getAppName() username Hadoop Application's owner app_obj.getUsername() queue Hadoop The queue the app is running in. app_obj.getQueue() cluster_id Hadoop The cluster the the app is running on. Note : default app_obj.getClusterId() For Hive on MapReduce applications the following property is available. (This method is currently unavailable for Spark or Tez.) getAppConf(\" parameter\" Any field which exists within the MR configuration object. e.g., app_obj.getAppConf (“hive.query.id”) app_obj.getAppConf(\" parameter_name See https:\/\/github.com\/unraveldata-org\/tagging for more examples of tagging scripts. Example Python Script The below script creates seven (7) tag_keys hive_query_id dept team auth scope unravel.workflow.name unravel.workflow.utctimestamp tagged workflows. The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path=\/usr\/scripts\/Tagging.py com.unraveldata.app.tagging.script.method.name=get_tags # filename: \/usr\/scripts\/Tagging.py\n\nfrom datetime import datetime\n\n# get_tags is the method so com.unraveldata.app.tagging.script.method.name=get_tags \ndef get_tags(obj):\n\n tags = {}\n\n# MR apps get the hive_query_id tag\n if app_obj.getAppType() == \"mr\":\n tags[\"hive_query_id\"] = app_obj.getAppConf(\"hive.query.id\")\n\n# every app gets a dept and team tag\n tags[\"dept\"] = app_obj.getAppName() + \"_\" + app_obj.getQueue())\n tags[\"team\"] = app_obj.getUsername()\n\n# Only apps with username=admin get this tag\n if app_obj.getUsername() == \"admin\": \n tags[\"auth\"] = \"admin\"\n\n# Every app gets a scope tag based upon queue they are in\n if app_obj.getQueue() == \"engr\":\n # All apps in the \"engr\" queue get this tag\n tags[\"scope\"] = \"engineering-application\"\n elif app_obj.getQueue() == \"qa\":\n # All apps in the \"qa\" queue get this tag\n tags[\"scope\"] = \"qa-application\"\n else:\n # All apps not in the\"engr\" or \"qa\" queues get this tag\n tags[\"scope\"] = \"daily-application\"\n\n# creates the workflow tags, these are Unravel tags and you should contact support@unraveldata.com before using them\n tags[\"unravel.workflow.name\"] = \"Workflow-\" + tags[\"team\"] \n tags[\"unravel.workflow.utctimestamp\"] = app_obj.getAppType() + \"-\" + str(datetime.utcnow())\n\n\n return tags Running Scripts The tags computed in the Python script feed into Unravel core ETL pipeline. The Python script is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. The first time an application is invoked and running it is not listed when applications are filtered by tags. Debug and print statements are logged multiple timesas the script is invoked multiple times over a run. " }, 
{ "title" : "APIs", 
"url" : "unravel-4-5/advanced-topics/apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs", 
"snippet" : "REST API Use Case - Auto Actions and Pagerduty...", 
"body" : " REST API Use Case - Auto Actions and Pagerduty " }, 
{ "title" : "REST API", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API", 
"snippet" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI ...", 
"body" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI with a curl command using the same credentials (see Sign In role All requests\/responses are in JSON Format. Applications Like the Applications here here Operations Provides detailed data on various ongoing activities\/resource usage of your cluster. These include memory and vcore usage by cluster, queue, application, and user. You can also obtain information about job status and recent alerts. See here Operations Reports Returns key cluster KPIs and the small files report. See here Reports | Data Insight Generates chargeback reports (by user, queue, app), and Cluster workload reports. See here Reports | Operational Insight Monitoring Provides lightweight daemon which allows you to monitor various Unravel components via the REST API, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. See here Available REST Resources This API supports a Representational State Transfer (REST) model for accessing a set of resources through a fixed set of operations. API Http Method Description Sign In POST \/signIn apps POST GET GET GET GET GET GET GET \/apps\/search \/apps\/events\/inefficient_apps\/ \/apps\/status\/running\/ \/apps\/status\/finished\/ \/apps\/resources\/cpu\/allocated \/apps\/resources\/memory\/allocated \/yarn_rm\/kill_app \/yarn_rm\/move_app Applications GET GET GET GET GET \/common\/app\/{app_id}\/recommendation \/common\/app\/{app_id}\/summary \/common\/app\/{app_id}\/status \/common\/app\/{app_id}\/errors \/common\/app\/{app_id}\/logs Data Insights GET GET \/reports\/data\/kpis \/reports\/data\/small_file_report_details auto actions GET GET GET \/autoactions \/autoactions\/recent_violations \/autoactions\/metrics jobs GET \/jobs\/count operations GET GET GET GET GET GET GET \/clusters\/resources\/cpu\/allocated \/clusters\/resources\/cpu\/total \/clusters\/resources\/memory\/allocated \/clusters\/resources\/memory\/total \/clusters\/nodes \/clusters\/resources\/tagged\/cpu \/clusters\/resources\/tagged\/memory Reports GET GET GET GET GET \/search\/cb\/appt \/search\/cb\/appt\/user \/search\/cb\/appt\/queue \/reports\/operational\/clusterstats \/reports\/operational\/clusterworkload Workflow GET GET \/workflows \/workflows\/missing_sla HTTP\/HTTPS methods The API resources listed below follow standard Create-Read-Update-Delete (CRUD) semantics; the HTTP request path defines the Unravel Server and the method the action to perform. Method Operation POST Create entries GET Read entries PUT Update or edit entries DELETE Delete entries Error Codes Upon failure one of the following errors are returned Error Code Description 400 Invalid request parameters; Malformed requests 401 Authentication failure 403 Authorization failure 404 Object not found 500 Internal API error 503 Response temporarily unavailable; the caller should retry later " }, 
{ "title" : "Sign In", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/rest-api-sign-in.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Sign In", 
"snippet" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you en...", 
"body" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you enter in the CLI. Click Authorize Post \/signIn Authorize Command Line \n ssh {ClusterName} # ssh@root {ClusterName} Once you have successfully logged in, enter the curl command from step 2 above. You should see output as shown below. # curl -X POST \"http:\/\/UNRAVEL_HOST:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=user1&password=Password1\" \n Response example {\n \"message\": \"ok\",\n \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTUzNzc3NDMwOSwiZXhwIjoxNTM3NzgxNTA5fQ.iD6NXDRj1UqRYr58H4xYlNRcdrWFcU9l3p8NmbpN30k\",\n \"role\": \"admin\",\n \"readOnly\": false,\n \"tags\": \"\",\n \"id\": \"admin\",\n \"username\": \"admin\"\n} The token is needed for each REST API curl command executed. Each time you log in a new token is generated to be used in commands for that session. " }, 
{ "title" : "Apps APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/rest-api-apps-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Apps APIs", 
"snippet" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | s...", 
"body" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | spark | pig | cascading | impala | tez appTypes : S(ucess) | F(ailed) | K(illed) | R(unning) | W | P(ending) | U(nkown) appStatus : valid users for the Unravel_host users : valid queues for the Cluster queues : \"key\":[\"value,value\"], e.g., \"dept\":[finance, mktg] taglist For Hive and MR you must specify at least one Status type. Code: 200\n{\n\"metadata\": {},\n\"results\": [\n{ }\n]\n} curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Returns all Apps filtered by app type, status, username, queue, and tags \/apps\/events\/inefficient_apps\/ All Failed, Killed and Unknown Hive and Tez Apps Apps All apps with the failed, killed or unknown status regardless of user, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"hive\",\"tez\"],\"appStatus\":[\"\"F\",\"K\",\"U\"],\"end_time\":\"2018-10-09T05:18:42.000Z\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App User All apps owned by a user regardless of status, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"users\":[\"root\"]}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search Sample Output {\n \"metadata\": {\n \"duration\": {\n \"max\": 21228577,\n \"min\": 0\n },\n \"resource\": {\n \"max\": 1,\n \"min\": 0\n },\n \"events\": {\n \"max\": 4,\n \"min\": 0\n },\n \"appTypes\": {\n \"mr\": 115,\n \"hive\": 67\n },\n \"appStatus\": {\n \"S\": 168,\n \"F\": 9,\n \"K\": 4,\n \"R\": 1\n },\n \"users\": {\n \"root\": 106,\n \"hdfs\": 68,\n \"user11\": 8\n },\n \"queues\": {\n \"root.users.root\": 83,\n \"root.users.hdfs\": 62,\n \"root.users.user11\": 8,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjhfsjhfhfsjkhfjhfdjhfj\": 6,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjh\": 4,\n \"root.abcdefghijklmnopqrstuvwxyz\": 1,\n \"root.pooja.pooja\": 1\n },\n \"clusters\": {\n \"default\": 157\n },\n \"totalRecords\": 182\n },\n \"results\": [\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1201\",\n \"nick\": \"1201\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:48:29\",\n \"start_time_long\": \"2018-08-30T11:48:29.315Z\",\n \"duration_long\": 88000,\n \"predDuration_long\": 0,\n \"io_long\": 327736439,\n \"read_long\": 293122770,\n \"write_long\": 34613669,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 5,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 5,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 67916,\n \"totalReduceSlotDuration\": 38679,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n }\n ]\n} Returns list of events Inefficient apps \/apps\/events\/inefficient_apps\/ Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Type of applications. 0=> MR, 1 => HIVE , 2 => SPARK, 16 =>IMPALA entity_type: Schema: '200':\nschema:\ntype: array\nitems:\nstring curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"F\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\"}' http:\/\/myserver.com:3000\/api\/v1\/apps\/search Returns list of running apps \/apps\/status\/running\/ No Parameters Schema: '200':\n{\n\"date\":\n\"appsRunning\":\n\"appsPendiing\"::}\n} curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Schema: {\n\"date\":1551947819180,\n\"appsRunning\":3,\n\"appsPending\":0}\n} Returns list of finished apps \/apps\/status\/finished\/ No Parameters Schema: '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for Allocated CPU by app type \/apps\/resources\/cpu\/allocated Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Schema: '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for allocated memory by app type \/apps\/resources\/memory\/allocated Parameters: from to Schema: '200':\nschema:\ntype: array\nitems:\nstring curl -X GET \"http:\/\/myserver.co:3000\/api\/v1\/apps\/resources\/memory\/allocated?from=2019-03-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" {\n \"1551787200000\":\"2\"\n}} \/yarn_rm\/kill_app - Returns the status code only Parameters: cluserid appid Schema: '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/kill_app?clusterid=ignite1&appid=application_1550764666755_0668\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" \/yarn_rm\/move_app - Returns the status code only Required Parameters: cluserid appid queue: Schema: '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/move_app?clusterid=ignite1&appid=application_1550764666755_0668&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" " }, 
{ "title" : "Auto Actions APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/rest-api-auto-actions-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Auto Actions APIs", 
"snippet" : "All commands require: Unravel_Host: Port: Output schema: schema: type: array items: type: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization: JWT TOKEN\" [{\"enabled\":...", 
"body" : "All commands require: Unravel_Host: Port: Output schema: schema:\ntype: array\nitems:\ntype: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization: JWT TOKEN\" [{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"5346557784540652874\",\n\"name_by_user\":\"Long running YARN application Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125124967,\"updated_at\":1551861204577,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"elapsedTime\",\"compare\":\">\",\"state\":\"*\",\"type\":\"mapreduce\",\"value\":10000}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false},{\"action\":\"kill_app\"}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user7\"],\"user_transform\":\"\",\"app_mode\":2,\"app_list\":[\"QuasiMonteCarlo\"],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":1,\"start_hour\":13,\"start_min\":59,\"end_hour\":13,\"end_min\":59,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"7944134729015777055\",\n\"name_by_user\":\"Long running Hive query Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125155192,\"updated_at\":1551787684684,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"duration\",\"compare\":\">=\",\"state\":\"*\",\"type\":\"hive\",\"value\":100}]}],\"actions\":[{\"action\":\"http_post\",\"urls\":[\"https:\/\/unraveldata.slack.com\/messages\/CA2RX1M35\/\"]}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":0,\"user_list\":[],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":5,\"end_hour\":12,\"end_min\":5,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"2556543518905386312\",\n\"name_by_user\":\"Resource contention in cluster allocated memory Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125186605,\"updated_at\":1551787687869,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedMB\",\"compare\":\">\",\"value\":1024},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user1\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"8761136669870544897\",\n\"name_by_user\":\"Resource contention in cluster allocated vcores Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125217281,\"updated_at\":1551787691083,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedVCores\",\"compare\":\">=\",\"value\":2},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">=\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user2\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"}] Returns list of violations \/autoactions\/violations Parameters: from YYYY-MM-DD to YYYY-MM-DD limit: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/recent_violations?from=2019-03-01&to=2019-03-06&limit=10\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Returns list of autoaction metrics \/autoactions\/metrics No parameters curl -X GET \"http:\/\/devcdh513k.unraveldata.com:3000\/api\/v1\/autoactions\/metrics\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" { \"appCount\",\"elapsedTime\",\"allocatedMB\",\"allocatedVCores\",\"runningContainers\",\"memorySeconds\",\"vcoreSeconds\",\"jobCount\",\"elapsedAppTime\",\"mapsTotal\",\"mapsCompleted\",\"reducesTotal\",\"reducesCompleted\",\"mapsPending\",\"mapsRunning\",\"reducesPending\",\"reducesRunning\",\"newReduceAttempts\",\"runningReduceAttempts\",\"failedReduceAttempts\",\"killedReduceAttempts\",\"successfulReduceAttempts\",\"newMapAttempts\",\"runningMapAttempts\",\"failedMapAttempts\",\"killedMapAttempts\",\"successfulMapAttempts\",\"badId\",\"connection\",\"ioError\",\"wrongLength\",\"wrongMap\",\"wrongReduce\",\"fileBytesRead\",\"fileBytesWritten\",\"fileReadOps\",\"fileLargeReadOps\",\"fileWriteOps\",\"hdfsBytesRead\",\"hdfsBytesWritten\",\"hdfsReadOps\",\"hdfsLargeReadOps\",\"hdfsWriteOps\",\"mapInputRecords\",\"mapOutputRecords\",\"mapOutputBytes\",\"mapOutputMaterializedBytes\",\"splitRawBytes\",\"combineInputRecords\",\"combineOutputRecords\",\"reduceInputGroups\",\"reduceShuffleBytes\",\"reduceInputRecords\",\"reduceOutputRecords\",\"spilledRecords\",\"shuffledMaps\",\"failedShuffle\",\"mergedMapOutputs\",\"gcTimeMillis\",\"cpuMilliseconds\",\"physicalMemoryBytes\",\"virtualMemoryBytes\",\"committedHeapBytes\",\"totalLaunchedMaps\",\"totalLaunchedReduces\",\"dataLocalMaps\",\"slotsMillisMaps\",\"slotsMillisReduces\",\"millisMaps\",\"millisReduces\",\"vcoresMillisMaps\",\"vcoresMillisReduces\",\"mbMillisMaps\",\"mbMillisReduces\",\"bytesRead\",\"bytesWritten\",\"duration\",\"totalDfsBytesRead\",\"totalDfsBytesWritten\",\"inputRecords\",\"outputRecords\",\"outputToInputRecordsRatio\",\"totalJoinInputRowCount\",\"totalJoinOutputRowCount\",\"inputPartitions\",\"outputPartitions\",\"joinInputRowCount\",\"joinOutputRowCount\",\"joinOutputToInputRowRatio\"\n}] " }, 
{ "title" : "Applications APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/ug-ev-applications-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Applications APIs", 
"snippet" : "All commands require: Unravel server Unravel_Host: port # Port: assigned application id app_id: : recommendation | summary | status | errors | logs query string Unravel's Recommendations for the Application \/common\/app\/{app_id}\/recommendation curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v...", 
"body" : "All commands require: Unravel server Unravel_Host: port # Port: assigned application id app_id: : recommendation | summary | status | errors | logs query string Unravel's Recommendations for the Application \/common\/app\/{app_id}\/recommendation curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v1\/common\/app\/job_1543784013107_1631\/recommendation\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTQwNzI5MiwiZXhwIjoxNTUxNDE0NDkyfQ.Kf8bIhSoWFVY2uaR0WHrPw_nq6k0iVH9piSnqbO6vLg\" Sample Output Summary [\n{\n\"parameter\": \"mapreduce.map.memory.mb\",\n\"current_value\": \"7596\",\n\"recommended_value\": \"3896\"\n},\n{\n\"parameter\": \"mapreduce.map.java.opts\",\n\"current_value\": \"-Xmx8192m\",\n\"recommended_value\": \"-Xmx3117m\"\n}\n] Application's Summary \/common\/app\/{app_id}\/summary curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/summary\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Summary {\n \"@class\": \"com.unraveldata.annotation.HiveQueryAnnotation\",\n \"vcoreSeconds\": 0,\n \"memorySeconds\": 0,\n \"cents\": 0,\n \"version\": 1,\n \"source\": \"post-db\",\n \"kind\": \"hive\",\n \"id\": \"parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\",\n \"nick\": \"Hive Query\",\n \"status\": \"S\",\n \"user\": \"parwinder\",\n \"mrJobIds\": [\n \"job_1550559654567_8334\"\n ],\n \"duration\": 82120,\n \"startTime\": 1551405626487,\n \"numMRJobs\": 0,\n \"totalMRJobs\": 1,\n \"totalMapTasks\": 0,\n \"sm\": 0,\n \"km\": 0,\n \"kmu\": 0,\n \"fm\": 0,\n \"fmu\": 0,\n \"totalReduceTasks\": 0,\n \"sr\": 0,\n \"kr\": 0,\n \"kru\": 0,\n \"fr\": 0,\n \"fru\": 0,\n \"totalMapSlotDuration\": 0,\n \"totalReduceSlotDuration\": 0,\n \"totalDfsBytesRead\": 0,\n \"totalDfsBytesWritten\": 0,\n \"queryString\": \"\\nINSERT OVERWRITE DIRECTORY '\\\/user\\\/benchmark-user\\\/benchmarks\\\/oozie\\\/workflows\\\/road_accident_db\\\/output\\\/'\\nROW FORMAT DELIMITED\\nFIELDS TERMINATED BY '\\\\t'\\nSTORED AS TEXTFILE\\nselect reflect(\\\"java.lang.Thread\\\", \\\"sleep\\\", bigint(60000))\",\n \"type\": \"DML\",\n \"numEvents\": 0\n} Application's Status \/common\/app\/{app_id}\/status curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/status\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output {\n \"status\": \"Success\",\n \"message\": \"The app status of parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF is Success\"\n} Application's Errors \/common\/app\/{app_id}\/errors curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/errors\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Noerrorsfound Application's Logs \/common\/app\/{app_id}\/logs curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/logs\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Nologviewfoundforapphive_20190126160707_1d3f4e51-55eb-4cc7-8a8b-bca05d598920\n " }, 
{ "title" : "Data APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/data-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Data APIs", 
"snippet" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Variable arguments within the command are indicated in RED KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are: s : number of days Number of Day Schema [ { \"st\": start time EPOCH_timesta...", 
"body" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Variable arguments within the command are indicated in RED KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are: s : number of days Number of Day Schema [\n {\n \"st\": start time EPOCH_timestamp,\n \"et\": end time EPOCH_timestamp,\n \"nlaTb\": # of tables accessed,\n \"nlaPr\": # of partitions accessed,\n \"nlaQr\": # of queries accessing the table,\n \"nlaRi\": Total Read I\/O due to accessing the Tables,\n \"nlcTb\": # of tables created,\n \"nlcPr\": # of partitions created,\n \"nlcTz\": size of tables created,\n \"nlcPz\": size of partitions created,\n \"ntoTb\": total number of tables in the system,\n \"ntoPr\": total number of partitions in the system,\n \"nhtTb\": ,\n \"nwaTb\": ,\n \"ncoTb\": ,\n \"nhtPr\": ,\n \"nwaPr\": ,\n \"ncoPr\": ,\n \"rp\": ,\n \"rs\": ,\n \"fs\": ,\n \"users\": [\n \/\/ comma separated list of users\n ]\n }\n] curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/kpis?numDays=1\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample Output [\n {\n \"st\": 1536795705,\n \"et\": 1536882105,\n \"nlaTb\": 15,\n \"nlaPr\": 0,\n \"nlaQr\": 57,\n \"nlaRi\": 101489411821,\n \"nlcTb\": 7,\n \"nlcPr\": 0,\n \"nlcTz\": 304768890,\n \"nlcPz\": 0,\n \"ntoTb\": 378,\n \"ntoPr\": 30061,\n \"nhtTb\": 19,\n \"nwaTb\": 0,\n \"ncoTb\": 359,\n \"nhtPr\": 1823,\n \"nwaPr\": 0,\n \"ncoPr\": 28238,\n \"rp\": 30061,\n \"rs\": 92544293666,\n \"fs\": 92544293666,\n \"users\": [\n \"root\",\n \"hdfs\"\n ]\n }\n] SMALL FILES curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are: Small file id: Schema {\n \"date\": date created EPOCH_timestamp,\n \"isSuccess\": report generation sucess\/failure,\n\n\/\/ report parameters\n \"avg_size_threshold\": small file size in bytes,\n \"num_files_threshold\": minimum number of small files,\n \"top_n_small_files\": # of directories to show,\n \"report_id\": report name\",\n \"root\": [\n \/\/ array[top_n_small files] directory\n {\n \"MaxFilesize\": maximum size file in directory,\n \"MinFilesize\": minimum size file in directory,\n \"NumFiles\": # of small files in the directory,\n \"DirPath\": directory path,\n \"TotalFilesize\": sum of file size in directory,\n \"AvgFilesize\": average file size\n }\n ]\n} curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/small_file_report_details?entity_id=small_files_1536927781_5674\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample Output {\n \"date\": 1536899695,\n \"isSuccess\": true,\n \"avg_size_threshold\": 100000,\n \"num_files_threshold\": 100,\n \"top_n_small_files\": 5,\n \"report_id\": \"small_files_1536927781_5674\",\n \"root\": [\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"21523359\",\n \"DirPath\": \"\\\/97ovg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"11302185\",\n \"DirPath\": \"\\\/98esm\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/99acx\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/97zqu\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/99kgg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n }\n ]\n} " }, 
{ "title" : "Operational APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/operational-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Operational APIs", 
"snippet" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: Cluster User Queue Jobs All commands require the following: Unravel_Host: Port: Start: End: token: sign in All timestamps are in EPOCH. You mu...", 
"body" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: \n Cluster \n User \n Queue \n Jobs All commands require the following: \n Unravel_Host: \n Port: \n Start: \n End: \n token: sign in All timestamps are in EPOCH. You must substitute your local or relevant information for fields indicated by RED CLUSTER Cluster Vcore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port resourceType Query End Interval Start token Required Parameters: \n Resoure Type cpu: vcores memory: returns memory in bytes \n Query Type allocated: allocated total: used \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : count\n} \n \n Allocated Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?to=1538666160&interval=30m&from=1536273311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output \n \n \n {\n \"1535360400000\": \"12\", \n \"1535364000000\": \"3.125\", \n \"1535367600000\": \"5.0303030303\"\n} \n Total Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/memory\/total?to=1536273841&interval=1h&from=1536187441\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output \n \n \n {\n \"1535364000000\": 47460,\n \"1535367600000\": 45087,\n \"1535371200000\": 37968\n} Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n Interval \n Start token Additional required parameters: \n Polling Interval Schema \n Note \n \n \n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n \"total\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"active\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"lost\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"unhealthy\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"decommissioned\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"rebooted\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }\n}\n \n Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/nodes?to=1536339111&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output \n \n \n {\n \"date\": [\n 1535688000000,\n 1535691600000,\n 1535695200000,\n 1535698800000\n ],\n \"total\": {\n \"1535688000000\": 3,\n \"1535691600000\": 3,\n \"1535695200000\": 3,\n \"1535698800000\": 3\n },\n \"active\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"lost\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n },\n \"unhealthy\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"decommissioned\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"rebooted\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n }\n} APPLICATION VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n AppType \n Interval \n Start token Required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Application Type \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : count\n} \n Mapreduce Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22MAPREDUCE%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n \n Spark Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22SPARK%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" User VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n UserName \n Interval \n Start token Required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n User \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : result\n} \n \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" QUEUE VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n QueueName \n Interval \n Start token Additional required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Queue \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : result\n} \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" JOBS Returns average number of jobs by Groupby type. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n GroupBy \n Interval \n Start token Required parameters \n Groupby: \n Polling Interval \n State Example Schema - State \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n \"RUNNING\": {\n EPOCH_timestamp: average\n },\n \"ACCEPTED\": {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536275822&groupBy=state&interval=1h&from=1535671022\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output \n \n \n {\n \"date\": [\n 1535626800000\n ],\n \"RUNNING\": {\n \"1535626800000\": \"1.3333333333\"\n },\n \"ACCEPTED\": {\n \"1535695200000\": \"1\"\n }\n} \n Application Type Schema - Application \n \n \n {\n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n\n APP_TYPE : { \n \/\/ APP_TYPE mr | hive | spark | pig | cascading | impala | tez \n \/\/ for each polling interval where the app type is running\n EPOCH_timestamp: count\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536280789&groupBy=applicationType&interval=1h&from=1535675989\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - Application \n \n \n {\n \"date\": [\n 1535644800000,\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000\n ],\n \"MAPREDUCE\": {\n \"1535644800000\": \"1\",\n \"1535695200000\": \"1\",\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1.5\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1.1538461539\"\n },\n \"SPARK\": {\n \"1535698800000\": \"1\"\n }\n} \n User Example Schema - User \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n userName: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536281730&groupBy=user&interval=1h&from=1535676930\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - User \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000\n ],\n \"root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\"\n },\n \"hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\"\n }\n} \n Queue Example Schema - Queue \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n queue Name: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536339111&groupBy=queue&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample - Queue \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000,\n 1536044400000,\n 1536048000000,\n 1536051600000,\n 1536055200000,\n 1536058800000,\n 1536076800000\n ],\n \"root.users.root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536076800000\": \"1\"\n },\n \"root.users.hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535961600000\": \"2\",\n \"1536040800000\": \"1.1304347826\",\n \"1536044400000\": \"1\",\n \"1536055200000\": \"1.3333333333\",\n \"1536058800000\": \"1\"\n },\n \"root.users.user11\": {\n \"1535698800000\": \"1\"\n },\n \"root.abcdefghijklmnopqrstuvwxyz\": {\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536044400000\": \"1\"\n }\n} " }, 
{ "title" : "Reports APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/rest-api-reports-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Reports APIs", 
"snippet" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel_Host: Port: app_id: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"ht...", 
"body" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel_Host: Port: app_id: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: Parameters gte lte Schena {\n \"cb\": [\n \/\/ array organizing by application type\n {\n \"ms\": memory usage seconds,\n \"count\": application count,\n \"v1\": \"application type (mr | spark)\",\n \"vs\": vcore usage in second\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt?from=1536670860<to=1536757260\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample Output {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Chargeback by user \/search\/cb\/appt\/user Required parameters: gte lte Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback by queue \/search\/cb\/appt\/queue Required parameters: gte lte Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback app type Return report by application type for a specific queue. curl -X GET \"http:\/\/ UNRAVEL_HOST AppType End Start QueueName Required parameters: AppType: Queue Name Start: End: Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"ms\": memory usage in milliseconds,\n \"count\": application count,\n \"v2\": userName | queueName,\n \"vs\": vcore usage in seconds\n }\n ]\n}\n Example App Type = mr # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/user?appt=mr>e=1536677580<e=1536763980&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Clusters Cluster Summary curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters Mode: Start: End: Output for mode user or queue {\n \"userStats\": [\n\/\/ array of users | queues currently on cluster\n {\n \"root\": {\n \"running\": {\n \"min\": minimum applications running,,\n \"max\": minimum applications running,\n \"mean\": average appication number running,\n \"stddev\": standard deviation\n },\n \"memory\": { \/\/ see running above },\n \"pending\": { \/\/ see running above },\n \"vcores\": { \/\/ see running above }\n }\n }\n ]\n} Example mode = user # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=user&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n Sample output mode=user {\n \"userStats\": [\n {\n \"root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Example mode = queue # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=queue&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n Sample output mode=queue {\n \"queueStats\": [\n {\n \"root.users.root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Output for mode=app { [ appID : application id , type : memorySeconds \/vcoreSeconds vcore : vcore value memory : memory value ] } Cluster Workload curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: Start: End: token: sign in reportby Schema \/\/ work load by month one of more months with the application count for month\n\/\/ minimum of one month\n { timestamp: appcount[,timestamp: appcount] }\n\n\/\/ work load by hour array of 25 hours\n [\n { timestamp: appcount }, ... { timestamp: appcount }\n ]\n\n\/\/ work load by day - array for each days contained with the time period (Mon - Sun)\n\/\/ minimum of one day\n [\n { timestamp: appcount }\n ]\n\n\/\/ work load by hour\/day:array for each day (Mon - Sun) by hour\n\/\/ minimum of 24 hours for one day\n [\n { timestamp: appcount }\n ]\n\n curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterworkload?gte=1536777000Z<=1536863400Z&reportBy=month\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample output mode {\"1536777000\":96,\"1536863400\":4} [ {\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2} [ {\"1536777000000\":96},{\"1536863400000\":7} ] [{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}] " }, 
{ "title" : "Workflow APIs", 
"url" : "unravel-4-5/advanced-topics/apis/rest-api-rest-api/rest-api-workflow-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ REST API \/ Workflow APIs", 
"snippet" : "endpoint All commands require: Unravel_Host: Port: app_id: Returns list of Workflows \/workflows No Parameters Response Schema Code: 200 type: array items: type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output wi...", 
"body" : " endpoint All commands require: Unravel_Host: Port: app_id: Returns list of Workflows \/workflows No Parameters Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with only One Missing SLA: { \"Benchmark: Road_Accident_2005-2016\": { \"key\": \"Benchmark: Road_Accident_2005-2016\", \"doc_count\": 565, \"duration_stats\": { \"count\": 565, \"min\": 21000, \"max\": 772000, \"avg\": 174877.8761061947, \"sum\": 98806000, \"sum_of_squares\": 17972790000000, \"variance\": 1227976236.197041, \"std_deviation\": 35042.49186626204, \"std_deviation_bounds\": { \"upper\": 244962.85983871878, \"lower\": 104792.8923736706 } } } } Returns list of Workflows which are missing SLA \/workflows\/missing_sla Parameters from to Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\/missing_sla?from=2019-02-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with no Missing SLAs. [] " }, 
{ "title" : "Use Case - Auto Actions and Pagerduty", 
"url" : "unravel-4-5/advanced-topics/apis/use-case---auto-actions-and-pagerduty.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ APIs \/ Use Case - Auto Actions and Pagerduty", 
"snippet" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done...", 
"body" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's Autoactions API. Currently, this action is initiated outside of Unravel's Server. For the integration you need to complete 2 setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerdutyinformation. The python script \n must pagerduty Using pagerduty for notifications Set up a pagerduty service. \n 1 Configuration Services https:\/\/www.pagerduty.com \n 2 Add New Service \n 3 Use our API directly Events API v2 \n 4 Add Service \n 5 I ntegration Key Integrations Run the Unravel API python script on your local computer. \n 1 https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 \n 2 cd unravel-api-demo-v1 # cd unravel-api-demo-v1\n# ls\nREADME.md api-test.py \n 3 # python .\/api-test.py \n 4 Unravel Autoactions API Address The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions UNRAVEL_HOST_IP https:\/\/playground.unraveldata.com. Enter the pagerduty integration key Pagerduty API key \n 5 Start To scroll within list, click within the Autoactions Name \n 6. Navigate to the Auto Actions page ( Manage Auto Actions Creating Auto Actions \n 7. In our example there were 2 active autoactions, \n Kill job hogging the cluster \n Rogue App AA #1. \n Kill job hogging the cluster \n 8. \n \n Kill job hogging the cluster \n Updated Unravel UI \n Updated Unravel API \n Kill job hogging the cluster \n 9 \n Sample sms message \n Sample email The python script \n must pagerduty " }, 
{ "title" : "Unravel Monitoring Service", 
"url" : "unravel-4-5/advanced-topics/unravel-monitoring-service.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service", 
"snippet" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monito...", 
"body" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monitors REST API " }, 
{ "title" : "Configuration", 
"url" : "unravel-4-5/advanced-topics/unravel-monitoring-service/configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Configuration", 
"snippet" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property Description Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 com.unraveldata.monitoring.js...", 
"body" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property Description Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 com.unraveldata.monitoring.js_rules.cool.off.period Alert action cool-off period. 1800 File System Related Rules File system related rules (disk usage) are evaluated separately. Property Description Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency, in seconds, to evaluate filesystem specific monitoring rules. 0: disables evaluation. 60 DB Status Monitoring Property Description Default com.unraveldata.monitoring.db.status.check.interval Frequency, in seconds, database basic status is queried 0: disables evaluation. 30 DB Performance Monitoring Property Description Default com.unraveldata.monitoring.db.performance.check.interval Frequency, in seconds, database performance metrics are gathered. 0: disables evaluation. 30 com.unraveldata.monitoring.db.performance.query Default query: select count(*) from (select 1 from blackboards limit 1000) b see note Zookeeper Monitoring Property Description Default com.unraveldata.monitoring.zookeeper.check.interval Frequency, in seconds, Zookeeper metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.zookeeper.history.size How many data sets consider in computing average values (for historical data). 5 Kafka Monitoring Property Description Default com.unraveldata.monitoring.kafka.check.interval Frequency, in seconds,Kafka metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.kafka.ignore.topics Zookeeper topics to be ignored during Kafka monitoring. Topics can be ignored from different reasons, e.g. all internal topics are ignored. __consumer_offsets, connect-configs, connect-offsets, connect-status com.unraveldata.monitoring.kafka.history.size Number of data sets stored in the memory and used as historical data. 5 ElasticSearch Monitoring Property Description Default com.unraveldata.monitoring.elastic.check.interval Frequency, in seconds,ElasticSearch metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.elastic.history.size Number of data sets stored in the memory and used as historical data. 5 " }, 
{ "title" : "Disk Monitoring", 
"url" : "unravel-4-5/advanced-topics/unravel-monitoring-service/disk-monitoring.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Disk Monitoring", 
"snippet" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.u...", 
"body" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. 90 Partitions (volumes) monitoring Given list of partitions are monitored. Once partition usage exceeds given threshold (configured as high watermark limit) email alert is sent. Another email is not sent until disk usage goes below low watermark limit and above high watermark limit again. Property Description Default com.unraveldata.monitoring.fs.partitions.check.interval Frequency to check monitored partitions. 60 com.unraveldata.monitoring.fs.partitions.csv Comma separated list of monitored partitions. Symbolic links are supported. \/srv\/unravel,\/usr\/local\/unravel com.unraveldata.monitoring.fs.partitions.high.watermark Trigger alert if disk usage is over this limit. Do not trigger next alert until disk usage is below low watermark limit. 85 com.unraveldata.monitoring.fs.partitions.low.watermark If disk usage goes below this limit then disk alert can be triggered again. 70 Deprecated Partition Properties The following properties have been deprecated and replaced with the above partition properties. We strongly suggest remove these deprecated properties from unravel.properties and replace them with the above properties. If you have both the new and deprecated properties defined, the value of the deprecated property is used. For instance, if you have the defined both: new: com.unraveldata.monitoring.fs.partitions.high.watermark=80 deprecated: com.unraveldata.kafka.monitor.disk.high.watermark=60 Unravel uses the deprecated property value, so triggering occurs when disk usage is > 60% not 80% Property Replaced by com.unraveldata.filesystem.volumes.csv com.unraveldata.monitoring.fs.partitions.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.monitoring.fs.partitions.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark com.unraveldata.monitoring.fs.partitions.low.watermark Folders monitoring Property Note Default com.unraveldata.monitoring.fs.folders.check.interval Frequency to check monitored folders (directories). 0 com.unraveldata.monitoring.fs.folder_limit.pairs.csv Comma separated list of monitored folders and their size limits, <foldername>:<folderlimit>[KB|MB|GB], e.g., \/srv\/unravel:100GB,\/usr\/local\/unravel:200GB. The folder must be a fully qualified and may be a symbolic link. If no size unit is specified, size is evaluated as bytes. \/srv\/unravel:100GB com.unraveldata.monitoring.fs.folders.low.watermark Percentage of folder limit above. Once an alert is triggered, a new alert is only triggered if the size first drops below this limit and then rises above the folder limit. The purpose is to prevent false and repeated alerts. Example: \/srv\/unravel:100GB with a low water mark of 80% The first time (\/srv\/unravel size > 100GB) the alert is triggered. No new alert is triggered unless (\/srv\/unravel size drops < 80GB) and then 80 " }, 
{ "title" : "JMX Client", 
"url" : "unravel-4-5/advanced-topics/unravel-monitoring-service/jmx-client.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ JMX Client", 
"snippet" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client d...", 
"body" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client displays monitoring beans. In the right panel below JSON response data is associated REST API call by which given data can be retrieved. " }, 
{ "title" : "Monitors REST API", 
"url" : "unravel-4-5/advanced-topics/unravel-monitoring-service/monitors-rest-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Monitors REST API", 
"snippet" : "sdfa Table of Contents PartitionInfo Monitor DbStatus Monitor DbPerformance Monitor Zookeeper Monitor Kafka Monitor Elastic Monitor Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response { \"request\": { \"mbean\": \"com.unraveldata:group=Database,name=DbStatu...", 
"body" : "sdfa Table of Contents PartitionInfo Monitor DbStatus Monitor DbPerformance Monitor Zookeeper Monitor Kafka Monitor Elastic Monitor Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response {\n \"request\": {\n \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\",\n \"attribute\": \"MBeanStatus\",\n \"type\": \"read\"\n },\n \"value\": {\n \"lastUpdated\": \"2018-05-15T08:31:36\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n },\n \"timestamp\": 1526373099,\n \"status\": 200\n} There is value Set MONITOR_HOST Since we can have more unravel nodes I used term \"monitor_host\" to point to node where monitoring service is running PartitionInfo Monitor MBean Status http:\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:30:17\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Partitions Info http:\/\/ MONITOR_HOST PartitionInfo - Output {\n \"\\\/home\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/home\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/tmp\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/tmp\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n }\n} DbStatus Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:31:49\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true DbPerformance Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:45:06\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true Last Query Duration http:\/\/ MONITOR_HOST LastQueryDuration - Output 14 Query Timed Out http:\/\/ MONITOR_HOST QueryTimedOut false Query Exception http:\/\/ MONITOR_HOST QueryException - Output RuntimeException: Cannot read configuration Zookeeper Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T11:33:32\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMax\": 0,\n \"leader\": false,\n \"follower\": false,\n \"created\": \"2018-05-14T11:33:52\",\n \"connectionsCount\": 0,\n \"mode\": null,\n \"latencyAvg\": 0,\n \"latencyMin\": 0,\n \"port\": 2000,\n \"outstandingCount\": 0,\n \"host\": \"localhost\",\n \"nodeCount\": 0,\n \"ok\": false\n }\n] Historical Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMinAvg\": 0,\n \"connectionsCountTrend\": 0,\n \"nodeCountTrend\": 0,\n \"latencyMinTrend\": 0,\n \"outstandingCountTrend\": 0,\n \"latencyAvg\": 0,\n \"latencyMaxTrend\": 0,\n \"port\": 2000,\n \"isOkCount\": 0,\n \"host\": \"localhost\",\n \"outstandingCountAvg\": 0,\n \"connectionsCountAvg\": 0,\n \"leaderCount\": 0,\n \"followerCount\": 0,\n \"latencyAvgTrend\": 0,\n \"latencyMaxAvg\": 0,\n \"nodeCountAvg\": 0,\n \"isNotOkCount\": 5\n }\n] Kafka Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T11:55:24\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST {\n \"consumerGroups\": [\n {\n \"groupName\": \"0_1790010567376612\",\n \"consumerTopicList\": [\n {\n \"consumerHost\": \"172.16.1.111\",\n \"clientId\": \"unravel_diag_meta\",\n \"lag\": 0,\n \"currentOffset\": 0,\n \"partitionId\": 0,\n \"consumerId\": \"unravel_diag_meta-1971faac-1d57-462b-b73e-c45e9c3cee52\",\n \"topicName\": \"meta\",\n \"logEndOffset\": 0\n }\n ]\n }\n ],\n \"created\": \"2018-05-14T12:06:38\",\n \"kafkaRunning\": true,\n \"topicList\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"meta\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ],\n \"topicsWithoutConsumer\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ]\n} Historical Data http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.kafka.history.size Elastic Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T12:15:02\",\n \"lastUpdateSuccessful\": false,\n \"errorMessage\": \"Cannot retrieve ElasticSearch data: Cannot get ElasticSearch data. Address: sako1:4171\",\n \"initialized\": true\n} RecentData http:\/\/ MONITOR_HOST RecentData {\n \"running\": true,\n \"nodes\": [\n {\n \"indices\": {\n \"search\": {\n \"fetchTimeInMillis\": 0,\n \"queryTimeInMillis\": 0,\n \"scrollTimeInMillis\": 0\n },\n \"docs\": {\n \"deleted\": 0,\n \"count\": 0\n },\n \"indexing\": {\n \"noopUpdateTotal\": 0,\n \"indexTimeInMillis\": 0,\n \"throttleTimeInMillis\": 0,\n \"indexCurrent\": 0,\n \"deleteTimeInMillis\": 0,\n \"deleteCurrent\": 0,\n \"indexTotal\": 0,\n \"indexFailed\": 0,\n \"deleteTotal\": 0,\n \"throttled\": false\n },\n \"get\": {\n \"missingTimeInMillis\": 0,\n \"existsTimeInMillis\": 0,\n \"timeInMillis\": 0\n },\n \"store\": {\n \"sizeInBytes\": 0,\n \"throttleTimeInMillis\": 0\n }\n },\n \"roles\": [\n \"master\",\n \"data\",\n \"ingest\"\n ],\n \"name\": \"unravel_s_1\",\n \"timestamp\": 1526327791715\n }\n ],\n \"port\": 4171,\n \"created\": \"2018-05-14T19:56:33\",\n \"host\": \"sako1\",\n \"clusterHealth\": {\n \"activeShardsPercentAsNumber\": 100,\n \"numberOfPendingTasks\": 0,\n \"numberOfInFlightFetch\": 0,\n \"timedOut\": false,\n \"activePrimaryShards\": 0,\n \"unassignedShards\": 0,\n \"numberOfFailedNodes\": 0,\n \"numberOfNodes\": 1,\n \"taskMaxWaitingInQueueMillis\": 0,\n \"initializingShards\": 0,\n \"numberOfDataNodes\": 1,\n \"relocatingShards\": 0,\n \"clusterName\": \"unravel18679\",\n \"activeShards\": 0,\n \"delayedUnassignedShards\": 0,\n \"numberOfSuccessfulNodes\": 1,\n \"status\": \"green\"\n }\n} Historica lData http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.elastic.history.size " }, 
{ "title" : "Unravel Properties", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties", 
"snippet" : "Table of Contents Required Properties for Unravel General properties for all platforms General Login Properties LDAP SAML Custom UI Banner\/Notification Airflow Auto Action Cluster Manager Cluster EMAIL Properties HBASE HBase - source.type=JMX HBase - source.type=AMBARI | CDH Hive Metastore Access Hi...", 
"body" : " Table of Contents Required Properties for Unravel General properties for all platforms General Login Properties LDAP SAML Custom UI Banner\/Notification Airflow Auto Action Cluster Manager Cluster EMAIL Properties HBASE HBase - source.type=JMX HBase - source.type=AMBARI | CDH Hive Metastore Access Hive-hook SSL Impala JDBC Configurations KAFKA Monitoring OOZIE RBAC Spark Tagging Tez HDInsight Azure Data Lake Unravel properties, unless otherwise, are located in \/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact support@unraveldata.com The Set By User Req Opt is blank if there is a default value Unit bool: CSL ms min ns sec path percent " }, 
{ "title" : "Required Properties for Unravel", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-RequiredPropertiesforUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Required Properties for Unravel", 
"snippet" : "Property Description Set By User Unit Default prepend username Unravel database user string - password password for unravel.jdbc.username string - url MySQL mariadb path jdbc: mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc: mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod prepend hive.hdfs.dir \/user\/unra...", 
"body" : " Property Description Set By User Unit Default prepend username Unravel database user string - password password for unravel.jdbc.username string - url MySQL mariadb path jdbc: mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc: mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod prepend hive.hdfs.dir \/user\/unravel\/HOOK_RESULT_DIR Req - customer.organization Opt - tmpdir path \/srv\/unravel\/tmp login.admins Admins who can write in the Unravel UI, e.g., update\/add auto actions. admin login.admins.readonly Admins who only have read-only. These have the same access as a read\/write Admin except in read-only mode. CLS - zk.quorum CSL 127.0.0.1:4181,127.0.0.1:4182,127.0.0.1:4183 es.cluster Unravel elastic search cluster name, e.g., unravel21650 advertised.url Defines the Unravel Server URL for HTTP traffic. Req url history.maxSize.weeks Number of weeks retained for search results in Elastic Search. count 26 retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database count " }, 
{ "title" : "General properties for all platforms", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Generalpropertiesforallplatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ General properties for all platforms", 
"snippet" : "Property Description Set By User Unit Default prepend job.collector.done.log.base HDFS path to \"done\" directory of MR logs path HDP CONFIGURE FOR DEF CLOUDERA - HDP spark.eventlog.location Spark event logs direcotry path maprfs:\/\/\/apps\/spark job.collector.log.aggregation.base An HDFS path that helps...", 
"body" : " Property Description Set By User Unit Default prepend job.collector.done.log.base HDFS path to \"done\" directory of MR logs path HDP CONFIGURE FOR DEF CLOUDERA - HDP spark.eventlog.location Spark event logs direcotry path maprfs:\/\/\/apps\/spark job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process path \/tmp\/logs\/*\/logs\/ max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application byte 500000000 (~500 MB) max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application byte 2000000000 (~2GB) min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) ms 60000 (10 mins) min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds) ms 6000 (1 min) attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application count 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs) Valid Options yarn " }, 
{ "title" : "General Login Properties", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-GeneralLoginProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ General Login Properties", 
"snippet" : "Property Description Set By User Unit Default prepend login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml open: users logs directly into Unravel UI Req path - kerberos.principal Spark event logs directory path kerberos.keytab.path An HDFS path that helps locate MR job ...", 
"body" : " Property Description Set By User Unit Default prepend login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml open: users logs directly into Unravel UI Req path - kerberos.principal Spark event logs directory path kerberos.keytab.path An HDFS path that helps locate MR job logs to process path LDAP Note: These properties are required when com.unraveldata.login.mode=ldap. Property Description Set By User Unit Default prepend Domain See Note customLDAPQuery See Note groupClassKey See Note groupDNPattern See Note groupFilter See Note groupMembershipKey See Note guidKey=uid See Note mailAttribute The mail attribute name in the LDAP response that Unravel server uses to extract the ldap user's email address. If not configured, Unravel server uses the attribute name \"mail\". See Note userDNPattern See Note userFilter See Note url See Note SAML Unravel supports saml2.0. These properties are located in \/usr\/local\/unravel\/etc\/saml.json Note: These properties are required when com.unraveldata.login.mode=saml. Property Description Set By User Example Values login.saml.config See saml.json \/usr\/local\/unravel\/etc\/saml.json entryPoint Identity provider entrypoint, It is Req to be spec-compliant when the request is signed. See Note \" http:\/\/c24.unravel.com:9080\/simplesaml\/saml2\/idp\/SSOService.php issuer Issuer string to supply to identity provider( Environment name). Should match the name configured in Idp See Note “Congo24”, “Localhost” , cert IDP's public signing certificate. See Note Idp Cert String unravel_mapping Mapping saml attributes to Unravel attributes. Specific to unravel Integration. See Note { \"username\":\"userid\", \"groups\":\"ds_groups\" } logoutUrl Base address to call with logout requests (default:entryPoint) See Note \" http:\/\/c24.unravel.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php privateCert Unravel private cert string to sign Auth requests See Note Unravel cert string " }, 
{ "title" : "Custom UI Banner\/Notification", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-CustomUIBannerNotification", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Custom UI Banner\/Notification", 
"snippet" : "Property Description Set By User Unit Default prepend display Displays a banner at the top of the Unravel UI. true text end.date false bool false text Text to display when display true The text end.date Opt string - end.date Date and Time to stop displaying the custom banner. There is no date\/time l...", 
"body" : " Property Description Set By User Unit Default prepend display Displays a banner at the top of the Unravel UI. true text end.date false bool false text Text to display when display true The text end.date Opt string - end.date Date and Time to stop displaying the custom banner. There is no date\/time limit. Format: YYYYMMDD T Z The text end.date Opt string - " }, 
{ "title" : "Airflow", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Airflow", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Airflow", 
"snippet" : "Property Description Set By User Unit Default prepend: available Notes if airflow is currently available false: not available true: available Req bool - server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Req url - protocol Type of connection, e.g., https or http https login.na...", 
"body" : " Property Description Set By User Unit Default prepend: available Notes if airflow is currently available false: not available true: available Req bool - server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Req url - protocol Type of connection, e.g., https or http https login.name Airflow login username. Req string - login.password Password for login username. Req string - status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow web UI. Default unlimited. bytes 0 no prepend airflow.look.back.num.days Date range for workflows, specified in days to look back. The value must start with a minus, -5 is past 5 days, -5 " }, 
{ "title" : "Auto Action", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-AutoAction", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Auto Action", 
"snippet" : "Property Definition Set By User Unit Default prepend auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts bool true auto.action.default.snooze.period.ms The time repeated violations are be ignore...", 
"body" : " Property Definition Set By User Unit Default prepend auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts bool true auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app 0: snooze is turned off > 0: no upper bound ms 3,600,000 (1 hour) " }, 
{ "title" : "Cluster Manager", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-ClusterManager", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Cluster Manager", 
"snippet" : "These properties need to be set for Cloud Reports Forecasting Report Impala Be sure to only set the properties for your cluster manager tool. Property Description Set by User Unit Default The following properties are defined by Cluster tool. {ManagerName} prepend {ManagerName} URL of Cluster Manager...", 
"body" : "These properties need to be set for Cloud Reports Forecasting Report Impala Be sure to only set the properties for your cluster manager tool. Property Description Set by User Unit Default The following properties are defined by Cluster tool. {ManagerName} prepend {ManagerName} URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8083 If the Cloudera Manager URL does not contain a port you must define port below Req - {ManagerName} user name for the manger Req - {ManagerName} password Req - cloudera.manager.port This is required only Opt - cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Opt - " }, 
{ "title" : "Cluster", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Cluster", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Cluster", 
"snippet" : "Property Description Set by User Unit Default prepend cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the displ...", 
"body" : " Property Description Set by User Unit Default prepend cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint. E.g,. http:\/\/HOST:8080\/api\/v1\/clusters\/ For Cloudera Manager, the cluster id is the \"name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:7180\/api\/v17\/clusters\/ cluster.type Possible values are HDP, CDH, or MAPR " }, 
{ "title" : "EMAIL Properties", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-EMAILProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ EMAIL Properties", 
"snippet" : "Property Set By User Set By User Unit Default prepend monitoring.alert.email.enabled Enables email alerts. true: enables alerts false: disables bool true report.user.email.domain Default email domain used for email alerts. localhost.local Opt string - login.admins Comma separated list of email recip...", 
"body" : " Property Set By User Set By User Unit Default prepend monitoring.alert.email.enabled Enables email alerts. true: enables alerts false: disables bool true report.user.email.domain Default email domain used for email alerts. localhost.local Opt string - login.admins Comma separated list of email recipients. Recipient can be defined by complete email address or email name with report.user.email.domain E.g., admin,support@unraveldata.com Req string - prepend mail.smtp.from Used for email \"from\" and \"reply-to\" headers Req string - mail.smtp2.from Used for email \"from\" and \"reply-to\" headers Opt string - mail.smtp.port Port number 25 mail.smtp.auth Enable\/ SMTP authentication. Note Req bool false mail.smtp.starttls.enable Use start-TLS. bool false mail.smtp.ssl.enable Use SSL right from the start. bool false mail.smtp.user Username for SMTP authentication Note Opt string - mail.smtp.pw Password for SMTP authentication Note Opt string - mail.smtp.host Host for SMTP server string l mail.smtp.localhost A domain name for apparent sender; must have at least one dot (e.g. organization.com) string localhost.local mail.smtp.debug Enable debug mode. bool false " }, 
{ "title" : "HBASE", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-HBASE", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ HBASE", 
"snippet" : "Property Definition Set By User Unit Default prepend source.type Source of metrics. Supported values - AMBARI, CDH, and JMX. Req - clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,clustername2,... Req CSL - metric.poll...", 
"body" : " Property Definition Set By User Unit Default prepend source.type Source of metrics. Supported values - AMBARI, CDH, and JMX. Req - clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,clustername2,... Req CSL - metric.poll.interval Polling interval in minutes minutes 5 http.conn.timeout Polling connection timeout in seconds sec 5 http.read.timeout Polling read timeout in seconds sec 10 http.poll.parallelism Polling parallelism, no. of cores number 2 alert.average.threshold Threshold factor above average value for alerts alerts sec 1.2 (120%) HBase-source.type=JMX Property Definition Set By User Unit Default prepend {clustername}.node.http.apis HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... Example: http:\/\/your.master.server:16010, http:\/\/your.region.server:16030\/ Req CSL - HBase-source.type=AMBARI | CDH Property Definition Set By User Unit Default prepend rest.url Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443) Format: http[s]:\/\/your.ambari.server[:port]\" Example: http:\/\/your.ambari.server http:\/\/your.ambari.serve r : Req URL - rest.user Username for rest api Req CSL - rest.pwd Password for rest api Req CSL - rest.ssl.enabled ase.ssl.enabled property value Opt bool true master.port hbase.master.info.port property value For AMBARI For CDH Opt number - regionserver.port hbase.master.info.port property value For AMBARI For CDH Opt number - service.name HBase service name if not the default - “HBASE” Format: clustername1=servicename1,clustername2=servicename2,... Opt CSL {clustername}=hbase " }, 
{ "title" : "Hive Metastore Access", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-HiveMetastoreAccess", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property Definition Set By User Unit Default prepend ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.m...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property Definition Set By User Unit Default prepend ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.mysql.jdbc.Driver Req string - ConnectionPassword Password used to access the data store. Req string - ConnectionUserName Username used to access the data store. Req string - ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc:{ DB_Driver} {HOST} : {PORT} Examples: } postgresql: jdbc: postgresql:\/\/congo21.unraveldata.com:7432\/hive Req url - " }, 
{ "title" : "Hive-hook SSL", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Hive-hookSSL", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Hive-hook SSL", 
"snippet" : "Property Definition Set By User Unit Default prepend Set in unravel.properties live.logreceiver.port.https HTTPS server port (negative value means HTTPS server is disabled) number -1 server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Req string - server.ssl.cert_password KeyS...", 
"body" : " Property Definition Set By User Unit Default prepend Set in unravel.properties live.logreceiver.port.https HTTPS server port (negative value means HTTPS server is disabled) number -1 server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Req string - server.ssl.cert_password KeyStore password Req string - Set in hive-site-xml or in Hive CLI (using --hiveconf) hive.hook.insecure.ssl false: SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store true: certificate is not validated, trust store not needed false hive.hook.use.ssl Enables SSL bool false hive.hook.ssl.trust_store Trust store string - hive.hook.ssl.trust_store_password Trust store password as plain text string - hive.hook.ssl.trust_store_password_file Path to file of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence string - port number 4043 Property Definition Set By User Unit Default prepend hive.hook.use.ssl number false hive.hook.insecure.ssl false port KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Yes string - server.ssl.cert_password KeyStore password Yes string - server.ssl.trust_store_path TrustStore file path Yes string - server.ssl.trust_store_password TrustStore password Yes string " }, 
{ "title" : "Impala", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Impala", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Impala", 
"snippet" : "Property Definition Set By User Unit Default prepend data.source Can be cm or impalad Opt cm impalad.nodes Node list in the form of IP Address:Port Req CSL impala.ddl Controls whether DDL statements will be imported bool false Cloudera Manager Properties prepend num.queries.limit Maximum number of q...", 
"body" : " Property Definition Set By User Unit Default prepend data.source Can be cm or impalad Opt cm impalad.nodes Node list in the form of IP Address:Port Req CSL impala.ddl Controls whether DDL statements will be imported bool false Cloudera Manager Properties prepend num.queries.limit Maximum number of queries that will be returned by a poll to the Cloudera Manager API count 1000 poll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 look.back.minutes Number of minutes to look back when polling the Cloudera Manager API min -5 skip.duration.millis Queries with duration shorter than this threshold will get captured but not analyzed ms 1000 prepend read.timeout.millis HTTP Read timeout for Cloudera Manager connections ms 5000 connect.timeout.millis HTTP Connect timeout for Cloudera Manager connections ms 30000 The following properties defaults should be fine and don't need to be changed. hitdoc.impala.operator.info.length 20480 impala.events.stalestats.threshold.bytes bytes 1000 impala.events.stalestats.ratio percent 0.2 impala.events.longop.time.millis ms 2000 impala.events.longop.ratio percent 0.2 impala.events.cost.diff.bytes bytes 500000000 impala.events.skew.time.millis ms 500000000 impala.events.skew percent 1.5 " }, 
{ "title" : "JDBC Configurations", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-JDBCConfigurations", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ JDBC Configurations", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Set By User Unit Default prepend acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from t...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Set By User Unit Default prepend acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. count 30 acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. ms 1000 breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false bool false maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. sec 0 maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. sec 0 maxpoolsize The maximum connections in the connection pool. count 5 OPTIONAL DATA PAGE prepend db.c3p0.idleconnectiontestperiod Opt 0 databasePattern Opt string dname * print.metastore.stats Opt bool false " }, 
{ "title" : "KAFKA Monitoring", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-KAFKAMonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ KAFKA Monitoring", 
"snippet" : "Property Definition Set By User Unit Example prepend clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Req CSL c1, c2 {cluster}.bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. R...", 
"body" : " Property Definition Set By User Unit Example prepend clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Req CSL c1, c2 {cluster}.bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. Req CSL localhost:9092, localhost:9093 {cluster}.jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. Req CSL kafkaNode-1,kafkaNode-2 {cluster}.jmx.{kafkaNode-1}.host Req localhost To locate Kafka and JMX ports: Cloudera Manager Alternatively, you may lookup up that information in the broker nodes of Zookeeper CLI. HDP JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template {cluster}.jmx.{kafkaNode-1}.port Req number 5005 {cluster}.jmx.{kafkaNode-2}.port Req number 2010 servers Req CSL kafka-1,kafka-2 insight.interval_min min 15 insight.sw_size 30 insight.num_ignored_intervals 2 insight.lag_threshold 100 prepend check.interval sec 30 ignore.topics comma separated list of topics CSL __consumer_offsets, connect-configs, connect-offsets, connect-status history.size count 5 " }, 
{ "title" : "OOZIE", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-OOZIE", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ OOZIE", 
"snippet" : "Property Description Set By User Unit Default prepend: server.url The Oozie server URL to be monitored by Unravel Req path server.username Opt string server.password Opt string log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x w...", 
"body" : " Property Description Set By User Unit Default prepend: server.url The Oozie server URL to be monitored by Unravel Req path server.username Opt string server.password Opt string log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x will be trimmed from the beginning and only last x characters are kept. count 1000000 prepend disable Whether to disable bringing in Oozie workflows into Unravel. The underlying jobs will not be affected. bool false fetch.num Number of workflows to pull in each API call. count 100 fetch.interval.sec sec 120 retry.sec sec 600 " }, 
{ "title" : "RBAC", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-RBAC", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ RBAC", 
"snippet" : "Property Description Set By User Unit Default prepend: enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName tagcmd string prepend {Mode} c...", 
"body" : " Property Description Set By User Unit Default prepend: enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName tagcmd string prepend {Mode} com.unraveldata.login.mode here login.admins.{ Mode Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - login.admins.readonly.{ Mode Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - rbac.{ Mode A comma separated list of the prefix of LDAP\/saml group to be used as the PROJECT { Mode - CLS - rbac.{ Mode Tag} Defines regular expression used to parse LDAP\/saml groups for generating the TENANTs PROJECT. Value = { Tag} -(REGEX) PROJECT com.unraveldata.rbac.{Mode}.tags. Note: PROJECT - CLS - " }, 
{ "title" : "Spark", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Spark", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Spark", 
"snippet" : "Property Description Set By User Unit Default prepend live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. true false bool true live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > ...", 
"body" : " Property Description Set By User Unit Default prepend live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. true false bool true live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline count 1000 master Default spark master mode to be used if not available from Sensor. yarn Event log processing prepend eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when string hdfs:\/\/\/user\/spark\/applicationHistory\/ eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize bytes 1000000000 (~1GB) eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. sec 1440 (1 day) hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. true false Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes false hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE CSL ,,.lz4,.snappy,.inprogress appLoading.maxAttempts Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry ms 2000 (2 s) tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related events.enableCaching Enables logic for executing caching events. false " }, 
{ "title" : "Tagging", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Tagging", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Tagging", 
"snippet" : "Property Definition Set By Use Unit Default prepend enabled Enables tagging functionality. bool true script.enabled Enables tagging. bool false script.path Specifies tagging script path to use when enabled=true path \/usr\/local\/unravel\/etc\/apptag.py script.method.name Method name that will be execute...", 
"body" : " Property Definition Set By Use Unit Default prepend enabled Enables tagging functionality. bool true script.enabled Enables tagging. bool false script.path Specifies tagging script path to use when enabled=true path \/usr\/local\/unravel\/etc\/apptag.py script.method.name Method name that will be executed as part of the tagging script. string generate_unravel_tags " }, 
{ "title" : "Tez", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-Tez", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Tez", 
"snippet" : "Property Definition Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Opt string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Opt string prepend yarn.timeline-service.w...", 
"body" : " Property Definition Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Opt string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Opt string prepend yarn.timeline-service.webapp.address Hostname of Application Timeline Server, e.g., http:\/\/$atshostname Req url yarn.timeline-service.port HTTP port of Application Timeline Server number 8188 tez.app.ats.connect.timeout.millis HTTP Connect timeout for ATS connections ms 30000 tez.app.ats.read.timeout.millis HTTP Read timeout for ATS connections ms 5000 tez.app.ats.poll.timeout.millis Controls the timeout after which we will stop trying to poll ATS if the polling is failing ms 120000 tez.ats.poll.interval.millis Interval between consecutive polls of ATS if the polling fails. ms 10000 tez.ats.poll.max.retries Maximum number of retries if the polling of ATS fails count 30 The following properties defaults shouldn't need to be changed. tez.events.low.tasks count 25 tez.events.low.tasks count 50 tez.events.min.task.millis ms 2000 tez.events.max.task.millis ms 50000 tez.events.task.percentage percent 0.2 " }, 
{ "title" : "HDInsight", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-HDInsight", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ HDInsight", 
"snippet" : "Note: These property are required if you are using Azure blob. Property Description Set By User Unit Default prepend hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster See Note string - hdinsight.primary-access-key Pr...", 
"body" : "Note: These property are required if you are using Azure blob. Property Description Set By User Unit Default prepend hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster See Note string - hdinsight.primary-access-key Primary storage account key See Note string - hdinsight.storage-account-name-2 Optional for Spark when HDInsight using blob storage Storage account name for the HDInsight cluster (same as account-name-1 See Note string - hdinsight.secondary-access-key Secondary storage account key See Note string - " }, 
{ "title" : "Azure Data Lake", 
"url" : "unravel-4-5/advanced-topics/unravel-properties.html#UUID-dddcd3b9-9119-40d5-1f0e-f9b518bc0742_id_UnravelProperties-AzureDataLake", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Azure Data Lake", 
"snippet" : "Note: These property are required if you are using Azure Data Lake. Property Description Set By User Unit Default prepend adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net See Note URL - adl.clientld An application ID. An application registration has ...", 
"body" : "Note: These property are required if you are using Azure Data Lake. Property Description Set By User Unit Default prepend adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net See Note URL - adl.clientld An application ID. An application registration has to be created in the Azure Active Directory See Note string - adl.clientKey An application access key which can be created after registering an application See Note string - adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal See Note string - adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. See Note URL - " }, 
{ "title" : "Configurations for OnDemand Reports", 
"url" : "unravel-4-5/advanced-topics/unravel-properties/configurations-for-ondemand-reports.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Configurations for OnDemand Reports", 
"snippet" : "Table of Contents General Celery Configurations HiveServer2 Sessions Small Files and File Reports Small Files File Reports Queue Analysis There are no Top X specific properties. Set By User Column notes whether you are required to set the property (Req) or optionally can set the property (Opt) Gener...", 
"body" : " Table of Contents General Celery Configurations HiveServer2 Sessions Small Files and File Reports Small Files File Reports Queue Analysis There are no Top X specific properties. Set By User Column notes whether you are required to set the property (Req) or optionally can set the property (Opt) General Property Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Req - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the ondemand installation.) boolean True Celery Configurations Property Description Set By User Unit Default Celery Configurations com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: sqla+mysql - unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: db+mysql+pymysql - HiveServer2 Property Description Set By User Unit Default prepend: host FQDN or IP-Address of the HiveServer2 instance port Port for the HiveServer2 instance number 10000 authentication \"KERBEROS\" or \"LDAP\" or \"CUSTOM\" When set to KERBEROS you must kerberos.service.name=hive - - kerberos.service.name This must hive unravel.hive.server2.authentication KERBEROS string - password Use only when unravel.hive.server2.authentication=LDAP CUSTOM string - thrift.transport or \"TTransportBase\" for custom advanced usage - prepend: kerberos.principal Required when unravel.hive.server2.authentication KERBEROS e.g., user1@xyz.com string - kerberos.keytab.path Required only when unravel.hive.server2.authentication=KERBEROS This keytab file will be used to init and renew Kerberos Tickets path - Sessions Property Description Set By User Unit Default prepend: session.enabled Enables On-demand Sessions features tab in the UI true: Sessions enabled false: Sessions disabled bool true session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 Small Files and File Reports Property Description Set By User Unit Default You must restart the unravel_ondemand and unravel_ngui daemons for any changes in the two properties given below to take effect NOTE: : com.unraveldata.ngui.sfhivetable.schedule. prepend enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true interval Controls the frequency with which Unravel fetches fsimage from the cluster. For 4.5.0.0, it is recommended to use the default setting. With this setting, Unravel triggers fetch fsimage at 00:00 UTC day 1d You must restart the unravel_ondemand daemon for any changes to take effect NOTE: : unravel.python.reporting.files prepend disable Enables or disables Unravel ability to generate Small Files and File Reports. : disables the functionality in the Backend and UI. true : enables the functionality in the Backend and UI to generate the Small Files\/File Reports. false boolean false skip_fetch_fsimage If hdfs admin privileges can not be granted, then setting this to true allows an externally fetched fsimage for use by unravel Ondemand process. Ondemand etl_fsimage process does not fetch fsimage from name node. Instead, the fsimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir true: boolean false external_fsimage_dir Directory for fsimage when skip_fetch_fsimag e This directory Note: must be different string - hive_database Hive Database where Ondemand creates 5 hive tables (4 temporary, 1 permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the hive queries used for this feature run against default MR queue. It must string default Hive database hive_mr_queue The hive queries ran by Ondemand process run against this MR queue. It must string default The following properties apply to Small Files and File Reports; each has equivalent \"local\" properties. If the equivalent property is not files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true : absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean - min_parent_dir_depth Directory depth to start search at. If depth=2, search begins below 2 levels i.e. starts with directory such as HDFS_root\/one\/two count - max_parent_dir_depth Directory depth to end search at. Maximum is 50. : Example Depth=5: search ends at HDFS_root\/one\/two\/three\/four\/five Depth=6: search ends at HDFS_root\/one\/two\/three\/four\/five\/six count - drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false t rue : given directory list is \/one\/two Example \/ - lists files in \/ false: \/one - lists files in one \/one\/two - lists files in \/one\/two \/ - lists files in \/, \/one and \/one\/two. true: \/one - lists files in \/one and \/one\/two \/one\/two - lists files in \/one\/two boolean - SmallFiles Property Description Set By User Unit Default NOTE: prepend small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. : absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false true true files.small_files_min_parent_dir_depth If depth=2, search begins below 2 levels i.e. starts with directory such as HDFS_root\/one\/two Directory depth to start search at. 0 files.small_files_max_parent_dir_depth Depth=6: search ends at HDFS_root\/one\/two\/three\/four\/five\/six Depth=5: search ends at HDFS_root\/one\/two\/three\/four\/five : Example Directory depth to end search at. Maximum is 50. 10 files.small_files_drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false t rue : given directory list is \/one\/two Example \/ - lists files in \/ false: \/one - lists files in one \/one\/two - lists files in \/one\/two \/ - lists files in \/, \/one and \/one\/two. true: \/one - lists files in \/one and \/one\/two \/one\/two - lists files in \/one\/two true File Reports Property Description Set By User Unit Default NOTE: unravel_ondemand prepend: huge_files_threshold_size file size >= threshold_size bytes 100GB huge_files_min_files number of files in directory >= min_files count 1 huge_files_top_n_dirs maximum number of directories to display count 10 medium_files_max_threshold_size file size <= max_threshold_size bytes 10GB medium_files_min_threshold_size file size >= min_threshold_size bytes 5GB medium_files_min_files number of files in directory >= min_files count 5 medium_files_top_n_dirs maximum number of directories to display count 20 tiny_files_threshold_size file size <= threshold_size bytes 100KB tiny_files_min_files maximum number of directories to display count 10 tiny_files_top_n_dirs maximum number of directories to display count 30 empty_files_min_files number of files in directory >= min_files count 10 empty_files_top_n_dirs maximum number of directories to display count 03 prepend: The following four properties are defined per file size Size Size See here boolean false Size See here count 0 Size See here count 10 Size See here boolean false Queue Analysis Property Definition Set By User Default Default prepend metrics.sensor.enabled Enables or disables queue metric sensor. boolean true poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 http.retries YARN Resource Manager HTTP connection retries. count 3 http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 prepend metrics.scale UI rendered graph metrics scale factor. number 1000 daterange.span UI report date picker range. days 30 Forecasting Reports & Cloud Reports Property Definition Set By User Unit Default unravel.python.reporting.cloudreport.enable Enable\/disable all ondemand cloud reports boolean true Set cluster properties. Set cluster manager properties. Be sure to only set the properties for your cloud provider, either Cloudera or Ambari. App Parameter Defaults Report N\/A " }, 
{ "title" : "Sensors", 
"url" : "unravel-4-5/advanced-topics/unravel-properties/sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Sensors", 
"snippet" : "Table of Contents All Sensors Resource Usage Sensor Spark Sensor Most Sensor properties are set via JVM arguments, when they can be set in the file the file name is noted. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JV...", 
"body" : " Table of Contents All Sensors Resource Usage Sensor Spark Sensor Most Sensor properties are set via JVM arguments, when they can be set in the file the file name is noted. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend client.rest.queue The queue length for outgoing REST HTTP requests 20000 client.rest.retryfail Cool-down period after unsuccessful attempt to make REST HTTP request in nanoseconds ns 30 seconds client.rest.conn.timeout.ms REST HTTP request timeout in milliseconds ms 100 client.rest.shutdown.ms Maximum time to wait for orderly shutdown of the REST client (if exceeded some messages still in the queue will be lost) ms 10 client.rest.dns.ttl The period to refresh the DNS info in milliseconds - IP is pre-resolved and kept till the next refresh if no failures are observed ms 6 hours client.rest.priority.retries Certain critical messages have priority flag and their transmission will be reattempted this many times 5 no prepend unravel.server.hostport Unravel server host:port information - Resource Usage Sensor Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend agent.metrics.enabled_keys Comma separated list of metric type names which are enabled for collection CLS availableMemory,cpuUtilization, processCpuLoad,systemCpuLoad, maxHeap, usedHeap, vmRss,gcLoad no prepend unravel.metrics.factor Sampling period scale down factor 1 Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . no prepend metricsCaptureFilter Format allow specifying single ordinals for component IDs as well as ranges and enumerations - e.g.,metricsCaptureFilter=1,2,5-10,turns on metrics collection for components 1, 2, 5 to 10 0-1500 Spark Sensor Property Definition Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend enableLiveUpdates Enable live updates for Spark apps boolean False enableCachingInfo Enable tracking caching info for Spark apps boolean False enableSampling Enable data sampling between operators for Spark apps boolean False Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . prepend clusterID The cluster ID - currently only used in Spark " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server...", 
"body" : " Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server " }, 
{ "title" : "Installing Sensors", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/installing-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors", 
"snippet" : "Individual Applications Submitted Through spark-submit Individual Hive Queries...", 
"body" : " Individual Applications Submitted Through spark-submit Individual Hive Queries " }, 
{ "title" : "Individual Applications Submitted Through spark-submit", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/installing-sensors/individual-applications-submitted-through-spark-submit.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP Obtain the Sensor. The...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP Obtain the Sensor. The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST_IP http:\/\/UNRAVEL_HOST_IP:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Locate the sensor file: unravel-agent-pack-bin.zip Run the Sensor to Intercept Spark Apps. Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit by first creating a destination directory that is readable by all users. We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}3000\/hh\/unravel-agent-pack-bin.zip\n# cd {UNRAVEL_SENSOR_PATH}\n# hdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\n# set UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions and spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: (UNRAVEL_SENSOR_PATH} unravel-agent-pack-bin.zip (UNRAVEL_SENSOR_PATH} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNRAVEL_SENSOR_PATH={UNRAVEL_SENSOR_PATH}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option B: If You Run Spark Apps in yarn-client Mode. To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST If you use multiple hosts as clients, on each client. # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip Define spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: (UNZIPPED_ARCHIVE_DEST} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNZIPPED_ARCHIVE_DEST={UNZIPPED_ARCHIVE_DEST}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --driver-java-options \"-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs=spark-$SPARK_VERSION\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR " }, 
{ "title" : "Individual Hive Queries", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/installing-sensors/individual-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard Map...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive Option C: Installing the MapReduce JVM Sensor on HDFS Option D: Installing the MapReduce JVM Sensor on the Local File System Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor and Configure Impala Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP " }, 
{ "title" : "Upgrading the Unravel Server and Sensors", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/upgrading-the-unravel-server-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Upgrading the Unravel Server and Sensors", 
"snippet" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Unravel Sensor Upgrade sensors on CDH cluster Upgrade sensors on HDP cluster Upgrade sensors on MAPR cluster Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some...", 
"body" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com \n Unravel Server \n Unravel Sensor \n Upgrade sensors on CDH cluster \n Upgrade sensors on HDP cluster \n Upgrade sensors on MAPR cluster Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Lines beginning with '\/\/' are comments. Copy the new RPM to each Unravel host. Stop each host simultaneously. \/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously. \/\/ host1\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host2\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host3\n# sudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties. You must enter add license key to unravel.properties After all the RPM upgrades finish, restart Unravel Server on each host simultaneously. \/\/ host1\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host2\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host23\n# sudo \/etc\/init.d\/unravel_all.sh start Complete any deployment-specific upgrade steps. Unravel Sensor \n HIGHLIGHTED \n UNRAVEL_HOST_IP \n SPARK_VERSION _X.Y.Z \n HIVE_VERSION_X.Y.Z Upgrade sensors on CDH cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Check you current sensor version. Log in to the Cloudera Manager and click the parcel () in the top menu bar. Click the Check for New Parcels button and look for UNRAVEL_SENSOR entries. If newer sensors are available, it will be shown as another entry like below. Click the Download and then Distribute button, then activate for the newer version of the sensors. When activating the new sensors, you will be notified that Hive and Spark services must be restarted. Once new sensor activation is completed, the old version is automatically disabled. Upgrade sensors on HDP cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client folders. Tar these two folders and scp to all cluster nodes. # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar Upgrade sensors on MAPR cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client folders. Tar these two folders and scp to all cluster nodes. # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Uploading Spark Programs to Unravel", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/uploading-spark-programs-to-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading Spark Programs to Unravel", 
"snippet" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their...", 
"body" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: \n HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their location on the spark-submit Example: In yarn-client mode local \n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {Comma separated list of files} \\\n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR The default value of spark.unravel.program.dir Example: In yarn-cluster mode \n --files {comma-separated-list-of-source-files} spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_SOURCE_FILE_DIRECTORY}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {comma-separated-list-of-source-files} \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option 2: Upload a Zip Archive Package all relevant source files into a zip archive. It's advisable to keep the archive small by including only the relevant driver source files. Example: In yarn-client mode local \n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Example: In yarn-cluster mode \n --files $SRC_ZIP_PATH --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $SRC_ZIP_PATH \\\n --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Unravel searches for source files in this order: \n spark.unravel.program.dir Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Applications Program " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-5/advanced-topics/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel # sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel\n# sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Appendices", 
"url" : "unravel-4-5/appendices.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices", 
"snippet" : "Server Daemon Reference HBASE Alerts and Metrics...", 
"body" : " Server Daemon Reference HBASE Alerts and Metrics " }, 
{ "title" : "Server Daemon Reference", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_hostN Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2_N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma_N Metrics Analyzer unravel_ngui aNGular web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s_N Elasticsearch unravel_sw_N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor \\ Impala unravel_zk_N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending o...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp HDFS com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) 30 JDBC unravel.jdbc.username MySQL (embedded or external) username for db unravel unravel.jdbc.password MySQL (embedded or external) password for db random generated for bundled MySQL unravel.jdbc.url This is JDBC URL without username and password jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prodc Kafka com.unraveldata.kafka.broker_list embedded Kafka 127.0.0.1:4091 mapreduce com.unraveldata.longest.job.duration.days Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) 2 Oozie com.unraveldata.oozie.fetch.interval.sec seconds between intervals for fetching Oozie workflow status 120 com.unraveldata.oozie.fetch.num Max number of jobs to fetch during an interval 100 oozie.server.url URL for accessing Oozie to track workflows http:\/\/localhost:11000\/oozie Zookeeper com.unraveldata.zk.quorum embedded Zookeeper ensemble in form host1:port1,host2:port2, … 127.0.0.1:4181 " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to jvm to override other settings unset HADOOP_CONF_DIR The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml as discovered by running \"hadoop fs -ls \" UNRAVEL_DATA_DIR A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data, zk_data tmp_data com.unraveldata.tmpdir \/srv\/unravel UNRAVEL_LISTEN_PORT The Web UI port on the primary or standalone Unravel installation ( service unravel_ngui 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable Root Environment Settings", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Root Environment Settings", 
"snippet" : "The optional file \/etc\/unravel_ctl Env Variable Description Default if not set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel...", 
"body" : "The optional file \/etc\/unravel_ctl Env Variable Description Default if not set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-5/appendices/server-daemon-reference.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and ...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~3.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. " }, 
{ "title" : "HBASE Alerts and Metrics", 
"url" : "unravel-4-5/appendices/hbase-alerts-and-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Alerts", 
"url" : "unravel-4-5/appendices/hbase-alerts-and-metrics.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-Alerts", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Alerts", 
"snippet" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offl...", 
"body" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region in transition beyond threshold period. If a region server is dead, this is common. If not run \"hbase hbck\" to see if your HBase cluster has corruptions. Server availability Dead region servers Check region server logs for more information Performance Region servers with reads > 20% of avg Region server hotspotting. Split regions or randomize the keys. Region servers with writes > 20% of avg Region server hotspotting. Split regions or randomize the keys. Regions within a table with reads > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a table with writes > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a regionserver with reads > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Regions within a regionserver with writes > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Load, osload > 20% of avg Check for compactions, regions in transition and server logs Balancer not running Enable Blancer # of compactions and length of compaction Disable periodic automatic major compactions by setting - hbase.hregion.majorcompaction to 0 Storage Regionservers with storage (storefilesie sum) > 20% of avg Split or randomize the keys Regions within a table with storage (storefilesie sum) > 20% of avg for that table Split or randomize the keys Temporal e.g. requests > 20% higher for the last 1 hour as compared to the prior 3 hours (just an example) Check master and region server alerts or environment issues which could be slowing down the read\/write " }, 
{ "title" : "HBase Cluster Metrics", 
"url" : "unravel-4-5/appendices/hbase-alerts-and-metrics.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-HBaseClusterMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ HBase Cluster Metrics", 
"snippet" : "Master\/Cluster Metrics JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in mi...", 
"body" : "Master\/Cluster Metrics JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseconds numDeadRegionServers Number of dead Region Servers count numRegionServers Number of live Region Servers count ritCount The number of regions in transition count ritCountOverThreshold The number of regions that have been in transition longer than a threshold time seconds ritOldestAge The age of the longest region in transition, in milliseconds millliseconds OS Metrics (Ambari Only) OS Metrics Description Unit jvm_* jvm metrics number rpc_* rpc metrics number " }, 
{ "title" : "Region Server Metrics", 
"url" : "unravel-4-5/appendices/hbase-alerts-and-metrics.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-RegionServerMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Region Server Metrics", 
"snippet" : "JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local Data...", 
"body" : "JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local DataNode, 0-100 percentage readRequestCount The number of read requests received count regionCount The number of regions hosted by the regionserver count slowOPCount The number of operations we thought were slow. OP: delete, get, put, increment, append count storeFileSize Aggregate size of the store files on disk bytes writeRequestCount The number of write requests received count OS Metrics (Ambari Only) OS Metrics Description Unit cpu_user cpu percentage disk.disk_free Amount of free disk space bytes disk.write_bps Number of bytes written per second to disk. bytes per second disk.read_bps Number of bytes read per second to disk. bytes per second load.load_one load number memory.mem_free Percentage of free memory. percentage network.bytes_in Total number incoming bytes to network. bytes network.bytes_out Total number outgoing bytes to network. bytes " }, 
{ "title" : "Table\/Region Metrics", 
"url" : "unravel-4-5/appendices/hbase-alerts-and-metrics.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-TableRegionMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Table\/Region Metrics", 
"snippet" : "Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served byt...", 
"body" : " Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served bytes readRequestCount Number of read requests this region server has answered count writeRequestCount Number of mutation requests this region server has answered count " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-5/troubleshooting.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Troubleshooting", 
"snippet" : "Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround:...", 
"body" : " Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround: " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "unravel-4-5/troubleshooting/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh Email the bundle to the Unravel support team. " }, 
{ "title" : "Release Notes", 
"url" : "unravel-4-5/release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes", 
"snippet" : "v4.5.0.4 Release Notes v4.5.0.3 Release Notes v4.5.0.2 Release Notes v4.5.0 Release Notes v4.5.x - Upgrade Instructions v4.5.x - Updates to Unravel Properties...", 
"body" : " v4.5.0.4 Release Notes v4.5.0.3 Release Notes v4.5.0.2 Release Notes v4.5.0 Release Notes v4.5.x - Upgrade Instructions v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "v4.5.0.4 Release Notes", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Version", 
"snippet" : "Release Date: 02\/11\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 02\/11\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/C...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ New Features", 
"snippet" : "Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363) Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in run...", 
"body" : " Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363) Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in running state from application page. (CUSTOMER-337) Bug Fixes RBAC tagcmd does not always work in all environments. (CUSTOMER-592) LDAP AUTH failing due to null pointer exception. (CUSTOMER-600) Tagged Spark jobs not showing in workflow view. (CUSTOMER-608) Diagnostics files are either empty or not complete. (CUSTOMER-621) OnDemand reports runs \"successfully\" but not showing in UI. (CUSTOMER-635) Queue analysis report generation fails with a \"x not in list\" error. (REPORT-277) Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-5/release-notes/v4-5-0-4-release-notes.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Known Issues", 
"snippet" : "HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characte...", 
"body" : " HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge. (UIX-1589) Cloud Reports: You must select an instance type Queue analysis data can be missing or skewed if queues were created or deleted in a middle of the report range. (REPORT-316) Top X Report: Hive-on-Tez apps will show vcoreseconds and memory seconds values as zero. (TEZLLAP-249) The group-by pie charts in Cluster Discovery may be empty if don't have enough historic data (CLOUD-221) Cloud discovery functionality in cloud reports may fail with \"UDObject\" object has no attribute \"warn\" Add support for custom SSL cert for Ambari and Cloudera Manager in ondemand reports (CUSTOMER-649) Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0.3 Release Notes", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/28\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01\/28\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Pa...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ New Features", 
"snippet" : "NONE...", 
"body" : "NONE " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node...", 
"body" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node. (CUSTOMER-419) Bug Fixes Impala Chargeback shows \"User\" field data in report. (CUSTOMER-605) Unravel Spark sensor supports clusterId with spaces. (CUSTOMER-595) Airflow connectivity fixed. (CUSTOMER-575) Airflow Workflow are now displayed on Workflow page. (CUSTOMER-458) unravel_ondemand daemon is now included in unravel_all.sh. (CUSTOMER-434, CUSTOMER-375) Queue Analysis collects metrics for nested queue on HDP when configured to use Fair Scheduler. (REPORT-294) Queue metric sensor works with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled, (PLATFORM-927, PLATFORM-1052) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-5/release-notes/v4-5-0-3-release-notes.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger a...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge.(UIX-1589) Cloud Reports: You must select an instance type Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0.2 Release Notes", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features None Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features None Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01\/21\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ New Features", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "None", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-None", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ None", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) Unravel_km es_clear.sh...", 
"body" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) Unravel_km es_clear.sh " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-5/release-notes/v4-5-0-2-release-notes.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supp...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supported, (CUSTOMER-440, CUSTOMER-552) Unravel Spark sensor doesn't support clusterId with spaces, (CUSTOMER-59 fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting not working properly in Edge(UIX-1589) may fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0 Release Notes", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01 \/14\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01 \/14\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "Supported Browsers", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported Browsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ New Features", 
"snippet" : "Applications Ability to interactively 'Kill' and 'Move' YARN Live view Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark jobs show completed stage, running stage and overall progress. ...", 
"body" : " Applications Ability to interactively 'Kill' and 'Move' YARN Live view Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark jobs show completed stage, running stage and overall progress. (CUSTOMER-111) Insights New Data, operator and SQL-level insights for Impala. New efficiency and failure events for Hive\/Tez. Cloud Reports Added several new API end points. Retrieve app recommendations. (CUSTOMER-396) Get finished and total number of jobs that ran on a particular date range . (CUSTOMER-367) Get status, error, logs, summary of individual Apps. (CUSTOMER-253) * Preview features are in beta and is subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend you deploy Preview features in production environment " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions Link to the offending application is contained in email notification. (CUSTOMER-251) Violation email is now sent to the owner using LDAP email address field. (CUST...", 
"body" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions Link to the offending application is contained in email notification. (CUSTOMER-251) Violation email is now sent to the owner using LDAP email address field. (CUSTOMER-350) Rules for catching named workflows that have missed their SLAs are applied are no longer indiscriminately to all running workflows. (CUSTOMER-400) Operations Chargeback Reports now support Impala queries. (IMPALA-118) Top-X Report now supports Spark and Hive-On-Tez. (REPORT-241) Sessions Improved Spark's session recommendations for bigger containers, effective broadcast size & shuffle partition size. (SESS-128, SESS-129, SESS-130) Spark com.unraveldata.spark.maste com.unraveldata.spark.eventlog.appDuration Reporting OnDemand is now supported via PostgreSQL DB. (REPORT-229) Added support to disable Small Files. (REPORT-204) Small File Report generation is faster. (REPORT-181) Small Files\/ File Reports can run even without dfsadmin privileges. (REPORT-143) Queue Analysis now is interactive, supports MapR disk metrics, and has UI presentation and performance improvements (REPORT-105) Tez Improvements in map tasks recommendations for Hive\/Tez apps (TEZLLAP-47) Removed recommendations to increase the number of tasks for ORDER BY reducer vertices. (TEZLLAP-35) Improvements in hive.tez.auto.reducer.parallelism recommendations for Hive\/Tez apps. (TEZLLAP-37) Improvements in failure events for Hive\/Tez apps. (TEZLLAP-80) Added support for Hive\/Tez on MapR 5.2 and 6.0. (TEZLLAP-103) A failure event is now triggered when a Tez DAG fails. (TEZLLAP-145) Hive queries executed as Tez apps are now connected with their Tez parent app in the UI. (TEZLLAP-199) Hive query metrics are now published to the Auto-Actions framework. (TEZLLAP-206) Impala Input tables used by Impala queries are now displayed in the Data Page. (IMPALA-121) DML statements are now captured. ( IMPALA-166) Cluster name is now available when using impalad as the data source. (IMPALA-78) UIX UI shows list of triggered auto actions on mouse hover over the AA2 badge. (UIX-1249) Queue Analysis Graphs are now interactive and clicking on any of the graphs will open up the cluster view for that point in time with a list of all apps that were running at that time (UIX-1443) Average values for each queue metric is now displayed next to its value when selecting a point on a graph (UIX-1493) Average run apps, used memory, used vcores, and used disk (MaprR only) now are included as columns in the list of analysed queues and can be sorted by these values (UIX-1535) All queue metric labels now have descriptions, when hovering over the metric label a popup window will be shown giving a complete description (UIX-1444) Raw queue metrics now have a retention policy to preventing possibility of the database running out of space, default is 90 days (PLATFORM-945) MapR disk metrics are supported, the 4th graph \"Disk Usage\" will be displayed alongside with Apps, Memory, VCore usage when monitoring MapR clusters (REPORT-220) Rendering performance of graphs was greatly improved in 4.5.0.0 and users should notice much less lag when opening queue metric graphs (UIX-1515) Queue metric collector sensor and queue analysis reports now fully support PostgreSQL as backend storage database alongside with MySQL (REPORT-274) HiveHook Sensors HiveHook Support for Hive 2.2.0 & Hive 2.3.0 Support for Single HiveHook class Bug Fixes Customer Fixes Provide 'path' filter options in Small Files Report. (CUSTOMER-371) Support for `com.unraveldata.rbac.tagcmd` property. (CUSTOMER-545) For inbound HTTPS connections to Unravel UI, provide support to restrict TLS protocols. (CUSTOMER-515) com.unraveldata.ldap.bind_pw=bigsecret needs to support encrypted passwords (CUSTOMER-473) Unravel using global resources to make connections to internet (CUSTOMER-449) Support for RHEL6 with OnDemand Framework (CUSTOMER-435) Spark Spark application\/job progress (CUSTOMER-475) Spark “Load Logs” and “Application Diagnostics” (CUSTOMER-508) Spark-shell is generating unwanted warnings (CUSTOMER-407) Spark execution graph display issue (CUSTOMER-283) Execution tab needs to show unique Stage IDs (CUSTOMER-236) Kafka Kafka Consumer Groups not displaying (CUSTOMER-366) Kafka page not showing all partitions in UI (CUSTOMER-544) Application Page Filters - add ability to select multiple users and multiple queues (CUSTOMER-491) Sorting doesn't take into account MB vs GB vs TB (CUSTOMER-383) Airflow Monitoring does not work consistently (CUSTOMER-304) Workflow compare panel only showing 24 hours worth of data (CUSTOMER-291) Queue analysis reports is empty and sometimes hits 1000 field limit in ES (CUSTOMER-471) Sessions Recommendations generated by Application are now c onsistent Inside and Outside Sessions (SESS-138) CMP API issue picks up data ~4 days later than the earliest data point viewable. ( SESS-60) Auto Actions Auto actions are not triggering for long running Hive apps with Tez as Execution Engine (PLATFORM-643) Reporting TOP-X UI Stability Fixes (REPORT-263) Error handling Improvements for Small Files \/ File Reports (REPORT-235) Small Files Data Inconsistencies Improvements (REPORT-209) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) Tez Tez apps are now visible in the UI regardless of missing or incorrect ATS configuration (TEZLLAP-50) A Hive\/Tez query is no longer associated with multiple DAGs in the Unravel UI. (TEZLLAP-198) Fixed NullPointerException raised in unravel_ew_1.log Fixed NumberFormatException raised when tez.am.grouping.max-size tez.grouping.max-size Fixed NullPointerException raised when dagMap is not populated. (TEZLLAP-219) Fixed NumberFormatException raised in MapR 6.0 when tez.grouping.min-size Tez apps are no longer stuck in running state in MapR 6.0. (TEZLLAP-231) Impala HTTP connection to CM will no longer wait indefinitely (IMPALA-139) Short-running Impala queries are no longer missed (IMPALA-165) Start and end query times use the correct timezone when using impalad as the data source (IMPALA-164) UIX Queue Analysis graphs are taking long time to render (UIX-1515) Added Report Archives Support for PostgresSQL. (REPORT-257) Added appsRun metric on queue analysis graph (UIX-1476) Global search no longer returns multiple results for single Hive Query ID. (UIX-1224) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) Spark Major performance improvement in compressed Event log parsing. (USPARK-164 ) Queue Analysis MapR queue analysis report is missing Disk metrics. (REPORT-220) Queue metric sensor is not collecting any data if PostgreSQL is used. (REPORT-274) ES reports index limit was increased to 10000. (PLATFORM-821) Queue Analysis report stuck at putting data to ES. (REPORT-291) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-5/release-notes/v4-5-0-release-notes.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting n...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting not working properly in Edge ( UIX-1589 ) may fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or TLS with authentication is enabled (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.x - Upgrade Instructions", 
"url" : "unravel-4-5/release-notes/v4-5-x---upgrade-instructions.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions", 
"snippet" : "Table of Contents Supported Installs\/Upgrade PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already b...", 
"body" : " Table of Contents Supported Installs\/Upgrade PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL either as an internal or external database 4.4.x you might be using with PostgreSQL which was bundled " }, 
{ "title" : "The following upgrade options arenotsupported:", 
"url" : "unravel-4-5/release-notes/v4-5-x---upgrade-instructions.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-Thefollowingupgradeoptionsarenotsupported", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ The following upgrade options arenotsupported:", 
"snippet" : "Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL...", 
"body" : " Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL " }, 
{ "title" : "Supported Installs\/Upgrade", 
"url" : "unravel-4-5/release-notes/v4-5-x---upgrade-instructions.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-SupportedInstallsUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ Supported Installs\/Upgrade", 
"snippet" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel...", 
"body" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel for MySQL Upgrade from 4.4.x If your database is MySQL If the MySQL data has not been migrated to the new partitioned tables, then follow all the steps in Upgrade from 4.3.1.X If MySQL data is already migrated to partitions, then follow all the steps in Upgrade from 4.3.1.X section but skip the \" MySql Partitioning and Data Migration To check if the tables are already partitioned, execute the following command: # echo \"show table status like '%blackboards%'\" | \/usr\/local\/unravel\/install_bin\/db_access.sh If your database is PostgreSQL Install RPM If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.3.x Embedded\/external MySQL to 4.5 using external MySQL Unravel 4.5 uses partitioned MySQL tables to manage the disk space and you must prepare for the upgrade. Prepare for upgrade Check amount of disk space used by MySQL (Space_Used) via the CLI with the command Embedded MySQL # du -sh \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs du -sh Check amount of disk space available in the database partition (Space_Available) via the CLI with the command Embedded MySQL # df -h \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs df -h If Space_Used > Space_Available then follow instructions to move MySQL Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ # mkdir -p \/usr\/local\/unravel\/share\/java\n# sudo cp \/usr\/local\/unravel\/dlib\/mybatis\/mysql-connector*.jar \/usr\/local\/unravel\/share\/java Upgrade Download RPM Install RPM Complete the instructions at MySql Partitioning and Data Migration RBAC properties have been changed. See here If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.2.x 4.2.x (embedded MySQL) -> 4.5 (external MySQL) 4.2.x (external MySQL) -> 4.5 (external MySQL) Upgrade to 4.3.1.7 then follow the instructions above " }, 
{ "title" : "v4.5.x - Updates to Unravel Properties", 
"url" : "unravel-4-5/release-notes/v4-5-x---updates-to-unravel-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties", 
"snippet" : "This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties Table of Contents New LDAP Custom Banner HBASE Hive-hook SSL Small Files and File Reports Small Files File Reports Forecasting & Cloud Reports Spark Application Liveness Experim...", 
"body" : " This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties Table of Contents New LDAP Custom Banner HBASE Hive-hook SSL Small Files and File Reports Small Files File Reports Forecasting & Cloud Reports Spark Application Liveness Experimental Renamed\/Replaced Properties Hive RBAC " }, 
{ "title" : "New", 
"url" : "unravel-4-5/release-notes/v4-5-x---updates-to-unravel-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-New", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ New", 
"snippet" : "Property Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replace...", 
"body" : " Property Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replaced hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.groupFilter replaced hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupDNPattern replaced hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.guidKey replaced hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.userDNPattern replaced hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userFilter replaced hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.groupMembershipKey replaced hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupClassKey replaced hive.server2.authentication.ldap.groupClassKey Custom Banner com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. True text end.date False - com.unraveldata.custom.banner.text Text to display when display - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the custom banner Format: YYYYMMDD T Z - HBASE (See here com.unraveldata.hbase. clustername HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... * Example: http:\/\/your.master.server:16010,http:\/\/your.region.server:16030 - Hive-hook SSL (See here com.unraveldata.live.logreceiver.port.https HTTPS server port (negative value means disabled HTTPS server) -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks - com.unraveldata.server.ssl.cert_password KeyStore password - com.unraveldata.server.ssl.trust_store_path TrustStore file path - com.unraveldata.server.ssl.trust_store_password TrustStore password - Small Files and File Reports (See here unravel.python.files_use_avg_file_size_flag true false - unravel.python.min_parent_dir_depth Directory depth to start search at. - unravel.python.max_parent_dir_depth Directory depth to end search at. Maximum is 50. - unravel.python.drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. false - Small Files (See here unravel.python.reports.files.small_files_use_avg_file_size_flag See here true unravel.python.reports.files.small_files_min_parent_dir_depth See here 0 unravel.python.reports.files.small_files_max_parent_dir_depth See here 10 reports.files.small_files_drill_down_subdirs_flag See here true File Reports (See here The following four properties are defined per file size Size unravel.python.reports.files. Size See here false unravel.python.reports.files. Size See here 0 unravel.python.reports.files. Size See here 10 unravel.python.reports.files. Size See here false Forecasting & Cloud Reports (see here com.unraveldata.ambari.manager.url URL of cloud manger, e.g., http:\/\/$clouderaserver:7180, http:\/\/$ambariserver:8080 For Cloudera, if the URL does not contain a port you must define manager.port - com.unraveldata.ambari.manager.username Username to log into the manager - com.unraveldata.ambari.manager.password Password for the username. - Spark com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor com.unraveldata.spark.eventlog.appDuration If application duration is more than configured value load the event log is not loaded. 1440 mins Application Liveness com.unraveldata.appstatus.refresh.mins Time interval in minutes to scan for running applications and marking the stale ones. 5 mins com.unraveldata.appstatus.stale_limit.mins Maximum number of minutes since the latest app update from RM before it is marked as stale. 10 mins Experimental Experimental features (not to be used in production) com.unraveldata.cluster_access.host Cluster Access Service host (where the service will be bound) 0.0.0.0 com.unraveldata.cluster_access.port Cluster Access Service port 4020 com.unraveldata.sregistry.hostport Service Registry host:port ${com.unraveldata.zk.quorum} com.unraveldata.sensor.polling.secs The base polling period of Unravel reactive sensors in seconds. 30s com.unraveldata.appevents.emitters.exclude.list Comma separated list of the application event emitter IDs which will be disabled\/excluded. - com.unraveldata.multicluster.enabled Allow Unravel to operate in multi-cluster mode. In this mode a service registry will be used to discover and access all registered (local and remote) clusters. false " }, 
{ "title" : "Renamed\/Replaced Properties", 
"url" : "unravel-4-5/release-notes/v4-5-x---updates-to-unravel-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-RenamedReplacedProperties", 
"breadcrumbs" : "Home \/ Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ Renamed\/Replaced Properties", 
"snippet" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPa...", 
"body" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPattern hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.guidKey hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userDNPattern hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.userFilter hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupMembershipKey hive.server2.authentication.ldap.groupClassKey com.unraveldata.ldap.groupClassKey RBAC You must update these properties manually. Property Replaced with com.unraveldata.rbac.mode com.unraveldata.login.mode com.unraveldata.rbac.user.operations.enabled com.unraveldata.ngui.user.mode " }, 
{ "title" : "", 
"url" : "unravel-4-4.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.4 •...", 
"body" : " Unravel 4.4 • " }, 
{ "title" : "Overview", 
"url" : "unravel-4-4/overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "unravel-4-4/overview.html#UUID-87d77c03-f385-8913-a4ea-b6ec587ca533_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. \n The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "unravel-4-4/overview.html#UUID-87d77c03-f385-8913-a4ea-b6ec587ca533_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. \n Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "unravel-4-4/overview.html#UUID-87d77c03-f385-8913-a4ea-b6ec587ca533_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API....", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. " }, 
{ "title" : "Installation Guides", 
"url" : "unravel-4-4/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides", 
"snippet" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Hortonworks Data Platform (HDP) HDP Pre-Installation Check Step 1: Install Unravel Server on HDP St...", 
"body" : " Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Hortonworks Data Platform (HDP) HDP Pre-Installation Check Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP Amazon Elastic MapReduce (Amazon EMR) Step 1: Install Unravel EC2 node for Amazon EMR Step 2: Connect Unravel EC2 node to new or existing EMR clusters Step 3: Set up AWS RDS for Unravel DB (Optional) Step 4: VPC Peering for Unravel EC2 node (Optional) EMR Support Matrix MapR MapR Pre-Installation Check Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Azure HDInsight Option 1: Install Unravel on a Separate Azure VM Create Azure Storage Account Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel Option 2: Install Unravel's Azure Marketplace App INTERNAL: Update the Azure Resource Manager if Needed INTERNAL: Step 3: Create Unravel VM Using ARM template INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions Upgrading the Unravel VM or App Finding Unravel properties' values in Microsoft's Azure MySQL Install and Configure MySQL for Unravel Moving Unravel MySQL to a Custom Location MySql Partitioning and Data Migration OnDemand Installation or Upgrade of OnDemand Library Versions and Licenses for OnDemand For more information on installations and configurations see Advanced Topics " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "unravel-4-4/install/install-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-4/install/install-cdh.html#UUID-3544bdfa-2c23-d97b-1312-006d1e2ea99b_id_ClouderaDistributionIncludingApacheHadoopCDHwithClouderaManagerCM-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Ordered Steps", 
"snippet" : "Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Unravel's reports: Sessions FIXLINK, Cluster Optimization FIXLINK, Capacity Forecasting FIXLINK, and Small Files FIXLINK require that you use MySQL FIXLINK as your database and in...", 
"body" : " Cloudera Pre-Installation Check Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Unravel's reports: Sessions FIXLINK, Cluster Optimization FIXLINK, Capacity Forecasting FIXLINK, and Small Files FIXLINK require that you use MySQL FIXLINK as your database and install Unravel's Ondemand. See OnDemand For more information on installations and configurations see Advanced Topics " }, 
{ "title" : "Cloudera Pre-Installation Check", 
"url" : "unravel-4-4/install/install-cdh/install-cdh-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Cloudera Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 5.12, 5.13, 5.14, 5.15 (including Spark 2.3.x) with Kerberos Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Hive versions 0.10.0 through 1...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 5.12, 5.13, 5.14, 5.15 (including Spark 2.3.x) with Kerberos Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Hive versions 0.10.0 through 1.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark Unravel host should be a client\/gateway (Hadoop and Hive commands in PATH) If Spark2 service is installed, Unravel host should be a client\/gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) On Unravel Edge-node server, do not NTP should be running and in-sync with the cluster Access Permissions Local user unravel:unravel is created during installation, but can be changed later If Kerberos is in use, a keytab for an Unravel Kerberos principal is required for access to: Access to YARN's \"done dir\" in HDFS Access to YARN's log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) For Impala support, a read-only account for Cloudera Manager API is needed Network The following ports must be open on the Unravel edge node: Port(s) Direction Description 3000 Both Traffic to and from Unravel UI If you plan to use Cloudera Manager to install Unravel Sensors, the Cloudera Manager service must also be able to reach the Unravel edge node on port 3000. HDFS ports In Traffic from Hadoop cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 4043 In UDP and TCP traffic from the entire cluster to Unravel Server(s) 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 3316, 4020, 4091, 4176, 4181-4189 Both Localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "unravel-4-4/install/install-cdh/install-cdh-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unra...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive, and optionally Spark2. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. Update Site-Specific Properties in unravel.properties \/\/ Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/\n 3. Configure Unravel Server (Basic\/Core Optional for CDH) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data If Kerberos is Enabled: \n \n Create Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE. unravel Run Unravel Daemons with Custom User If Sentry is Enabled: \n Define your own alt principal with narrow privileges. The alt principal can be unravel rpm X \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n Your alt principal \n read \n Spark event log \n \n \n hdfs:\/\/user\/spark\/spark2ApplicationHistory \n Your alt principal \n read \n Spark 2 event log \n \n \n hdfs:\/\/user\/history \n Your alt principal \n read \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n Your alt principal \n read \n YARN aggregation folder \n \n \n hdfs:\/\/user\/hive\/warehouse \n Your alt principal \n read \n Obtain table partition sizes with \"stat\" only Please see Configure Permission for Unravel daemons on CDH Sentry Secured Cluste You can find the principal by using 'klist -kt KEYTAB_FILE' If you are using KMS and HDFS encryption, you might need to adjust kms-acls.xml . If you are using \"JNI\" based groups for HDFS (a setting in CM), then you will need to add \" export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native\" to \/usr\/local\/unravel\/etc\/unravel.ext.sh Switch User Depending on your cluster security configuration, you will need to run the \n switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Hive Metastore Access Hive metastore is accessed by Unravel server to analyze table usage in conjunction with Hive job instrumentation. Information is gathered using a Hive API that works very much like beeline connections which leverage the jdbc database connection protocol. As a quick-start approach, you can set Unravel to use the already-defined 'hive' user that is also used by HiveServer2. Alternatively, you can define a read-only metastore database user. If you want a custom user, then do the following steps for the particular kind of database that is used for Hive metastore: Connect to the Hive metastore using the normal conversational interface (mysql or psql, etc.) as an admin that can create new users. Create a user, e.g., unravel Grant select on all table in the hive database. As the new user, use the conversational interface (mysql or psql, etc.) from the Unravel server to verify their access. To complete the integration of the Hive metastore with the user, follow the steps in Hive Metastore Access Restart Unravel Server After the edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. 4. Log into Unravel Web UI Using a web browser, navigate to http:\/\/{UNRAVEL_HOST_IP} admin unraveldata UNRAVEL_HOST_IP. For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 5. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"url" : "unravel-4-4/install/install-cdh/install-cdh-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"snippet" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job...", 
"body" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. These instructions apply to Unravel Sensor 4.3 Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM \n Workflow Summary Obtain and distribute the parcel from Unravel Server. Put the Hive Hook JAR in AUX_CLASSPATH Configure the gateway automatic deployment of Hive instrumentation. Configure the gateway automatic deployment of Spark instrumentation. \n HIGHLIGHTED When Active Directory Kerberos is used, UNRAVEL_HOST_IP To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 1. Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, you can deactivate it now. Then click Activate 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh no subsitutions HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is Enabled: Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT {ROLE} # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE {ROLE} 3. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath 4. Configure the Gateway Automatic Deployment of Hive Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways In Cloudera Manager (CM): Go to Hive service. Select the Configuration Search for hive-site.xml Add the xml snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks \n com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger, com.unraveldata.dataflow.hive.hook.HivePostHook \n IMPORTANT! However, the \"HiveServer2 Advanced Configuration Snippet for hive-site.xml\" should have all 3 classes: 2 from Navigator and 1 from Unravel. Add the xml snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Save the changes with optional comment \"Unravel snippet in hive-site.xml \" Perform action Deploy Hive Client Configuration ) or by using the Actions Restart the Hive service. (Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. ) Again, monitor the situation to see if all Hive queries are failing with a class not found or permission problems. If they are failing hive-site.xml Troubleshooting Check Unravel Web UI If queries are running fine and appearing in Unravel Web UI, then you are done. 5. Configure the Gateway Automatic Deployment of Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \" spark-defaults In the Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use the fully qualified DNS or logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) \n spark-conf\/ UNRAVEL_HOST_IP SPARK_VERSION-X.Y \n SPARK_VERSION -X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true 5. Save changes. 6. Deploy client configuration by clicking the deploy glyph ( Actions Monitor the situation to see if all Spark queries are failing with a class not found or permission problems. If they are failing spark-defaults.conf In the case of yarn-client mode applications, the Spark default configuration won't be sufficient because the driver JVM starts before the configuration set through the SparkConf is applied. (See Apache's Spark Configuration here per-application profiling cluster-wide profiling 6. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide For instructions, see CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) 7. Enable Impala APM with CM as the Data Source In order to configure Unravel Server to retrieve Impala query data from ClouderaManager (CM) you need to: Add com.unraveldata.data.source=cm \/usr\/local\/unravel\/etc\/unravel.properties Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n \n com.unraveldata.cloudera.manager.url \n CM internal URL. Must start with http:\/\/ \n \n com.unraveldata.cloudera.manager.port \n (Optional) CM port number. You only need to specify this if your Cloudera Manager is not on port 7180. \n \n com.unraveldata.cloudera.manager.username \n CM username \n \n com.unraveldata.cloudera.manager.password \n CM password For example: \n \n \n \n com.unraveldata.data.source=cm \n com.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret \n com.unraveldata.cloudera.manager.port=9997 \n com.unraveldata.cloudera.manager.username=mycmname \n com.unraveldata.cloudera.manager.password=mycmpassword Make sure that the CM user in com.unraveldata.cloudera.manager.username curl \n curl --user CM_username: password CM CM \n curl --user CM_username:CM_password 'http:\/\/ CM _url: CM By default, the ImpalaSensor task is enabled. To disable it, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw 8. (Optional) Advanced Configuration Configuration for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Integrating LDAP Authentication for Unravel Web UI Troubleshooting \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR Verify that unravel \/user\/unravel\/ \n \n \n ClassNotFound com.unraveldata.dataflow.hive.hook.HivePreHook \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST References \n {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "Hortonworks Data Platform (HDP)", 
"url" : "unravel-4-4/install/install-hdp.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Hortonworks Data Platform (HDP)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-4/install/install-hdp.html#UUID-8f16a148-92d8-2a49-3101-b61298b362d3_id_HortonworksDataPlatformHDP-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Ordered Steps", 
"snippet" : "HDP Pre-Installation Check Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP Unravel's Reports: Sessions Cluster Optimization Capacity Forecasting Small Files require that you use MySQL OnDemand For more information on installations and configu...", 
"body" : " HDP Pre-Installation Check Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP Unravel's Reports: Sessions Cluster Optimization Capacity Forecasting Small Files require that you use MySQL OnDemand For more information on installations and configurations see Advanced Topics " }, 
{ "title" : "HDP Pre-Installation Check", 
"url" : "unravel-4-4/install/install-hdp/install-hdp-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ HDP Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 2.6.3.0 with Kerberos & spnego enabled Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache ver. equiv.) Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: ...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 2.6.3.0 with Kerberos & spnego enabled Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache ver. equiv.) Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway Verify that all clients are installed by checking that RPMs are installed; Unravel utilizes clients and associated libraries You need to register edge node to Ambari LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for an Unravel Kerberos principal is required for access to: Access to YARN's \"done dir\" in HDFS Access to YARN's log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active If you plan to use the Auto Actions feature (move \/ kill), you'll need to add the Unravel username to YARN yarn.admin.acl JDBC access to the Hive Metastore (read-only user is sufficient) Application Timeline Server (ATS) read-only Network The following ports must be open on Unravel Server's host machine(s): Port(s) Direction Description 3000 Both Traffic to and from Unravel UI HDFS ports In Traffic from Hadoop cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 4043 In UDP and TCP traffic from the entire cluster to Unravel Server(s) 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 3316, 4020, 4091, 4176, 4181-4189 Both Localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "unravel-4-4/install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel ...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Installation 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh Done The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include 3. Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example Values \n \n \n com.unraveldata.customer.organization \n \n Update \n \n Company_and_org \n \n com.unraveldata.tmpdir \n \n Update \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.login.admins \n \n Update admin \n \n admin \n \n \n com.unraveldata.advertised.url \n \n Add \n \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.history.maxSize.weeks \n \n Add \n \n 26 \n \n \n com.unraveldata.s3.batch.monitoring.interval.sec \n \n Add \n \n 120 \n \n \n com.unraveldata.spark.eventlog.location \n \n Add \n hdfs:\/\/\/user\/spark\/applicationHistory\/,hdfs:\/\/\/user\/spark\/sparkApplicationHistory\/,hdfs:\/\/\/user\/spark\/spark2ApplicationHistory\/ \n \n \n oozie.server.url \n \n Add \n http:\/\/example.localdomain:11000\/oozie \n \n \n yarn.resourcemanager.webapp.address \n \n Add \n deprecated in 4.4 - now read from yarn-site.xml If Kerberos is Enabled: Add authentication for HDFS... Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using properties klist -kt KEYTAB_FILE . unravel Run Unravel Daemons with Custom User If Ranger is Enabled: Add these permissions... Define your own alt principal with narrow privileges. The alt principal can be unravel rpm \n X \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/spark-history \n Your alt principal \n read+execute \n Spark event log \n \n \n hdfs:\/\/spark2-history \n Your alt principal \n read+execute \n Spark2 event log \n \n \n hdfs:\/\/mr-history\/done \n Your alt principal \n read+execute \n MapReduce logs \n \n \n hdfs:\/\/app-logs \n Your alt principal \n read+execute \n YARN aggregation folder \n \n \n hdfs:\/\/apps\/hive\/warehouse (default value of hive.metastore.warehouse.dir) \n Your alt principal \n read+execute \n Obtain table partition sizes \n \n Hive Metastore database GRANT \n hive \n read+execute \n Hive table information You must set yarn properties in Ambari. Log into Ambari and from the dashboard select YARN Configs Advanced Set yarn.acl.enable Add the Unravel user specified in com.unraveldata.kerberos.principal above yarn.admin.acl. Save your changes. Disable Impala Sensor Impala is not officially supported on HDP clusters therefore you should disable the ImpalaSensor by modifying the \/usr\/local\/unravel\/etc\/unravel.properties Open unravel.properties com.unraveldata.sensor.tasks.disabled com.unraveldata.sensor.tasks.disabled=iw 4. Update Unravel Installation with HDP configuration properties Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Edit the values in unravel.properties \n \n \n Property \n Description \n Example Values \n \n javax.jdo.option.ConnectionURL \n \n Update \n \n e.g. jdbc:mysql:\/\/ns1.example.com\/hive \n \n javax.jdo.option.ConnectionDriverName \n \n Update \n e.g. com.mysql.jdbc.Driver \n \n javax.jdo.option.ConnectionUserName \n \n Update \n e.g. hive \n \n javax.jdo.option.ConnectionPassword \n \n Update \n e.g. <hive_password> Note: This change will stick after later RPM upgrades; it does not need to be done each time. Switch User Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh {user} {group} where {user} {group} Run Unravel Daemons with Custom User Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. 5. Log into Unravel Web UI Using a web browser, navigate to http:\/\/{UNRAVEL_HOST_IP} admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 6. Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "unravel-4-4/install/install-hdp/install-hdp-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us HIGHLIGHTED UNRAVEL_HOST_IP 1. Convert Your Unravel Installation to HDP (If you have already run this step following the instructions on page Step 1: Install Unravel Server on HDP # sudo \/etc\/init.d\/unravel_all.sh ...", 
"body" : "Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Convert Your Unravel Installation to HDP (If you have already run this step following the instructions on page Step 1: Install Unravel Server on HDP # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change remains after RPM upgrades. 2. Update Site-Specific HDP Properties in unravel.properties Add these properties to\/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n \n com.unraveldata.yarn.timeline-service.webapp.address \n The http address of the Timeline service web application \n \n http:\/\/localhost \n \n \n com.unraveldata.yarn.timeline-service.port \n Timeline service port \n 8188 For example, com.unraveldata.yarn.timeline-service.webapp.address=http:\/\/172.16.1.101\ncom.unraveldata.yarn.timeline-service.port=8188 3. Verify Properties in unravel.properties Comparing to Equivalents in Hadoop Properties Open \/usr\/local\/unravel\/etc\/unravel.properties If you are using Spark1 and Spark2 you must com.unraveldata.spark.eventlog.location com.unraveldata.spark.eventlog.location =hdfs:\/\/\/spark-history\/, Check unravel.properties and update with the equivalent values in Ambari Web UI \/\/ Application logs directories \ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/\n \n\/\/ Hive Metastore database information for Unravel Hive Config \njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/{hostname}:3306\/{database_name} \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName={HiveMetastoreUserName} \njavax.jdo.option.ConnectionPassword={HiveMetastorePassword} Cross check the entries in unravel.properties against Ambari. Log into Ambari Web UI and verify the following properties have been set correctly in your unravel.properties file. The values listed in unravel.properties should match the equivalent properties in Ambari. On the left-hand menu of Ambari dashboard, click MapReduce2 Configs Advanced Advanced mapred-site Verify com.unraveldata.job.collector.done.log.base mapreduce.jobhistory.done-dir. On the left-hand menu of Ambari dashboard, click YARN Configs Advanced Node Manager Verify com.unraveldata.job.collector.done.log. aggregation.base yarn.nodemanager.remote-app-log-dir On the left-hand menu of Ambari dashboard, click Hive Configs Advanced Hive Metastore Verify javax.jdo.option.ConnectionURL Database Host Database URL Verify javax.jdo.option.ConnectionDriverName JDBC Driver Class Verify javax.jdo.option.ConnectionUserName Database Username 4. Start Unravel Server Note: Unravel must be up for the next step to complete. # sudo \/etc\/init.d\/unravel_all.sh start 5. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers Install Unravel Hive Hook and Spark Sensors (JAR files) onto HDP servers as follows (substitute the correct fully qualified host name or IP address for UNRAVEL_HOST_IP Login as root and ensure wget # yum install -y wget From Unravel server (e.g. edge node), run the following unravel_hdp_setup.sh script, and copy the resulting directories and JARs onto each host that will use Unravel Sensors Be sure to substitute valid values. \n SPARK_VERSION_X.Y.Z 1.3.0 1.5.0 1.6.0 2.0.0 2.1.0 2.2.0 2.3.0 \n HIVE_VERSION 1.2.0 0.13.0 1.2.0 # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --hive-version {HIVE_VERSION} --spark-version {SPARK_VERSION} As a result of executing this script, sensor files are installed locally on the edge node under the following directories: \n \/usr\/local\/unravel_client (Hive hook JARs) \n \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor JARs) Copy these two directories and JARs onto all nodes of the cluster (worker \/ edge \/ master). In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (this will be host2 6. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar oozie.libpath 7. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU Completion of this step requires a restart of all affected Hive services in Ambari UI. In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook \ncom.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR \ncom.unraveldata.hive.hook.tcp=true \ncom.unraveldata.host={add unravel gateway internal IP hostname} \n \n\/\/ Find below properties as it may already exists, concatenate it with a comma & no spaces\nhive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook \nhive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook \nhive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook If LLAP is enabled copy the above settings in Custom hive-interactive-site Manual edit hive-site.xml (no AWU) \n hive-site.xml \/etc\/hive\/conf\/ Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar If LLAP is enabled copy above line of code into Advanced hive-interactive-env You can manually edit hive-env.sh without using AWU. The hive-env.sh \/etc\/hive\/conf\/ Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env export HADOOP_CLASSPATH export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar 8. Optionally for Tez, Enable Unravel Tez Instrumentation on all of HDP's Services in the Cluster Completion of this step requires a restart of all affected Hive services in Ambari UI. Confirm that hive-execution.engine is set to tez. set hive.execution.engine=tez; Using the Ambari Web UI (AWU), configure the Btrace agent for Tez: Append -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 tez.am.launch.cmd-opts tez.task.launch.cmd-opts Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment you need to modify tez.am.view-acls 9. Optionally for Spark on YARN. Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster Completion of this step requires a restart of all affected Spark services in Ambari UI. Be sure to substitute valid values.for UNRAVEL_HOST_IPX SPARK_VERSION_X.Y. \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark- \n \n 2.2 spark- \n \n 2.3 Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults You can manually edit spark-defaults.conf without using AWU. The default location for spark-defaults.conf \/usr\/hdp\/current\/SPARK_VERSION_X.Y The cluster only has one spark 1.X version: \/usr\/hdp\/current\/spark-client\/conf For spark 2.X version: \/usr\/hdp\/current\/spark2-client\/conf Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043\nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs={SPARK_VERSION_X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs={SPARK_VERSION_X.Y} \nspark.eventLog.enabled=true 10. Optionally for YARN If yarn.acl.enable=true \n yarn.acl.enable=false, \n yarn.admin.acl=userName Unravel only requires yarn.admin.acl 11. Run a test application on Tez and confirm Unravel Web UI displays Tez applications Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Unravel Web UI may take a few seconds to load Tez data. " }, 
{ "title" : "Amazon Elastic MapReduce (Amazon EMR)", 
"url" : "unravel-4-4/install/install-emr.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR)", 
"snippet" : "This guide is compatible with Amazon EMR 5.x EMR 5.14 (preferred)...", 
"body" : " This guide is compatible with Amazon EMR 5.x EMR 5.14 (preferred) " }, 
{ "title" : "Option 1: Install Unravel on an EC2 instance", 
"url" : "unravel-4-4/install/install-emr.html#UUID-23133315-3690-77e0-3a01-a9c3b8fa059a_id_AmazonElasticMapReduceAmazonEMR-Option1InstallUnravelonanEC2instance", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Option 1: Install Unravel on an EC2 instance", 
"snippet" : "Step 1: Install Unravel EC2 node for Amazon EMR Step 2: Connect Unravel EC2 node to new or existing EMR clusters Step 3: Set up AWS RDS for Unravel DB (Optional) Step 4: VPC Peering for Unravel EC2 node (Optional) EMR Support Matrix...", 
"body" : " Step 1: Install Unravel EC2 node for Amazon EMR Step 2: Connect Unravel EC2 node to new or existing EMR clusters Step 3: Set up AWS RDS for Unravel DB (Optional) Step 4: VPC Peering for Unravel EC2 node (Optional) EMR Support Matrix " }, 
{ "title" : "Option 2: Install Unravel via AWS Marketplace (WIP)", 
"url" : "unravel-4-4/install/install-emr.html#UUID-23133315-3690-77e0-3a01-a9c3b8fa059a_id_AmazonElasticMapReduceAmazonEMR-Option2InstallUnravelviaAWSMarketplaceWIP", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Option 2: Install Unravel via AWS Marketplace (WIP)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 1: Install Unravel EC2 node for Amazon EMR", 
"url" : "unravel-4-4/install/install-emr/install-emr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Step 1: Install Unravel EC2 node for Amazon EMR", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.4.X on AWS EC2 instance that will be used to monitor existing or newly created EMR clusters. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Setup the EC2 instance for unra...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.4.X on AWS EC2 instance that will be used to monitor existing or newly created EMR clusters. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Setup the EC2 instance for unravel installation. Install the Unravel EMR RPM on the created EC2 instance. Log into Unravel Web UI. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 instance type & size Security Group \/ IAM role AWS EMR 5.x (5.14 preferred) EMR Support Matrix Hive 0.9.x 2.3.x Spark 1.3.x, 1.6.x - 2.3.x Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI e.g. ami-02e98f78 (in us-east-1) Minimum Recommended Virtualization type: HVM Root device type: EBS – 100 - 500GB Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 EC2 instance profile that has permission to Read the S3 bucket used in the EMR clusters. Allowed port for inbound access to EMR master node: Port 8020 (Name node access for spark event log) Allowed port for inbound access to EMR core node: Port 50010 (Data node access for spark event log) Port 50020 Below is an example of security group used for Unravel EC2 node. Inbound rule 1. Setup the EC2 instance for unravel installation. Provision an EC2 Instance Instance Type: r4.2xlarge; recommended r4.4xlarge; the instance type: HVM with EBS.[ OS: centos7.4 (ami-02e98f78) Minimum root disk space is 100GB. Recommendation: 200GB or above with \"Provisioned IOPS SSD (IO1)\" storage. The EC2 instance must Create a S3 ReadAccess only IAM role and assign it to Unravel EC2 node to READ the archive logs on the S3 bucket configured for the EMR cluster. For instance, create an IAM role that contains the policy that only READ the specific S3 bucket used on EMR cluster. Then create a EC2 instance profile and add the IAM role to it. See Creating IAM Role Create Instance Profile CLI Security Group or policy required for Unravel EC2 node: Unravel EC2 node works with multiple EMR clusters, including existing and newly created clusters. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node. You must create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address, or put the member of security group used on EMR cluster in this rule. Below is an example of security group used for Unravel EC2 node. Inbound rule Type Protocol Port Range Source All traffic All All SG ID of this group or Subnet IP block (e.g. 10.10.0.0\/16) SSH TCP 22 0.0.0.0\/0 Custom TCP Rule TCP 3000 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Custom TCP Rule TCP 4043 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Outbound rule Type Protocol Port Range Source All traffic All All 0.0.0.0\/0 Configure the EC2 node at first login Disable selinux # sudo setenforce Permissive\n Edit the file to make sure the setting persists after reboot and make sure \"SELINUX=permissive\". # vi \/etc\/selinux\/config\n Install libaio.x86_64 and lzop.x86_64. # sudo yum install -y libaio.x86_64\n# sudo yum install -y lzop.x86_64 Start ntp # sudo service ntpd start\n# sudo ntpq -p Create a new user hadoop # sudo useradd hadoop In a PoC or evaluation, the minimal root disk size 100GB should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend to set minimal root disk from 300 - 500GB \"Provisioned IOPS\" EBS volume with 3000 IOPS. For production unravel use case, 200GB root disk Provisioned IOPS EBS and RDS are recommended 2. Install the Unravel EMR rpm on the created EC2 instance. Reminder AWS EMR 5.x support is available via Unravel 4.3.1.6 only Get the Unravel Server RPM; see Download Unravel Software Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. # sudo rpm -U unravel-4.3.1.*-EMR-latest.rpm \n Run the specified await_fixups.sh await_fixups.sh Done\" # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n# \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Edit the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem=false For monitoring EMR spark service, add the following properties in the unravel.properties com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel ( \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used; you can switch to use AWS RDS Security Remind We do not recommend making Unravel Server UI accessible on the public Internet. 3. Login to Unravel Web UI Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Create a ssh # ssh -i ssh_key.pem centos@{UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 From your workstation browser, navigate to URL http:\/\/127.0.0.1:3000. Login to the unravel UI with user id : admin unraveldata. connect to your existing or new EMR clusters " }, 
{ "title" : "Step 2: Connect Unravel EC2 node to new or existing EMR clusters", 
"url" : "unravel-4-4/install/install-emr/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Step 2: Connect Unravel EC2 node to new or existing EMR clusters", 
"snippet" : "Introduction This topic explains how to setup and configure your EMR cluster so that Unravel EC2 node can start monitoring your running jobs on the connected EMR clusters. In order for Unravel EC2 node able to monitor the EMR cluster, the cluster nodes are required to run the Unravel EMR bootstrap s...", 
"body" : "Introduction This topic explains how to setup and configure your EMR cluster so that Unravel EC2 node can start monitoring your running jobs on the connected EMR clusters. In order for Unravel EC2 node able to monitor the EMR cluster, the cluster nodes are required to run the Unravel EMR bootstrap script. The bootstrap script performs the following: On the Master Node: If a hive cluster, updates hive-site.xml \/etc\/hive\/conf\/ if a spark cluster, updates spark-defaults.conf \/etc\/spark\/conf\/ Updates mapred-site.xml \/etc\/hadoop\/conf\/ Updates yarn-site.xml \/etc\/hadoop\/conf\/ If TEZ is installed, updates tez-site.xml \/etc\/tez\/conf\/ Installs and starts the unravel_es daemon \/usr\/local\/unravel_es Installs the Spark & MapReduce sensors in \/usr\/local\/unravel-agent Installs the Hive hook sensor in \/usr\/lib\/hive\/lib\/ On all other nodes: Installs the Spark & MapReduce sensors in \/usr\/local\/unravel-agent Assumption Unravel node is created and unravel daemon running, tcp ports 3000 and 4043 are allowed for EMR cluster nodes. EMR cluster nodes allow all traffic from Unravel node . Both EMR cluster and Unravel node are created in same VPC, same subnet and SG is allowing all traffic from same subnet. For existing an EMR cluster connection located on a different VPC Connect Unravel EC2 node to New EMR cluster Download Unravel EMR bootstrap python script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py # curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the Unravel EMR bootstrap script unravel_emr_bootstrap.py for-EMR # aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce In the AWS console, choose EMR service and click Create cluste In the Create Cluster - Quick Options Go to advanced options . On the Advanced Options Step 1: Software and Steps emr-5.14.0 Release Next. In Hardware Configuration Advanced Options - Step 2 Network EC2 Subnet \n Next. In General Options Advanced Options - Step 3 Cluster name S3 folder Add bootstrap action Custom action Configure and add In the Add Bootstrap Action Script location Optional arguments Add \n \n \n Example Script location \n s3:\/\/aws-logs-217619106665-us-east-1\/elasticmapreduce\/unravel_emr_bootstrap.py \n \n Optional arguments \n --unravel-server UNRAVEL_EC2_IP In the Bootstrap Actions Nex Advanced Options In Security Options Advanced Options - Step 3) EC2 key pair EC2 security groups Create cluster Maste Core & Task If everything was entered correctly; your new EMR cluster should finish the bootstrap process and be in the Waiting Connect Unravel EC2 node to Existing EMR cluster Substitute your local values for text in {red brackets} After Unravel Upgrade repeat the following process to upgrade Unravel Sensors Locate the public IP address of Master and Core nodes. Locate the nodes using the Amazon EMR console. \n The Master Node \n The Core Nodes Locate the nodes using the command line interface (CLI). Get the instance group ID from the EMR cluster ID, {emr_cluster_id} # aws emr describe-cluster --cluster-id j-{emr_cluster_id} |grep Id |grep ig\n Using the two instance groups ID from above, {instance_group_id_1} {instance_group_id_2} # aws emr list-instances --cluster-id j-{emr_cluster_id} --instance-group-id ig-{instance_group_id_1} |grep PublicIpAddress\n# aws emr list-instances --cluster-id j-{emr_cluster_id} --instance-group-id ig-{instance_group_id_2} |grep PublicIpAddress Example output: \n ssh \/tmp # curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Run the Unravel EMR bootstrap script on each EMR cluster node, Master and Core & Task nodes . UNRAVEL_SERVER # sudo python \/tmp\/unravel_emr_bootstrap.py --unravel-server {UNRAVEL_SERVER} Example script output from Master node. Example script output from EMR Core and Task node. Restart hive-server2 on master node if hive component is installed # sudo stop hive-server2\n# sudo start hive-server2 Once Unravel EMR bootstrap script has been run on all EMR cluster nodes, the unravel setting should be configured on the cluster. You can run some jobs on this EMR cluster and monitor the metric data displayed on the Unravel UI (http:\/\/unravel_ec2_node_public_IP:3000). If the EMR cluster is created in a different VPC, see Step 4 for configuring VPC peering connection for an Unravel node. " }, 
{ "title" : "Step 3: Set up AWS RDS for Unravel DB (Optional)", 
"url" : "unravel-4-4/install/install-emr/install-emr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Step 3: Set up AWS RDS for Unravel DB (Optional)", 
"snippet" : "Introduction Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow ...", 
"body" : "Introduction Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group. Create new DB subnet group Create new DB parameter group that has custom MySQL properties. Setup MySQL RDS in AWS Create Unravel RDS instance In AWS portal → RDS, click Create database Choose MySQL Next Choose Production - MySQL Change the following properties, leave all others as the default. \n License model \n \n DB engine version \n \n DB instance class \n \n Multi-AZ deployment \n \n Storage type \n \n Allocated storage \n \n Provisioned IOPS Create a new DB instance and Master user and password. Click Next \n DB Instance identifier \n Master username \n Master password In the Advanced Settings . \n \n Network & Security Settings \n Virtual Private Cloud \n Subnet group \n Public accessibility \n \n \n Availability zone \n VPC security group Create a new DB subnet group in advance. It is required for Multi-AZ deployment. The VPC should at least contains two subnets in at least two Availability zones in a given region. For further information please check AWS documentation. Screenshot for DB subnet group. Create a new DB parameter group in advance, and this group is based on mysql 5.5. Alter the parameters base on the custom db parameters. \n \n \n key_buffer_size = 268435456 \n max_allowed_packet = 33554432 \n table_open_cache = 256 \n read_buffer_size = 262144 \n read_rnd_buffer_size = 4194304 \n max_connect_errors=2000000000 \n net-read-timeout = 300 \n net-write-timeout = 600 \n open_files_limit=9000 \n innodb_open_files=9000 \n character_set_server=utf8 \n collation_server = utf8_unicode_ci \n innodb_autoextend_increment=100 \n innodb_additional_mem_pool_size = 20971520 \n innodb_log_file_size = 134217728 \n innodb_log_buffer_size = 33554432 \n innodb_flush_log_at_trx_commit = 2 \n innodb_lock_wait_timeout = 50 \n \n \n Database Options Settings \n Database name \n Port \n DB parameter group For other RDS options such as Encryption Backup Monitoring Maintenance Create database Connecting unravel node to the unravel RDS instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. Locate the MySQL database endpoint in the RDS dashboard. Look for the security group used for unravel RDS instance from RDS dashboard. Edit the inbound rule of the security group. Add new rule allow either from unravel node's Security Group, or subnet IP block which unravel node located. The SG or IP block works provided the RDS instance is located on the same region as the VPC. Verify the myql connection from unravel node. # \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Click here to see a sample screenshot. Create unravel db schema in RDS unravel database Stop the Unravel server. # sudo \/etc\/init.d\/unravel_all.sh stop Configure the following properties in unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Locate and modify the properties below so that they reflect your particular values. If the property isn't found, add it. Use the actual values you set in the above steps, here here encrypted unravel.jdbc.username=unravel\nunravel.jdbc.password={unraveldata}\nunravel.jdbc.url=jdbc:mysql:\/\/{unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod}\n Ensure the schema is up to date using the schema upgrade utility provided by Unravel server. The script step connects to the database and applies schema deltas in-order until the schema is up to date. The success or failure of the update is noted. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user admin # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Start unravel daemon Disable the bundled db on the Unravel server. Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command produces an error that can be ignored. # sudo chkconfig unravel_db off\n# sudo chkconfig unravel_pg off Start the Unravel server. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: VPC Peering for Unravel EC2 node (Optional)", 
"url" : "unravel-4-4/install/install-emr/install-emr-part4.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ Step 4: VPC Peering for Unravel EC2 node (Optional)", 
"snippet" : "Follow these steps only Introduction This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assu...", 
"body" : " Follow these steps only Introduction This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. \n Assumptions The VPC where Unravel EC2 located is in the same region where the EMR cluster located (e.g., us-east-1). The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in EMR cluster. Network ACL on both VPC for Unravel EC2 and EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. In the following steps, we have both Unravel EC2 node and EMR cluster located in us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. \n \n \n Resources \n Internal IP Address \n Subnet ID \n Subnet IP Block \n VPC ID (Name) \n IP block in VPC \n Security Group ID (Name) \n \n Unravel EC2 node \n 10.10.0.7 \n subnet-03b82c56b2c26dbd1 \n 10.10.0.0\/24 \n vpc-0b0e17b01c4a3b54a (Unravel_VPC) \n 10.10.0.0\/16 \n sg-0e0a03084398287c9 (Unravel-EC2_SG) \n \n EMR Cluster Master node \n 10.11.0.53 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) \n \n EMR Cluster Core nodes \n 10.11.0.76 10.11.0.130 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) Create VPC Peering in VPC Dashboard From the AWS console → VPC services → Peering Connections Create Peering Connection VPC (Requester) VPC (Accepter) Create Peering Connection. A success message should appear in the screen. Click OK Accept the VPC Peering Request In the VPC Dashboard Pending Acceptance Action Accept Request Click Yes Accept Close Create Routes between peered VPC Go to VPC Dashboard → Route Tables Edit Add another route Find the Unravel_VPC route table. Enter the IP block of EMR VPC, e.g., 10.11.0.0\/16 In the Destination Target Save. Find the Test_EMR_VPC route table. Set the Destination Target Save. pcx-0a57a978ef9a525e2 Target Save Update Security Groups Go to VPC Dashboard → Security Group Add another rule Type ALL traffic Protocol ALL. Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block, e.g., 10.11.0.0\/16 in the Source Save. Locate the security group used on EMR cluster node. Enter the Unravel VPC IP block, e.g., 10.10.0.0\/16 in the Source Save Verify Connection between Unravel EC2 node and EMR master node \n ssh ALL Traffic Then on Unravel EC2 node, telnet On the EMR master node, telnet . If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network acl, security groups, and route tables used on both VPC. \n See unsupported VPC peering configurations on AWS documentation. " }, 
{ "title" : "EMR Support Matrix", 
"url" : "unravel-4-4/install/install-emr/install-emr-support-matrix.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Amazon Elastic MapReduce (Amazon EMR) \/ EMR Support Matrix", 
"snippet" : "This support matrix is based on recent test on unravel 4.3.1.4 EMR Version Hive Version Spark Version Hive Support Spark Support 5.15.0 2.3.3 2.3.0 No Yes (Unravel 4.3.1.5 and later ONLY) 5.14.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.13.0 2.3.2 2.3.0 ...", 
"body" : "This support matrix is based on recent test on unravel 4.3.1.4 EMR Version Hive Version Spark Version Hive Support Spark Support 5.15.0 2.3.3 2.3.0 No Yes (Unravel 4.3.1.5 and later ONLY) 5.14.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.13.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.12.1 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.12.0 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.11.1 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.11.0 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.10.0 2.3.1 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.9.0 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.2 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.1 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.0 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.7.0 2.1.1 2.1.1 Yes Yes 5.6.0 2.1.1 2.1.1 Yes Yes 5.5.2 2.1.1 2.1.0 Yes Yes 5.5.1 2.1.1 2.1.0 Yes Yes 5.5.0 2.1.1 2.1.0 Yes Yes 5.4.0 2.1.1 2.1.0 Yes Yes 5.3.1 2.1.1 2.1.0 Yes Yes 5.3.0 2.1.1 2.1.0 Yes Yes 5.2.2 2.1.0 2.0.2 Yes Yes 5.2.1 2.1.0 2.0.2 Yes Yes 5.2.0 2.1.0 2.0.2 Yes Yes 5.1.0 2.1.0 2.0.1 Yes Yes 5.0.3 2.1.0 2.0.1 Yes Yes 5.0.0 2.1.0 2.0.0 Yes Yes 4.9.4 1.0.0 1.6.3 Yes Yes 4.8.3 1.0.0 1.6.3 Yes Yes " }, 
{ "title" : "MapR", 
"url" : "unravel-4-4/install/install-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-4/install/install-mapr.html#UUID-5b375c07-7188-1160-0edb-5fe2bfcdd243_id_MapR-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MapR \/ Ordered Steps", 
"snippet" : "MapR Pre-Installation Check Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Unravel's Reports: Sessions Cluster Optimization Capacity Forecasting Small Files require that you use MySQL OnDemand For more information on installations and conf...", 
"body" : " MapR Pre-Installation Check Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Unravel's Reports: Sessions Cluster Optimization Capacity Forecasting Small Files require that you use MySQL OnDemand For more information on installations and configurations see Advanced Topics " }, 
{ "title" : "MapR Pre-Installation Check", 
"url" : "unravel-4-4/install/install-mapr/install-mapr-pre.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MapR \/ MapR Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR 6.0.0 with MapR Expansion Packs 4.1.1 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware A...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR 6.0.0 with MapR Expansion Packs 4.1.1 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: Access to YARN's \"done dir\" in HDFS Access to YARN's log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network The following ports must be open on Unravel Server's host machine(s): Port(s) Direction Description 3000 or 4020 Both Traffic to and from Unravel UI HDFS ports In Traffic from Hadoop cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 4043 In UDP and TCP traffic from the entire cluster to Unravel Server(s) 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 3316, 4020, 4091, 4176, 4181-4189 Both Localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on MapR", 
"url" : "unravel-4-4/install/install-mapr/install-mapr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2 on the MapR converged data platform. These instructions apply to Unravel Server 4.2. Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log i...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2 on the MapR converged data platform. These instructions apply to Unravel Server 4.2. \n Workflow Summary Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access If you do not already have a gateway\/edge\/client host provisioned for Unravel server, follow these steps which are needed to enable the hadoop fs For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands on Unravel Server as root NAME, CLDB_LIST,HISTORY_SERVER. # sudo yum install mapr-client.x86_64\n# sudo \/opt\/mapr\/server\/configure.sh -N {NAME} -c -C {CLDB_LIST} -HS {HISTORY_SERVER}\n# sudo yum install mapr-hive.noarch\n# sudo yum install mapr-spark.noarch Configure the Host Before installing the RPM Run the following commands on Unravel Server as root # sudo useradd -g mapr unravel\n# hadoop fs -mkdir \/user\/unravel\n# hadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr \/etc\/unravel_ctl Check available RAM to ensure availability: # free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 (only change this setting on the Unravel gateway\/client machine). And the restart mfs. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv and symlink after installing). Do it before the first install if there is insufficient space on the target paths \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fizup.sh await_fizup.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL Do Host-Specific Post-Installation Actions Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh 3. Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable additional daemons at this point, see Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n Company_and_org \n \n com.unraveldata.tmpdir \n \n Optional \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n 26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n \n Optional N N ThousandJobsPerDay \n 1 \n \n \n com.unraveldata.job.collector.done.log.base \n HDFS path to \"done\" directory of MR logs \n \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done \n \n \n com.unraveldata.job.collector.log.aggregation.base \n An HDFS path that helps locate MR job logs to process \n \/tmp\/logs\/*\/logs\/ \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n admin \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n maprfs:\/\/\/apps\/spark \n \n \n yarn.resourcemanager.webapp.address \n Resource Manager web app address \n http:\/\/example.localdomain:8088 \n \n \n yarn.resourcemanager.webapp.username \n Resource Manager username to login \n \n \n yarn.resourcemanager.webapp.password \n Resource Manager password to login \n \n \n https.protocols \n Enable https access to Resource Manager \n TLSv1.2 \n \n \n javax.jdo.option.ConnectionURL \n A JDBC connection URL \n \n jdbc:mysql:\/\/example.localdomain:3306\/hive OR jdbc:postgresql:\/\/example.localdomain:7432\/hive_zzzzzz \n \n \n javax.jdo.option.ConnectionDriverName \n JDBC driver \n com.mysql.jdbc.Driver OR \n \n \n javax.jdo.option.ConnectionUserName \n Hive metastore user name \n hiveuser \n \n \n javax.jdo.option.ConnectionPassword \n Hive metastore password \n \n \n com.unraveldata.metastore.databasePattern \n \n Optional \n s*|t*|d* \n \n \n oozie.server.url \n Oozie URL \n http:\/\/example.localdomain:11000\/oozie If Kerberos is Enabled: \n Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can find and verify the principal the keytab by # klist -kt KEYTAB_FILE\n The keytab file should have chmod bits 500 and be owned by unravel Run Unravel Daemons with Custom User If Sentry is Enabled: \n Define your own alt principal with narrow privileges. The alt user can be admin unravel rpm X \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n Your alt principal \n read \n Spark event log \n \n \n hdfs:\/\/usr\/history\/done \n Your alt principal \n read \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n Your alt principal \n read \n YARN aggregation folder \n \n \n hdfs:\/\/user\/hive\/warehouse \n Your alt principal \n read \n Obtain table partition sizes \n \n \n Hive Metastore access \n hive \n read \n Hive table information Switch User Depending on your cluster security configuration, you will need to run the \n switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. 4. Log into Unravel Web UI Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} admin unraveldata For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. Check Unravel Web UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel Web UI, see the User Guide 5. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the Advanced Topics " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-4/install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unra...", 
"body" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. 1. Enable Additional Instrumentation on Unravel Server's Host Substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION _X.Y.Z The following unravel_mapr_setup.py script need to run on all nodes of the mapr cluster. # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n\n\n# sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only For Sensor Upgrade only # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only Dry-Run (test\/check instrumentation, this does not change any configuration file): # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --dry-run Hive hook jar is installed under: \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2\/MapR 6.0) are made to the following files, (< SPARK_VERSION X.Y.Z> \/opt\/mapr\/spark\/spark-<SPARK VERSION X.Y.Z>\/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-site.xml \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-env.sh \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/yarn-site.xml \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/mapred-site.xml \/usr\/local\/unravel\/etc\/unravel.properties Copy of original configuration is saved in same directory named *. \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml.preunravel Once the files are present on edge host where Unravel rpm is installed, you can tar tar 2. For Oozie, Copy Unravel Hive Hook and Metrics JARs to the HDFS Shared Library Path Copy the Hive Hook JAR in \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-agent\/ oozie.libpath 3. Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server: someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 4. Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> 5. Enable Additional Instrumentation on Other Hosts in the Cluster To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml Run the shell script unravel_mapr_setup.sh Copy the newly edited yarn-site.xml Do a rolling-restart of HiveServer2 6. Enable Instrumentation manually by updating the following files: hive-site.xml hive-env.sh spark-defaults.conf hadoop-env.sh mapred-site.xml Once the files are updated on edge host where Unravel rpm is installed, you can scp Be sure to substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION _X.Y.Z Update hive-site.xml Copy the content in \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-site.xml {UNRAVEL_HOST_IP}) <property>\n <name>com.unraveldata.host<\/name>\n <value>{UNRAVEL_HOST_IP}<\/value>\n <description>Unravel hive-hook processing host<\/description>\n<\/property>\n\n<property>\n <name>com.unraveldata.hive.hook.tcp<\/name>\n <value>true<\/value>\n<\/property>\n\n<property>\n <name>com.unraveldata.hive.hdfs.dir<\/name>\n <value>\/user\/unravel\/HOOK_RESULT_DIR<\/value>\n <description>destination for hive-hook, Unravel log processing<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.driver.run.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HiveDriverHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.pre.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HivePreHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.post.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HivePostHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.failure.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HiveFailHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<\/configuration> Update hive-env.sh In \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-env.sh ({HIVE_VERSION_X.Y.Z}). export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-{HIVE_VERSION X.Y.Z}-hook.jar\nexport HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}:\/usr\/local\/unravel_client Update spark-defaults.conf In \/opt\/mapr\/spark\/spark-SPARK_VERSION _X.Y.Z\/conf\/ spark-defaults.conf {UNRAVEL_HOST_IP} {SPARK_VERSION_X.Y.Z} spark.unravel.server.hostport {UNRAVEL_HOST_IP}:4043\nspark.eventLog.dir maprfs:\/\/\/apps\/spark\nspark.history.fs.logDirectory maprfs:\/\/\/apps\/spark\nspark.driver.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=driver\nspark.executor.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=executor Update hadoop-env.sh In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh ({HIVE_VERSION_X.Y.Z}). export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-{HIVE_VERSION_X.Y.Z}.0-hook.jar Update mapred-site.xml In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/mapred-site.xml {UNRAVEL_HOST_IP} <property>\n <name>mapreduce.task.profile<\/name>\n <value>true<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.maps<\/name>\n <value>0-5<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.reduces<\/name>\n <value>0-5<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.params<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value>\n<\/property>\n\n<property>\n <name>yarn.app.mapreduce.am.command-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=172.36.1.126:4043<\/value>\n<\/property>\n\n Make sure the original value of yarn.app.mapreduce.am.command-opts " }, 
{ "title" : "Azure HDInsight", 
"url" : "unravel-4-4/install/install-hdi.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight", 
"snippet" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters....", 
"body" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters. " }, 
{ "title" : "Unravel VM", 
"url" : "unravel-4-4/install/install-hdi.html#UUID-682787b1-111b-e8f4-3050-c1c360062d60_id_AzureHDInsight-UnravelVM", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Unravel VM", 
"snippet" : "Installing Unravel as a separate Azure VM...", 
"body" : " Installing Unravel as a separate Azure VM " }, 
{ "title" : "Unravel App", 
"url" : "unravel-4-4/install/install-hdi.html#UUID-682787b1-111b-e8f4-3050-c1c360062d60_id_AzureHDInsight-UnravelApp", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Unravel App", 
"snippet" : "Installing Unravel as an Azure Marketplace app...", 
"body" : " Installing Unravel as an Azure Marketplace app " }, 
{ "title" : "Option 1: Install Unravel on a Separate Azure VM", 
"url" : "unravel-4-4/install/install-hdi/install-hdi-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM", 
"snippet" : "This option involves the following steps: Create Azure Storage Account Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel...", 
"body" : "This option involves the following steps: Create Azure Storage Account Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Create Azure Storage Account", 
"url" : "unravel-4-4/install/install-hdi/install-hdi-part1-option1/create-azure-storage-account.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Create Azure Storage Account", 
"snippet" : "Introduction This topic explains how to create an Azure storage account appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. Windows Azure Storage Blob: By default HDInsight 3.6 u...", 
"body" : "Introduction This topic explains how to create an Azure storage account appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. Windows Azure Storage Blob: By default HDInsight 3.6 uses Blob storage, which is general-purpose for Big Data. One of the most important attributes of Blob storage is that it is a key-value store with a flat namespace. Additionally, it has full support for analytics workloads; batch, interactive, streaming analytics, and machine learning data such as log files, IoT data, click streams, large datasets. Azure Data Lake Storage generation 1 (ADLS v1): The other major option for Hadoop clusters is ADLS v1. One of the most important attributes of ADLS is that it is a hierarchical file system. Additionally, it has full support for analytics workloads; batch, interactive, streaming analytics, and machine learning data such as log files, IoT data, click streams, large datasets.Azure Data Lake Storage (ADLS v2) generation 2: Azure Data Lake Storage generation 2 in preview (ADLS v2): ADLS generation 2 combines the features of Blob storage and Azure Data Lake Storage Gen1 Features from Azure Data Lake Storage generation 1, such as file system semantics, file-level security and scale are combined with low-cost, tiered storage, high availability\/disaster recovery capabilities from Azure Blob Storage Note that Unravel has not been tested with ADLS v2 See https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-comparison-with-blob-storage In the rest of this document, we will refer to these three types of storage systems as Blob and ADLS for short. Review prerequisites. You must already have an Azure account and able to login from https:\/\/portal.azure.com Resource Group: You must already have a Resource Group created, which is a container that holds related resources for an Azure solution. Create storage on Azure. Navigate to https:\/\/portal.azure.com Storage accounts + Enter values for the following fields: Subscription Resource Group Storage Account Name Location Performance Standard Premium Kind Blob Storage StorageV2 StorageV2 is still in preview and not currently supported by Unravel. Replication Access Tier Blob ADLS v2 hot Set advanced options. A. Replication Locally redundant storage (LRS) Zone-redundant storage (ZRS) Geo-redundant storage (GRS) Read-access geo-redundant storage (RA-GRS) FIXLINK: Handles failures in the data-center, zone, region, and allows read-access in another region. Durability guarantee is 16 9's. B. Click Next: Advanced HTTPS Virtual Networks C. Finally, click Review + Create Create Related Resources Finding the Unravel properties in your Azure Data Lake Storage account Azure - creating a storage account Difference between Replication types " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"url" : "unravel-4-4/install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"snippet" : "Introduction This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. 1. Review Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group yo...", 
"body" : "Introduction This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. 1. Review Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network in a resource group. This virtual network will be shared by your Hadoop cluster and the Unravel VM. You must have root privilege You must have already created a storage system. For instructions, see Create Azure Storage Account Your VM host must meet the requirements below. Support Chart and VM Requirements: \n \n \n Azure HDI cluster compatibility \n HDInsight 3.6 with Windows Azure Blob HDP 2.6.5 Spark 1.6.3, 2.1.0, 2.2.0, 2.3.0 (see \"Known Limitations\" below) Hive 2.1 Kafka 0.10.0, Kafka 1.0, Kafka 1.1 \n \n Unravel VM OS \n RHEL or CentOS 7.2 - 7.6 Note that the actual HDInsight Kafka\/Spark cluster can run another OS. \n \n Min requirement \n Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 Cores: 8 min RAM: 64 GB min \n \n Disk requirement \n min 100GB for \/srv \/srv \n \n Network requirement \n Unravel VM should be located in the same VNET and VSNET of the HDInsight cluster Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server HDFS ports open from Hadoop cluster to Unravel Server Hive MetaStore DB port open to Unravel Server for partition reporting For Oozie, port 11000 open to Unravel Server \n \n Security requirement \n Allows inbound ssh to the unravel VM Allows outbound Internet access and all traffic within the subnet (VSNET). Allows TCP port 3000 and 4043 to Unravel VM from HDInsight cluster \n \n Known Limitations \n \n Storage: Unravel currently only works with Windows Azure Blob WASB ADLS v1 It does not support multiple Azure Data Lake Storage accounts or ADLS v2. \n Spark: yarn.log-aggregation.file-formats <property>\n <name>yarn.log-aggregation.file-formats<\/name>\n <value>TFile<\/value>\n<\/property> \n HIGHLIGHTED \n UNRAVEL_HOST_IP 2. Provision an Azure VM for Unravel Server Set up Unravel VM on the same VNET and subnet as the target HDInsight cluster(s) In a web browser, go to https:\/\/portal.azure.com Navigate to Virtual machines + Add On the Create a virtual machine On the Basics \n Resource Group \n Virtual machine name \n Region \n Size \n Image \n Availability options Optional but recommended: Select to authenticate using an SSH public key, which you can generate using ssh-keygen Set Inbound Port Rules: On the Disks \n OS disk type Premium SSD Standard SSD \n Advanced managed disks If you don't have a disk ready, click on \" Create and attach new disk On the Networking It is imperative that the VM be created on the same virtual network as that of the cluster(s) you plan to manage. \n Virtual network \n Subnet tip \n Security Group: A TCP and UDP connection is needed from the \" head node Add an inbound security policy to allow SSH access and 443 access to the Unravel node. The default security policy should allow all access within the VNET. Default rules start with a priority of 6500. Once you're satisfied, select the Review + create Create It should take around 2 mins to create your VM. Your should see your host's dynamic or static IP address. Open an SSH session to your host. # ssh -i {ssh_key} {user}@{IP}\nVerify your IP as expected, example:\n# ifconfig\neth0 Link encap:Ethernet HWaddr 00:0d:3a:1b:c2:48\n inet addr:10.10.1.96 3. Configure the VM at First Login Install ntpd \n ntpd # sudo su -\n# yum install ntp\n# ntpd -u ntp:ntp Disable \" Security Enhanced Linux # sudo setenforce Permissive Edit the file to make sure the setting persists after reboot, be sure to set, SELINUX=permissive # vi \/etc\/selinux\/config\nSELINUX=permissive\n\n:wq Install libaio.x86_64. # sudo yum -y install libaio.x86_64 Install lzop.x86_64. \n # sudo yum install lzop.x86_64 Disable the firewall and check your iptable rules. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L Prepare the second disk (for example, \/dev\/sdc fdisk -l # sudo su -\n\n\nList all disks and partitions\nYou should see one called \"sdc\" if you attached a 500-1000 GB disk.\n# fdisk -l\nFormat the disk\n# \/usr\/sbin\/mkfs -t ext4 \/dev\/sdc\n# mkdir -p \/srv\n\n# DISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\n# echo $DISKUUID\n\nMount the disk on \/srv\n# echo \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\nSet permissions for Unravel and symlink Unravel's directories to the \/srv mount\n# mkdir -p \/srv\/local\/unravel\n# chmod -R 755 \/srv\/local\n# ln -s \/srv\/local\/unravel \/usr\/local\/unravel\n# chmod 755 \/usr\/local\/unravel Create the hdfs hadoop # sudo useradd hdfs\n# sudo groupadd hadoop\n# sudo usermod -a -G hadoop hdfs 4. Install the Unravel Server RPM on the VM Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM. For instructions, see Download Unravel Software # cd \/tmp\n# Note that the same RPM is used for both EMR and HDInsight.\n# curl -u {USERNAME}:{PASSWORD} -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.3\/unravel-4.4.3.0-EMR-latest.rpm -o unravel-4.4.3.0-EMR-latest.rpm Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. The rpm rpm U # sudo rpm -U unravel-4.4.3.0-EMR-latest.rpm Run the specified await_fixups.sh If you're doing a routine upgrade, you can start all Unravel daemons, but don't stop or restart them until await_fixups.sh DONE # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\nDONE Useful Information \n Dirs \/usr\/local\/unravel\/ \n unravel.properties \n \/etc\/init.d\/unravel_* \n \/etc\/init.d\/unravel_all.sh \n Subsequent RPM upgrades don't change unravel.properties \n User: unravel \n DB \/srv\/unravel\/ \n Config \/usr\/local\/unravel\/etc\/unravel.properties \n Logs \/usr\/local\/unravel\/logs\/ Grant access to Unravel Server Security Reminder Do not make Unravel Server UI TCP port 3000 accessible on the public Internet because doing so would violate your licensing terms. By default, a Public IP should be assigned to the Unravel VM . Create a security policy that allows ssh It is recommended that you use an SSH key to access the Unravel node. 5. Modify Properties and Start Unravel Daemons Open an SSH session to the Unravel VM. # ssh -i {ssh_private_key} {ssh_user}@{UNRAVEL_HOST_IP} Set correct permissions on the Unravel configuration directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh Check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions # Find the version of HDP that is installed by checking the HDP symlink. Take the first 2 digits, such as 2.6\n# You can also check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions \n# hdp-select status | grep hadoop\nhadoop-client - 2.6.5.3005-27\n\n# Append this classpath based on the version you found\n# echo \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Run the \"switch user\" script. # \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop In \/usr\/local\/unravel\/etc\/unravel.properties \n com.unraveldata.onprem This is optional at this time but is required later. echo \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties Modify other values in unravel.properties \n \n \n Property \n Description \n Required \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n Yes \n \n http:\/\/{LAN_DNS}:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n Yes \n \n Company_and_org \n \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n mandatory and is set by installation \n \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n optional \n \n 26 \n \n \n com.unraveldata.login.admins \n Unravel UI admin \n mandatory and is set by installation \n \n admin \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster \n mandatory if using Azure blob \n \n fs.azure.account.key. blob.core.windows.net \n \n \n com.unraveldat \n Primary storage account key \n mandatory if using Azure blob \n \n ABCDABCDABCDABCDABCD\/ABCDABCDABCDABCDABCDABCDABCD \n \n \n com.unraveldata.hdinsight.storage-account-name-2 \n Optional for Spark when HDInsight using blob storage Storage account name for the HDInsight cluster (same as account-name-1 \n mandatory if using Azure blob \n \n fs.azure.account.key. blob.core.windows.net \n \n \n com.unraveldat a.hdinsight \n Secondary storage account key \n mandatory if using Azure blob \n \n ABCDEF\/ABCDEFGHIJKLM\/ABCDABCDABCDABCDABCDABCDAB \n \n \n com.unraveldata.adl.accountFQDN \n The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net \n mandatory if using Azure Data Lake \n \n \n datalake0001.azuredatalakestore.net \n \n \n com.unraveldata.adl.clientld \n An application ID. An application registration has to be created in the Azure Active Directory \n mandatory if using Azure Data Lake \n \n 12345678-1234-1234-1234-123456789ABC \n \n \n com.unraveldata.adl.clientKey \n An application access key which can be created after registering an application \n mandatory if using Azure Data Lake \n \n ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABC= \n \n \n com.unraveldata.adl.accessTokenEndpoint \n The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal \n mandatory if using Azure Data Lake \n \n https:\/\/login.microsoftonline.com\/ABCD \n \n \n com.unraveldata.adl.clientRootPath \n It is the path in the Data lake store where the target cluster has been given access. \n mandatory if using Azure Data Lake \n \n \/clusters\/{CLUSTERNAME} \n \n \n com.unraveldata.ext.kafka.clusters \n Name of Kafka cluster. The display name show on the Unravel UI to define Kafka cluster. Other Unravel Kafka properties depends on this name {CLUSTERNAME} \n mandatory for HDI Kafka \n \n udkafka \n \n \n com.unraveldata.ext.kafka. {CLUSTERNAME} \n Kafka cluster bootstrap server and port (usually are two worker nodes) \n mandatory for HDI Kafka \n \n wn0-UDKAFK:9092,wn1-UDKAFK:9092 \n \n \n com.unraveldat a.ext.kafka. \n Define Kafka cluster broker servers names \n mandatory for HDI Kafka \n \n broker-1,broker-2,broker-3 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 host \n mandatory for HDI Kafka \n \n wn0-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 port \n mandatory for HDI Kafka \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 host \n mandatory for HDI Kafka \n \n wn1-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 port \n mandatory for HDI Kafka \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 host \n mandatory for HDI Kafka \n \n wn2-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 port \n mandatory for HDI Kafka \n \n 9999 For an HDInsight cluster using blob storage or Data Lake Storage you need to update the following properties. The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel properties' values in Microsoft's Azure When using blob storage \n \n \n Property \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n \n \n com.unraveldat \n \n \n com.unraveldat a.hdinsight \n \n \n com.unraveldat a.hdinsight When using Data Lake storage \n \n \n Property \n \n \n com.unraveldat a.adl.accountFQDN \n \n \n com.unraveldat a.adl.clientld \n \n \n com.unraveldat a.adl.clientKey \n \n \n com.unraveldat a.adl.accessTokenEndpoint \n \n \n com.unraveldat a.adl.clientRootPath Restart Unravel Server Whenever making edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties The echo If you are using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: # sudo \/etc\/init.d\/unravel_all.sh restart\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" 6. Log into Unravel UI Create an SSH # ssh -i {ssh_private_key} {ssh_user}@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to \n http:\/\/127.0.0.1 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Proceed to Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"url" : "unravel-4-4/install/install-hdi/install-hdi-part1-option1/step-2--use-script-action-to-configure-hdinsight-cluster-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"snippet" : "Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure \" Script action There are ...", 
"body" : "Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure \" Script action There are two kinds of Unravel \" script actions Note: For HDInsight cluster without Internet access, you can download these scripts and store them in your Azure blob storage and use the blob storage URI on the script action's \" Bash script URI \n \n \n Cluster Type \n Download path \n Supported HDI cluster(s) \n Apply to cluster node type(s) \n \n Hadoop, Hive, or Spark \n \n unravel_hdi_spark_bootstrap_3.0.sh \n Hadoop 2.7.3 Hive 2.1 Spark 2.0, 2.1, 2.3 \n Head Node, Worker Node, Edge node \n \n Kafka \n \n unravel_hdi_kafka_bootstrap.sh \n Kafka 0.10.0, Kafka 1.0.0, Kafka 1.1.0 \n Head Node Checks before running script action Read the latest documentation on the ports required by HDInsight: https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-hadoop-port-settings-for-services Ensure Unravel service is running on Unravel VM and ports 3000 and 4043 are reachable from the Azure HDInsight cluster master node before running the the Unravel \"script action\" script. E.g., # ssh -i {ssh_key} {ssh_user}@{UNRAVEL_HOST_IP}\n# sudo su -\n# netstat -anp | grep 3000\ntcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN 65072\/node\n# hostname\n\nOn one of the cluster's head nodes.\n# ping {UNRAVEL_HOST_IP} Depending on the type of cluster you are deploying, please follow one of these options: \n Option A: Deploy a new cluster \n Option B: Configure an existing Hadoop, Hive, or Spark cluster \n Option C: Configure an existing Kafka cluster Prerequisites You must already have an Unravel VM on Azure HDInsight running and the Unravel UI available on port 3000. For instructions, see Step 1: Install Unravel Server for Azure HDInsight Cluster If you plan to create a cluster, have the following information handy: Virtual Network and subnet of the Unravel VM Storage account (Windows Azure Blob Storage or Azure Data Lake Storage generation 1). Please see how to Create Azure Storage Account Option A: Deploy a New Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight clusters Add In the Security + networking In the Storage In the \" Cluster size \n Optional Script action In the Summary - Confirm configurations Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, Click on Script actions Submit new Create \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh \n \n Node type(s) \n Head, Worker, Edge (only if you have deployed edge node) \n \n Parameters \n \n --unravel-server unravel_server_private_ip spark_version e.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0\n\nTo UNDO the changes use --uninstall parameter\ne.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 --uninstall \n \n Persist this script action \n Checked. Note that persistence only applies on new Head and Worker nodes Option C: Configure an Existing Kafka Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, If the Kafka cluster has no Internet access; then download the HDInsightUtilities-v01.sh script and scp\/download it to the Kafka \"head node\" \/tmp folder . wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click Script actions Submit new \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh \n \n Node type(s) \n Head \n \n Parameters \n \n --unravel-server unravel_server_private_ip e.g. --unravel-server 10.10.1.10:3000 \n \n Persist this script action \n Checked. Note that persistence only applies on new Head nodes Click Create After the Kafka script action script completed successfully, ssh to the Kafka cluster's \"head node\" and append the content of \/tmp\/unravel\/unravel.ext.properties nravel.ext.properties # Adding Kafka properties\ncom.unraveldata.ext.kafka.clusters=<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.bootstrap_servers=wn0-<cluster_name>:9092,wn1-<cluster_name>:9092\ncom.unraveldata.ext.kafka.<cluster_name>.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.host=wn0-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.host=wn1-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.port=9999 Unravel VM must have access to the Kafka worker nodes' broker port 9092 and Kafka JMX port 9999 After updating the Kafka properties, you will need to restart the Unravel server. sudo \/etc\/init.d\/unravel_all.sh restart Troubleshooting Tips From the Azure portal, you can check if a script action finished successfully by checking the SCRIPT ACTION HISTORY . If script action process fails, you can check the error messages from the HDInsight cluster's Ambari dashboard, which has a balloon next to the cluster name on the top menu bar with the recent operations. Click on the \"ops\" button and search for the most recent \"run_customscriptaction\" command and inspect the log messages. You may see multiple entries of \"run_customscriptaction\" which were created by previous runs. The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new \"script action\" script with a different name. " }, 
{ "title" : "Option 2: Install Unravel's Azure Marketplace App", 
"url" : "unravel-4-4/install/install-hdi/install-hdi-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Option 2: Install Unravel's Azure Marketplace App", 
"snippet" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. 1. Launch a Spark 2.1 Cluster Login to the Azure ...", 
"body" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. 1. Launch a Spark 2.1 Cluster Login to the Azure port. Click or choose HDInsight service. Create a new cluster; choosing Spark as the cluster type version Next. 2. Set Up a Storage Account for the Cluster Either create a new storage account or use existing one. Fill in the storage account information for the spark cluster. 3. Find the Unravel app on Azure Marketplace Enter UNRAVEL Available applications Click OK Create After you accepted the \"Terms of Use\" and \"Privacy Policy\" click Next 4. Launch the Cluster and Unravel App Review the summary. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. Click Create 5. Find the URL of the Unravel App After Unravel app and the spark2 cluster is successfully launched, go to HDInsight service, look for the spark2 cluster, and click on it. Click on Application In most cases, the Unravel HDInsight app user interface is in the following format https:\/\/<clusterName>-unr.apps.azurehdinsight.net\/ 6. Log into the Unravel UI Start your browser and navigate to the Unravel app webpage URL https:\/\/clusterName- apps.azurehdinsight.net The default admin login credential is admin unraveldata. When you log in, the dashboard appears. Step 8. (Optional) Start\/Stop Unravel Daemons \n ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Step 9. Enter Your License By default Unravel app doesn't contains any license keys, and runs without any issue during the initial 30 days trial period. To continue using Unravel app and technical support, contact our sales team. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Step 10. Getting started to use Unravel Please read the Unravel User Guide Getting Started User Guide Step 11. Unravel API (special note for Unravel app) Unravel provides REST API to perform some operations. To try the API, click on the API tab on the dashboard An API page with available command options are displayed and explained. You can try the API by clicking \"Try it out\" → Execute buttons; it will display the corresponding curl commands for that REST api call. See below screen capture. From the Unravel user interface, trying out the api will always has \"TypeError: Failed to fetch\". Because the generated curl command is not using https. Copy the generated curl commands and modify it to include default user credential and using https protocol. ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n The api output will be in JSON format shown below and the long numeric string displayed is the epoch time {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "INTERNAL: Update the Azure Resource Manager if Needed", 
"url" : "unravel-4-4/install/install-hdi/internal--update-the-azure-resource-manager-if-needed.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed", 
"snippet" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters. Please see ARM Template for Spark2 HDInsight Cluster with Unravel Script Actions ARM Template for Kafka HDInsight Cluster with Unravel Script Actions...", 
"body" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters. Please see ARM Template for Spark2 HDInsight Cluster with Unravel Script Actions ARM Template for Kafka HDInsight Cluster with Unravel Script Actions " }, 
{ "title" : "INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"url" : "unravel-4-4/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-3--create-unravel-vm-using-arm-template.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"snippet" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or cr...", 
"body" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or create your own template files. This example template creates an Azure \"standard E8s V3\" VM in the existing VNET and subnet, and it adds a data disk on the VM for \"\/ srv\" You will need to update VNET, subnetRef, vmName, adminPassword before running the ARM template to create VM Optionally change the size of data disk; it is currently set to 500G. If you change the disk size, update unravel-setup.sh For Centos 7.3 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.parameters.json For Redhat 7.4 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.parameters.json Both ARM template and parameter files have to be modified to fit your Azure environment This ARM template embedded with Azure Extension script to download and install Unravel RPM. The Azure Extension script for Unravel RPM installation Extension Script for CentOS 7.3 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/unravel-setup.sh Extension Script for CentOS 7.4 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/unravel-setup.sh The custom extension script will fix most of the basic unravel configuration; however you will have to manually edit \/usr\/local\/unravel\/etc\/unravel.properties file for blob storage account or data lake store access information. Please see Step 1 Below is the content of this extension script # Download unravel rpm\n\/usr\/bin\/wget http:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.2.7-Azure-latest.rpm\n\n\/usr\/bin\/sleep 5\n\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=1p6ed4s492012j5rb242rq3x3w702z1l455g501z2z4o2o4lo675555u3h\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start 1. Download the ARM template and parameter JSON files onto your configured Azure CLI workstation. 2. Use the Azure CLI to deploy Unravel VM using this template and parameters JSON file. To Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 2. Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually start unravel daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"url" : "unravel-4-4/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-4--arm-template-for-spark2-hdinsight-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"snippet" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. T...", 
"body" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. This ARM template uses the existing VNET, Subnet, and Storage Account on the same resource group. You need to update those values in the parameter variables to reflect your Azure environment. A Spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use Spark edge node for accessing the cluster and testing\/hosting your client applications. Substitute your local values for text in red, i.e., UNRAVEL_IP. You will need to deploy unravel VM and update the script action parameters --unravel-server UNRAVEL_IP Optionally you can change the VM size of header, worker and edge nodes; and currently they are all using VM size of \"Standard_D3_v2\" \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh After modifying this template please validate it before applying. HDInsight cluster creation takes about 15 - 25 minutes Install using CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 3. Use the Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Optionally, Install manually on an existing Spark2 cluster. 1. From Azure portal, click the resource of the target Spark2 cluster under your resource group and click \"Script actions\". In Script Actions dialog box: Click Submit new Select script type \"- Custom\" Enter a Name for this script, e.g., \" unravel-spark-setup\" Enter the script path from above e.g., https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Use input parameters: --unravel-server UNRAVEL_VM_IP_Address:30000 --spark-version 2.1.0 Check the box Persist this script action to rerun when .. Click Create The checkbox for Persist this script action to rerun when ... does not Script Action You can upload the unravel_hdi_bootstrap.sh 2. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed. 3. After completion, login to Ambari and check the Ambari task status. Install additional edge node on existing HDinsight cluster. The ARM template for install edge node with Unravel Script Action only \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json Use the ARM template to install the edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example). In this example, an edge node will be created first. Next, it runs emptynode-setup.sh unravel_hdi_bootstrap.sh \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires change in scriptActionUri path and application name in variables and also parameters for cluster name. Adjust the ARM templates for your setup and validate it before using. " }, 
{ "title" : "INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"url" : "unravel-4-4/install/install-hdi/internal--update-the-azure-resource-manager-if-needed/internal--step-5--arm-template-for-kafka-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"snippet" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. Th...", 
"body" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. The worker nodes in this Kafka cluster use two data disks per node. You need to deploy unravel VM and update the Script Actions UNRAVEL__IP \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Validate the modified ARM template before applying. The HDInsight cluster creation takes about 15 - 25 minutes Apply Script Actions via CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Modify parameter file. Change the parameter values to reflect your Azure environment, i.e., VNET, Subnet, StorageAccount, Cluster name, etc. You can change the VM size of header, worker nodes. By default they use \"Standard_D3_v2\" as the VM size. 3. Validate the template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 4. Use Azure CLI to deploy the Kafka cluster using the template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 5. After the Kafka cluster is successfully created, the unravel Script Actions \/usr\/local\/unravel\/etc\/unravel.properties The following is the example of the lines appended to unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 6. Once unravel.properties unravel_km # \/etc\/init.d\/unravel_km restart Alternatively, apply Unravel Kafka Script Actions scripts manually on an existing Kafka cluster. If you already have an existing Kafka cluster, you can apply Unravel's Kafka script via Azure portal. 1. From Azure portal, click the resource of the target Kafka cluster under your resource group and click Script Actions 2. In the Script Actions dialog box: Click Submit New Select script type, e.g., \"- Custom\" Enter a Name for this script, e.g. \" unravel-kafka-setup\" Enter the script path from above, e.g. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Input parameters: UNRAVEL_VM_IP_Address Check the box \" Persist this script action to rerun You can upload the unravel_hdi_kafka_bootstrop.sh 3. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed 4. After completion, login to Ambari of the cluster and check the Ambari task status. 4. Login to unravel VM and restart the unravel Kafka monitor daemon, unravel_km. # \/etc\/init.d\/unravel_km restart " }, 
{ "title" : "Upgrading the Unravel VM or App", 
"url" : "unravel-4-4/install/install-hdi/upgrading-the-unravel-vm-or-app.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Upgrading the Unravel VM or App", 
"snippet" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll a...", 
"body" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this requires you to re-submit the Unravel action scripts to head, worker, and edge nodes. " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-4/install/install-hdi/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ Azure HDInsight \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "MySQL", 
"url" : "unravel-4-4/install/install-mysql.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MySQL", 
"snippet" : "Install and Configure MySQL for Unravel Moving Unravel MySQL to a Custom Location MySql Partitioning and Data Migration...", 
"body" : " Install and Configure MySQL for Unravel Moving Unravel MySQL to a Custom Location MySql Partitioning and Data Migration " }, 
{ "title" : "Install and Configure MySQL for Unravel", 
"url" : "unravel-4-4/install/install-mysql/install-mysql-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MySQL \/ Install and Configure MySQL for Unravel", 
"snippet" : "Pre-install Steps Install MySQL Server 5.5 Configure and Start MySQL Server Post Install Steps Download and Extract MySQL JDBC Driver Copy MySQL JDBC JAR to target directories Configure Unravel to connect MySQL Server Pre-install Steps Perform the following steps on the MySQL database host server (t...", 
"body" : " Pre-install Steps Install MySQL Server 5.5 Configure and Start MySQL Server Post Install Steps Download and Extract MySQL JDBC Driver Copy MySQL JDBC JAR to target directories Configure Unravel to connect MySQL Server Pre-install Steps Perform the following steps on the MySQL database host server (this can be the same host as Unravel server, or a separate host). Install MySQL Server 5.5 Install MySQL database. CentOS 6 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm\n# sudo yum install yum-utils\n# sudo rpm -ivh mysql80-community-release-el6-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql55-community\n# sudo yum install mysql-community-server CentOS 7 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el7-1.noarch.rpm\n# sudo rpm -ivh mysql80-community-release-el7-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql55-community\n# sudo yum install mysql-community-server Configure and Start MySQL Server Stop MySQL server if it is running. # sudo service mysqld stop If MySQL was running previously, backup old InnoDB log files to a directory of your choosing, { Backup_Path \/var\/lib\/mysql\/ib_logfile # mv \/var\/lib\/mysql\/ib_logfile* {Backup_Path}\n\n\/\/ OR\n\n# rm -rf \/var\/lib\/mysql\/ib_logfile* Append the following properties at the end of [mysqld] \/etc\/my.cnf datadir \/srv\/unravel\/db_data port = 3316 key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M max_connections = 500 max_connect_errors = 2000000000 open_files_limit = 10000 port-open-timeout = 121 expire-logs-days = 2 character_set_server = utf8 collation_server = utf8_unicode_ci innodb_open_files = 2000 innodb_file_per_table = 1 innodb_data_file_path = ibdata1:100M:autoextend # The innodb_buffer_pool_size depends on load and cluster size. # On a dedicated machine, it can be 50% of the RAM size. # Using 1G is the absolute minimum. For a large cluster, we use 48G. innodb_buffer_pool_size = 4G innodb_flush_method = O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency = 20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format = mixed # if SSD disk is used uncomment the line below #innodb_io_capacity = 4000 For a large cluster with a lot of jobs running, increase the max_allowed_packet Set MySQL server to start at boot. CentOS 6 # sudo chkconfig mysqld on CentOS 7 # sudo systemctl enable mysqld Start MySQL server. CentOS 6 # sudo service mysqld start CentOS 7 # sudo systemctl start mysqld Check disk space used by MySQL's datadir from the MySql configuration file (eg., ( \/etc\/my.cnf # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs du -sh Check available file system disk space for MySQL's datadir from the MySql configuration file (eg., \/etc\/my.cnf) # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs df -h Post Install Steps After installing or upgrading Unravel RPM, complete the following steps on the Unravel server. Download and Extract MySQL JDBC Driver Download MySQL JDBC driver to \/tmp # wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz cd \/tmp # cd \/tmp\n# tar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz Copy MySQL JDBC JAR to target directories Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ and \/usr\/local\/unravel\/dlib\/mybatis # mkdir -p \/usr\/local\/unravel\/share\/java\n# mkdir -p \/usr\/local\/unravel\/dlib\/mybatis\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/mybatis Configure Unravel to connect MySQL Server Update the following properties in \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.username=unravel unravel.jdbc.password={password} # If MySQL JDBC driver is installed replace jdbc:mariadb with jdbc:mysql unravel.jdbc.url=jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod Log into mysql as root and create a Database and user for Unravel. Substitute <unravel-hostname> & <password> with the relevant values for your environment. # mysql \nmysql> CREATE DATABASE unravel_mysql_prod;\nmysql> CREATE USER 'unravel'@'<unravel_hostname>' IDENTIFIED BY '<password>';\nmysql> GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'<unravel_hostname>'; Create schema for Unravel tables. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create default admin for Unravel UI. # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Once you have successfully configured Unravel to use the MySQL database, ensure that the unravel_pg service unravel_pg stop; chkconfig unravel_pg off If this is a new install of Unravel # \/etc\/init.d\/unravel_all.sh restart If this is an upgrade of an existing install of Unravel next section " }, 
{ "title" : "Moving Unravel MySQL to a Custom Location", 
"url" : "unravel-4-4/install/install-mysql/install-mysql-move.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MySQL \/ Moving Unravel MySQL to a Custom Location", 
"snippet" : "If you are currently using Unravel bundled MySQL see here An external MySQLsee here Whichever move you perform, you must use slow shutdown If you need to move MySQL to another host please see here UsingMySQL which had been bundled with Unravel Daemon user must have Perform a slow shutdown Get DB roo...", 
"body" : "If you are currently using Unravel bundled MySQL see here An external MySQLsee here Whichever move you perform, you must use slow shutdown If you need to move MySQL to another host please see here UsingMySQL which had been bundled with Unravel Daemon user must have Perform a slow shutdown Get DB root password from \/root\/unravel.install.include \/root\/unravel.install.include .include # grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include Run the following commands to set MySQL clean shutdown. # \/usr\/local\/unravel\/mysql\/bin\/mysql -uroot --port=3316 --host=127.0.0.1 -p\nmysql> SET GLOBAL innodb_fast_shutdown=0; Stop unravel_db daemon. # \/etc\/init.d\/unravel_db stop Backup MySQL database folder \/srv\/unravel\/db_data # cd \/srv\/unravel\n# tar cvf unravel_db_data.tar db_data\/\n Restore MySQL datadir to the custom path. Replace { NEW_DB_LOCATION} # scp unravel_db_data.tar {NEW_DB_LOCATION}\n# ssh to {NEW_DB_LOCATION}\n# cd {NEW_DB_LOCATION}\n# tar xvf unravel_db_data.tar Update unravel.install.include In \/root\/unravel.install.include NEW_DB_LOCATION If using bundled Unravel database, in \/usr\/local\/unravel\/mysql\/unravel_mysql.cnf innodb_data_home_dir innodb_log_group_home_dir NEW_DB_LOCATION If using external MySQL database, In \/etc\/my.cnf innodb_data_home_dir innodb_log_group_home_dir NEW_DB_LOCATION Restart Unravel # \/etc\/init.d\/unravel_all.sh restart Using MySQL that has been installed independently \/ external to Unravel (non bundled) MySQL user must have Perform a slow shutdown Run the following commands to set MySQL clean shutdown. # mysql -uroot -p\nmysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon # service mysqld stop Backup MySQL database folder \/var\/lib\/mysql # cd \/var\/lib\n# tar cvf unravel_db_data.tar mysql\/\n Restore MySQL datadir to the custom path. Replace { NEW_DB_LOCATION} # scp unravel_db_data.tar {NEW_DB_LOCATION}\n# ssh to {NEW_DB_LOCATION} \n# cd {NEW_DB_LOCATION}\n# tar xvf unravel_db_data.tar Update MySQL cnf file In \/etc\/my.cnf NEW_DB_LOCATION Restart Unravel # \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "MySql Partitioning and Data Migration", 
"url" : "unravel-4-4/install/install-mysql/install-mysql-partition.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ MySQL \/ MySql Partitioning and Data Migration", 
"snippet" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for it's use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS 1....", 
"body" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for it's use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS 1. Upgrade the Unravel Server. See here for instructions. 2. Stop all daemons and make sure MySQL is running. # \/etc\/init.d\/unravel_all.sh stop\n 3. Perform the data migration. N.B. *** THE FOLLOWING MIGRATION COULD TAKE A LONG TIME DEPENDING ON SIZE OF DATABASE - PLEASE RUN IN A 'SCREEN' OR 'NOHUP' OR 'TMUX' *** # \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh migration:migration_partitioning During migration process the status of the tables is written to stdout. The output is continually updated until the migration for the table is complete. Start migration: migration_partitioning\nStart table rows calculation...\nTable blackboards_old has 0 rows total\nTable impala_queries_old has 0 rows total\nTable jobs_old has 205437 rows total\nTable event_instances_old has 0 rows total\nTable oozie_workflow_jobs_old has 0 rows total\nTable hive_queries_old has 0 rows total\nTable rows calculation finished.\nMigrating records for table: blackboards, chunk size: 10000, chunk count: 10...\nMigrated records for table: blackboards, rows total: 0, chunk size: 10000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: impala_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: impala_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 5000, chunk size: 500, chunk count: 10, finished: false, time: 32 seconds, progress 2,43 %\nMigrating records for table: event_instances, chunk size: 50000, chunk count: 10...\nMigrated records for table: event_instances, rows total: 0, chunk size: 50000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: oozie_workflow_jobs, chunk size: 1500, chunk count: 10...\nMigrated records for table: oozie_workflow_jobs, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: hive_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: hive_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 10000, chunk size: 500, chunk count: 10, finished: false, time: 33 seconds, progress 4,87 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\n When all the tables have been successfully migrated you will see: Migration: migration_partitioning is finished\n During the process a temporary migration_partitioning keeps track the tables' status. You can view the table using The chunks field increases until the migration for the table is complete. Upon completion the table's finished column is set to 1. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql> select * from migration_partitioning;\n+---------------------+--------------------+--------+----------+\n| migr_table_name | lowest_id_migrated | chunks | finished |\n+---------------------+--------------------+--------+----------+\n| blackboards | 11018258 | 119 | 0 |\n| event_instances | 0 | 1 | 1 |\n| hive_queries | 0 | 1 | 1 |\n| impala_queries | 0 | 1 | 1 |\n| jobs | 240884 | 100 | 0 |\n| oozie_workflow_jobs | 0 | 1 | 1 |\n+---------------------+--------------------+--------+----------+\n\n To see the oldest migrated records per each table you can use the following MySql query. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql>\nselect \"blackboards\" as \"table_name\", min(created_at) as \"oldest_migrated_record\" from blackboards\nunion select \"event_instances\", min(created_at) from event_instances\nunion select \"hive_queries\", min(created_at) from hive_queries\nunion select \"impala_queries\", min(created_at) from impala_queries\nunion select \"jobs\", min(created_at) from jobs\nunion select \"oozie_workflow_jobs\", min(created_at) from oozie_workflow_jobs;\n+---------------------+------------------------+\n| table_name | oldest_migrated_record |\n+---------------------+------------------------+\n| blackboards | 2018-08-30 00:57:45 |\n| event_instances | 2018-08-30 00:57:55 |\n| hive_queries | 2018-08-30 00:59:10 |\n| impala_queries | NULL |\n| jobs | 2018-08-30 00:57:50 |\n| oozie_workflow_jobs | 2018-09-07 10:25:22 |\n+---------------------+------------------------+ If the migration process is interrupted or killed, you can run shell script again. However, if the process has failed you must truncate and reset ID's 4. During migration the original files were renamed to <TableName>_old. Upon completion delete these and the migration_partitioning table. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nDROP TABLE blackboards_old;\nDROP TABLE event_instances_old;\nDROP TABLE hive_queries_old;\nDROP TABLE impala_queries_old;\nDROP TABLE jobs_old;\nDROP TABLE oozie_workflow_jobs_old;\nDROP TABLE migration_partitioning;\nquit\n\n 5. Reinstate JDBC JAR Copy the JDBC JAR to \/usr\/local\/unravel\/dlib\/mybatis\/ # sudo cp \/usr\/local\/unravel\/share\/java\/*.jar \/usr\/local\/unravel\/dlib\/mybatis\/ \n# ls -lt \/usr\/local\/unravel\/dlib\/mybatis\/ 6. Restart daemons. # \/etc\/init.d\/unravel_all.sh start How to restart a failed migration. Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning # \/etc\/init.d\/unravel_all.sh stop\n\n# \/usr\/local\/unravel\/install_bin\/db_access.sh\n\ntruncate blackboards;\ntruncate event_instances;\ntruncate hive_queries;\ntruncate impala_queries;\ntruncate jobs;\ntruncate oozie_workflow_jobs;\n\ntruncate migration_partitioning;\n\ncall resetAutoIncrementId('blackboards_old', 'blackboards');\ncall resetAutoIncrementId('event_instances_old', 'event_instances');\ncall resetAutoIncrementId('hive_queries_old', 'hive_queries');\ncall resetAutoIncrementId('impala_queries_old', 'impala_queries');\ncall resetAutoIncrementId('jobs_old', 'jobs');\ncall resetAutoIncrementId('oozie_workflow_jobs_old', 'oozie_workflow_jobs');\n\n Return to Step 3 and complete the remaining steps. " }, 
{ "title" : "OnDemand", 
"url" : "unravel-4-4/install/install-ondemand.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ OnDemand", 
"snippet" : "Installation or Upgrade of OnDemand Library Versions and Licenses for OnDemand...", 
"body" : " Installation or Upgrade of OnDemand Library Versions and Licenses for OnDemand " }, 
{ "title" : "Installation or Upgrade of OnDemand", 
"url" : "unravel-4-4/install/install-ondemand/install-ondemand-details.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ OnDemand \/ Installation or Upgrade of OnDemand", 
"snippet" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before insta...", 
"body" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before installing OnDemand. (Optional) 1. Install External MySQL on the Unravel Host You can skip these steps if you plan to use Postgres with OnDemand. Install MySQL. See MySQL Verify MySql is installed correctly by running netstats # netstat -tunlp | grep :3306\ntcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld For fresh installations ondemand_tables.sh https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/ \/tmp ondemand_tables.sh You can use this version of ondemand_tables.sh Contact Unravel Support for {USERNAME} {PASSWORD} # wget --user {USERNAME} --password {PASSWORD} \\\n-O \/tmp\/ondemand_tables.sh \\\nhttps:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.1\/ondemand_tables.sh Navigate to the \/tmp chmod +x # cd \/tmp\n# chmod +x ondemand_tables.sh\n# sudo .\/ondemand_tables.sh For fresh installation of OnDemand versions 4.4.2 or 4.4.1 report_instances # cat \/usr\/local\/unravel\/sql\/mysql\/20180517031500.sql | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh Run db_access.sh ondemand_tasks ondemand_sessions report_instances # sudo \/usr\/local\/unravel\/install_bin\/db_access.sh\nmysql> show tables; 2. Install the OnDemand Service on the Unravel Host Download the OnDemand package from https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/ \/tmp Contact Unravel Support for {USERNAME} {PASSWORD} For example, if your host operating system is Red Hat Enterprise Linux 7 (RHEL 7): # cd \/tmp\n# wget --user {USERNAME} --password {PASSWORD} \\\nhttps:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/OnDemand.rhel7.tar.gz For example, if your host operating system is Red Hat Enterprise Linux 6 (RHEL 6): # cd \/tmp \n# wget --user {USERNAME} --password {PASSWORD} \\\nhttps:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.2\/OnDemand.rhel6.tar.gz Navigate to the the \/tmp ondemand # cd \/tmp\n# sudo rm -rf \/usr\/local\/unravel\/ondemand\n (Optional) If you want Unravel to generate Small Files reports and you've customized hive_properties.hive hive_properties.hive ondemand # cp \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive \\\nondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive\n Extract the contents of the tarball. # tar xvf OnDemand.rhel<6 or 7>.tar.gz Run the installation script. # sudo mv ondemand\/ \/usr\/local\/unravel\/\n# cd \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\n# chmod +rwx install\/ondemand_quick_install.sh\n# sudo .\/install\/ondemand_quick_install.sh\n# sudo \/etc\/init.d\/unravel_all.sh restart In unravel.properties For details, see Unrave lProperties: OnDemand Configurations for Reports Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart If your host operating system is SELinux, you might get alerts like these after restarting Unravel Server, depending on your environment. You can ignore them: # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Howver, if you encounter problems, contact Unravel Support. 3. Start the OnDemand Daemon Execute the following four commands in the order shown, replacing { run_as_user run_as_group switch_to_user For RHEL 7.x: # sudo systemctl stop unravel_ondemand.service\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo systemctl start unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand stop\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo service unravel_ondemand start Confirm that the OnDemand service is running: You should see output similar to the example below. Process IDs will vary dynamically based on the number of processors in Unravel Server, number of current tasks, and so on. For RHEL 7.x: # sudo systemctl status unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand status You can also use ps # ps -ef | grep ondemand | grep -v grep\nroot 11159 1 0 Sep21 ? 00:00:00 su - -c bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh\nroot 11163 11159 0 Sep21 ? 00:00:00 -bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh\nroot 11450 11176 0 Sep21 ? 00:00:42 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/flask run\nhdfs 11452 11176 0 Sep21 ? 00:27:19 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3\nhdfs 11670 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3\nhdfs 11671 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3\nhdfs 11672 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3\n\n 4. Enable Various OnDemand-based Reports or Features Enabling or Disabling Queue Analysis Reports Enabling or Disabling Cluster Optimization Reports Enabling or Disabling Cloud Reports Enabling or Disabling Forecasting Reports Enabling or Disabling Small Files Reports and Files Reports Enabling or Disabling Top X Reports Enabling or Disabling Sessions " }, 
{ "title" : "Library Versions and Licenses for OnDemand", 
"url" : "unravel-4-4/install/install-ondemand/install-ondemand-libraries-licenses.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Installation Guides \/ OnDemand \/ Library Versions and Licenses for OnDemand", 
"snippet" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency ...", 
"body" : " Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency of FBprophet) fbprophet>=0.1.1 BSD scipy>=0.19.1 BSD seasonal>=0.3.1 MIT statsmodels>=0.8.0 BSD gatspy>=0.3 BSD 3-Clause numpy>=1.13.1 BSD PyAstronomy>=0.12.0 MIT python_dateutil>=2.6.1 Simplified BSD fastdtw>=0.3.2 MIT requests>=2.14.2 Apache 2.0 seaborn>=0.8.1 BSD 3-Clause sqlalchemy>=1.2.3 MIT License pymysql>=0.8.0 MIT cython 0.27.3 Apache License, Version 2.0 kombu 4.1.0 BSD 3-Clause \"New\" Thrift-sasl 0.3.0 Apache License, Version 2.0 " }, 
{ "title" : "Post Installation Steps", 
"url" : "unravel-4-4/post-installation-steps.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Post Installation Steps", 
"snippet" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication LDAP properties If mode=ldap see enable SAML authentication SAML properties Configure the email properties Set com.unraveldata.customer.organiz...", 
"body" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication LDAP properties If mode=ldap see enable SAML authentication SAML properties Configure the email properties Set com.unraveldata.customer.organization Configure Hive Metastore Permissions Set up HBASE Configuration Configure Kafka Monitoring Configure Oozie Configure the following Tez yarn.ats.webapp.username yarn.ats.webapp.password yarn.timeline-service.webapp.address yarn.timeline-service.port " }, 
{ "title" : "Other Configuration Options", 
"url" : "unravel-4-4/post-installation-steps.html#UUID-5968993c-5e7b-69bd-39b0-328ebecf8676_id_PostInstallationSteps-OtherConfigurationOptions", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Post Installation Steps \/ Other Configuration Options", 
"snippet" : "Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data...", 
"body" : " Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data " }, 
{ "title" : "Further Configuration Options", 
"url" : "unravel-4-4/post-installation-steps.html#UUID-5968993c-5e7b-69bd-39b0-328ebecf8676_id_PostInstallationSteps-FurtherConfigurationOptions", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Post Installation Steps \/ Further Configuration Options", 
"snippet" : "Custom Configurations Security Configurations...", 
"body" : " Custom Configurations Security Configurations " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-4/user-guide.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optim...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-4/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Getting Started", 
"snippet" : "Use Case Videos Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements...", 
"body" : " Use Case Videos Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-4/user-guide/getting-started.html#UUID-02dd8feb-3bec-6ac7-f159-d258e6820d33_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "The Use Case videso below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Pe...", 
"body" : " The Use Case videso below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Common UI Features", 
"url" : "unravel-4-4/user-guide/common-ui-features.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Common UI Features", 
"snippet" : "Every page has the Unravel Docs Supported Roles Role Based Access Control If you are end-user restricted by Role Based Access Control Applications About Logout If you are unrestricted end-user or an admin, you have all the pages available with possible read\/write restrictions. The pull-down menu has...", 
"body" : " Every page has the Unravel Docs Supported Roles Role Based Access Control If you are end-user restricted by Role Based Access Control Applications About Logout If you are unrestricted end-user or an admin, you have all the pages available with possible read\/write restrictions. The pull-down menu has Manage About Logout If your admin has disabled Support If you can configure the date range time period cluster(s) When there are multiple tabs, click on the tab to display its contents. When detailed or further information is available open section ( To expand section to the width of the entire tile click on the double arrows displayed ( Clicking on the application name\/id\/workflow usually bring ups information on the appplication, fragment etc, i.e., the Spark Application Manager, table information, etc. Lists\/Tables Can be sorted by a column, i.e., start time, in ascending or descending order. The sort column highlights the arrow indicating the sort order ( Clicking on a column being used reverses the sort order. If you can chose which columns to display a plus ( When applicable, the application status is color coded: Clicking on the app name\/id\/workflow usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. When applicable, the Notifications When relevant there is an Auto Actions\/Events column ( When an application has a parent a link to it will appear in the GoTo A block glyph ( Graphs (see Operations | Usage Details | Infrastructure Hovering over a line in a graph causes the information to be displayed in a text box ( When \" Applications running at mm\/dd\/yy hh:mm:ss\" Clicking Show More ( If graph can be displayed based upon Group By Tags Metric Click If you can zoom in\/out of a diagram\/execution graph the magnifiers ( " }, 
{ "title" : "The Operations Page", 
"url" : "unravel-4-4/user-guide/the-operations-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Operations Page", 
"snippet" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Detail Operations Dashboard Note Click here...", 
"body" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: \n Dashboard Usage Detail Operations Dashboard Note Click here " }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-4/user-guide/the-operations-page.html#UUID-94268a47-cf8b-f22e-76f1-27ba586e0806_id_TheOperationsPage-DashboardDashboard", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Operations Page \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applicat...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applications Tile The line graphs display the successful, failed, and killed jobs for the time period time incremen cluster Clicking on Open Section Applications | Applications Finding Applications Running YARN Application Tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Clicking on Open Section Operations | Usage Details | Jobs here Resources Tile Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations | Usage Details | Infrastructure . below Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications | Applications Finding Applications Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. A separate entry appears for each time a particular Auto Action was triggered. In the image below, the same auto action triggered at 23:56 and 2:58. Clicking an event\/alert brings up a Cluster Resource view ( Operations | Usage Detail | Infrastructure Auto Actions Overview Add a New AutoAction or Alert Clear " }, 
{ "title" : "Usage Details", 
"url" : "unravel-4-4/user-guide/the-operations-page.html#UUID-94268a47-cf8b-f22e-76f1-27ba586e0806_id_TheOperationsPage-ChartsUsageDetails", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Operations Page \/ Usage Details", 
"snippet" : "Infrastructure Jobs Nodes Impala Usage Kafka HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details recommendations and insights All the charts and tables are...", 
"body" : " \n \n Infrastructure \n Jobs \n Nodes \n Impala Usage \n Kafka \n HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details \n recommendations and insights All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, e.g., change the date range, click on a point within in a graph. When disabled a Refresh Refresh By default the Usage Details tab opens showing the Infrastructure tab. For all charts, click on the menu bars ( Show more Reset Graph Infrastructure Infrastructure This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, i.e., Application Type User Queue Business Tags Clicking within a chart (1) displays the applications running for that point in time. You can chose how to display the bottom two graphs by clicking on the View By Showing View Showing x Infrastructure Application Type above show more To View by use the Business Tags Showing Jobs Graphs the running and accepted jobs as applicable. You can Group by Nodes This chart graphs the Total \n Total Active Unhealthy Where: \n Active: \n Unhealthy: You can toggle the display of an item by clicking on its name. Impala Usage Graphs memory MB consumption and Query Number. The # Queries Tags Group By Kafka Lists all the configured Kafka clusters. See Kafka Application Manager Kafka Use Case Clicking the cluster name brings detailed information about the Kafka Cluster HBase Please see HBase Configuration Clusters View Clusters page lists all the available HBase clusters Click on a cluster name to bring up the cluster's information the HBase Cluster view. Cluster View This view is divided into four (4) sections. When an component's health is noted, hovering over it's health glyph brings up details, \n Cluster Information A bar at shows what cluster you are displaying with a pull-down which allows you to switch between clusters. Listed immediately below are the cluster metrics. You can choose to tab between clusters by choosing all cluster \n Region Servers Lists the the cluster regional services, with their KPI's and health. You can search on the region server by name. Click on the server's name to bring up its details. \n Region Servers KPI Graphs the regional server metrics, the graphs are linked with the table list below them. Click within a graph to see up the tables associated servers that point in time. Hover over a point to bring up a popup displaying the information for that point in time. Click Show More \n Tables List all the tables associated with the cluster, their KPIs and the table's health. Click on the table name to bring up its information. You can search for a table by name; any table with a name matching or containing the string is displayed. Region Server View Server, Operational, and OS Metrics are displayed. Hover over the metric for its description. For more information on the metrics see here Table View Table has two tabs, Table Region Table Table Regions Lists all the regions with their KPIs and health. " }, 
{ "title" : "The Applications Page", 
"url" : "unravel-4-4/user-guide/the-applications-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page", 
"snippet" : "Table of Contents Applications Tab Finding Applications Workflow Tab Sessions Tab Creating a Session Manual Session Auto Tune Session Via the Events Panel Session Typical Application Manager's Layout Application-Specific Managers Cascading and Pig Application Managers Key Performance Indicators Tabs...", 
"body" : "\n Table of Contents \n Applications Tab \n Finding Applications \n Workflow Tab \n Sessions Tab \n Creating a Session \n Manual Session \n Auto Tune Session \n Via the Events Panel \n Session \n Typical Application Manager's Layout \n Application-Specific Managers \n Cascading and Pig Application Managers \n Key Performance Indicators \n Tabs \n Hive Application Manager \n Key Performance Indicators \n Tabs \n Impala Application Manager \n Key Performance Indicators \n Tabs \n Kafka Application Manager \n Key Performance Indicators \n Cluster View \n Consumer Group View \n Unravel Insights for Kafka \n MapReduce Application Manager \n Key Performance Indicators \n Tabs \n Spark Application Manager \n Tez Application Manager \n Key Performance Indicators \n Tabs \n Workflow Manager \n Key Performance Indicators \n Tabs \n Event Panel Examples \n Recommendations \n Efficiency \n MapReduce Job Example \n Tez DAG Example \n Appendices \n Event List \n Resource Metrics The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron Unravel currently supports the the following application frameworks: Cascading\/Pig Hive (on Map-Reduce) Kafka Impala Map-Reduce Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL \n events Event Panel Examples The Applications Page has three tabs: Applications, Workflows, and Sessions Note Click here " }, 
{ "title" : "Applications Tab", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-ApplicationsTabApplicationsTab", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Applications Tab", 
"snippet" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application...", 
"body" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application(s) in a variety of ways: The left sidebar allows you to filter you App Name App type Status Tags , Queue User Cluster Duration Number of Events. By time period, If the job is part of a Hive query, Pig script, or a Workflow, a link to it is noted in the job's Go To " }, 
{ "title" : "Workflow Tab", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-WorkflowTabWorkflowTab", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Workflow Tab", 
"snippet" : "The layout of this window mirrors the Applications Workflow Manager Application Manager. Click Add Workflow Add Selected Workflows Add Selected Workflows....", 
"body" : "The layout of this window mirrors the Applications Workflow Manager Application Manager. Click Add Workflow Add Selected Workflows Add Selected Workflows. " }, 
{ "title" : "Sessions Tab", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-SessionsTab", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Sessions Tab", 
"snippet" : "This Report does not work with Postgres. You must be using MySQL and have the OnDemand package installed Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first pr...", 
"body" : " This Report does not work with Postgres. You must be using MySQL and have the OnDemand package installed Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliability: in attempting to reduce resources Unravelprioritizes memory allocation to ensure the application doesn't fail due to \"out of memory\" exceptions. Why use sessions when Unravel already offers insights and recommendations on an application's run? You direct the tuning goal. You can provide multiple runs of an application providing a larger data pool for Unravel to analyze. You can have Unravel apply the recommendations for you and run the newly configured application. You can see the effects, both positive and negative, the tuning has on an applications run. You can compare runs configurations. You can repeatedly tune the application until Unravel has no more recommendations. Your session is saved and can be run again, e.g., new runs added, cluster configuration changed. You can tune: Spark Hive on MapReduce Sessions can serve simply as a tool to compare two runs of the same application. The Sessions tabs opens displaying all current sessions sorted on Sessions Name Start Time Number of Apps The four (4) KPI's Duration IO vCore Seconds Memory Seconds Duration Cluster ID You can search for a session by name. Enter the string in the search box; any session name matching or containing the string will be displayed. Creating a Session You can use sessions \n manually \n automatically Manual Session Click Create Session. Application Type Tuning Goal App IDs +Add another App ID Add If you are tuning a Spark App you must supply the JAR path and Class Name. Auto Tune Session You have the additional option to specify the maximum number of runs. If not specified, iterates continues until no recommendations are available. When specified, the iteration stops at the maximum number or lack of recommendations, whichever comes first. Via the Events Panel If an applications has events Session The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applications \n Right Tabs \n Progress Tab - \n Trends \n Compare - See Sessions Use Case for more information on using this feature. " }, 
{ "title" : "Typical Application Manager's Layout", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-TypicalAPMLayoutTypicalApplicationManagersLayout", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Typical Application Manager's Layout", 
"snippet" : "A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the pa...", 
"body" : " A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the parent's type. Clicking on it brings up its APM. Unravel's Intelligence Engine can provide insights into an application and may provide recommendations, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) \n Event icon No Events Event Panel Examples \n Job icon: \n Job information: \n KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation Common Tabs: \n Errors: Keywords \n Conf\/Configuration \n Tags project dept group11 hr " }, 
{ "title" : "Application-Specific Managers", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-Application-SpecificManagers", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Application-Specific Managers", 
"snippet" : "Cascading and Pig Application Managers The only Key Performance Indicators Events Event Panel Examples Duration Data I\/O Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Tabs The left Tabs: Navigation Gantt chart Click here for a screenshot. Tags here Th...", 
"body" : "Cascading and Pig Application Managers The only Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Tabs The left Tabs: \n Navigation \n Gantt chart Click here for a screenshot. \n Tags here The right Tabs \n Task Attempts entire \n Attempts Click here screenshots of the Attempts graphs. The wall clock time is noted in the upper left hand corner. The computer slot usage is noted below the graph. Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of YARN apps Tabs By default the Hive APM opens displaying the Navigation Query The left Tabs are: \n Navigation MapReduce Application Manager \n Execution Graph Click here for more information and to see a screenshot. The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other along some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). Click on a table name to bring up the table information Close Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. \n Gantt Chart \n Errors here \n Tags here The right Tabs are: \n Query window Copy Query \n Tables: Click here to for more information and to see a screenshot. Click on the table name to bring up the Table Detail. See here \n Task Attempts entire Click here to for more information and to see a screenshot. \n Attempts Click here to for more information and to see a screenshot. The wall clock time the job started is listed in the upper left hand corner. The total Map and Reduce slot duration time is noted below the graph. Impala Application Manager The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Fragments \n Number of Operators Tabs The left Tabs are: \n Fragments More L ess \n This window shows the Fragment and it's KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking on the operator brings up the operator window. (See Operators below for more information.) You can view the Query Plan \n Instance View: \n Operators Click here to screenshots. You can search the operators name. Click on the operator to display its details. \n Scan HDFS details \n Aggregate Details \n Exchange Details \n Gannt Chart Click to see a screenshot. Hover over a section see the operation and it's KPI's. \n Query Plan Click here to see screenshots. Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. \n Tags here The right Tabs are: \n Query Query Copy window \n Mem Usage Click here to see a screenshot. Kafka Application Manager The Kafka Application Manager provides Multi-Cluster support for monitoring : Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled \n Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators \n Bytes in\/sec \n Bytes out\/sec \n Messages in\/sec \n Total Fetch Requests per \/sec \n Number of Active Controller \n Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafka Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators \n Number of Topics \n Number of Partitions The Topic lists displays the KPIs; when details are available a more info \n You can chose both the Partition Metric th offset Partition Details' \n The Kafka View has two tabs, Topic Detail Partition Detail \n Consumer Details' \n Kafka Topic Detail By default the Kafka Topic Detai Topic Detail \n Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is \n Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. \n Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: \n OK, \n Warning: \n Error MapReduce Application Manager The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O Tabs By default the MapReduce APM opens in the Graphs | Attempts \n Graphs \n Attempts \n Containers, Vcores, \n Timeline Click here for more details and to see a screenshot. The Timeline tab is divided into two sections: a Distribution Map Reduce a bottom table which lists either the tasks by stages on servers or teh list of tasks and their associated KPIs' The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map Reduce Timeline Selected \n Metrics Click here to see the screenshot. \n Logs: Click here for more details and to see a screenshot. Click on the tab to see the listing for that type (Map, Reduce, or Application Master). Click on an item to see the log. Click here for an example log. \n Configuration: Click here to see a screenshot \n Resource Usage Click here to see a screenshot Initially all the executors are displayed using the Metric Metric nly Show All \n Errors here \n Tags here Spark Application Manager See Spark Application Manager page Tez Application Manager The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). \n \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O Tabs By default the Tez APM opens showing the Navigation and Program Tabs. The left Tabs are: \n Navigation Click here for more information and screenshots of the Dag detail. The DAG detail has six tabs: \n Query: \n Graph: \n Counter \n Vertex Timeline Wall Clock Total Run \n All Vertices \n All Task \n All Task Attempts \n Changed Configuration \n Configuration \n Tags here The right Tabs are: \n Program \n Graphs \n Containers, Vcores, \n Resources Click here for more information and to see a screenshot. By default the Resource systemCpuLoad Select series Metric Get Data Workflow Manager The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications | Applications Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Yarn Apps Tabs The APM opens showing the Navigation Compare The left Tabs \n Navigation More Click here for more information and a screenshot Below the second Oozienode is shown, it is comprised of one MapReduce job and three Hive jobs. The hive jobs comprise one or more tasks, so that too can be expanded. In the example below, the second Oozienode has been expanded along with the first hive job within it. You can click on any job to see the application manager for it. In the example, below you can click on the expanded hive job to bring up the hive application manager. Similarly you can click on the mapreduce job within the hive job to go directly to it. Click on Less \n Execution Click here for more information and a screenshot Click to zoom in, and to zoom in. Click to return to the initial display. Hover over a node within the graph to see a text box which information about the node task. \n Errors here \n Tags here The right Tabs: \n Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events above \n Task Attempts Click to see a screenshot. \n Attempts Click to see a screenshot. Event Panel Examples The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job Example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. \n Recommendations \n Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). \n Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . \n Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). \n Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG Example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. \n Recommendations \n Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. \n Efficiency 2: Tez DAG Resulted in one recommendation (#1). \n Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Appendices", 
"url" : "unravel-4-4/user-guide/the-applications-page.html#UUID-5d1b1316-3fb1-1fe8-4c7f-ac32deddec8f_id_TheApplicationsPage-Appendices", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Appendices", 
"snippet" : "Event List A list of all events generated by Unravel. Resource Metrics A list of resource metrics collected by Unravel....", 
"body" : "Event List A list of all events generated by Unravel. Resource Metrics A list of resource metrics collected by Unravel. " }, 
{ "title" : "Impala Events", 
"url" : "unravel-4-4/user-guide/the-applications-page/impala-events.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Impala Events", 
"snippet" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the ev...", 
"body" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the event box brings up the event panel where you can scroll through the multiple insights. Each insight has two sections, the analysis (brown bar on the left), and actions you can take (green bar on the left). When Fragments and Operators are referred to, their ID's are always noted so you can refer back to their views\/tiles in the Impala APM Time Breakdown Analysis The analysis breaks down where the time is spent during the query execution across the following phases: Query planning, i.e., time spent on parsing the SQL and optimizing the query plan, Admission control, i.e., time spent waiting on a queue\/resource pool, Query execution time until the first row becomes available, and Results fetching, which is a combination of row fetching, execution, and idle time. Next, it displays the longest phase and provides additional analysis of where time spent as part of that phase. In the example below, the longest phase is query execution (56.829s). The details on the operator taking the longest time are noted; in this case the Hash Join took 41 out of the 57 seconds. When possible, the event has insights into why this operator took long time to run, and makes tuning suggestions. (See Impala Slow Operator Even Missing Optimizer Statistics Displays a list of tables accessed by the query that had missing statistics and recommends running \"COMPUTE STATS\" to collect statistics for these tables. Underestimated Count of Rows In this case, the query scanned a table for which the optimizer statistics were outdated resulting in the optimizer underestimating the number of rows to be returned. This, potentially, led to a bad execution plan and the query to run slower. Unravel recommends refreshing the statistics by running \"COMPUTE STATS\". Time Skew Analysis This insight detects whether one or more operator instances took substantially longer than other instances and provides a potential root cause for the time skew. It notes whether the time skew is correlated with either: a bottleneck node, e.g., due to slow disk I\/O on that particular node, or data skew, e.g., if the bottleneck node processed much larger volumes of data compared to the rest of the nodes. Slow Operator Analysis This insight indicates that a query operator took significant amount of time to execute. It shows details such as the operator execution time and the time spent in different phases of the operator's execution. Depending on the operator type, this insight provides a root cause analysis that can explain the operator's poor performance. For example, for a SCAN operator the following potential causes of poor performance are examined: Missing or stale statistics on the table the operator scanned, Less efficient storage format of the scanned table, Inefficient partition pruning, and Remote or non-circuit byte reads For a JOIN operator the following issues are examined: Inefficient join algorithm used, e.g., a broadcast join was executed when a partitioned join would be significantly faster, Inefficient order of joined tables in a hash join, e.g., the right hand side table contains more rows than the left hand side, and Missing or stale statistics on one or both joined tables. Based on the issues identified this insight provides corresponding recommendations to improve the performance of the operator. For example, if a partitioned (shuffle) join would be faster, we recommend you use the \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join. Non-Columnar Storage Formats This lists the tables that were accessed which use a less efficient storage format, such as TEXT or (non-splittable) GZIP. This insight is generated when scans on these tables take long time to execute. In order to address this, we recommend modifying the storage format to a columnar one such as PARQUET or ORC. Inefficient Partition Pruning This insight shows the tables the query accessed which: have table partitions, and all the partitions were used during the query execution. This could lead to poor query performance for all scan operators accessing the tables, especially when the number of partitions is large. Often older partitions, e.g., partitions containing \"cold\" or outdated data, are not useful. In such cases we recommend rewriting the SQL query by adding a predicate on the partition key, such that the older\/cold partitions will get pruned. " }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-4/user-guide/the-applications-page/spark-application-manager.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Applications Page \/ Spark Application Manager", 
"snippet" : "Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source co...", 
"body" : "Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. \n Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source code from the stage tile, spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an application's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark driver and executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. Unravel provides insights into Spark applications and potentially tuning recommendations; see Spark Event List There are multiple Spark application types and the Spark APM information can vary by the application type. Currently Unravel distinguishes between: \n Scala, Java, and PySpark \n SQL-Query \n Streaming Regardless of the application type and how they are are submitted (e.g., from Notebooks, spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark Application Manager's Basic Layout \n A black title bar notes the type of tile (Spark, Job, Stage, etc). The title bar's right side are glyphs for adding a comment, and to minimize or close the tile if possible. If it has a link to the parent, there is an up arrow ( Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions, or insights into how to improve the application's run. When there are insights, a bar appears immediately below the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples Spark Event List The next section contains the Key Performance Indicators (KPIs) and general application information. \n Event icon No Events Event Panel Examples Spark Event List \n Application icon: application status \n Application information: \n Key Performance Indicators (KPIs): The last section, divided in half, has specific information related to the application. The sections for a specific Spark Application (e.g., Streaming) go into more detail. If the application is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Stream Common Tabs Regardless of the application type, the Application Manager view is split into two blocks and each block contains the following tabs. Left Tabs \n Errors Lists all errors associated with the application. The errors are color coded (fatal Keywords Click here to see a screenshot. \n \n Logs Lists the critical logs that were collected for this Spark application. It is noted when no logs are available. \n Below is an excerpt of the executor-20 \n Conf Click here to see a screenshot and for further information. You can narrow the list by choosing the configuration type to display; to see the spark version select metadata. Metadata and driver are selected in the example below and the list narrowed from 1042 to two (2) properties. Some properties appear in multiple categories, e.g., spark.executor.extraJavaOptions is listed under Memory, Driver and Executor. You can search by name; searching on yarn displays every property containing the word yarn. Click Reset Right Tabs \n Program Uploading Spark Programs. \n Task Attempts Click here to see a screenshot and for further information. The donut graphs show the percentage of successful (green), failed (orange), and killed (red) tasks. The legend on the right lists the number of each. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination. Hovering over the chart tells you the percentage of each. \n Graphs \n Running Containers \n Vcores \n Memory Click here to see screenshots of the graphs. \n Running Containers \n Allocated Vcores \n Allocated Memory \n \n Resource Click here for an example and further information. By default the graph displays all executors using the metric availableMemory Get Data You chose one or more series to display using the Select X Hovering only resource highlights it, clicking on it toggles the display, i.e., if currently displayed it is removed from the graph. Conversely if you click the Only Show All Below is an example of the JSON when clicking on Get Data Common Tiles Spark Job A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators \n Duration: \n Number of Stages: It has three tabs: \n \n Stages start time \n \n Gannt Chart Click here to see screenshot. \n \n \n Metadata Click here to see screenshot. Spark Stage The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators \n Duration: \n Date IO It has two tabs, by default it opens in the graph \n Graph \n \n \n Task Attempt Displays the number of total tasks for the stage and number of total attempts made to run these tasks. The number of tasks is noted on the left side of the bar with the number of attempts on the right. The donut chart graphically displays the successful ( Click here to see a screenshot. \n \n \n \n \n Program Details Click here to see a screenshot. Time Line Tab The Time Line tab has two sections, \n Distribution Charts Three tabs below the chart: \n Time Line \n Timeline Breakdown \n Selected Tasks \n Time Line The Time Line tab has two sections, \n Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown Selected Tasks Be default the Time Line Distribution Charts ShuffleMap Disk Bytes Spilled Memory Bytes Spilled Records Read. The lower section opens displaying the Time Line , \n Timeline Breakdown This is useful to identify bottlenecks. \n For each executor used in the current stage, multiple metrics are graphed: Scheduler Delay, Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Using the ratio of Executor Computing Time performing actual work thrashing, or waiting for scheduling. \n Selected Tasks Spark - Scala, Java, and PySpark Key Performance Indicators \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Stages Left Five Tabs Navigation Lists the application's jobs with their relevant KPIs: Status Start Time Duration Partitions\/Tasks Read Write # Stages Start Time \n The job block lists the KPIs Duration # of Stages Stages Metadata Status Start Time Duration Partitions\/Tasks Read Write Input Output Start Time Spark Stage Details \n Execution The execution graph shows a RDD DAG of the application. The application's RDDs Operations Program Click here for more details and screenshots of the Execution DAG. If the program tab displays the code it is linked with the DAG. If you display the execution and program tab simultaneously, as shown below. Click on the vertex to highlight relevant code. Below we see the corresponding code for vertex 18. Below we expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call and finally the number of stages which were involved. Below RDD represented by vertices 17-16 involved two (2) stages, while 15-16 had five (5). Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. \n Gantt Chart Displays the stages using a Gantt Chart. Click here to see a screenshot. The table is sorted on Start Time Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf Four Right Tabs Program When available it displays the program associated with the application. See above Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resource Spark - SQL-Query Key Performance Indicators \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Stages Left Five Tabs Navigation Lists the application's jobs with their relevant KPIs: Status Start Time Duration Paritions\/Tasks Read Write # Stages. above Execution A execution graph of the query. There are times when the DAG is too large to display and it will be noted. See above Gantt Chart Displays the stages using a Gantt Chart. For more details see above Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf Four Right Tabs Program This tab connects all the pieces of a SQL query. The table lists all queries with significant KPI's and the top five stages, i.e., the stages with the longest duration. The lower section contains two tabs, SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Click here to see screenshots of the SQL plan. Scroll down to see the entire SQL Plan. Click on query Click here to see more of the above query plan. \n Task Attempts, Graphs and Resources Task Attempts Graphs Resource Spark - Streaming Enable Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf. After the Unravel sensor has been deployed and installed on the cluster open spark-defaults.conf spark.driver.extraJavaOptions X.Y. \n javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-X.Y. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited. Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Jobs \n Number of Stages Unlike other Spark Application Managers this has a Stream tab Stream Program Left Four Tabs \n Stream Displays the core of an Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Scheduling Delay Metric Scheduling Delay, Processing Time Total Delay. This graph is composed of two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven (7) pages. In the example below, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one (1) page, versus the seven (7) above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time Click on a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. Click to see detailed information about the Stream Batch block. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. The KPI's, Duration Processing Delay Scheduling Delay Total Delay Output Operation Input The example below is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two (2) jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click on the Job ID Spark Job The Input Tab \n Errors, Log and Conf Errors Logs Conf Right Four Tabs \n Program The program (if uploaded by the user) is shown in this tab. \n Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resource " }, 
{ "title" : "The Reports Page", 
"url" : "unravel-4-4/user-guide/the-reports-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Reports Page", 
"snippet" : "Table of Contents Operational Insights Chargeback Cluster Summary Cluster Compare Cluster Optimization Queue Analysis Cluster Workload Data Insights Overview Details Table List Forecasting (Disk Capacity) Small Files File Reports Top X Hive Scheduling Reports Reports Archive Scheduled Reports Unrave...", 
"body" : "\n Table of Contents \n Operational Insights \n Chargeback \n Cluster Summary \n Cluster Compare \n Cluster Optimization \n Queue Analysis \n Cluster Workload \n Data Insights \n Overview \n Details \n Table List \n Forecasting (Disk Capacity) \n Small Files \n File Reports \n Top X Hive \n Scheduling Reports \n Reports Archive \n Scheduled Reports Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. \n Operational Insights \n Data Insights \n Report Archives \n Scheduled Reports Reports opens displaying Operational Insights Note Click here " }, 
{ "title" : "Operational Insights", 
"url" : "unravel-4-4/user-guide/the-reports-page.html#UUID-6821fb64-0b7e-4fb7-a938-f364bb36e6c5_id_TheReportsPage-OpInsightsOperationalInsights", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Reports Page \/ Operational Insights", 
"snippet" : "Chargeback Cluster Summary Cluster Compare Cluster Optimization Queue Analysis Cluster Workload If can specify a date range, a pull down menu is on the right hand side of the tab bar. By default Operational Insights Chargeback Application Type, Chargeback You can generate ChargeBack Group By Applica...", 
"body" : " \n Chargeback \n Cluster Summary \n Cluster Compare \n Cluster Optimization \n Queue Analysis \n Cluster Workload If can specify a date range, a pull down menu is on the right hand side of the tab bar. By default Operational Insights Chargeback Application Type, Chargeback You can generate ChargeBack Group By Application Type, User, Queue Tags. Application Type Donut graphs showing the top results for the Group By Charge back report showing costs, sorted by the Group By List of Yarn applications running. Generate Charge Back Report You can set the report's date range and clusters in the Operational Insights Group By Group By User dept User dept VCore\/Hour Memory MB\/Hour Update Report CSV A new charge back report is generated each time you change the Group By must Update Report Cluster Summary The Cluster Summary Applications User Queue User Applications You can sort applications on vCore or memory seconds. User Queue Cluster Compare This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use Group By User Queue Time Range Compare With Range Any deviation in metrics across the time ranges is highlighted (3). A green red Time Compare With Group By Cluster Optimization This Report does not work with Postgres. You must be using MySQL and have the OnDemand package installed This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize your cluster throughput, resources, and performance. Currently this feature only supports Hive on MapReduce. You can these reports you can: Fine tune your cluster to maximize its performance and minimize your costs, and Compare your cluster's performance between two (2) time periods. Report are generated on an ad hoc or scheduled basis. All reports are archived and can accessed via the Reports Archive Download or Generate a Report Click Download JSON Reports Archive Click Generate New Report Date Range Run Running Run Generate New Report Click Schedule schedule your report Optimization Report The Report has three (3) sections. \n Header \n KPIs Number of Jobs: per day average Number of vCore Hours: per day average Number of MapReduce Containers Percent used for Map Percent used for Reduce Amount of Memory from of MapReduce Containers Percent from Map containers Percent from Reduce containers The KPIs are a per-day average for the number of days in the report. In this case we generated a report for a two (2) day period. All the insights\/recommendations are based upon the analysis of all jobs, in this case113. \n \n Insights\/Recommendations This section contains a tab for each area, with the relevant properties under consideration for tuning. These are cluster wide properties and are the defaults for all applications. Applications, however, can override these properties on an application by application basis. \n MapReduce: mapreduce.map.memory.mb,mapreduce.reduce.memory.mb,mapreduce.input.fileinputformat.split.maxsize,mapreduce.job.reduce.slowstart.completedmaps \n Hive: hive.exec.reducers.bytes.per.reducer,hive.exec.parallel You can expand the insight tile to the full width of the window. Further below we go into greater detail on two of the insights to explain the contents. Insight\/Recommendations Tile Details \n \n Tune the size of the map containers Each tile is entitled with what's being tuned. Below is the expanded view of the first tile \n \n . Immediately below the title is the property to tune, in this case mapreduce.map.memory.mb. Click on on the Next ( 1 2 \n As expected 51% of the jobs (58) used the default, while 33% (37) used 512MB with the remaining jobs distributed across the remaining values. The graph shows Unravel's analysis of the property potential values. It shows each candidate % of memory saved for the input workload % of jobs from the workload that would still run with the candidate When there are tuning instructions it is noted above the graph ( 3 \n \n Tune the number of the map containers \n \n Tune the number of reduce containers in Hive queries In this case, the information was simply informative. There can be cases where tuning suggestions for specific apps are offered. Queue Analysis This Report does not work with PostgreSQL. You must be using MySQL and have the OnDemand package Generates a report of active queues for specified date range. The report analyzes queue activity by apps, vcores. and memory. As with all reports, it can be generated on an ad hoc or scheduled basis. The reports are archived and can accessed via the Reports Archive Download or Generate a Report Click New Report Date Range Run Running Run Generate New Report Click Schedule schedule your report If the report was successfully generated a light green bar appears and a table listing all the queues in existence during the time range is displayed. You can download the report for all queues by clicking on the Download JSON New Report Filter By Click in a queue's row to see its three graphs displaying the queue's apps, vcore, and memory usage. Click in the selection box to chose from the available metrics to graph. Clicking on any point within a graph brings up pop-up in each graph displaying its metrics and their respective values. For instance, in Apps you can graph any combination of submitted, completed, pending, failed, killed, average time, and number of ops. Cluster Workload Displays your cluster(s) yarn applications' workload across a date range using the following views: \n Month \n Hour \n Day \n \n Hour\/Day You can filter each view by App Count Vcores Hour Memory Hour To measure the Vcores or Memory Hour usage is straightforward; at any given point the memory or vcore is being used or it's not. The App Count is not a count of unique apps instance The App Count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore an app can be counted multiple times in a view. on multiple dates, e.g., on 11 & 12 October (2 days) in multiple hours, e.g., 10pm, 11pm & 12am hours on multiple days, Thursday & Friday, and in multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day App Count) Sum(App Counts for dates representing the day) App Count for Wednesdays (10, 17 & 24 October) = 2492, and App Count across Hour\/Day We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. By default the tab opens in the Month App Count Date Range App Count Vcores Hour Memory Hou View By Hour Day \n Hour\/Day Average Sum See Drilling Down Month Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is Therefore, the color of any particular day varies in context to the other days being displayed, e.g., when only one day is displayed it is colored Use Previous Next Hour, Day and Hour\/Day These graphs do not link jobs to any specific date at the graph level. For instance, the Hour at Day on Hour\/Day at a date Month above By default each view opens using the metric selected for the prior view. For instance, if Vcores Hour Month Day Vcores Hour When the Date Range \n Sum \n Average Hour Breaks out information by hour. The interval label indicates the start, i.e., 2AM is 2-3AM. Hover over an interval for its details. Click on the interval to drill down Day Displays the jobs run on a specific weekday. Hover over an interval for its details. Click on the interval to drill down Hour\/Day This views displays the intersection of Hour and Day graphs. The \n Hour \n Day Drilling Down in a Workload View Click on an interval to bring up its information. In our example, we selected 11 October in the \n Month App Count Click to display User Queue User Queue App Type: MR, User: HDFS, or User: ROOT, We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Applications | Applications App Count Vcores Hour Memory Hou r " }, 
{ "title" : "Data Insights", 
"url" : "unravel-4-4/user-guide/the-reports-page.html#UUID-6821fb64-0b7e-4fb7-a938-f364bb36e6c5_id_TheReportsPage-DataInsights", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Reports Page \/ Data Insights", 
"snippet" : "The first two tabs provides data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. The Reports Page#Overview The Reports Page#Details See Hive Metastore Configuration Hot Warm Co ld policy configuration The last two provide provide disk ...", 
"body" : " The first two tabs provides data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. \n The Reports Page#Overview \n The Reports Page#Details See Hive Metastore Configuration Hot Warm Co ld policy configuration The last two provide provide disk management insights, help you manage your disk usage both in terms of capacity and cluster performance. \n Forecasting \n Small Files \n File Reports \n Top X Hive In order to use Small Files, File Reports and Top X Hive you must be using MySQL and have the OnDemand package installed Overview The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. \n Table KPIs \n Partition KPIs The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Partition KPIs sections contain: \n # Accessed \n # Created \n Size Created \n Total Number The Table KPI's also contains: \n Accessed Queries \n Total Read IO Donut Charts These display the Current Label Distribution policy configuration Details The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In this example, the second through fourth table are selected and graphed. Use the Metric Read IO, Total Users, Total Attempts, Total Size Reset Graph Read IO Table List You can Search Show All Read IO More Info Table Detail . \n Configuration Policy Download CSV Table Detail This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI's are displayed: Read IO User # Apps, Attempts There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Read IO, Total Users, Total Attempts, Total Size Application Detail Application Tab Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition S ize MR jobs By default the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View Configuration This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data | Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save Forecasting (Disk Capacity) This Report does not work with PostgreSQL. You must be using MySQL and have the OnDemand package Currently works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapolate capacity trends allowing you to more effectively plan for, and allocate your disk resources. The tab opens displaying the last forecasting report, if any, generated. The graph displays the trend from the historical range start date to the forecast range end date (x-axis). The trend line (in blue) shows the lower, middle, and upper bounds of Unravel's prediction. The y-axis is determined by your actual physical disk capacity. The report parameters are listed above the table headings. Currently this report only works for Cloudera and Hortonworks. Click New Report History (Date Range) Forecasting Run Schedule Scheduling Reports. While Unravel prepares to generate the report Run Running New Report New Report You can download the report currently displayed by clicking the Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives Small Files This Report does not work with PostgreSQL. You must be using MySQL and have the OnDemand package See here Each small file is accessed by a single mapper, therefore a large number of small files can lead to a large number of mappers. In turn, mappers are costly to run so applications using a large number of small files drive up your costs. This report helps you to identify users who create\/use an excessive amount of small files, allowing you to take corrective action such combine multiple files into large files, or notify, limit, or block\/ users who create\/use an excessive amount. in order to correct and prevent future performance degradation, and lower your costs to run applications. The small file window by defaults open with the last report, if any that was generated. The report parameters are listed above the table headings (1), and you can search the path list by string. The list is sorted in descending order on the total number of small files in the directory. Click Download CSV Click New Report \n Small File Size (bytes): \n Minimum # of Small Files: \n # of Directories to Show: Click Run Schedule While Unravel prepares to generate the report Run Running New Report New Report Click Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives File Reports This Report does not work with PostgreSQL. You must be using MySQL and have the OnDemand package This report requires hdfs privileges and currently only works on HDP\/CDH. See here here This report same as Small Files except they are automatically generated using the File Reports properties The default size for the files are: huge file is more than 100GB, medium file is 5GB - 10GB, and tiny file is less than 100KB. Click on the size buttons ( Large Medium Tiny \n Empty Download CSV Top X Hive This Report does not work with PostgreSQL. You must be using MySQL and have the OnDemand package This report lists the Top X Hive jobs which have consumed: the most cluster usage. have the longest duration, and have the largest amount of data I\/O. Click New Report History (Date Range) Top X Run Running Run New Report Download JSON button. The display is composed of three (3) tiles: \n Applications The Applications tile notes the Hive query total for the time range and the number which were successful. Three tables list the hive queries by categories. The tables only vary by the column displaying the report category, i.e., cluster duration. The tables are sorted on the category in descending order. Hover over the Query Snippet \n Resources and Data These tiles display the cumulative totals for the queries. Resources Data Read Write I\/O \n \n \n \n {\n \"Cluster\": [\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"27s \",\n \"queryString\": \" insert into table unravel_hdfs_fsimage_temp_orc select unravel_hdfs_fsimage_master_orc.path, N, unravel_hdfs_fsimage_master_orc.deltimearray, temp_int(unravel_hdfs_fsimage_master_orc.repl,unravel_hdfs_fsimage.repl), temp_time(unravel_hdfs_fsimage_mas...\",\n \"id\": \"hive_20181017030101_de11e114-6117-47e1-84aa-0fec026ac081-u_15afe482-6e29-42e8-afa6-6c1f18ebdcd4\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"27s \",\n \"queryString\": \" insert into table unravel_hdfs_fsimage_temp_orc select unravel_hdfs_fsimage_master_orc.path, N, unravel_hdfs_fsimage_master_orc.deltimearray, temp_int(unravel_hdfs_fsimage_master_orc.repl,unravel_hdfs_fsimage.repl), temp_time(unravel_hdfs_fsimage_mas...\",\n \"id\": \"hive_20181017070101_8dbe75b5-520a-4bfc-8157-1a2524dbe6b6-u_4172ca3e-0300-46fa-8c85-3485ae2e54ac\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"27s \",\n \"queryString\": \" insert into table unravel_hdfs_fsimage_temp_orc select unravel_hdfs_fsimage_master_orc.path, N, unravel_hdfs_fsimage_master_orc.deltimearray, temp_int(unravel_hdfs_fsimage_master_orc.repl,unravel_hdfs_fsimage.repl), temp_time(unravel_hdfs_fsimage_mas...\",\n \"id\": \"hive_20181016190101_aaf4b06f-6d06-45ad-a030-09444f2f31a1-u_87f76094-0914-4e4a-a3e1-08dae51e16bc\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n }\n ],\n \"Parameters\": {\n \"startDate\": 1539628200000,\n \"endDate\": 1539800999000,\n \"x\": 3\n },\n \"IO\": [\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"IO\": \"60.44 MB\",\n \"queryString\": \" insert into table unravel_hdfs_fsimage select path string, repl int, modification_time string, accesstime string, preferredblocksize int, blockcount double, filesize double, nsquota bigint, dsquota int, permission string, username string, groupname str...\",\n \"id\": \"hive_20181017070000_c5723682-302a-4e6a-a8c9-3189719d8e52-u_92a18a35-0ba5-48ea-b108-6f14e7909292\",\n \"memorySeconds\": \"0 B\",\n \"duration\": \"-\"\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"IO\": \"60.31 MB\",\n \"queryString\": \" insert into table unravel_hdfs_fsimage select path string, repl int, modification_time string, accesstime string, preferredblocksize int, blockcount double, filesize double, nsquota bigint, dsquota int, permission string, username string, groupname str...\",\n \"id\": \"hive_20181017060000_2e0f4bf6-a5bc-4fd4-a370-ce9d50b2579f-u_7ee9ab57-6c17-4e0e-8245-34e75304e4f5\",\n \"memorySeconds\": \"0 B\",\n \"duration\": \"-\"\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"IO\": \"60.20 MB\",\n \"queryString\": \" insert into table unravel_hdfs_fsimage select path string, repl int, modification_time string, accesstime string, preferredblocksize int, blockcount double, filesize double, nsquota bigint, dsquota int, permission string, username string, groupname str...\",\n \"id\": \"hive_20181017050000_7376b321-839f-4f98-9971-fbdc7330e54d-u_19bdf30c-ba36-4ab2-9e91-44e5557f982e\",\n \"memorySeconds\": \"0 B\",\n \"duration\": \"-\"\n }\n ],\n \"Duration\": [\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"43s \",\n \"queryString\": \" select Unravel, regexp_extract(path,^(\\\/[^\\\/]+\\\/[^\\\/]+\\\/[^\\\/]+)(\\\/[^\\\/]+),1) as dir, count(1) as num_files, cast(avg(filesize[size(filesize)-1].s) as bigint) as avg_size, cast(sum(filesize[size(filesize)-1].s) as bigint) as total_size, cast(min(filesize[si...\",\n \"id\": \"hive_20181015141111_bb6bc68d-da80-46df-9044-7dbaaa42faf0-u_ad1b1fa6-54e0-4a71-a145-7e1f78fad9ba\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"43s \",\n \"queryString\": \" select Unravel, regexp_extract(path,^(\\\/[^\\\/]+)(\\\/[^\\\/]+),1) as dir, count(1) as num_files, cast(avg(filesize[size(filesize)-1].s) as bigint) as avg_size, cast(sum(filesize[size(filesize)-1].s) as bigint) as total_size, cast(min(filesize[size(filesize)...\",\n \"id\": \"hive_20181016061212_2a9e2681-7172-4caf-9a4c-34b89035d8da-u_ba89a5fc-e037-4a9f-80c1-8cc5b7f5ec6f\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n },\n {\n \"status\": \"Success\",\n \"vcoreSeconds\": \"0 B\",\n \"numEvents\": 2,\n \"user\": \"hdfs\",\n \"duration\": \"43s \",\n \"queryString\": \" select Unravel, regexp_extract(path,^(\\\/[^\\\/]+\\\/[^\\\/]+\\\/[^\\\/]+\\\/[^\\\/]+)(\\\/[^\\\/]+),1) as dir, count(1) as num_files, cast(avg(filesize[size(filesize)-1].s) as bigint) as avg_size, cast(sum(filesize[size(filesize)-1].s) as bigint) as total_size, cast(min(files...\",\n \"id\": \"hive_20181017060808_ae661be6-1a68-4427-a295-3c2d9ab6b62a-u_af5a2fc4-b19a-416d-80fb-18fd0f8afa3d\",\n \"memorySeconds\": \"0 B\",\n \"IO\": null\n }\n ],\n \"Queues\": {\n \"root.users.souser1\": 1,\n \"root.users.root\": 387,\n \"root.users.hdfs\": 1240\n },\n \"Total\": {\n \"FailedHive\": 0,\n \"TotalHive\": 2706,\n \"SuccessfulHive\": 2525\n },\n \"Data\": {\n \"totalReadIo\": \"9.15 GB\",\n \"totalWriteIo\": \"1.77 GB\"\n },\n \"Resources\": {\n \"successful_reducers\": 2525,\n \"total_mappers\": 2770,\n \"total_reducers\": 2525,\n \"reduce_time\": \"2h 47m 34s \",\n \"successful_mappers\": 2770,\n \"map_time\": \"3h 24m \"\n }\n} Scheduling Reports Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule capacity forecasting You can not alter the Report Daily Notification Save Schedule Scheduled Reports " }, 
{ "title" : "Reports Archive", 
"url" : "unravel-4-4/user-guide/the-reports-page.html#UUID-6821fb64-0b7e-4fb7-a938-f364bb36e6c5_id_TheReportsPage-ReportsArchiveReportsArchive", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Reports Page \/ Reports Archive", 
"snippet" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to vie...", 
"body" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to view it. The report is displayed in a pop-up as if in the original reports window, with all functionality. Click " }, 
{ "title" : "Scheduled Reports", 
"url" : "unravel-4-4/user-guide/the-reports-page.html#UUID-6821fb64-0b7e-4fb7-a938-f364bb36e6c5_id_TheReportsPage-ScheduledReportsScheduledReports", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ The Reports Page \/ Scheduled Reports", 
"snippet" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule Cluster Optimization Capacity Forecasting Small Files More info ( Close...", 
"body" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule Cluster Optimization Capacity Forecasting Small Files More info ( Close " }, 
{ "title" : "Auto Actions", 
"url" : "unravel-4-4/user-guide/auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions", 
"snippet" : "Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Supported cluster metrics Running Auto Action Demos...", 
"body" : " Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Supported cluster metrics Running Auto Action Demos " }, 
{ "title" : "Auto Actions Overview", 
"url" : "unravel-4-4/user-guide/auto-actions/auto-actions-overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Auto Actions Overview", 
"snippet" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, such as resource contention or stuck jobs. Additionally, it can...", 
"body" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, such as resource contention or stuck jobs. Additionally, it can be set to automatically kill an application or move it to a different queue. The Unravel Server processes auto actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: : The conditions which cause a violation, e.g., the number of jobs running, memory used. Prerequisite conditions : Who\/what\/when can cause the violation, e.g., user, applications Defining conditions for Unravel Server to execute whenever it detects a rule violation. Actions Manage | Auto Actions The auto actions tab provides a quick way to view auto actions and quickly see their status, along with its defined actions and scope. The tab displays all defined auto actions separated into an Active Inactive define new auto actions Hovering over the auto action's name gives you the description which was entered when defining the auto action. Hovering over action or scope glyph brings up its detail. For example, for the active auto action above: rule description: , email action: an email is sent to only one (1) person, , and queue scope: is three queues: . The Actions Scope quicktest must Expert Rule quicktest in MR History of Runs By default all actions are off. Possible actions are: Send an Email ( ), Kill the App ( ), Move the app to another queue ( ), and Send a Http post ( ). By default the various scopes apply to all, i.e., all applications and constantly on. The scopes are: User ( ) Queue ( ), Cluster ( ), Application ( ), Time ( ), and Sustained Violation: This is not shown in the auto actions list. If you have not defined a particular action or scope, i.e., it's using the default, the glyph is grey ( The history of runs contains an entry for each time the auto action was triggered Click on a run's Link Operations Usage Details | Infrastructure Applications | Applications Notifications 'Snoozing' Auto Actions The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., the new violation is essentially noise. See Snooze Feature Property Definition Possible Value Default com.unraveldata.auto.action.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app or user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app Value is in millliseconds. 0: snooze is turned off. > 0: snooze is on, there is no no upper bound. 1 hour (3,600,000 seconds) When you change the snooze time period all applications currently snoozed are reset. Upon next violation the application is \"snoozed\" using new snooze value. To change the snooze time On Unravel Server open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart Auto Actions Property See here " }, 
{ "title" : "Creating Auto Actions", 
"url" : "unravel-4-4/user-guide/auto-actions/creating-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Creating Auto Actions", 
"snippet" : "Select Manage Auto Actions The tab displays all defined auto actions separated into an Active Inactive To create a new auto action, click either Create from Template Build Rule Expert Rule Create from Template Build Rule Expert Rule Using Create from Template or Build Rule to define you auto action....", 
"body" : " Select Manage Auto Actions The tab displays all defined auto actions separated into an Active Inactive To create a new auto action, click either Create from Template Build Rule Expert Rule Create from Template Build Rule Expert Rule Using Create from Template or Build Rule to define you auto action. Whether using Create from Template Build Rule The sections are: Template Name - Create from Template Build Rule Expert Rule Auto Action Name and Description Ruleset Options Actions Name and Description The name is mandatory; the description is optional. The name is used by the UI for all auto actions' displays; we recommend using a name that reflect the auto action's purpose. The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name the description is displayed. This example is from Create from Template Build Rule Ruleset At least one rule type ( User Queue, Apps Expert Rule metric type state Expert Rule Build Rule defines a rule \"metric\" \"comparision operator\" \"value\". Metric See Supported Cluster Metrics metrics The comparison value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type mapreduce, yarn, tez, spark, impala, workflow and hive The State new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: Or, And Same Or And work Same Logical Operator Same The Ruleset User Queue, Cluster App Expert Rule Add Queue metric type state Expert Rule must In the example below, Metric Type Queue metric comparison operato type state above , Apps , Same Or And Same Logical Operator Same Or And . Click Close trash This template has the Ruleset metric comparison type state Metric Type or State metric Same'd Same Logical Operator Options Define the scope ( User Queue Cluster Application Name Time Sustained Selected options default to All, Time Sustained Violation For Build Rule When using Create from Template Queue . Queue Cluster Application Name Check User Queue Cluster Application Name Only Except Only rule Except all but those specified Transform Application Name t Except Add Application except Create from Template All The Time start end click Sustained violation minimum maximum Minimum Maximum Actions Defines the actions to take when the auto action is triggered. Build Rule Create from Template, Send an email Http Post Post to Slack Move App to Queue Kill App Build Rule Expert Action. Expert Mode Auto Actions and Pagerduty You can chose one of more actions. Check the text box to chose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. For Send Email Add Recipient Include Owner For HTTP post Add URL Post to Slack https:\/\/slack.com\/api\/chat.postMessage public Slack channel, private channel, or direct message\/IM channel, It provides a better integration with Slack Service and allows you to send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: test tokens custom bot user token FIXLINK - generic bot icon, with generic \"bot\" username Slack App user token chat:write:bot Slack App bot user token FIXLINK - inherits Slack App's icon, with generic \"bot\" username Move app to queue Kill App. The Move App Kill App Kill App Move App Have directly caused the rule violation, and Have allocated resources, i.e. in allocated or running states. Move App Kill App The Build Rule Expert Rules Both the Email Http Post Cluster View auto action history Sample Email Auto Action policy \"ROGUE APPLICATION #1\" violation detected.\n\nPolicy description: \"Identify applications that are using too much of the cluster resources\"\n\nApplication \"application_1498514199803_2411\" has 1 violation:\n\n1. Sum of memory in MB allocated to containers is 1GB >= 1MB\n\nTimestamp: 07\/19\/2017 08:41:35 +0000\n\nReach Unravel server at http:\/\/localhost:3000\/\n\nSee cluster history at http:\/\/localhost:3000\/ops_dashboard\/charts\/resources?from=1499817549571&to=1499817669570&at=1499817609571&interval=1m Sample Slack Post To create an auto action, click eitherCreate from Template, Build Rule, or Expert Rule. 4a. Create From Template Click on Create a Template above Resource contention Resource contention in cluster Resource contention in queu Rogue Identification Rogue user Rogue application Rogue Impala query (HDFS Read\/Write) Long Running Jobs Long running YARN application Long running Hive query Long running workflow Long Impala query 4b. Build Rule Click on Build Rule. above 4c. Expert Rule The Auto Actions engine is capable of much more than is available through the templates. Expert Rule Expert Rule This flexibility and power makes this mode dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. In this mode you must specify prerequisite and defining conditions, and actions to execute when the auto action triggers. Prerequisite conditions Defining conditions Actions See the Expert Rule Sample Auto Actions Click Save Auto Action Your auto action is now listed in the Manage Auto Action " }, 
{ "title" : "Expert Rule", 
"url" : "unravel-4-4/user-guide/auto-actions/expert-rule.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Expert Rule", 
"snippet" : "Table of Contents Overview Defining the Rule Header Rules: Defining conditions Logical Operators for Evaluating Multiple Rules A Single Rule A Rule Array Options - Policy\/Scope: Prerequiste conditions Options - Policy\/Scope Rule Actions: action(s) to implement upon violation Single Action Multiple A...", 
"body" : "\n Table of Contents \n Overview \n Defining the Rule \n Header \n Rules: Defining conditions \n Logical Operators for Evaluating Multiple Rules \n A Single Rule \n A Rule Array \n Options - Policy\/Scope: Prerequiste conditions \n Options - Policy\/Scope Rule \n Actions: action(s) to implement upon violation \n Single Action \n Multiple Actions \n Send_email \n Actions can be Ignored When in Conflict \n Action(s) Fail if the Required Information is Invalid or not Specified. \n An Expert Rule Example \n Auto Actions Examples Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts and capabilities, along with JSON, which is used to define the auto action. \n This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule In this mode you must specify \n Prerequisite condition s \n Defining conditions \n Actions When using Expert Rule must rules actions {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} \n Header: \n Rules \n \n Options - \n Actions: Defining the Rule Header You must define a header. \n \n \n Attributes Name \n Definition \n Possible Value \n Required \n Default Value \n \n \n enabled \n Whether the auto action is active or not. True: active\/enabled. False: inactive\/disabled \n True | False \n √ \n - \n \n \n policy_name \n Value defined by Unravel. \n \n AutoActions2 \n √ \n \n AutoActions2 \n \n \n policy_id \n Value defined by Unravel. \n \n 10 \n √ \n \n 10 \n \n \n instance_id \n Any unique value \n √ \n - \n \n \n name_by_user \n Any unique string. The name is used when the auto action is displayed in the UI. \n √ \n - \n \n \n description_by_user \n Description of the auto action. \n - \n \n \n created_by \n Value defined by Unravel. \n \n admin \n √ \n \n admin \n \n \n last_edited_by \n Value defined by Unravel. \n \n admin \n √ \n \n admin \n \n \n created_at \n Time created. Date and time is in the form of a Epoch\/Unix timestamp. \n √ \n - \n \n \n updated_at \n Time updated. Date and time is in the form of a Epoch\/Unix timestamp. \n √ \n - \n \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: Defining conditions You must define at least one rule. \n \n \n Field Name \n Definition \n Possible Values \n Required\/ Required by \n Default Value \n \n \n scope \n The rule scope. \n app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container ,containers, multi_ containers queue, queues, multi_queue, user, users, multi_user \n Note \n √ \n - \n \n \n target \n Application name \n any valid application name \n when scope is by_name \n - \n \n \n metric \n Metric used for comparison. \n see supported metrics per type \n - \n \n \n comparison \n Comparison operator \n >, >=, ==, <=, < \n \n metric \n - \n \n \n value \n Value for comparison. The value form varies by metric. \n number \n \n metric \n - \n \n \n state \n Scope state \n new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * \n - \n \n \n type \n Job type \n mapreduce, yarn, tez, spark, workflow, hive \n - Logical Operators for Evaluating Multiple Rules \n \n \n Operator \n Condition for a Violation \n \n \n OR \n At least one rule evaluates to true. \n \n \n AND \n All rules evaluate to true. \n \n \n \n SAME \n All the rules evaluate to true and See Same Logical Operator A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n\n \/\/ at least one of the following\n\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \n \"state\":\"\",\n\n \"type\":\"\"\n }\n] \n \n Violation occurs when the application is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] \n Violation occurs when the state \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] \n Violation occurs when the state type \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n \n Note \n Multi_X X. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n \n OR Example When they are OR'ed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] \n AND Example When AND'ed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] \n SAME Example When SAME'd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the auto action is not triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However if My_App violates both rules (allocatedMB andallocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the auto action is triggered for My_App but not Your_App. \n OR, AND and SAME Given the same ruleset, evaluation becomes more restrictive. OR: the auto action is triggered if one or more of conditions is true. AND: the auto action is triggered if all of conditions are true. SAME: the auto action is triggered if all of the conditions are true within Options - Policy\/Scope:Prerequiste conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. \n \n \n Field Name \n Definition \n Conditions \n Required\/ Required by \n Possible Values \n Default Value \n \n \n X_mode where X \n The mode defines how the rules are applied to type X 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type X. 2 - the rules are evaluated for only X X 3 - the rules are evaluated for everything but X X \n By Definition. You must define at least one option\/policy. \n 0, 1, 2, 3, \n 0 \n \n \n X_list \n A list of X type \n Only applicable if mode is set to 2 (only) or 3 (except). \n if X X_ \n empty, single item or comma separated list. \n - \n \n \n X_transform \n A list of regex used to generate a list of X \n Only applicable if mode is set to 2 (only) or 3 (except). \n if X X \n empty, single regex or comma separated regex list \n - \n \n \n Time \n The daily time the Auto Action is trigger. \n any time period spanning less than 24 hours. \n - \n \n \n Sustained Violation \n Set a minimum or maximum time period for the auto action to be triggered. See here \n any time period less than 24 hours. \n - Options - Policy\/Scope Rule where X \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , \n \n Cluster \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", \n Queue \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", \n User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", \n Application Name except \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", \n User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action(s) to implement upon violation You do not have to define any actions, but it defeats to purpose not to. If no actions are defined, the UI keeps track of the auto action and when triggered, who triggers it. Both the prerequisite defining \n \n \n Field Name \n Definition \n Required\/ Required by \n Possible Value \n Default Value \n \n \n action \n The action to be taken. \n at least one \n send_email, http_post, post_in_slack, move_to_queue, kill_app \n - \n \n \n to \n Email recipients. \n send_mail if to_owner \n One or more recipients in a comma delimited list. \n - \n \n \n to_owner \n Send email to owner. \n send_mail if to \n false: do not send email true: send email \n false \n \n \n urls \n URLs for Http post. \n http_post \n One or more URLs in a comma delimited list. \n - \n \n \n token \n Token generated by slack \n post_in_slack \n Slack token \n - \n \n \n channels \n Slack channel. \n post_in_slack \n One or more channels in a comma delimited list \n - \n \n \n queue \n Queue name. \n move_to_queue \n The name of a valid queue to move the app to. \n - Single Action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple Actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] \n \n Example Actions There are five (5) main actions : send_email http_post post_in_slac , move_to_queue kill_app \n send_email, http_post, post_in_slack Creating Auto Actions send_email http_post post_in_slack Note You must take care when entering information. A specified action fails if you enter the incorrect information, i.e., bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template Build Rule to_owner \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] \n http_post Just like send_email \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] \n post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma delimited list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] \n \n move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] \n \n kill_app This is straight forward, but kill_app is a destructive action \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] Actions can be Ignored When in Conflict Below we specified two actions, move_to_queue kill_app kill_app move_to_queue actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n] Action(s) Fail if the Required Information is Invalid or not Specified. Below are two actions with invalid information. In send_mail http_post Operations Dashboard , history of runs cluster view \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] An Expert Rule Example This auto action triggers on applications using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the applications, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the application is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} Auto Actions Examples See Sample Auto Actions Build Rule " }, 
{ "title" : "Same Logical Operator", 
"url" : "unravel-4-4/user-guide/auto-actions/same-logical-operator.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Same Logical Operator", 
"snippet" : "SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition \"rules\":[ \"SAME\":[ { \"scope\":\"users\", \"metric\":\"appCount\", \"operator\":\">\", \"v...", 
"body" : " SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition \"rules\":[\n \"SAME\":[\n {\n \"scope\":\"users\",\n \"metric\":\"appCount\",\n \"operator\":\">\",\n \"value\":10,\n state\":\"running\"\n },\n {\n \"scope\":\"users\",\n \"metric\":\"appCount\",\n \"operator\":\">\",\n \"value\":5,\n \"state\":\"pending\"\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and 3 pending apps user B has 7 running and 1 pending apps user C has 21 running and 11 pending apps First, the two simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has 7 → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has 3 → FALSE User B has 1 → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called \"linking\" of rules (see Ruleset.java). Cluster \"User A\", link count = 1. User A > 10 running apps? → TRUE User A > 5 pending apps? → FALSE Cluster \"User B\", link count = 0. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Cluster \"User C\", link count = 2. User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule reset for the group. Cluster \"User A\" has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE FALSE User A > 5 pending apps? → FALSE Cluster \"User B\", link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster \"User C\", link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User Auto Action, therefore User C triggers the Auto Action and the alert is sent and\/or the actions performed. Comparison with AND Both User A and User C would have triggered the above rule were AND SAME any AND any To achieve the same result as the above example using AND SAME each and every user ( Username AND Username " }, 
{ "title" : "Snooze Feature", 
"url" : "unravel-4-4/user-guide/auto-actions/snooze-feature.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App ...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App An auto action specifying a Kill App Move App Snooze is set the first time the app violates the rule. The auto action itself continues to run uninterrupted whether zero (0) or all apps currently covered by the auto action are snoozed. The auto action takes action for any app not snoozing If an app is still violating upon awaking, snoozed Example Rule\/Action Two apps Snooze time at 20:00 A > 1GB → email is sent + snooze set (runs until 20:30). B < 1GB → app not violating so nothing is done. at 20:10 A > 1GB → snoozing B > 1GB → email is sent + snooze set (runs until 20:40). at 20:20 A > 1GB → snoozing B > 1GB → snoozing at 20:30 A > 1GB → application wakes B > 1GB → snoozing at 20:40 A > 1GB → snoozing, B < 1GB → app wakes To change the snooze time Property Definition Possible Value Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app Value is in milliseconds. 0: snooze is turned off > 0: no upper bound 1 hour (3,600,000 seconds) On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Sample Auto Actions", 
"url" : "unravel-4-4/user-guide/auto-actions/sample-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Sample Auto Actions", 
"snippet" : "Table of Contents 1. Sample JSON rules 1.1 Alert Examples Alert if Hive query duration > 10 minutes Alert if Tez query duration > 10 minutes Alert if any workflow's duration > 20 minutes Alert if workflow named \"foo\" and duration > 10 minutes Alert if workflow named \"foo\" and totalDfsBytesRead > 100...", 
"body" : "\n Table of Contents \n 1. Sample JSON rules \n 1.1 Alert Examples \n Alert if Hive query duration > 10 minutes \n Alert if Tez query duration > 10 minutes \n Alert if any workflow's duration > 20 minutes \n Alert if workflow named \"foo\" and duration > 10 minutes \n Alert if workflow named \"foo\" and totalDfsBytesRead > 100 MB and duration > 20 minutes \n Alert if Hive query in Queue \"foo\" and duration > 10 minutes \n 1.2 Kill App Example \n When workflow name is \"prod_ml_model\" and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != 'sla_queue' \n 2. Auto Actions Rules, Predefined Templates v Expert Mode \n 2.1 Map Reduce \n Alert on Map Reduce jobs using > 1 TB of memory. \n Alert on Map Reduce jobs using > 1000 vcores. \n Alert on Map Reduce jobs running more than 1 hour. \n Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. \n Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. \n 2.2 Spark \n Alert on only Spark jobs using > 1 TB of memory. \n Alert on only Spark jobs using > 1000 vcores. \n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or \"rogue\" queries. \n Alert if a Spark SQL has lots of output partitions. \n 2.3 Hive \n Alert if a Hive query duration is running longer than expected. \n a. Check if a Hive query started between 1 am and 3 am in queue 'prod' runs longer than > 20 minutes. \n b. Check if any Hive query is started between 1 am and 3 am in any queue except 'prod'. \n Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. \n a. Check if a Hive query writes out more than 100GB in total. \n b. Check if a Hive query reads in more than 100GB in total. \n Detect inefficient and \"stuck\" Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. \n 2.4 Tez \n Alert if a Tez query duration is running longer than expected. \n a. Check if a Tez query started between 1 am and 3 am in queue 'prod' runs longer than > 20 minutes. \n b. Check if any Tez query is started between 1 am and 3 am in any queue except 'prod'. \n Alert if a Tez query has extensive I\/O, wich may affect HDFS and other apps. \n a. Check if a Tez query writes out more than 100GB in total. \n b. Check if a Tez query reads in more than 100GB in total. \n Detect inefficient and \"stuck\" Tez queries, i.e., alert if a Tez query has not read lots of data but running for a longer time. \n 2.5 Workflow \n Alert if a workflow is taking longer than expected. \n a. Check if any workflow is running for longer than 5 hours. \n b. Check if a SLA bound workflow named 'market_report' is running for longer than 30 minutes. \n Alert if a SLA bound workflow is reading more data than expected. \n Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. \n 2.6 User \n Alert for Rogue User - Any user consuming a major portion of cluster resources. \n a. Check for any user where the allocated vcores aggregated over all their applications is > 1000. \n b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. \n 2.7 Queue \n Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. \n a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. \n b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. \n 2.7 Applications \n Alert for Rogue application - any application which is consuming a major portion of cluster resources. \n a. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue \n b. If any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. \n Related articles Information on Demo Auto Actions can be found here 1. Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode 1.1 Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named \"foo\" and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named \"foo\" and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue \"foo\" and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only \"foo\": 1.2Kill App Example When workflow name is \"prod_ml_model\" and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != 'sla_queue' In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} 2. Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. 2.1 Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the \"root.adhocd\" queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. 2.2 Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or \"rogue\" queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. 'outputToInputRowRatio' > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application 'outputPartitions' > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n 2.3 Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n \n Alert if SLA bound query is taking longer than expected. a. Check if a Hive query started between 1 am and 3 am in queue 'prod' runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Hive query is started between 1 am and 3 am in any queue except 'prod'. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and \"stuck\" Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n 2.4 Tez Alert if a Tez query duration is running longer than expected. Check if a Tez query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. a. Check if a Tez query started between 1 am and 3 am in queue 'prod' runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Tez query is started between 1 am and 3 am in any queue except 'prod'. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Tez query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Tez query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Tez query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and \"stuck\" Tez queries, i.e., alert if a Tez query has not read lots of data but running for a longer time. Check if any Tez query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n 2.5 Workflow Alert if a workflow is taking longer than expected. a. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} b. Check if a SLA bound workflow named 'market_report' is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named ''market_report'' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named 'prod_ml_model' and duration > 2h then kill jobs with allocated_vcores >= 20 and user != 'sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} 2.6 User Alert for Rogue User - Any user consuming a major portion of cluster resources. a. Check for any user where the allocated vcores aggregated over all their applications is > 1000. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} 2.7 Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} 2.7 Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Alert for Rogue application - any application which is consuming a major portion of cluster resources. a. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue You can use the Rogue Application or memory Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Set Time rule condition as: Set Move app rule as: b. If any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application or memory Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n}\n or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\":\"allocated_mb\",\n \"compare\": \">\",\n \"value\": [X]\n} Set Queue rule conditions as: Set Move app action as: Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Supported cluster metrics", 
"url" : "unravel-4-4/user-guide/auto-actions/supported-cluster-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Supported cluster metrics", 
"snippet" : "Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an ...", 
"body" : "Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on all live running Monitoring is only Hive\/Workflow Metrics Metric Definition duration total time taken by the application totalDfsBytesRead total hdfs bytes read totalDfsBytesWritten total hdfs bytes written stevev mun MapReduceApplication Master Metrics MapReduce Metrics Type Metric Definition elapsedAppTime time since the application was started Map mapsCompleted number of completed maps mapsPending number of maps still to be run mapsRunning number of running maps mapsTotal total number of maps Map Attempts failedMapAttempts number of failed map attempts killedMapAttempts number of killed map attempts newMapAttempts number of new map attempts runningMapAttempts number of running map attempts Reduce reducesCompleted number of completed reduces reducesPending number of reduces still to be run reducesRunning number of running reduces reducesTotal total number of reduces Reduce Attempts failedReduceAttempts number of failed reduce attempts killedReduceAttempts number of killed reduce attempts newReduceAttempts number of new reduce attempts runningReduceAttempts number of running reduce attempts successfulReduceAttempts number of successful reduce attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduceCounters File System Counters Metric Definitions fileBytesRead mount of data read from local file system fileBytesWritten amount of data written to local file system fileReadOps number of read operations from local file system fileLargeReadOps number of read operations of large files from local file system fileWriteOps number of write operations from local file system hdfsBytesRead amount of data read from HDFS hdfsBytesWritten amount of data written to HDFS hdfsReadOps number of read operations from HDFS hdfsLargeReadOps number of read operations of large files from HDFS hdfsWriteOps number of write operations to HDFS Job Counters Type Metric Definition Map dataLocalMaps number of map tasks which were launched on the nodes containing required data mbMillisMaps total megabyte-seconds taken by all map tasks millisMaps total time spent by all map tasks slotsMillisMaps total time spent by all executing maps in occupied slots vcoresMillisMaps total vcore-seconds taken by all map tasks Reduce mbMillisReduces total megabyte-seconds taken by all reduce tasks millisReduces total time spent by all reduce tasks slotsMillisReduces total time spent by all executing reduces in occupied slots totalLaunchedReduces total number of launched reduce tasks vcoresMillisReduces total vcore-seconds taken by all reduce tasks File Input\/Output Format Counters Metric Definition bytesRead amount of data read by every tasks for every filesystem bytesWritten amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Map-Reduce Framework Counters Type Metric Definition Map failedShuffle total number of mappers which failed to undergo through shuffle phase mapInputRecords total number of records processed by all of the mappers mapOutputBytes total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes amount of (compressed) data which was actually written to disk mapOutputRecords total number of records produced by by all of the mappers mergedMapOutputs total number of mapper output files undergone through shuffle phase shuffledMaps total number of mappers which undergone through shuffle phase Reduce reduceInputGroups total number of unique keys reduceInputRecords total number of records processed by all reducers reduceOutputRecords total number of records produced by all reducers reduceShuffleBytes amount of data processed in shuffle and reduce phase Records combineInputRecords total number of records processed by combiners combineOutputRecords total number of records produced by combiners spilledRecords total number of map and reduce records that were spilled to disk Time gcTimeMillis wall time spent in Java Garbage Collection cpuMilliseconds cumulative CPU time for all tasks Memory committedHeapBytes total amount of memory available for JVM physicalMemoryBytes total physical memory used by all tasks including spilled data splitRawBytes amount of data consumed for metadata representation during splits virtualMemoryBytes total virtual memory used by all tasks Shuffle Errors Metric Definition badId total number of errors related with the interpretations of IDs from shuffle headers connection total number of established network connections ioError total number of errors related with reading and writing intermediate data wrongLength total number of errors related to compression and decompression of intermediate data wrongMap total number of errors related to duplication of the mapper output data wrongReduce total number of errors related to the attempts of shuffling data for wrong reducer Spark Metrics In addition to the metric set supported by MapReduce applications, Spark applications can be polled on: Join joinInputRowCount the total input rows of the first join of the SQL query, aggregated for all the queries that are part of the application totalJoinInputRowCount total number of input rows count for all join operators of all SQL queries that are part of the application totalJoinOutputRowCount total number of output rows count for all join operators of all SQL queries that are part of the application joinOutputRowCount the total output rows of the first join of the SQL query, aggregated for all the queries that are part of the application Partitions inputPartitions total number of input partitions for all SQL queries that are part of the application outputPartitions total number of output partitions for all SQL queries that are part of the application Records inputRecords cumulative number of input records for all SQL queries that are part of the application (collected at stage level) outputRecords cumulative number of output records for all SQL queries that are part of the application (collected at stage level) outputToInputRecordsRatio outputRecords \/ inputRecords if inputRecords > 0, else 0 YARN Resource Manager metrics allocatedMB The sum of memory in MB allocated to the application's running containers allocatedVCores The sum of virtual cores allocated to the application's running containers appCount total number of applications elapsedTime The elapsed time since the application started (in ms) runningContainers The number of containers currently running for the application memorySeconds The amount of memory the application has allocated (megabyte-seconds) vcoreSeconds The amount of CPU resources the application has allocated (virtual core-seconds) For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "unravel-4-4/user-guide/auto-actions/running-auto-action-demos.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED 1...", 
"body" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". \n HIGHLIGHTED 1. Unpack and Install the Auto Action Demos 1.1 Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory. \n DEMO_PATH auto-actions-demos # ls auto-actions-demos\ndemos\/ setup\/ 1.2. Go to DEMO_PATH\/setup directory. Open .\/settings 1.3 Execute the .\/setup-all script. # .\/setup-all The auto action rules that include time specification are automatically adjusted to the current time period, e.g., from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. 1.4 After running the script go the the Unravel Server UI and select Admin->Manage->Auto Actions tab. You should see all the Auto-Actions demos listed under Active Auto Actions 2. Executing the demos 2.1. Go to DEMO_PATH\/demos directory. For each auto actions rules listed in Admin->Manage->Auto Cctions there is a corresponding script in the demo's directory. Each script will trigger the corresponding auto action demo. Some auto actions demo scripts trigger multiple auto actios. This side effect can happen when running your own defined auto actions due to autos action having overlapping definitions. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ 2.2. Execute \".\/demo-tag\" script to trigger the corresponding \"AA-tag\" rule. Each script is designed to simulate violation conditions for the corresponding Auto Action on the target Hadoop cluster, i.e., to trigger AA-Spark-1c you run the demo-Spark-1c script. Some auto action's demo scripts trigger multiple auto actions. This side effect can happen when running your own defined auto actions due to auto actions having overlapping definitions. 3. Cleaning up demos 3.1 Go to DEMO_PATH\/setup directory. 3.1 Run .\/clean-all script. # .\/clean-all This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 Auto Actions demos list \n \n \n \n Application type \n \n Use case \n \n Auto Action \n Triggering Script \n \n Notes \n \n \n MapReduce \n Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. \n Alert if any MapReduce job allocated memory > 20GB. \n AA-MR-1a Demo-MR-1a \n Submits to \"root.sla\" queue. \n \n Alert if any MapReduce job allocated vcores > 10. \n AA-MR-1b Demo-MR-1b \n Submits to \"root.sla\" queue. \n \n Alert if any MapReduce job is running for longer than 10 minutes. \n AA-MR-1c Demo-MR-1c \n Submits to \"root.sla\" queue. May trigger MR-1b. \n \n Alert if a MapReduce job may affect any production SLA jobs running on a cluster. \n Alert if any application not in the queue 'sla_queue' and running between X and Y and allocated memory > 20GB. \n AA-MR-2a Demo-MR-2a \n Will also trigger MR-1a as well. \n \n Alert if any application not in the queue 'sla_queue' and running between X and Y and allocated vcores greater than 10. \n AA-MR-2b Demo-MR-2b \n Will also trigger MR-2a as well. \n \n Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. \n Alert if any MapReduce job allocated vcores > 10 between X and Y in queue 'root.adhoc'. \n AA-MR-3a Demo-MR-3a \n Submits to \"root.adhoc\" queue. Will also trigger MR-1a and MR-2a. \n \n Alert if any MapReduce job allocated memory > 20GB between X and Y in queue 'root.adhoc'. \n AA-MR-3b Demo-MR-3b \n Submits to \"root.adhoc\" queue. Will also trigger MR-1b and MR-2b. \n \n \n Spark \n Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. \n Alert if any Spark application has allocated more than 20GB of memory. \n AA-Spark-1a Demo-Spark-1a \n \n Alert if any Spark application allocated vcores > 8. \n AA-Spark-1b Demo-Spark-1b \n \n Alert if any Spark application is running longer than 10 minutes \n AA-Spark-1c Demo-Spark-1c \n \n Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or \"rogue\" queries. \n Alert if any Spark application is generating lots of rows in comparison with input,i.e. 'outputToInputRowRatio' > 1000. \n TBD \n \n \n Hive \n Alert if a Hive query duration is running longer than expected. \n Alert if a Hive query duration > 5 minutes. \n AA-Hive-1a Demo-Hive-1a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if SLA bound query is taking longer than expected. \n Alert if a Hive query started between A:00 and B:00 in queue 'root.prod' and duration > 10 minutes. \n AA-Hive-2a Demo-Hive-2a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if any Hive query is started between A:00 and B:00 in any queue except 'root.prod'. \n AA-Hive-2b Demo-Hive-2b \n Very short query. \n \n Alert if a Hive query is writing lots of data. \n Alert if a Hive query writes out more than 200MB in total. \n AA-Hive-3a Demo-Hive-3a \n \n Alert if a Hive query reads in more than 10GB in total. \n AA-Hive-3b Demo-Hive-3b \n \n Detect inefficient and \"stuck\" Hive queries. \n Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. \n AA-Hive-4a Demo-Hive-4a \n \n Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. \n AA-Hive-4b Demo-Hive-4b \n \n \n Workflow \n Alert if a workflow is taking longer than expected. \n Alert if any workflow is running for longer than 10 minutes, might be stuck. \n AA-WF-1a Demo-WF-1a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if a SLA bound workflow named 'market_report' is running for longer than 5 minutes. \n AA-WF-1b Demo-WF-1b \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if a workflow is reading more data than expected. \n Related articles Page: Running Auto Action Demos Page: Auto Actions Overview Page: Snooze Feature Page: Sample Auto Actions Page: Setting Up Email for Auto Actions and Collaboration " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-4/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-4/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-4/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Kafka Insights", 
"url" : "unravel-4-4/user-guide/use-cases/kafka-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, \n OK \n Lagging \n Stalled A Topic's status is set to the lowest status among it's Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Example 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Note: Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric offset, test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI's. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "unravel-4-4/user-guide/use-cases/optimizing-the-performance-of-spark-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web...", 
"body" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Event List", 
"url" : "unravel-4-4/user-guide/event-list.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Event List", 
"snippet" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception. HIVE Application failure HIveFailureIncorrectHe...", 
"body" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception. HIVE Application failure HIveFailureIncorrectHeaderEvent Indicates that the Hive query fails with \"incorrect header check\" exception. HIVE Application failure HiveFailureReturnCodeEvent Indicates that the Hive query fails and shows return code. HIVE Application failure HiveMapJoinMemoryExhaustionEvent Indicates that the Hive query fails because it is out of memory in map join, and recommends turning off mapjoin. HIVE Application failure HiveOutOfMemoryErrorEvent Indicates that the Hive query fails because it is out of memory. HIVE Informational HiveKillFailEvent Indicates that the Hive query is killed or failed with lots of wasted work. HIVE Informational HiveSuccWithKillFailEvent Indicates that the Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. HIVE Informational HiveShuffleBytesEvent Indicates that the Hive query has lots of data shuffle from map to reduce side. HIVE Informational HiveTimeBreakdownEvent Identifies where time is spent on for the query and points out significant events, including MR-level skew events. HIVE Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true. HIVE Speedup HiveSingleReduceCountDistinct Indicates that the Hive query has a job with a long single reducer because the query has \"count distinct\". HIVE Speedup HiveSingleReduceLongWait Indicates that the Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side. HIVE Speedup HiveSingleReduceOrderBy Indicates that the Hive query has a job with a long single reducer because the query has \"order by\". HIVE Speedup HiveTooFewReduceEvent Indicates that the Hive query is using too few reducers. HIVE Speedup\/ Resource Utilization HiveTooLargeMapEvent Indicates that mappers in the Hive query are requesting too much memory. HIVE Speedup\/ Resource Utilization HiveTooLargeReduceEvent Indicates that reducers in the Hive query are requesting too much memory. HIVE Speedup\/ Resource Utilization HiveTooManyMapEvent Indicates that the Hive query is using too many mappers. HIVE Speedup\/ Resource Utilization HiveTooManyReduceEvent Indicates that the Hive query is using too many reducers. IMPALA Application failure ImpalaFailureEvent Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible). IMPALA Informational\/ Speedup\/ Resource Utilization ImpalaTimeBreakdownEvent Displays the longest phase in the Impala query. If the longest phase is query execution then it displays the longest operator in the Impala query. If applicable, this event shows insights as to why this operator took the most time, and makes tuning suggestions. (See also ImpalaSlowOperatorEvent). IMPALA Speedup ImpalaNonColumnarTablesEvent Indicates that a table is in non-columnar format and a scan on this table represents significant percentage of the total query execution time. IMPALA Speedup ImpalaNonPrunedPartitionsEvent Indicates that a scan accessed all partitions of a table (i.e. no partition pruning occurred). IMPALA Speedup ImpalaSlowScanOperatorEvent Indicates that a scan operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if statistics for the scanned table are missing or stale, this insight recommends refreshing the statistics.. IMPALA Speedup ImpalaSlowJoinOperatorEvent Indicates that a join operator took significant amount of time to execute and shows details such as the size of the left and right hand side inputs, the join algorithm and type, and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if a broadcast join was used but a partitioned (shuffle) join would be faster, this insight recommends using \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join instead. IMPALA Speedup ImpalaSlowAggregateOperatorEvent Indicates that an aggregate operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. IMPALA Speedup ImpalaSlowSortOperatorEvent Indicates that a sort operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. IMPALA Speedup ImpalaSlowExchangeOperatorEvent Indicates that an exchange operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. IMPALA Speedup ImpalaTablesMissingStatsEvent Indicates that the Impala query accessed tables with missing optimizer statistics and recommends collecting these statistics. IMPALA Speedup ImpalaTimeSkewEvent Detects whether one of the operator instances took much longer than the other instances and indicates the bottleneck node. If the skewness is significant enough, this event indicates whether the skewness is correlated with data skew (i.e. the bottleneck node had to process much larger volumes of data) or because of slow disk I\/O on the bottleneck node. IMPALA Speedup ImpalaUnderestimatedCounfOfRowsEvent Indicates that an estimate is outdated and that statistics for the corresponding table should be refreshed. MR Application Failure MRClassNotFoundEvent Indicates that MR job fails due to a \"class not found\" exception. MR Application Failure MRFailureCompressLibNotAvailable Indicates that MR job fails due to a \"compression library not available\" exception. MR Application Failure MRFailureFileNotFoundEvent Indicates that MR job fails due to a \"file not found\" exception. MR Application Failure MRFailureIllegalArgumentEvent Indicates that MR job fails due to an \"illegal argument\" exception. MR Application Failure MRFailureNumberFormatEvent Indicates that MR job fails due to a \"number format\" exception. MR Application Failure MRFailureTimeOutEvent Indicates that MR job fails due to a \"time out\" exception . MR Application Failure MRGcOverheadLimitExceededMapEvent Indicates that the MR job fails because the GC overhead limit is exceeded on map side. MR Application Failure MRGcOverheadLimitExceededReduceEvent Indicates that the MR job fails because the GC overhead limit is exceeded on reduce side. MR Application Failure MRJavaOutOfMemoryMapEvent Indicates that MR job fails because it is out of memory on map side. MR Application Failure MRJavaOutOfMemoryReduceEvent Indicates that MR job fails because it is out of memory on reduce side. MR Informational MRKillFailEvent Indicates that the MR job is killed or failed with lots of wasted work. MR Informational MRSuccWithKillFailEvent Indicates that the MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. MR Informational MRShuffleBytesEvent Indicates that the MR job has lots of data shuffle from map to reduce side. MR Informational MRTimeBreakdownEvent Identifies where time is spent on for the job and points out significant events. MR Speedup MRLongTasksFromSlowNodeEvent Indicates that the MR job has long-running map\/reduce tasks from slow nodes. MR Speedup MRMapSkewDataIOEvent Indicates that the map phase of the MR job has a time skew with strong correlation with IO. MR Speedup MRReduceSkewDataIOEvent Indicates that the reduce phase of the MR job has a time skew with strong correlation with IO. MR Speedup MRTooFewReduceEvent Indicates that the MR job is using too few reducers. MR Speedup\/ Resource Utilization MRTooLargeMapEvent Indicates that mappers in the MR job are requesting too much memory. MR Speedup\/ Resource Utilization MRTooLargeReduceEvent Indicates that reducers in the MR job are requesting too much memory. MR Speedup\/ Resource Utilization MRTooManyMapEvent Indicates that the MR job is using too many mappers. MR Speedup\/ Resource Utilization MRTooManyReduceEvent Indicates that the MR job is using too many reducers. SPARK Application Failure DriverOomeEvent Indicates that a driver failed with \"OutOfMemory\" error. SPARK Application Failure ExecutorOomeEvent Indicates that an executor failed with \"OutOfMemory\" error. SPARK Application Failure YarnContainerKilledEvent Indicates that there are containers killed by YARN. SPARK Resource Utilization ContainerSizingUnderutilizationEvent Indicates that container resources are underutilized. SPARK Resource Utilization InefficientInputSplitSizeEvent Indicates an inefficient input split size. SPARK Resource Utilization TooFewPartitionsEvent Indicates that there are too few partitions with respect to available parallelism. SPARK Resource Utilization UnderutilizedCpuEvent Indicates that there is low utilization of CPU resources. SPARK Resource Utilization UnderutilizedNodeMemoryEvent Indicates that there is low utilization of memory resources. SPARK Resource Utilization UnderutilizedStorageMemoryEvent Indicates that the Spark storage memory has low utilization. More RDDs can be cached in memory. SPARK Speedup CachingOpportunityEvent Indicates that there is an (unused) opportunity for RDD caching. SPARK Speedup ContendedCpuEvent Indicates that there is contention for CPU resources. SPARK Speedup ExcessiveGcEvent Indicates that there is high garbage collection overhead. SPARK Speedup ExecutorImbalanceEvent Indicates that there is load imbalance among executors. SPARK Speedup ExhaustedStorageEvent Indicates that the Spark storage memory is getting exhausted. SPARK Speedup LightExecutorEvent Indicates that there is large idle time for one or several executors. SPARK Speedup LongStageEvent Indicates that there is load imbalance among tasks for the longest stage of the application. SPARK Speedup ContendedDriverEvent Indicates that the driver is a bottleneck in the application. It monitors data transfers to the driver (i.e., the time spent in fetching the data from executors). TEZ Application Failure TezOutOfMemoryErrorEvent Indicates that the TEZ query ran out of memory. TEZ Resource Utilization\/ Inefficiency TezNoDAGEvent Indicates that the TEZ session was created and TEZ DAG was not submitted. TEZ Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true. TEZ Speedup TezNoColStatsEvent Indicates that the Hive query accessed tables with missing column statistics and recommends collecting these statistics. TEZ Speedup\/ Resource Utilization TezMapVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few map tasks. TEZ Speedup\/ Resource Utilization TezMapVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many map tasks. TEZ Speedup\/ Resource Utilization TezReduceVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few reducer tasks. TEZ Speedup\/ Resource Utilization TezReduceVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many reducer tasks. Note: In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook. WORKFLOW Informational WorkflowTimeBreakdownEvent Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. WORKFLOW Informational\/ Application Failure WorkflowGeneralFailureEvent For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. WORKFLOW Informational\/ Resource Utilization WorkflowIRSummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources. WORKFLOW Informational\/ SLA analysis WorkflowDurationAnomalousEvent If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. WORKFLOW Informational\/ Speedup WorkflowIASummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-4/user-guide/resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory avail...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory available for launching new processes avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations committedHeap bytes Committed heap size committedNonHeap bytes Committed non-heap size committedVirtualMemory bytes The committed virtual memory in the operating system currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems edenPeakUsage bytes Maximum memory usage in the eden space freePhysicalMemory bytes The free physical memory in the operating system freeSwap bytes The free swap size fullGcCount COUNT Number of full GC runs fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms initHeap bytes Initial heap size initNonHeap bytes Initial non-heap size maxHeap bytes Maximum heap size maxNonHeap bytes Maximum non-heap size minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC oldPeakUsage bytes Maximum memory usage in the old space processCpuLoad PERCENT Average process CPU load for the last minute (all cores) snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started survivorPeakUsage bytes Maximum memory usage in the survivor space systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) totalPhysicalMemory bytes The total physical memory in the operating system totalSwap bytes The total swap size usedHeap bytes Used heap size usedNonHeap bytes Used non-heap size vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "unravel-4-4/user-guide/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Some Keywords and Error Messages", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type....", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. " }, 
{ "title" : "Spark Keywords", 
"url" : "unravel-4-4/user-guide/some-keywords-and-error-messages.html#UUID-a3575368-e465-85a4-5180-bbed44fd7eb9_id_SomeKeywordsandErrorMessages-SparkKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Some Keywords and Error Messages \/ Spark Keywords", 
"snippet" : "Spark Key Term Explanation Deploy mode Specifies where the driver runs. In \"cluster\" mode the driver runs on the cluster. In \"client\" mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application ...", 
"body" : " Spark Key Term Explanation Deploy mode Specifies where the driver runs. In \"cluster\" mode the driver runs on the cluster. In \"client\" mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application on a worker node Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset spark.default.parallelism Default number of partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.executor.memory Related to executor memory spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.sql.shuffle.partitions Number of SparkSQL partitions spark.yarn.executor.memoryOverhead YARN memory overhead SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SparkConf Spark configuration object SQLContext Main Spark SQL entry point StreamingContext Main Spark Streaming entry point " }, 
{ "title" : "Spark Error Messages", 
"url" : "unravel-4-4/user-guide/some-keywords-and-error-messages.html#UUID-a3575368-e465-85a4-5180-bbed44fd7eb9_id_SomeKeywordsandErrorMessages-SparkErrorMessages", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Some Keywords and Error Messages \/ Spark Error Messages", 
"snippet" : "Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer....", 
"body" : " Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. " }, 
{ "title" : "MapReduce\/Hive Keywords", 
"url" : "unravel-4-4/user-guide/some-keywords-and-error-messages.html#UUID-a3575368-e465-85a4-5180-bbed44fd7eb9_id_SomeKeywordsandErrorMessages-MapReduceHiveKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ User Guide \/ Some Keywords and Error Messages \/ MapReduce\/Hive Keywords", 
"snippet" : "Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record bo...", 
"body" : " Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-4/advanced-topics.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics", 
"snippet" : "Backing-up, Disaster Recovery and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom ...", 
"body" : " Backing-up, Disaster Recovery and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Enabling SAML Authentication for Unravel Web UI Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Connectivity Hive Metastore Access Connecting to a Hive Metastore Hive Metastore Configuration Connecting to\/Configuration of a Kafka Stream Creating Application Tags Roles and Role Based Access Control Role Based Access Control (RBAC) Supported Roles Running Verification Scripts and Benchmarks Unravel APIs REST API Sign In Application APIs Data APIs Operational APIs Reports APIs Use Case - Auto Actions and Pagerduty Unravel Monitoring Service Configuration Disk Monitoring JMX Client Monitors REST API Unravel Properties Unravel Servers and Sensors Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Uninstalling Unravel Server Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Workflows Tagging Workflows Tagging a Hive on Tez Query Monitoring Oozie Workflows Monitoring Airflow Workflows How to Write Jolokia JMX MBean Configure JVM Sensor HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-4/advanced-topics/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster Wide Report", 
"url" : "unravel-4-4/advanced-topics/cluster-wide-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's curren...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of these recommendations revolve around these parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. Step-by-step guide Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/usr\/local\/unravel\/install_bin\/ClusterReport cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz to cd dbin Input.txt # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report # su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "unravel-4-4/advanced-topics/configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations", 
"snippet" : "Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retent...", 
"body" : " Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Enabling SAML Authentication for Unravel Web UI Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace " }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Se...", 
"body" : " Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source " }, 
{ "title" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/configure-permission-for-unravel-daemons-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"snippet" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user access Purpose hdfs:\/\/user\/un...", 
"body" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt command login as admin user psql cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/configuring-multiple-hosts-for-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ HIGHLIGHTED UNRAVEL_HOST_IP Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db is identical on all Unravel hosts in the ensemble. \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig Stop Unravel Server. On each Unravel host, run this command: # sudo \/etc\/init.d\/unravel_all.sh stop Modify unravel.properties Pick a machine to be host1 If the bundled db is in use \/usr\/local\/unravel\/etc\/unravel.properties host1 must be a fully qualified DNS or IP address. Replace UNRAVEL_HOST_IP 3316 unravel_mysql_prod To find your fully qualified hostname # {hostname} -I unravel.jdbc.url=jdbc:mysql:\/\/ UNRAVEL_HOST_IP Copy host1's unravel.properties Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh \/etc\/unravel_ctl host1 host2 host3 host1 # scp \/usr\/local\/unravel\/etc\/unravel.properties {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/usr\/local\/unravel\/etc\/unravel.ext.sh {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/etc\/unravel_ctl {host2}:\/etc\/ Verify that the ownership of unravel.properties unravel.properties unravel:unravel \/etc\/unravel_ctl root:root. The scripts invoked below will make an identical change to the unravel.properties Assign roles. Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh {host1} {host2}\n on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh {host1} {host2} For a 3-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh {host1} {host2} {host3}\n on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh {host1} {host2} {host3} on host3 #sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh {host1} {host2} {host3} These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id.properties Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n on host2 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host3 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-zk \n on host2 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host3 # sudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15 seconds for Zookeeper quorum to settle: # sleep 15 Start up Kafka ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-k \n on host2 # sudo \/etc\/init.d\/unravel_all.sh start-k on host3 # sudo \/etc\/init.d\/unravel_all.sh start-k Wait 10 seconds for Kafka coordination: # sleep 10 Create the Kafka topics (only on one host): on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh\n Start Unravel Server Finish multi-host installation by starting up Unravel Server: on host1 # sudo \/etc\/init.d\/unravel_all.sh start \necho \"http:\/\/{UNRAVEL_HOST_IP}:3000\/\" \n on host2 # sudo \/etc\/init.d\/unravel_all.sh start Edit Hive-site snippet for Hive-Hook. The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 Snapshot unravel.properties " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # sudo chkconfig ...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 \n# sudo chkconfig --add unravel_ma_2 20000-30000 jobs per day # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3\n# sudo chkconfig --add unravel_ma_3 Greater than than 30000 jobs per day # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4\n# sudo chkconfig --add unravel_ma_4 Start Unravel Server Start the workers' daemons. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Defining a Custom Web UI Port", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/defining-a-custom-web-ui-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom Web UI Port", 
"snippet" : "These instructions apply to any platform deployment. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh 18080 export NGUI_PORT=18080 Note that port numbers under 1024 are restricted to root setuid After making this change, restart the affected daemon: # sudo \/etc\/init.d\/unravel_ngui restart...", 
"body" : " These instructions apply to any platform deployment. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh 18080 export NGUI_PORT=18080 Note that port numbers under 1024 are restricted to root setuid After making this change, restart the affected daemon: # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Email Alerts", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/email-alerts.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Email Alerts", 
"snippet" : "Configure the following property to set up email alerts. See here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.domain Default email domain used for email aler...", 
"body" : "Configure the following property to set up email alerts. See here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.domain Default email domain used for email alerts. Note: This is required for Auto Action email alerts. - com.unraveldata.login.admins Comma separated list of email recipients. A Recipient can be defined by complete email address or by email local-part, e.g., admin,support@unraveldata.com When no email domain is specified then default domain is used. admin mail.smtp2.from Used for email \"from\" and \"reply-to\" headers when at least one email recipient has unraveldata.com unravel.noreply@unraveldata.com mail.smtp.from Used for email \"from\" and \"reply-to\" headers when no email recipienthas unraveldata.com Note: This is required for Auto Action email alerts unravel.noreply@unraveldata.com " }, 
{ "title" : "HBASE Configuration", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/hbase-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ HBASE Configuration", 
"snippet" : "Configuration Please see here Edit \/usr\/local\/unravel\/etc\/unravel.propertiesand define the following properties. If the property is not found, add it, be sure to substitute your local values for the bracketed text red AMBARI and CDH You must here com.unraveldata.hbase.source.type=hbase_source_type c...", 
"body" : "Configuration Please see here Edit \/usr\/local\/unravel\/etc\/unravel.propertiesand define the following properties. If the property is not found, add it, be sure to substitute your local values for the bracketed text red AMBARI and CDH You must here com.unraveldata.hbase.source.type=hbase_source_type com.unraveldata.hbase.rest.url=Ambari_or_Cloudera_base_urlcom.unraveldata.hbase.rest.user=hbase_rest_usernamecom.unraveldata.hbase.rest.pwd=hbase_rest_password com.unraveldata.hbase.clusters=comma_separated_cluster_names You can optionally here com.unraveldata.hbase.rest.ssl.enabled=boolean com.unraveldata.hbase.master.port=Port# com.unraveldata.hbase.regionserver.port=Port# com.unraveldata.hbase.metric.poll.interval=Seconds com.unraveldata.hbase.http.conn.timeout=Seconds com.unraveldata.hbase.http.read.timeout=Seconds com.unraveldata.hbase.http.poll.parallelism=Number com.unraveldata.hbase.alert.average.threshold=Number Restart the HBase service unravel_us_1. # sudo service unravel_us_1 restart Verify unravel_us_1is running. # sudo service unravel_us_1 status unravel_us_1 is running Verify themetrics are collected in Elasticsearch. A separate index file is created for each week with an alias hb-search,e.g., hb-20180813_19. Troubleshooting Ifunravel_us_1is not running check the logs for any errors. \/usr\/local\/unravel\/logs\/unravel_us_1.out \/usr\/local\/unravel\/logs\/unravel_us_1.log If hb-* index is not created or no data in Elasticsearch, verify the following daemons are running. unravel_us_1 unravel_s_1 unravel_hl unravel_k_1 Resources Service name unravel_us_1 Service logs \/usr\/local\/unravel\/logs\/unravel_us_1.log, \/usr\/local\/unravel\/logs\/unravel_us_1.out Configuration file \/usr\/local\/unravel\/etc\/unravel.properties Elasticsearch: Template File \/usr\/local\/unravel\/etc\/template_hbase_metrics.json Index Name hb-* " }, 
{ "title" : "Run Unravel Daemons with Custom User", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/run-unravel-daemons-with-custom-user.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Run Unravel Daemons with Custom User", 
"snippet" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to...", 
"body" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to Switch User Run the following command to switch running Unravel daemons to user {USER} {GROUP} # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh {USER} {GROUP} Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\" foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\" hdfs hadoop Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh \/etc\/unravel_ctl RUN_AS USE_GROUP HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL env vars are removed from \/usr\/local\/unravel\/etc\/unravel.ext.sh \/usr\/local\/unravel\/ \/srv\/unravel\/* RUN_AS: \/srv\/unravel\/tmp_hdfs\/ logs in \/srv\/unravel\/log_hdfs \/usr\/local\/unravel\/logs \/srv\/unravel\/log_hdfs the umask of the run-as the chmod bits of \/usr\/local\/unravel \/srv\/unravel " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.recent.maxSize.weeks com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.recent.maxSize.weeks com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space; you can store approximately 1000 jobs per 1GB of disk. Open \/usr\/local\/unravel\/etc\/unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Search for and set the following properties. If not found, add the properties. com.unraveldata.retention.max.days com.unraveldata.recent.maxSize.weeks com.unraveldata.history.maxSize.weeks com.unraveldata.retention.max.days If recent activity in Unravel gives you too many results consider decreasing com.unraveldata.recent.maxSize.weeks com.unraveldata.history.maxSize.weeks These two properties are closely related and the following condition must be true: 1 < com.unraveldata.recent.maxSize.weeks com.unraveldata.history.maxSize.week In other words: (com.unraveldata.recent.maxSize.weeks >= 2) AND (com.unraveldata.recent.maxSize.weeks < com.unraveldata.history.maxSize.weeks) After changing any of the properties, restart unravel_td # sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Spark Properties for Spark Worker daemon @ Unravel", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/spark-properties-for-spark-worker-daemon---unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel", 
"snippet" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an ap...", 
"body" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:,,.lz4,.snappy,.inprogress (This makes sure that uncompressed, lz4 and snappy compressed and inprogress event log files are processed) com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. 1000 Executor log processing Property Definition Default com.unraveldata.job.collector.log.aggregation.base Base path to look for aggregated executor logs Note { yarn.nodemanager.remote-app-log-dir}\/*\/{ yarn.nodemanager.remote-app-log-dir-suffix \/tmp\/logs\/*\/logs\/ \"*\" is replaced by the user running the application com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) 60000 (10 mins) com.unraveldata.min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds) 6000 (1 min) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs) Valid Options yarn Tagging Property Definition Default com.unraveldata.tagging.enabled Enables tagging functionality. True com.unraveldata.tagging.script.enabled Enables tagging. False com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g. , \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_ke aws_secret_access_key access_key\/secretKey - EMR\/HDInsight specific properties Property Definition Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False True The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel properties' values in Microsoft's Azure Block storage specific properties (for HDInsight) For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property Definition Default com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Data Lake (ADL) specific data properties Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster \"spk21utj02\" has been given access to \"\/clusters\/spk21utj02\" on Data Lake store. retrieve from Microsoft Azure " }, 
{ "title" : "Using the Impala Daemon (impalad) as a Data Source", 
"url" : "unravel-4-4/advanced-topics/configurations/custom-configurations/using-the-impala-daemon--impalad--as-a-data-source.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Using the Impala Daemon (impalad) as a Data Source", 
"snippet" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics...", 
"body" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html CM as the data source Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM On Unravel Server, add the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port For example: com.unraveldata.data.source=impalad com.unraveldata.impalad.nodes=IP:port,IP:port,IP:port References https:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Security Configurations", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabl...", 
"body" : " Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Enabling SAML Authentication for Unravel Web UI " }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui res...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui restart " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/adding-ssl-and-tls-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure ...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure cookies are NOT supported when using this Apache2 reverse-proxy method, see instead Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4 and support listening on port 443. Install needed packages: # sudo yum install httpd mod_ssl Note: There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf unravelhost_FQDN SSLCertificate* <VirtualHost *:80>\n ServerName unravelhost_FQDN\n Redirect permanent \/ https:\/\/unravelhost_FQDN\/\n<\/VirtualHost>\n\n<VirtualHost *:443>\n\n DocumentRoot \/var\/www\/html\n ServerName unravelhost_FQDN\n # use this if http to https errors #RequestHeader set X-FORWARDED-PROTO 'https'\n\n SSLEngine on\n SSLCertificateFile \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt\n SSLCertificateKeyFile \/etc\/certs\/wildcard_unravelhost_RSA_private.key\n SSLCertificateChainFile \/etc\/certs\/IntermediateCA.crt\n\n # set this off for reverse proxy security\n ProxyRequests Off\n # might be helpful in logs\n ProxyPreserveHost On\n ProxyPass \/ http:\/\/localhost:3000\/ connectiontimeout=180 timeout=180\n ProxyPassReverse \/ http:\/\/localhost:3000\/\n\n <Location \/>\n Order deny,allow\n Deny from all\n Allow from all\n <\/Location>\n\n<\/VirtualHost>\n Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties :port com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_tc daemon: # sudo service unravel_ngui restart Start the httpd daemon: # sudo service httpd start Visit https:\/\/unravelhost_FQDN Troubleshooting To enable verbose logging in Apache2, add these lines: LogLevel debug\n where LogLevel debug trace1 trace8 Note: Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ add this line: RequestHeader set X-FORWARDED-PROTO 'https'\n after ServerName line in the virt. host httpd config for Unravel. Then restart apache2. " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/alternate-kerberos-principal-for-cluster-access-on-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in part 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in part 2 of the installation guide. HIGHLIGHTED 1. Check HDFS Default umask For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see sub-directories and read files. In Cloudera Manager check the value of dfs.umaskmode fs.permissions.umask-mode 2. Enable ACL Inheritance In Cloudera Manager, HDFS Configuration, search for \"namenode advanced configuration snippet\", and set dfs.namenode.posix.acl.inheritance.enabled https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 3. Restart Cluster When you are ready, restart the cluster to effect a change in dfs.namenode.posix.acl.inheritance.enabled 4. Change ACL of Target HDFS Directories Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented): Set ACL for future directories. The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse Please make sure you set the permissions at the \/user\/history \/user\/history \/user\/history\/done Set ACL for existing directories # hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse 5. Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target user ('unravel' or 'hdfs' or 'mapr' or custom), with a valid kerberos ticket corresponding to keytab principal (substitute your values for KEYTAB_FILE PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n " }, 
{ "title" : "Configure HDFS Permission for Unravel on CDH Sentry Secured Cluster", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/configure-hdfs-permission-for-unravel-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure HDFS Permission for Unravel on CDH Sentry Secured Cluster", 
"snippet" : "The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_...", 
"body" : " The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH please read the Cloudera Documentation here " }, 
{ "title" : "Configure Hive Metastore Permissions", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/configure-hive-metastore-permissions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Hive Metastore Permissions", 
"snippet" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metast...", 
"body" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt command login as admin user psql cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE Metastore Access Information See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Disabling Browser Telemetry", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/disabling-browser-telemetry.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Browser Telemetry", 
"snippet" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/et...", 
"body" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.do.not.track false com.unraveldata.do.not.track=false Restart the Unravel UI. # sudo service unravel_ngui restart Re-Enable Mixpanel To enable Mixpanel follow the above steps but in step 2 set com.unraveldata.do.not.track true " }, 
{ "title" : "Disabling Support\/Comments Panel", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/disabling-support-comments-panel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Support\/Comments Panel", 
"snippet" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel...", 
"body" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled false com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. # sudo service unravel_ngui restart Your title bar should be missing the support button like below. Show\/Re-enable Support Button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled true unravel.properties Pop-up Support Box " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/enabling-tls-to-unravel-web-ui-directly.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. See also Defining a Custom Web UI Port HIGHLIGHTED On Un...", 
"body" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. See also Defining a Custom Web UI Port HIGHLIGHTED On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Update or add the following properties. For example, to enable ssl #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true\n#PATH TO CERT FILE\ncom.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate\n#PATH TO KEY FILE\ncom.unraveldata.ngui.ssl.key.file=\/etc\/certs\/wildcard_unravelhost_RSA_private.key\n#OPTIONAL - COMMA SEPARATED LIST OF CA FILES \ncom.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt\n#OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE\ncom.unraveldata.ngui.ssl.passphrase=test Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/{unravel.example.com}:3000 Restart Unravel web UI: # sudo service unravel_ngui restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/encrypting-passwords-in-unravel-properties-and-settings.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh: # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you ...", 
"body" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh: # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter (Return key), it will emit something like: ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/integrating-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "You configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web U If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then follow the steps below. In Unravel server 4.3.1.2, the prope...", 
"body" : "You configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web U If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then follow the steps below. In Unravel server 4.3.1.2, the property com.unraveldata.ldap.use_jndi If you used com.unraveldata.ldap.search_bind_authentication If you set the bind_dn bind_pw You must replace all text noted in red For instance if your domain was thisLocalDomain hive.server2.authentication.ldap.Domain={MYDOMAIN}becomes hive.server2.authentication.ldap.Domain =thisLocalDomain. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties For { MYDOMAIN} If SSL is in use: For ldap:\/\/LDAP_HOST ldaps:\/\/LDAP_HOST A truststore unravel\/jre\/ unravel\/etc\/unravel.ext.sh You can append a port number, if needed; for example, ldap:\/\/ldap_host:9999 Knowing your precise DN is essential for some configuration examples below. If you are uncertain about what a normal DN is ldapsearch If you use AD, the Windows domain can be found using ldapsearch userPrincipalName How To Debug: see the property com.unraveldata.ldap.verbose Active Directory AD domain This is the simplest to implement and enables the widest access to Unravel server. The web UI looks at the login name entered, if the login has no domain name appended, i.e.,thisUser@MYDOMAIN, then the @MYDOMAIN is appended before binding to LDAP. if the name contains the domain name, it assumes the user entered the domain explicitly. If it is possible to bind to the AD (LDAP) server with name@MYDOMAIN then the login is approved. Note: The login name can appear in AD or be seen in an ldapsearch userPrincipalName For { MYDOMAIN} com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.Domain={MYDOMAIN} An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern AD with base DN defined This example works by composing a DN from the entered user account name in the web UI. For a user login \"foo\" the settings below would make a DN \"uid=foo,ou=myou,dc=domain,dc=com\" and try to bind\/authenticate with that. com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldaps:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myou,dc=domain,dc=com\nhive.server2.authentication.ldap.guidKey=uid An optional hive.server2.authentication.ldap.userFilter can be combined with this to narrow who can access Unravel. See example 3 below. Change LDAP_HOST, ou=myou,dc=domain,dc=com to appropriate value for your installation. For guidKey, the default username attribute in windows AD is uid but could also be sAMAccountName . AD with user pattern Here we match users with one or more colon-separated patterns, combined with an optional inclusion filter (userFilter). This is helpful when you want to allow users from multiple OUs or base DNs. The optional inclusion filter is a secondary check on the list of users allowed to access Unravel server. In this example, if the user enters name@MYDOMAIN then the LDAP bind uses that directly, otherwise the string entered is used to compose a DN by replacing the %s seen below with the string that the user entered. The string entered in the login form can have spaces embedded and is not normally case sensitive. Multiple userDNPatterns can be defined by using a colon ':' separator. The userFilter property is optional to further narrow down who can login to Unravel. We recommend that you first make sure your userDNPattern is working before you add this filter. Notice that spaces are significant, they are part of the name. When this property is specified, a user must get past the userDNPattern first, and then the simple login name must be also present in this list. com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.userDNPattern=CN=%s,CN=Users,DC=domain,DC=com\nhive.server2.authentication.ldap.userFilter=user1,user2,user with space,user3\n Change LDAP_HOST, CN=%s,CN=Users,DC=domain,DC=com to appropriate value for your installation, based on DN for the users to whom you want to give access. The identifier before the =%s can also be uid instead of CN in some cases. AD with group pattern Here we match users with one or more patterns, and verify group membership in order to approve login. In this example, if the user enters name@MYDOMAIN then the LDAP bind uses that directly, otherwise the string entered is used to compose a DN by replacing the %s seen below with the string that the user entered and the result is used for the bind. The string entered in the login form can have spaces embedded and is not normally case sensitive. Multiple groupDNPatterns can be defined by using a colon ':' separator. The template text below for groupDNPattern, would correspond to a group DN like if the groupFilter clause contained mygroup memberOf=CN=mygroup,OU=myou,DC=subdomain,DC=domain,DC=com Spaces (' ') are significant in the Unravel web login form. Use values relevant to your installation for {LDAP_HOST},{%s}, {myou}, {subdomain}, and {com} {%s} uid {UNRAVEL_GROUP} com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.groupDNPattern=CN={%s},OU={myou},DC={subdomain},DC={domain},DC={com}\nhive.server2.authentication.ldap.groupFilter=<UNRAVEL_GROUP>\nhive.server2.authentication.ldap.groupMembershipKey=member\nhive.server2.authentication.ldap.groupClassKey=group\n For Open LDAP LDAP Example 1 Use values relevant to your installation for {LDAP_HOST},{myunit}, {example}, and {com}. uid An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.baseDN=ou={myunit},dc={example},dc={com}\nhive.server2.authentication.ldap.guidKey={uid} LDAP example 2 Below we use a typical DN to be uid=%s,ou=myunit,dc=example,dc=com %s cn uid Multiple userDNPatterns can be defined by using a colon ':' separator. An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Optional Master Bind Account When user accounts cannot do an LDAP search, you can add the properties below to specify a search account. This can be combined with any of the examples above. com.unraveldata.ldap.bind_dn=CN=unravel,ou=myunit,dc=example,dc=com\ncom.unraveldata.ldap.bind_pw=bigsecret The password can be encrypted Web UI Login Syntax In most cases, people want to enter a simple login account name in the Web UI login page of Unravel server and the instructions here reflect that. However, if name@windowsDomain is entered for values that are valid for your site, that will be used instead of appending a suffix in the AD example 1 case above. The name field of the Web UI allows spaces to be entered. That way a name like \"john smith\" can be entered and matched as a CN in a DN. 2. Restart unravel_ngui Restart unravel_ngui # sudo \/etc\/init.d\/unravel_ngui restart Advanced Properties and Details Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The Unravel login process is described next. Property names are shortened for readability, but full names should be used. The SIMPLE LDAP authentication mechanism is used. Authentication Process For Unravel server 4.3.1.2 or later If a bind_dn and bind_pw are set, then authentication starts out by binding with the given account if the bind works, then the verbose log will show \"Connected using bindDN\" an LDAP search is for the user attempting login by matching inetOrgPerson AND either sAMAccountName OR guidKey attribute matches the simple login name. If guidKey is not set, then \"uid\" is default. if an entry is found, the userPassword is fetched and compared to the entered password. On success, \"pw is a match\" is logged when verbose. This method only works with un-hashed passwords which are often not available. if password for the LDAP server is unavailable or the compare fails, then another bind is attempted with a DN composed by the entered name in the login Web UI and the baseDN (or each userDNPattern supplied) using the guidKey if the bind or password matches for candidate login user succeeds, then login is approved pending further qualifications for authorization in next part. The bind made with the bind_dn will be used during authorization for searches. if Windows domain is set, bind as username + at sign + Windows domain, using the given password. verbose log will show Connecting Connected using principal bind with a DN composed by the entered name and the baseDN (or each userDNPattern supplied) using the guidKey is attempted if the bind succeeds, \"Connected using DN\" is logged when verbose. If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds Authorization Steps These extra authorization steps are optional to further narrow who can access Unravel server. The searches will be done using a bindDN account, if specified, otherwise the user account bind is used. If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful if there are no results from the custom query, then login fails each search result will be matched with the simple name entered in the Web UI login user or group filters will be ignored if custom query is used if a user filter is specified, it is matched with the simple name entered in the Web UI login no additional LDAP search is done in this case this allows access to Unravel to be controlled with an explicit list in Unravel. If a group pattern or filter is specified, it is checked a query is made to find the groups to which a user belongs the user membership list is scanned and if one of the groups is in the specified list of allowed groups, then this authorization step succeeds. verbose log will show results list and the match arguments in effect by logging \"Checking group\" Property Description Example Value hive.server2.authentication.ldap.baseDN OR com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey OR com.unraveldata.ldap.user_name_attr LDAP attribute name whose values are unique in this LDAP server. Default is \"uid\"; not used when custom query is specified. uid or CN or sAMAccountName hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). hiveuser,impalauser,hiveadmin,hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member, uniqueMember or memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group, groupOfNames or groupOfUniqueNames hive.server2.authentication.ldap.url OR com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. ldap:\/\/host ldaps:\/\/host ldap:\/\/host:9999 ldaps:\/\/host1:9999 ldaps:\/\/host2:9999 com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_ngui.log Can be true or false or not set; default is false " }, 
{ "title" : "Kafka Security", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/kafka-security.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Kafka Security", 
"snippet" : "You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace...", 
"body" : "You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For Single Kafka clients 1. Create a file named consumerConfig.properties. Add the following properties and copy\/move the file to \/usr\/local\/unravel\/etc. You can locate youSSL + Kerberos configuration ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Kerberos configuration sasl.mechanism = GSSAPI security.protocol = SASL_PLAINTEXT sasl.kerberos.service.name= kafka sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; 2. Copy\/move consumerConfig.properties to \/usr\/local\/unravel\/etc. 3. Edit \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. You should find com.unraveldata.ext.kafka.clusters=ClusterName Add the following property using the ClusterName above: com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties 4. Restart the kafka monitor daemon unravel_km. # service unravel_km restart For Multiple Kafka clients Each cluster must have a separate consumerConfig.properties 1. Open \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. The property will be defined with a comma separated list. If there is only one cluster name see above com.unraveldata.ext.kafka.clusters=ClusterName1, ClusterName 2. Create a file named consumerConfigClusterName.properties for each cluster. Add the following properties to each file ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; 3. Copy\/move each file to \/usr\/local\/unravel\/etc. 4. Edit \/usr\/local\/unravel\/unravel.properties. For each cluster add the following property: com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties 5. Restart the kafka monitor daemon unravel_km. # service unravel_km restart KafkaAuthorizations Unravel consumes message to topic __consumer_offsets UnravelOffsetConsumer SentryAuthorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe HOST=*->TOPIC=__consumer_offsets→action=read HOST=*->TOPIC=__consumer_offsets→action=write HOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization Kafka with Ranger Authorization The following privilege must be granted using Ranger for the topic __consumer_offsets Publish Consume Describe For further details, see Security - Create a Kafka Polic References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "Restricting Direct Access to Unravel UI", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/restricting-direct-access-to-unravel-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Restricting Direct Access to Unravel UI", 
"snippet" : "On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME #SET NGUI_HOSTNAME export NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart...", 
"body" : " On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME #SET NGUI_HOSTNAME \nexport NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/using-a-private-certificate-authority-with-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with \"sudo \/etc\/init.d\/unravel_all.sh restart\" after making the change. HIGHLIGHTED \/path\/to\/jks_keystore Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts \/usr\/local\/unravel\/etc\/unravel.ext.sh bin\/java \/usr\/java\/jdkl1.8 For example: export JAVA_HOME {\/usr\/java\/jdk1.8} Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts jssecacerts # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for mycompanyca something.cer # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias {mycompanyca} -file {something.cer}\n# sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts " }, 
{ "title" : "Enabling SAML Authentication for Unravel Web UI", 
"url" : "unravel-4-4/advanced-topics/configurations/security-configurations/enabling-saml-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling SAML Authentication for Unravel Web UI", 
"snippet" : "SAML authentication is available In Unravel from v4.4.2 and on. To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/u...", 
"body" : "SAML authentication is available In Unravel from v4.4.2 and on. To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. \n \n \n \n com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json Edit saml.config.json file \n \n \n Property \n Description \n Req \n Example Values \n \n entryPoint \n Identity provider entrypoint, Ping IdP address (SSO URL) Note: Identity provider entrypoint is required to be spec-compliant when the request is signed. \n Yes \n \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SSOService.php\" \n \n issuer \n Name of app that will connect to the saml server. Issuer string to supply to identity provider(Environment name). Should match the name configured in Idp. \n Yes \n \"unravel-myHost\" \n \n cert \n IDP's public cert to validate auth response signature Note: You retrieve this from saml host. \n Yes \n Idp Cert String \n \n logoutUrl \n Base address to call with logout requests Default:entryPoint \n No \n \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\" \n \n logoutEnabled \n If true logs you out from every app. \n No \n false \n \n unravel_mapping \n Mapping saml auth response attributes to Unravel attributes. \n Yes \n { \"username\":\"userid\", \"groups\":\"ds_groups\" } \n \n privateCert \n Unravel private cert string to sign Auth requests. \n No \n Unravel cert string Click here to see a saml.json example. { \"entryPoint\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\", \"issuer\":\"localhost\", \"logoutUrl\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\", \/\/ generate by saml host \"cert\":\"MIIDXTCCAkWgAwIBAgIJALmVVuDWu4NYMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEwHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdAcQf2CGAaVfwTTfSlzNLsF2lW\/ly7yapFzlYSJLGoVE+OHEu8g5SlNACUEfkXw+5Eghh+KzlIN7R6Q7r2ixWNFBC\/jWf7NKUfJyX8qIG5md1YUeT6GBW9Bm2\/1\/RiO24JTaYlfLdKK9TYb8sG5B+OLab2DImG99CJ25RkAcSobWNF5zD0O6lgOo3cEdB\/ksCq3hmtlC\/DlLZ\/D8CJ+7VuZnS1rR2naQ==\", \"privateCert\":\"-----BEGIN PRIVATE \/\/ generated by unravel node KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDEt4Ma2k4DUkoW\\nG9QDHUBnY7S\/iS\/+u2BjPZqUG2JktzYZl30J05zA6i642i2VDn8eUIPHqt2Hw249\\nZ3nHKL4YnBVqa3yTfEkdMB\/6GSAkoCbnufaD3IsGcFJnlW5raDiT\/GZMy+1WnDfJ\\npB0\/.......vD8kRkcmEi9t3KLmKVy3SO15\/YHAhLxP9oTnTFGkPnIqZLRM0Y55UfwbRSZDlgH\/\\ny9GGmsV5IaIwhepuALJMdkHp\\n-----END PRIVATE KEY-----\\n\", \"unravel_mapping\": { \"username\":\"userid\", \"groups\":\"ds_groups\" } } For Ping, the IdP certificate can be obtained as follows: In the Server Configuration Certificate Management and Digital Signing & XML Decryption Keys & Certificates. Click Export Select Certificate Only Next Click Export Configure SAML Server Configure the following properties on the SAML server. Replace UNRAVEL_HOST_IP \n \n \n Property \n Description \n Req \n PingFederate Specific configuration \n \n AssertionConsumerService \/ ACS Url \n http(s):\/\/ UNRAVEL_HOST \n Yes \n \n https:\/\/docs.pingidentity.com\/bundle\/p1_enterpriseEditAnApplication_cas\/page\/p1_t_EditASAMLApplication.html \n https:\/\/documentation.pingidentity.com\/pingfederate\/pf84\/#concept_settingAssertionConsumerServiceUrlsSaml.html#concept_settingAssertionConsumerServiceUrlsSaml \n \n Entity Identifier \n unravel-Congo24 \n Yes \n Should be same as the issuer in saml.json \n \n Single Logout Endpoint \n http:\/\/ UNRAVEL_HOST \n Yes \n \n https:\/\/documentation.pingidentity.com\/pingfederate\/pf84\/index.shtml#task_specifyingSloServiceUrlsSaml2_0.html#task_specifyingSloServiceUrlsSaml2_0 \n \n Single Logout Response Endpoint \n http:\/\/ UNRAVEL_HOST \n No " }, 
{ "title" : "Autoscaling HDInsight Spark Cluster using Unravel API", 
"url" : "unravel-4-4/advanced-topics/configurations/autoscaling-hdinsight-spark-cluster-using-unravel-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Autoscaling HDInsight Spark Cluster using Unravel API", 
"snippet" : "Install requests using pip # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click Link to see installation instruction After install Azure CLI 1.0 Run the following command to login # azure login Once you login to Azure you should see existing HDInsight...", 
"body" : " Install requests using pip # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click Link to see installation instruction After install Azure CLI 1.0 Run the following command to login # azure login Once you login to Azure you should see existing HDInsight clusters using this command # azure hdinsight cluster list Download the customizable script from https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/unravel-autoscaling\/unravel_HDInsight_autoscaling.py Open unravel_HDInsight_autoscaling.py Property Notes Example Value unravel_base_url http:\/\/localhost:3000\/ memory_threshold Scale up\/down when memory_usage higher\/lower 80% 80 cpu_threshold Scale up when cpu_usage higher\/lower 10% 10 min_nodes Minimum worker nodes 4 max_nodes Maximum worker nodes can scale up to 10 resource_group UNRAVEL01 cluster_name estspk2rh75 Run the auto scaling script # python unravel_HDInsight_autoscaling.py Below is a screenshot ( Operations Dashboard " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-4/advanced-topics/configurations/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "Define HOST Variable for Unravel Server as an FQDN. (Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST Define REALM variable. (Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM Create the Active Directory (AD) kerberos principals and keytabs. Use the two variables you defined above to replace...", 
"body" : " Define HOST Variable for Unravel Server as an FQDN. (Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST Define REALM variable. (Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM Create the Active Directory (AD) kerberos principals and keytabs. Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires UN Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass \\\n-out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass \\\n-out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "unravel-4-4/advanced-topics/configurations/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configurations \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring: Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdent...", 
"body" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring: Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier FreeStorageSpace Next In Alarm Threshold - for this Database Metrics (e.g. RDS_FreeStorageSpace_for_MySQL-A) Name - describe what the above database metrics name you entered (e.g. Disk space monitor of RDS MySQL-A) Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 You can tune Whenever FreeStorageSpace Add 10 for 10 consecutive period(s) Under Actions Send notifications to This SNS topic should already be setup before you add it. Click Create Alarm The UI displays the alarm you just created. If you see an \"INSUFFICIENT DATA\" message, you can ignore it because the alarm hasn't been triggered yet. Once the alarm is triggered, the UI displays ALARM Alarms Click Create Alarm " }, 
{ "title" : "Connectivity", 
"url" : "unravel-4-4/advanced-topics/connectivity.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Connectivity", 
"snippet" : "Hive Metastore Access Connecting to\/Configuration of a Kafka Stream...", 
"body" : " Hive Metastore Access Connecting to\/Configuration of a Kafka Stream " }, 
{ "title" : "Hive Metastore Access", 
"url" : "unravel-4-4/advanced-topics/connectivity/hive-metastore-access.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access", 
"snippet" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore Details Step 2: Configuring Unravel to Access Hive Metastore...", 
"body" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore Details Step 2: Configuring Unravel to Access Hive Metastore " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-4/advanced-topics/connectivity/hive-metastore-access/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from th...", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API. From CDH version 5.5 onward, use the REST API http:\/\/ CMGR_HOSTNAME_IP Look at the response body, a JSON-like text format as in the image below. Search the response body for metastore Edit \/usr\/local\/unravel\/etc\/unravel.properties javax.jdo.option.ConnectionURL=hive_metastore_database_host ;javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver \njavax.jdo.option.ConnectionPassword=hive_metastore_database_password ; \njavax.jdo.option.ConnectionUserName=hive_metastore_database_user ; Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in Applications | Applications For HDP and MapR Please talk to your cluster administrator. " }, 
{ "title" : "Hive Metastore Configuration", 
"url" : "unravel-4-4/advanced-topics/connectivity/hive-metastore-access/hive-metastore-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration", 
"snippet" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver jars. If you are using MySql as the Hive Metastore DB, you have two options: Use mariadb driver. Download the MySql jdbc connector jar from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/m...", 
"body" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver jars. If you are using MySql as the Hive Metastore DB, you have two options: Use mariadb driver. Download the MySql jdbc connector jar from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/mybatis\/. All parameters are defined in \/usr\/local\/unravel\/etc\/unravel.properties. Hive Metastore Access You must configure the following properties for the Data Insights tab to populate its information correctly. Property Definition Example Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. postgresql org.postgresql.Driver - mariadb org.mariadb.jdbc.Driver MySql org.mysql.jdbc.Driver javax.jdo.option.ConnectionPassword Password used to access the data store. OIhwSFl - javax.jdo.option.ConnectionUserName Username used to access the data store. hive - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT postgresql jdbc:postgresql:\/\/congo21.unraveldata.com:7432\/hive - JDBC Configurations You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false False com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. 5 Reference [1] c3p0 project page: http:\/\/www.mchange.com\/projects\/c3p0 " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "unravel-4-4/advanced-topics/connectivity/connecting-to-configuration-of-a-kafka-stream.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Connectivity \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka stream and add some properties to unravel.properties Change the configuration of the Kafka stream. Export an available port for JMX_PORT export JMX_PORT=<port_num> The default JMX port for kafka in CD...", 
"body" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka stream and add some properties to unravel.properties Change the configuration of the Kafka stream. Export an available port for JMX_PORT export JMX_PORT=<port_num> The default JMX port for kafka in CDH is 9393 In HDP you would export this parameter under Advanced kafka-env kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS kafka-run-class.sh -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Djava.rmi.server.hostname=127.0.0.1\n-Djava.net.preferIPv4Stack=true Not required for HDP Verify the configuration changes on the Kafka stream. Restart the Kafka broker. Configure Unravel Server to monitor the Kafka stream. The unravel daemon, unravel_km, Kafka Monitoring Properties In \/usr\/local\/unravel\/etc\/unravel.properties variables com.unraveldata.ext.kafka.clusters\ncom.unraveldata.ext.kafka. ClusterID ClusterID ClusterID JMX_ServerID ClusterID JMX_ServerID Example: com.unraveldata.ext.kafka.clusters=c1,c2\ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093 com.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193\ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2\ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010 c\nom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Creating Application Tags", 
"url" : "unravel-4-4/advanced-topics/creating-application-tags.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Creating Application Tags", 
"snippet" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. W...", 
"body" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. What is a Tag in Unravel? A tag is key-value pair <k,v> A Thus, application A <k1,v1> <k2,v2> How Does Unravel Use Tags? Unravel Server and UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights What Types of Tags Are There? There are two types of tags: Unravel tags and user-created tags. Unravel Tags All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp User-Created Tags You can create tags based on your use cases. " }, 
{ "title" : "How Do I Create Tags?", 
"url" : "unravel-4-4/advanced-topics/creating-application-tags.html#UUID-bbab6dd6-bf5a-2d82-19ab-6586038a76c1_id_CreatingApplicationTags-HowDoICreateTags", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Creating Application Tags \/ How Do I Create Tags?", 
"snippet" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,.....", 
"body" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,... Injecting Tags Through a Python Script You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches a consistent state.For example, Spark receives data from Resource Manager, event log file, YARN aggregated logs, and sensors. Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. " }, 
{ "title" : "Precedence of Tags", 
"url" : "unravel-4-4/advanced-topics/creating-application-tags.html#UUID-bbab6dd6-bf5a-2d82-19ab-6586038a76c1_id_CreatingApplicationTags-PrecedenceofTags", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Creating Application Tags \/ Precedence of Tags", 
"snippet" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script...", 
"body" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script " }, 
{ "title" : "Sample Use Cases", 
"url" : "unravel-4-4/advanced-topics/creating-application-tags.html#UUID-bbab6dd6-bf5a-2d82-19ab-6586038a76c1_id_CreatingApplicationTags-SampleUseCases", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Creating Application Tags \/ Sample Use Cases", 
"snippet" : "Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants...", 
"body" : " Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants " }, 
{ "title" : "Roles and Role Based Access Control", 
"url" : "unravel-4-4/advanced-topics/roles-and-role-based-access-control.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Roles and Role Based Access Control", 
"snippet" : "Role Based Access Control (RBAC) Supported Roles...", 
"body" : " Role Based Access Control (RBAC) Supported Roles " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "unravel-4-4/advanced-topics/roles-and-role-based-access-control/role-based-access-control--rbac-.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Role Based Access Control (RBAC)", 
"snippet" : "Role Based Access Control allows Admin to limit what a specific end-user can access based upon tags\/defaults. See the Role Matrix When RBAC mode is enabled a end-users' access to the Unravel UI is filtered by tags. Two different modes are available: Extended Mode Application | Applications Operation...", 
"body" : "Role Based Access Control allows Admin to limit what a specific end-user can access based upon tags\/defaults. See the Role Matrix When RBAC mode is enabled a end-users' access to the Unravel UI is filtered by tags. Two different modes are available: Extended Mode \n Application | Applications \n Operations | Usage Details | Infrastructure \n Operations | Usage Details | Impala Usage \n Reports | Operational Insights | Chargeback Restricted mode \n Application | Applications You can always toggle the status of RBAC via the UIX; however the unravel property com.unraveldata.login.mode ldap LDAP Supported Roles 1. Go to Manage | Role Manager for access to the Role Manager. The RBAC default is set via com.unraveldata.rbac.enabled. 2. Once RBAC is activated you can add filters for specific end-users. Any end-user roles you have previously set are displayed. If the Unravel daemon was restarted after you added end-user roles, those entries are lost. You can add end-users one at at time via Add New Role csv login.mode ldap Select role file Add New Role 3. Adding Roles You limit end-user access through tags. In the example below only two tags are available, project tenant department csv The tags used here must also be loaded as Application Tagging Workflows Add role directly Clicking on Add New Role Add one or more roles via a role file Click on the Select role file csv csv \n first row is a header row defining the columns tags user, tagKey[,tagKey]* \n \n tagKey \n one or more rows defining user and tag values user, tagValue[,tagValue]* \n \n tagValue tagType tagString \n \n tagString tagValue \n * Note: The file must user After you add your last tagValue .csv \n tagValue No special characters or spaces are allowed in file. The . csv user,project,tenant\nuser72,\"group1,group2\",mm\nuser25,,\"3n,3m\"\nuserNew,groupNew\nuser33,\"group3,group2\",\"3m,mm\"\n 4. Editing or Deleting Roles To edit a role, click the edit glyph ( To delete a role, click the delete glyph. Effect of Role Access Control End-user's Access with RBAC turned off The user has access to all the Unravel UI features and all applications. End-user's Access with RBAC turned on. " }, 
{ "title" : "Supported Roles", 
"url" : "unravel-4-4/advanced-topics/roles-and-role-based-access-control/supported-roles.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Supported Roles", 
"snippet" : "Unravel supports two roles: Admin End-user Admin Admins can access all Unravel's pages and features in \"read-write\" mode. End-user End-users have \"read-only\" access to Unravel pages and features. Using Role Based Access Control (RBAC) Applications Page applications containing specific tags RBAC If R...", 
"body" : "Unravel supports two roles: Admin End-user Admin Admins can access all Unravel's pages and features in \"read-write\" mode. End-user End-users have \"read-only\" access to Unravel pages and features. Using Role Based Access Control (RBAC) Applications Page applications containing specific tags RBAC If RBAC is: Off On Tags defined (user specific or default) Mode Unravel UIX Access Matrix By Role RBAC Available Pages Admin read\/write read-only End-user (read-only) Off All Unravel's Pages ✓ ✓ On All Unravel's Pages ✓ Specific tags defined for user? No Yes Extended Mode below Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage Reports | Operational Insights | Chargeback N\/A Pages filtered by default value. See below Pages filtered by specific tags. See Adding Role s Restricted Mode below Application | Applications N\/A Pages filtered by default value. See below Pages filtered by specific tags. See Adding Role s Role Based Access Control General Properties Property Definition Default com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off false com.unraveldata.login.admins.readonly Admins who only have read-only. These have the same access as a read\/writeAdmin except in read-only mode. RBAC does not affect the pages they see. - com.unraveldata.rbac.default Determines how the End-user's views are filtered when no specific tags are set for a end-user. userName com.unraveldata.ngui.user.mode Determines pages a end-user can see when RBAC is on. extended: Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage, Reports | Operational Insights | Chargeback restricted: Application | Applications extended Using LDAP with Role Based Access Control If you have LDAP you can use it in conjunction with RBAC through the following properties in \/usr\/local\/unravel\/etc\/unravel.properties. Property Definition Default Example com.unraveldata.login.admins.ldap.groups Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. - a-admin,b-admin com.unraveldata.login.admins.readonly.ldap.groups Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. - a-adminRO,b-adminRO com.unraveldata.rbac.ldap.tags A comma separated list of the prefix of LDAP group to be used as the PROJECT - proj,dept,div com.unraveldata.rbac.ldap.tag.Tag.regex.find Defines regular expression used to parse LDAP groups for generating the TENANTs PROJECT. Tag-(REGEX) PROJECT com.unraveldata.rbac.ldap.tags. Note: PROJECT example 2 - proj Generating Projects and Tenants for RBAC from LDAP groups See Adding Roles RBAC tags restrict a end-user to their applications with matching tags. Therefore, the RBAC tags must also be used as Application Tags loaded via setting the below properties are only used if RBAC is turned on. You can turn RBAC via properties ( com.unraveldata.rbac.enabled=true RBAC Define the tags and regex; below we are adding project dept com.unraveldata.rbac.ldap.tags.find=project,dept com.unraveldata.rbac.ldap.tag.project.regex.find=project-(.*) com.unraveldata.rbac.ldap.tag.dept.regex.find=dept-(.*) Examples When a user logs on, their LDAP groups is read and used to create the tags (if any). Using the above properties , User LDAP Groups Tags Project Tenant 1 [\"dept-hr\",\"dept-sales\",\"dept-finance\"] { \"dept\":[\"hr\",\"sales\",\"finance\"]} dept hr, sales, finance 2 [\"project-group01\",\"project-group02\",\"project-group03\"] { \"project\":[\"group01\",\"group02\",\"group03\"]} project group01, group02, group03 3 [\"project-group01\",\"project-group02\",\"project-group03\", \"dept-hr\",\"dept-sales\",\"dept-finance\"] { \"project\":[\"group01\",\"group02\",\"group03\"]} project group01, group02, group03 4 [\"division-div01\",\"division-div02\",\"division-div03\"] n\/a n\/a n\/a User 1 User 2 User 3 project Role Based Access Control User 4 division, " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-4/advanced-topics/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "unravel-4-4/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-9b68a478-13c3-0df7-18f2-4c9ffda24e31_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel's application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (\"Smoke Tests\") Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the sect...", 
"body" : " Verification tests highlight the value of Unravel's application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (\"Smoke Tests\") Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for MapR # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "unravel-4-4/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-9b68a478-13c3-0df7-18f2-4c9ffda24e31_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages \n \n \n Package Name \n Location \n \n Benchmarks 1.6.x \n \n https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz \n \n Benchmarks 2.0.x \n \n https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where { LOCATION FNAME # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz\n\n71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , \n cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ \n lib: \n scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh \n src: \n tpch-query-instances: \n cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. \n Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \n \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. \n \n example3 \n A Scala program containing a SparkSQL \n How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. \n \n example4 \n A Scala-based application. This application generates its input and applies multiple transformations to the generated data. \n How Unravel helps to root-cause a failed \n \n example5 \n A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group \n Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only \n Unravel's insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n see example1 in Benchmarks for Spark 1.6.x \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce \n How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. \n \n example3 example4 example5 \n see example3 - example5 in Benchmarks for Spark 1.6.x \n \n example6 \n A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. " }, 
{ "title" : "Unravel APIs", 
"url" : "unravel-4-4/advanced-topics/unravel-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs", 
"snippet" : "REST API Use Case - Auto Actions and Pagerduty...", 
"body" : " REST API Use Case - Auto Actions and Pagerduty " }, 
{ "title" : "REST API", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API", 
"snippet" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI ...", 
"body" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI with a curl command using the same credentials (see Sign In role All requests\/responses are in JSON Format. Applications Like the Applications here Operations Provides detailed data on various ongoing activities\/resource usage of your cluster. These include memory and vcore usage by cluster, queue, application, and user. You can also obtain information about job status and recent alerts. See here Operations Reports Returns key cluster KPIs and the small files report. See here Reports | Data Insight Generates chargeback reports (by user, queue, app), and Cluster workload reports. See here Reports | Operational Insight Monitoring Provides lightweight daemon which allows you to monitor various Unravel components via the REST API, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. See here Available REST Resources This API supports a Representational State Transfer (REST) model for accessing a set of resources through a fixed set of operations. API Http Method Description Sign In POST \/signIn Operations GET GET GET GET GET GET GET GET GET \/clusters\/resources\/cpu\/allocated \/clusters\/resources\/cpu\/total \/clusters\/resources\/memory\/allocated \/clusters\/resources\/memory\/total \/clusters\/nodes \/clusters\/resources\/tagged\/cpu \/clusters\/resources\/tagged\/memory \/jobs\/count \/autoactions\/recent_violations Application Search POST \/apps\/search Reports GET GET GET GET GET \/search\/cb\/appt \/search\/cb\/appt\/user \/search\/cb\/appt\/queue \/reports\/operational\/clusterstats \/reports\/operational\/clusterworkload Data Insights GET GET reports\/data\/kpis \/reports\/data\/small_file_report_details HTTP\/HTTPS methods The API resources listed below follow standard Create-Read-Update-Delete (CRUD) semantics; the HTTP request path defines the Unravel Server and the method the action to perform. Method Operation POST Create entries GET Read entries PUT Update or edit entries DELETE Delete entries Error Codes Upon failure one of the following errors are returned Error Code Description 400 Invalid request parameters; Malformed requests 401 Authentication failure 403 Authorization failure 404 Object not found 500 Internal API error 503 Response temporarily unavailable; the caller should retry later " }, 
{ "title" : "Sign In", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api/sign-in.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Sign In", 
"snippet" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you en...", 
"body" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you enter in the CLI. Click Authorize Post \/signIn Authorize Command Line \n ssh {ClusterName} # ssh@root {ClusterName} Once you have successfully logged in, enter the curl command from step 2 above. You should see output as shown below. # curl -X POST \"http:\/\/UNRAVEL_HOST:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=user1&password=Password1\" \n Response example {\n \"message\": \"ok\",\n \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTUzNzc3NDMwOSwiZXhwIjoxNTM3NzgxNTA5fQ.iD6NXDRj1UqRYr58H4xYlNRcdrWFcU9l3p8NmbpN30k\",\n \"role\": \"admin\",\n \"readOnly\": false,\n \"tags\": \"\",\n \"id\": \"admin\",\n \"username\": \"admin\"\n} The token is needed for each REST API curl command executed. Each time you log in a new token is generated to be used in commands for that session. " }, 
{ "title" : "Application APIs", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api/application-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Application APIs", 
"snippet" : "All queries are made with the http\/s request POST in a curl command using JSON for the query format. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port All commands require: Unravel_Host: Port: Start: ...", 
"body" : "All queries are made with the http\/s request POST in a curl command using JSON for the query format. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port All commands require: \n Unravel_Host: \n Port: \n Start: \n End: For the following parameters for when specifying more than one use a comma separated list. \n appTypes \n appStatus \n users \n queues \n taglist Note: You can not specify time in the Start and End times. For Hive and MR you must specify at least one Status type. \n \n \n { \"metadata\": {}, \"results\": [ { } ] } Examples Get All Failed Apps All apps regardless of status, user, queues, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"F\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search All Running and Pending Apps All running and pending apps regardless of user, queues, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"R\",\"P\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search All Failed, Killed and Unknown Hive and Tez Apps Apps All apps with the failed, killed or unknown status regardless of user, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"hive\",\"tez\"],\"appStatus\":[\"\"F\",\"K\",\"U\"],\"end_time\":\"2018-10-09T05:18:42.000Z\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App User All apps owned by a user regardless of status, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"users\":[\"root\"]}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App Queues All apps regardless of status, user, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"queues\":[\"root.users.hdfs\"]}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App Tags All apps owned by a user regardless of status, user, or queue. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"appTags\":{\"dept\":[\"mktg\"],\"project\":[]}}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search \n \n \n \n {\n \"metadata\": {\n \"duration\": {\n \"max\": 21228577,\n \"min\": 0\n },\n \"resource\": {\n \"max\": 1,\n \"min\": 0\n },\n \"events\": {\n \"max\": 4,\n \"min\": 0\n },\n \"appTypes\": {\n \"mr\": 115,\n \"hive\": 67\n },\n \"appStatus\": {\n \"S\": 168,\n \"F\": 9,\n \"K\": 4,\n \"R\": 1\n },\n \"users\": {\n \"root\": 106,\n \"hdfs\": 68,\n \"user11\": 8\n },\n \"queues\": {\n \"root.users.root\": 83,\n \"root.users.hdfs\": 62,\n \"root.users.user11\": 8,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjhfsjhfhfsjkhfjhfdjhfj\": 6,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjh\": 4,\n \"root.abcdefghijklmnopqrstuvwxyz\": 1,\n \"root.pooja.pooja\": 1\n },\n \"clusters\": {\n \"default\": 157\n },\n \"totalRecords\": 182\n },\n \"results\": [\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1201\",\n \"nick\": \"1201\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:48:29\",\n \"start_time_long\": \"2018-08-30T11:48:29.315Z\",\n \"duration_long\": 88000,\n \"predDuration_long\": 0,\n \"io_long\": 327736439,\n \"read_long\": 293122770,\n \"write_long\": 34613669,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 5,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 5,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 67916,\n \"totalReduceSlotDuration\": 38679,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1200\",\n \"nick\": \"1200\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-34)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"K\",\n \"status_long\": \"Killed\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:47:22\",\n \"start_time_long\": \"2018-08-30T11:47:22.972Z\",\n \"duration_long\": 297000,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 2,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": null,\n \"sr\": null,\n \"fm\": null,\n \"fr\": null,\n \"km\": 1,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 289561,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-34)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1199\",\n \"nick\": \"1199\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-1)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:47:22\",\n \"start_time_long\": \"2018-08-30T11:47:22.743Z\",\n \"duration_long\": 55000,\n \"predDuration_long\": 0,\n \"io_long\": 350185086,\n \"read_long\": 57113366,\n \"write_long\": 293071720,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 2,\n \"sr\": 6,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 2,\n \"totalReduceTasks\": 6,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 43929,\n \"totalReduceSlotDuration\": 61132,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-1)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"appt\": \"UNKNOWN\",\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"hdfs_20180830044747_9306c5ba-a6e4-4843-b80b-a475ff0227ba-u_sutl\",\n \"nick\": null,\n \"name\": \"\\ncreate view q2_min_ps_supplyc...\\tp_partkey\",\n \"queue\": \"-\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"-\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:47:11\",\n \"start_time_long\": \"2018-08-30T11:47:11.318Z\",\n \"duration_long\": 770,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 0,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 0,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": null,\n \"sr\": null,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": [\n \"tpch_flat_orc_10.supplier\",\n \"tpch_flat_orc_10.part\",\n \"tpch_flat_orc_10.partsupp\",\n \"tpch_flat_orc_10.region\",\n \"tpch_flat_orc_10.nation\"\n ],\n \"outputTables\": [\n \n ],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"Hive\",\n \"kindLong\": \"Hive\",\n \"name_long\": \"create view q2_min_ps_supplyc...\\tp_partkey\",\n \"kind_url\": \"hive_queries\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1198\",\n \"nick\": \"1198\",\n \"name\": \"select\\n\\tl_returnflag,...nflag,\\n\\tl_linestatus(Stage-2)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:19:48\",\n \"start_time_long\": \"2018-08-30T11:19:48.933Z\",\n \"duration_long\": 17000,\n \"predDuration_long\": 0,\n \"io_long\": 8730,\n \"read_long\": 8730,\n \"write_long\": 0,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 1,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 3121,\n \"totalReduceSlotDuration\": 3111,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": \"20180726T055507Z-1188411941450587713\",\n \"wn\": \"Papiya\",\n \"wt\": \"20180726T055507Z\",\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\tl_returnflag,...nflag,\\n\\tl_linestatus(Stage-2)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1197\",\n \"nick\": \"1197\",\n \"name\": \"select\\n\\tl_returnflag,...nflag,\\n\\tl_linestatus(Stage-1)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:19:28\",\n \"start_time_long\": \"2018-08-30T11:19:28.789Z\",\n \"duration_long\": 17000,\n \"predDuration_long\": 0,\n \"io_long\": 16374,\n \"read_long\": 16278,\n \"write_long\": 96,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 1,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 3565,\n \"totalReduceSlotDuration\": 3286,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": \"20180726T055507Z-1188411941450587713\",\n \"wn\": \"Papiya\",\n \"wt\": \"20180726T055507Z\",\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\tl_returnflag,...nflag,\\n\\tl_linestatus(Stage-1)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830041919_14f943db-fd7b-4e98-a6c9-1ed72605b126-u_dCcu\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830035151_90a5a80c-cd1e-4253-87f3-7a4eabfc8016-u_SK8w\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830035151_90a5a80c-cd1e-4253-87f3-7a4eabfc8016-u_SK8w\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1196\",\n \"nick\": \"1196\",\n \"name\": \"select\\n\\tsum(l_extendedprice* (1...PERSON'\\n\\t)(Stage-1)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"F\",\n \"status_long\": \"Failed\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:51:51\",\n \"start_time_long\": \"2018-08-30T10:51:51.057Z\",\n \"duration_long\": 3304000,\n \"predDuration_long\": 0,\n \"io_long\": 656194626,\n \"read_long\": 656194626,\n \"write_long\": 0,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 2,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 8,\n \"sr\": null,\n \"fm\": null,\n \"fr\": 4,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 8,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 218708,\n \"totalReduceSlotDuration\": 3253532,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\tsum(l_extendedprice* (1...PERSON'\\n\\t)(Stage-1)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830035151_90a5a80c-cd1e-4253-87f3-7a4eabfc8016-u_SK8w\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"appt\": \"UNKNOWN\",\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"hdfs_20180830035151_90a5a80c-cd1e-4253-87f3-7a4eabfc8016-u_SK8w\",\n \"nick\": null,\n \"name\": \"select\\n\\tsum(l_extendedprice* (...PERSON'\\n\\t)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"F\",\n \"status_long\": \"Failed\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"clusterName=\",\n \"start_time\": \"08\\\/30\\\/18 10:51:45\",\n \"start_time_long\": \"2018-08-30T10:51:45.863Z\",\n \"duration_long\": 3311551,\n \"predDuration_long\": 0,\n \"io_long\": 656194626,\n \"read_long\": 656194626,\n \"write_long\": 0,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 1,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"job_1534930112573_1196\"\n ],\n \"appIds\": [\n \n ],\n \"sm\": 8,\n \"sr\": null,\n \"fm\": null,\n \"fr\": 4,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 8,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 218708,\n \"totalReduceSlotDuration\": 3253532,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": [\n \"tpch_flat_orc_10.lineitem\",\n \"tpch_flat_orc_10.part\"\n ],\n \"outputTables\": [\n \n ],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"Hive\",\n \"kindLong\": \"Hive\",\n \"name_long\": \"select\\n\\tsum(l_extendedprice* (...PERSON'\\n\\t)\",\n \"kind_url\": \"hive_queries\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1195\",\n \"nick\": \"1195\",\n \"name\": \"create table q18_large_volume_customer...100(Stage-4)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:51:12\",\n \"start_time_long\": \"2018-08-30T10:51:12.282Z\",\n \"duration_long\": 16000,\n \"predDuration_long\": 0,\n \"io_long\": 58176,\n \"read_long\": 51866,\n \"write_long\": 6310,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 1,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 2992,\n \"totalReduceSlotDuration\": 3281,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"create table q18_large_volume_customer...100(Stage-4)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1194\",\n \"nick\": \"1194\",\n \"name\": \"create table q18_large_volume_customer...100(Stage-3)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:50:52\",\n \"start_time_long\": \"2018-08-30T10:50:52.328Z\",\n \"duration_long\": 16000,\n \"predDuration_long\": 0,\n \"io_long\": 110131,\n \"read_long\": 64863,\n \"write_long\": 45268,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 1,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 1,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 3237,\n \"totalReduceSlotDuration\": 3157,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"create table q18_large_volume_customer...100(Stage-3)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1193\",\n \"nick\": \"1193\",\n \"name\": \"create table q18_large_volume_customer...100(Stage-2)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:48:54\",\n \"start_time_long\": \"2018-08-30T10:48:54.392Z\",\n \"duration_long\": 114000,\n \"predDuration_long\": 0,\n \"io_long\": 1313700842,\n \"read_long\": 1313652174,\n \"write_long\": 48668,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 12,\n \"sr\": 41,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": 2,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 12,\n \"totalReduceTasks\": 41,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 493978,\n \"totalReduceSlotDuration\": 1102257,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"create table q18_large_volume_customer...100(Stage-2)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1192\",\n \"nick\": \"1192\",\n \"name\": \"create table q18_large_volume_customer...100(Stage-6)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:46:58\",\n \"start_time_long\": \"2018-08-30T10:46:58.343Z\",\n \"duration_long\": 111000,\n \"predDuration_long\": 0,\n \"io_long\": 1199989630,\n \"read_long\": 175531330,\n \"write_long\": 1024458300,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 3,\n \"sr\": 8,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 3,\n \"totalReduceTasks\": 8,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 131099,\n \"totalReduceSlotDuration\": 140407,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"create table q18_large_volume_customer...100(Stage-6)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1191\",\n \"nick\": \"1191\",\n \"name\": \"create table q18_large_volume_customer...100(Stage-1)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:46:57\",\n \"start_time_long\": \"2018-08-30T10:46:57.382Z\",\n \"duration_long\": 97000,\n \"predDuration_long\": 0,\n \"io_long\": 288981846,\n \"read_long\": 288960797,\n \"write_long\": 21049,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 7,\n \"sr\": 26,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": 1,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 7,\n \"totalReduceTasks\": 26,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 292067,\n \"totalReduceSlotDuration\": 657445,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"create table q18_large_volume_customer...100(Stage-1)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"appt\": \"UNKNOWN\",\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"hdfs_20180830034646_524c9dcf-4f79-4e1f-99f1-d31b5093e2b6-u_A0T6\",\n \"nick\": null,\n \"name\": \"\\n\\ncreate view q18_tmp_cached a...l_orderkey\",\n \"queue\": \"-\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"-\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 10:46:53\",\n \"start_time_long\": \"2018-08-30T10:46:53.213Z\",\n \"duration_long\": 570,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 0,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 0,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": null,\n \"sr\": null,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": [\n \"tpch_flat_orc_10.lineitem\"\n ],\n \"outputTables\": [\n \n ],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"Hive\",\n \"kindLong\": \"Hive\",\n \"name_long\": \"create view q18_tmp_cached a...l_orderkey\",\n \"kind_url\": \"hive_queries\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"appt\": \"UNKNOWN\",\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"hdfs_20180830034646_b0f86cab-7e50-41b9-b6d6-80f029f574ce-u_hRvT\",\n \"nick\": null,\n \"name\": \"\\n\\ncreate table q18_large_volum...\\nlimit 100\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"clusterName=\",\n \"start_time\": \"08\\\/30\\\/18 10:46:53\",\n \"start_time_long\": \"2018-08-30T10:46:53.213Z\",\n \"duration_long\": 278474,\n \"predDuration_long\": 0,\n \"io_long\": 2802840625,\n \"read_long\": 1778261030,\n \"write_long\": 1024579595,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 5,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \"job_1534930112573_1191\",\n \"job_1534930112573_1194\",\n \"job_1534930112573_1195\",\n \"job_1534930112573_1193\",\n \"job_1534930112573_1192\"\n ],\n \"appIds\": [\n \n ],\n \"sm\": 24,\n \"sr\": 77,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": 3,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 24,\n \"totalReduceTasks\": 77,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 923373,\n \"totalReduceSlotDuration\": 1906547,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": [\n \"tpch_flat_orc_10.lineitem\",\n \"tpch_flat_orc_10.q18_tmp_cached\",\n \"tpch_flat_orc_10.orders\",\n \"tpch_flat_orc_10.customer\",\n \"Temp Tables\"\n ],\n \"outputTables\": [\n \"tpch_flat_orc_10.tpch_flat_orc_10.q18_large_volume_customer_cached\"\n ],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"Hive\",\n \"kindLong\": \"Hive\",\n \"name_long\": \"create table q18_large_volum...\\nlimit 100\",\n \"kind_url\": \"hive_queries\",\n \"kind_parent_url\": \"app\"\n }\n ]\n} " }, 
{ "title" : "Data APIs", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api/data-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Data APIs", 
"snippet" : "The query types fall in the following categories: KPIs Small files Required parameters for all commands: Unravel_Host token: sign in All output timestamps are in EPOCH. Variable arguments within the command are indicated in RED KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST numDays Additional required param...", 
"body" : "The query types fall in the following categories: KPIs Small files Required parameters for all commands: Unravel_Host token: sign in All output timestamps are in EPOCH. Variable arguments within the command are indicated in RED KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST numDays Additional required parameters are: : number of days numDays Output Format [\n {\n \"st\": start time EPOCH_timestamp,\n \"et\": end time EPOCH_timestamp,\n \"nlaTb\": # of tables accessed,\n \"nlaPr\": # of partitions accessed,\n \"nlaQr\": # of queries accessing the table,\n \"nlaRi\": Total Read I\/O due to accessing the Tables,\n \"nlcTb\": # of tables created,\n \"nlcPr\": # of partitions created,\n \"nlcTz\": size of tables created,\n \"nlcPz\": size of partitions created,\n \"ntoTb\": total number of tables in the system,\n \"ntoPr\": total number of partitions in the system,\n \"nhtTb\": ,\n \"nwaTb\": ,\n \"ncoTb\": ,\n \"nhtPr\": ,\n \"nwaPr\": ,\n \"ncoPr\": ,\n \"rp\": ,\n \"rs\": ,\n \"fs\": ,\n \"users\": [\n \/\/ comma separated list of users\n ]\n }\n] curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/kpis?numDays=1\" \\\n-H \"accept: application\/json\" -H \"Authorization: JWT token\" Output [\n {\n \"st\": 1536795705,\n \"et\": 1536882105,\n \"nlaTb\": 15,\n \"nlaPr\": 0,\n \"nlaQr\": 57,\n \"nlaRi\": 101489411821,\n \"nlcTb\": 7,\n \"nlcPr\": 0,\n \"nlcTz\": 304768890,\n \"nlcPz\": 0,\n \"ntoTb\": 378,\n \"ntoPr\": 30061,\n \"nhtTb\": 19,\n \"nwaTb\": 0,\n \"ncoTb\": 359,\n \"nhtPr\": 1823,\n \"nwaPr\": 0,\n \"ncoPr\": 28238,\n \"rp\": 30061,\n \"rs\": 92544293666,\n \"fs\": 92544293666,\n \"users\": [\n \"root\",\n \"hdfs\"\n ]\n }\n] SMALL FILES: curl -X GET \"http:\/\/ UNRAVEL_HOST Additional required parameters are: Small file id: Output Format {\n \"date\": date created EPOCH_timestamp,\n \"isSuccess\": report generation sucess\/failure,\n\n\/\/ report parameters\n \"avg_size_threshold\": small file size in bytes,\n \"num_files_threshold\": minimum number of small files,\n \"top_n_small_files\": # of directories to show,\n \"report_id\": report name\",\n \"root\": [\n \/\/ array[top_n_small files] directory\n {\n \"MaxFilesize\": maximum size file in directory,\n \"MinFilesize\": minimum size file in directory,\n \"NumFiles\": # of small files in the directory,\n \"DirPath\": directory path,\n \"TotalFilesize\": sum of file size in directory,\n \"AvgFilesize\": average file size\n }\n ]\n} curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/small_file_report_details?entity_id=small_files_1536927781_5674\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" {\n \"date\": 1536899695,\n \"isSuccess\": true,\n \"avg_size_threshold\": 100000,\n \"num_files_threshold\": 100,\n \"top_n_small_files\": 5,\n \"report_id\": \"small_files_1536927781_5674\",\n \"root\": [\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"21523359\",\n \"DirPath\": \"\\\/97ovg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"11302185\",\n \"DirPath\": \"\\\/98esm\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/99acx\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/97zqu\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/99kgg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n }\n ]\n} " }, 
{ "title" : "Operational APIs", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api/operational-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Operational APIs", 
"snippet" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: Cluster Applications User Queue Jobs Recent Alerts All commands require the following: Unravel_Host: Port: Start: End: token: sign in All time...", 
"body" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: \n Cluster \n Applications \n User \n Queue \n Jobs \n Recent Alerts All commands require the following: \n Unravel_Host: \n Port: \n Start: \n End: \n token: sign in All timestamps are in EPOCH. You must substitute your local or relevant information for fields indicated by RED CLUSTER Cluster Vcore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port resourceType Query End Interval Start token Additional required parameters: \n Resoure Type \n cpu \n memory \n Query Type \n allocated \n total \n Polling Interval \n \n \n \n {\n EPOCH_timestamp : count\n} \n \n Allocated Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?to=1538666160&interval=30m&from=1536273311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n \n \n \n \n {\n \"1535360400000\": \"12\", \n \"1535364000000\": \"3.125\", \n \"1535367600000\": \"5.0303030303\"\n} \n Total Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/memory\/total?to=1536273841&interval=1h&from=1536187441\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n \n \n \n {\n \"1535364000000\": 47460,\n \"1535367600000\": 45087,\n \"1535371200000\": 37968\n} Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n Interval \n Start token Additional required parameters: \n Polling Interval \n Note: total = active + unhealthly \n \n \n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n \"total\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"active\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"lost\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"unhealthy\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"decommissioned\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"rebooted\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }\n}\n \n Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/nodes?to=1536339111&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n \n \n \n \n {\n \"date\": [\n 1535688000000,\n 1535691600000,\n 1535695200000,\n 1535698800000\n ],\n \"total\": {\n \"1535688000000\": 3,\n \"1535691600000\": 3,\n \"1535695200000\": 3,\n \"1535698800000\": 3\n },\n \"active\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"lost\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n },\n \"unhealthy\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"decommissioned\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"rebooted\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n }\n} APPLICATION VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n AppType \n Interval \n Start token Additional required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Application Type \n Polling Interval \n \n \n \n {\n EPOCH_timestamp : count\n} \n Mapreduce Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22MAPREDUCE%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n \n Spark Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22SPARK%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" User VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n UserName \n Interval \n Start token Additional required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n User \n Polling Interval \n \n \n \n {\n EPOCH_timestamp : result\n} \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" QUEUE VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n QueueName \n Interval \n Start token Additional required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Queue \n Polling Interval \n \n \n \n {\n EPOCH_timestamp : result\n} \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" JOBS Returns average number of jobs by Groupby type. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n GroupBy \n Interval \n Start token Additional required parameters \n Groupby: \n Polling Interval \n State Example \n \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n \"RUNNING\": {\n EPOCH_timestamp: average\n },\n \"ACCEPTED\": {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536275822&groupBy=state&interval=1h&from=1535671022\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n \n \n \n {\n \"date\": [\n 1535626800000\n ],\n \"RUNNING\": {\n \"1535626800000\": \"1.3333333333\"\n },\n \"ACCEPTED\": {\n \"1535695200000\": \"1\"\n }\n} \n Application Type \n \n \n \n {\n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n\n APP_TYPE : { \n \/\/ APP_TYPE mr | hive | spark | pig | cascading | impala | tez \n \/\/ for each polling interval where the app type is running\n EPOCH_timestamp: count\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536280789&groupBy=applicationType&interval=1h&from=1535675989\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n \n \n \n {\n \"date\": [\n 1535644800000,\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000\n ],\n \"MAPREDUCE\": {\n \"1535644800000\": \"1\",\n \"1535695200000\": \"1\",\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1.5\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1.1538461539\"\n },\n \"SPARK\": {\n \"1535698800000\": \"1\"\n }\n} \n User Example \n \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n userName: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536281730&groupBy=user&interval=1h&from=1535676930\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000\n ],\n \"root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\"\n },\n \"hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\"\n }\n} \n Queue Example \n \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n queue Name: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536339111&groupBy=queue&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000,\n 1536044400000,\n 1536048000000,\n 1536051600000,\n 1536055200000,\n 1536058800000,\n 1536076800000\n ],\n \"root.users.root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536076800000\": \"1\"\n },\n \"root.users.hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535961600000\": \"2\",\n \"1536040800000\": \"1.1304347826\",\n \"1536044400000\": \"1\",\n \"1536055200000\": \"1.3333333333\",\n \"1536058800000\": \"1\"\n },\n \"root.users.user11\": {\n \"1535698800000\": \"1\"\n },\n \"root.abcdefghijklmnopqrstuvwxyz\": {\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536044400000\": \"1\"\n }\n} RECENT ALERTS curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n GroupBy \n Interval \n Start token Additional required parameters \n limit \n \n \n \n {\n \"string\"\n} \n Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/autoactions\/recent_violations?to=1536342595&limit=500&from=1536256195\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n [\n {\n \"_index\": \"ev-20180909_15\",\n \"_type\": \"event\",\n \"_id\": \"8066732434887157252\",\n \"_score\": null,\n \"_source\": {\n \"eventName\": \"Long running Hive query 456\",\n \"eventType\": \"AV\",\n \"eventNumber\": 80002,\n \"eventTime\": 1536581601777,\n \"entityGroup\": 0,\n \"entityType\": 0,\n \"entityId\": \"6962622378701593007\",\n \"user\": \"N\\\/A\",\n \"queue\": \"N\\\/A\",\n \"clusterName\": \"N\\\/A\",\n \"staticRank\": 80,\n \"dynamicRank\": -1\n },\n \"sort\": [\n 1536581601777\n ]\n },\n {\n \"_index\": \"ev-20180909_15\",\n \"_type\": \"event\",\n \"_id\": \"2330589360884687327\",\n \"_score\": null,\n \"_source\": {\n \"eventName\": \"Long running Hive query 456\",\n \"eventType\": \"AV\",\n \"eventNumber\": 80002,\n \"eventTime\": 1536581150567,\n \"entityGroup\": 0,\n \"entityType\": 0,\n \"entityId\": \"6962622378701593007\",\n \"user\": \"N\\\/A\",\n \"queue\": \"N\\\/A\",\n \"clusterName\": \"N\\\/A\",\n \"staticRank\": 80,\n \"dynamicRank\": -1\n },\n \"sort\": [\n 1536581150567\n ]\n }\n] " }, 
{ "title" : "Reports APIs", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/rest-api/reports-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Reports APIs", 
"snippet" : "The query types fall in the following categories: Chargeback Clusters Variable parameters within the command are indicated in RED Chargeback Chargeback Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required para...", 
"body" : "The query types fall in the following categories: \n Chargeback \n Clusters Variable parameters within the command are indicated in RED Chargeback Chargeback Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"http:\/\/ UNRAVEL_HOST \n Start End token Required parameters: \n Unravel_Host \n Start: \n End: \n token: sign in \n \n \n \n {\n \"cb\": [\n \/\/ array organizing by application type\n {\n \"ms\": memory usage seconds,\n \"count\": application count,\n \"v1\": \"application type (mr | spark)\",\n \"vs\": vcore usage in second\n }\n ]\n} Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt?from=1536670860<to=1536757260\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n \n \n \n \n {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Chargeback by user or queue curl -X GET \"http:\/ UNRAVEL_HOST QueryType \n Start End token Required parameters: \n Unravel_Host \n Query Type: \n Start: \n End: \n token: sign in \n \n \n \n {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} Example Query Type = user # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" \n \n \n \n {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Example Query Type = queue # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" \n \n \n \n {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback app type Return report by application type for a specific queue. curl -X GET \"http:\/\/ UNRAVEL_HOST AppType \n End \n Start QueueName Required parameters: \n Unravel_Host \n AppType: \n Queue Name \n Start: \n End: \n token: sign in \n \n \n \n {\n \"cb\": [\n \/\/ array by application type\n {\n \"ms\": memory usage in milliseconds,\n \"count\": application count,\n \"v2\": userName | queueName,\n \"vs\": vcore usage in seconds\n }\n ]\n}\n Example App Type = mr # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/user?appt=mr>e=1536677580<e=1536763980&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n \n \n \n \n {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Clusters Cluster Summary curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters: \n Unravel_Host \n Mode: \n Start: \n End: \n token: sign in \n Output for mode user or queue \n \n \n {\n \"userStats\": [\n\/\/ array of users | queues currently on cluster\n {\n \"root\": {\n \"running\": {\n \"min\": minimum applications running,,\n \"max\": minimum applications running,\n \"mean\": average appication number running,\n \"stddev\": standard deviation\n },\n \"memory\": { \/\/ see running above },\n \"pending\": { \/\/ see running above },\n \"vcores\": { \/\/ see running above }\n }\n }\n ]\n} Output for mode app \n \n \n \n { [ appID : application id , type : memorySeconds \/vcoreSeconds vcore : vcore value memory : memory value ] } Example mode = user # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=user&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n \n \n \n \n {\n \"userStats\": [\n {\n \"root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Example mode = queue # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=queue&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n \n \n \n \n {\n \"queueStats\": [\n {\n \"root.users.root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Cluster Workload curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: \n Unravel_Host \n Start: \n End: \n token: sign in \n reportby \n \n \n \n \/\/ work load by month one of more months with the application count for month\n\/\/ minimum of one month\n { timestamp: appcount[,timestamp: appcount] }\n\n\/\/ work load by hour array of 25 hours\n [\n { timestamp: appcount }, ... { timestamp: appcount }\n ]\n\n\/\/ work load by day - array for each days contained with the time period (Mon - Sun)\n\/\/ minimum of one day\n [\n { timestamp: appcount }\n ]\n\n\/\/ work load by hour\/day:array for each day (Mon - Sun) by hour\n\/\/ minimum of 24 hours for one day\n [\n { timestamp: appcount }\n ]\n\n curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterworkload?gte=1536777000Z<=1536863400Z&reportBy=month\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n \n \n \n \n {\"1536777000\":96,\"1536863400\":4} [ {\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2} [ {\"1536777000000\":96},{\"1536863400000\":7} ] [{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}] " }, 
{ "title" : "Use Case - Auto Actions and Pagerduty", 
"url" : "unravel-4-4/advanced-topics/unravel-apis/use-case---auto-actions-and-pagerduty.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel APIs \/ Use Case - Auto Actions and Pagerduty", 
"snippet" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done...", 
"body" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's Autoactions API. Currently, this action is initiated outside of Unravel's Server. For the integration you need to complete 2 setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerdutyinformation. The python script \n must pagerduty Using pagerduty for notifications Set up a pagerduty service. \n 1 Configuration Services https:\/\/www.pagerduty.com \n 2 Add New Service \n 3 Use our API directly Events API v2 \n 4 Add Service \n 5 I ntegration Key Integrations Run the Unravel API python script on your local computer. \n 1 https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 \n 2 cd unravel-api-demo-v1 # cd unravel-api-demo-v1\n# ls\nREADME.md api-test.py \n 3 # python .\/api-test.py \n 4 Unravel Autoactions API Address The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions UNRAVEL_HOST_IP https:\/\/playground.unraveldata.com. Enter the pagerduty integration key Pagerduty API key \n 5 Start To scroll within list, click within the Autoactions Name \n 6. Navigate to the Auto Actions page ( Manage Auto Actions Creating Auto Actions \n 7. In our example there were 2 active autoactions, \n Kill job hogging the cluster \n Rogue App AA #1. \n Kill job hogging the cluster \n 8. \n \n Kill job hogging the cluster \n Updated Unravel UI \n Updated Unravel API \n Kill job hogging the cluster \n 9 \n Sample sms message \n Sample email The python script \n must pagerduty " }, 
{ "title" : "Unravel Monitoring Service", 
"url" : "unravel-4-4/advanced-topics/unravel-monitoring-service.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Monitoring Service", 
"snippet" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monito...", 
"body" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monitors REST API " }, 
{ "title" : "Configuration", 
"url" : "unravel-4-4/advanced-topics/unravel-monitoring-service/configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Monitoring Service \/ Configuration", 
"snippet" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property Description Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 com.unraveldata.monitoring.js...", 
"body" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property Description Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 com.unraveldata.monitoring.js_rules.cool.off.period Alert action cool-off period. 1800 File System Related Rules File system related rules (disk usage) are evaluated separately. Property Description Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency, in seconds, to evaluate filesystem specific monitoring rules. 0: disables evaluation. 60 DB Status Monitoring Property Description Default com.unraveldata.monitoring.db.status.check.interval Frequency, in seconds, database basic status is queried 0: disables evaluation. 30 DB Performance Monitoring Property Description Default com.unraveldata.monitoring.db.performance.check.interval Frequency, in seconds, database performance metrics are gathered. 0: disables evaluation. 30 com.unraveldata.monitoring.db.performance.query Default query: select count(*) from (select 1 from blackboards limit 1000) b see note Zookeeper Monitoring Property Description Default com.unraveldata.monitoring.zookeeper.check.interval Frequency, in seconds, Zookeeper metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.zookeeper.history.size How many data sets consider in computing average values (for historical data). 5 Kafka Monitoring Property Description Default com.unraveldata.monitoring.kafka.check.interval Frequency, in seconds,Kafka metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.kafka.ignore.topics Zookeeper topics to be ignored during Kafka monitoring. Topics can be ignored from different reasons, e.g. all internal topics are ignored. __consumer_offsets, connect-configs, connect-offsets, connect-status com.unraveldata.monitoring.kafka.history.size Number of data sets stored in the memory and used as historical data. 5 ElasticSearch Monitoring Property Description Default com.unraveldata.monitoring.elastic.check.interval Frequency, in seconds,ElasticSearch metrics should be queried. 0: disables monitoring. 30 com.unraveldata.monitoring.elastic.history.size Number of data sets stored in the memory and used as historical data. 5 " }, 
{ "title" : "Disk Monitoring", 
"url" : "unravel-4-4/advanced-topics/unravel-monitoring-service/disk-monitoring.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Monitoring Service \/ Disk Monitoring", 
"snippet" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.u...", 
"body" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. 90 Partitions (volumes) monitoring Given list of partitions are monitored. Once partition usage exceeds given threshold (configured as high watermark limit) email alert is sent. Another email is not sent until disk usage goes below low watermark limit and above high watermark limit again. Property Description Default com.unraveldata.monitoring.fs.partitions.check.interval Frequency to check monitored partitions. 60 com.unraveldata.monitoring.fs.partitions.csv Comma separated list of monitored partitions. Symbolic links are supported. , \/srv\/unravel \/usr\/local\/unravel com.unraveldata.monitoring.fs.partitions.high.watermark Trigger alert if disk usage is over this limit. Do not trigger next alert until disk usage is below low watermark limit. 85 com.unraveldata.monitoring.fs.partitions.low.watermark If disk usage goes below this limit then disk alert can be triggered again. 70 Deprecated Partition Properties The following properties have been deprecated and replaced with the above partition properties. We strongly suggest remove these deprecated properties from unravel.properties and replace them with the above properties. If you have both the new and deprecated properties defined, the value of the deprecated property is used. For instance, if you have the defined both: new: com.unraveldata.monitoring.fs.partitions.high.watermark=80 deprecated: com.unraveldata.kafka.monitor.disk.high.watermark=60 Unravel uses the deprecated property value, so triggering occurs when disk usage is > 60% not 80% Property Replaced by com.unraveldata.filesystem.volumes.csv com.unraveldata.monitoring.fs.partitions.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.monitoring.fs.partitions.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark com.unraveldata.monitoring.fs.partitions.low.watermark Folders monitoring Property Note Default com.unraveldata.monitoring.fs.folders.check.interval Frequency to check monitored folders (directories). 0 com.unraveldata.monitoring.fs.folder_limit.pairs.csv Comma separated list of monitored folders and their size limits, <foldername>:<folderlimit>[KB|MB|GB], e.g., \/srv\/unravel:100GB,\/usr\/local\/unravel:200GB. The folder must be a fully qualified and may be a symbolic link. If no size unit is specified, size is evaluated as bytes. \/srv\/unravel:100GB com.unraveldata.monitoring.fs.folders.low.watermark Percentage of folder limit above. Once an alert is triggered, a new alert is only triggered if the size first drops below this limit and then rises above the folder limit. The purpose is to prevent false and repeated alerts. Example: \/srv\/unravel:100GB with a low water mark of 80% The first time (\/srv\/unravel size > 100GB) the alert is triggered. No new alert is triggered unless (\/srv\/unravel size drops < 80GB) and then 80 " }, 
{ "title" : "JMX Client", 
"url" : "unravel-4-4/advanced-topics/unravel-monitoring-service/jmx-client.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Monitoring Service \/ JMX Client", 
"snippet" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client d...", 
"body" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client displays monitoring beans. In the right panel below JSON response data is associated REST API call by which given data can be retrieved. " }, 
{ "title" : "Monitors REST API", 
"url" : "unravel-4-4/advanced-topics/unravel-monitoring-service/monitors-rest-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Monitoring Service \/ Monitors REST API", 
"snippet" : "Each monitor can be accessed by REST API (which is exposed by Jolokia). { \"request\": { \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\", \"attribute\": \"MBeanStatus\", \"type\": \"read\" }, \"value\": { \"lastUpdated\": \"2018-05-15T08:31:36\", \"lastUpdateSuccessful\": true, \"errorMessage\":...", 
"body" : "Each monitor can be accessed by REST API (which is exposed by Jolokia). {\n \"request\": {\n \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\",\n \"attribute\": \"MBeanStatus\",\n \"type\": \"read\"\n },\n \"value\": {\n \"lastUpdated\": \"2018-05-15T08:31:36\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n },\n \"timestamp\": 1526373099,\n \"status\": 200\n} There is value Set MONITOR_HOST Since we can have more unravel nodes I used term \"monitor_host\" to point to node where monitoring service is running PartitionInfo Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:30:17\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Partitions Info http:\/\/ MONITOR_HOST PartitionInfo - Output {\n \"\\\/home\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/home\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/tmp\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/tmp\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n }\n} DbStatus Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:31:49\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true DbPerformance Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:45:06\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true Last Query Duration http:\/\/ MONITOR_HOST LastQueryDuration - Output 14 Query Timed Out http:\/\/ MONITOR_HOST QueryTimedOut false Query Exception http:\/\/ MONITOR_HOST RuntimeException: Cannot read configuration Zookeeper Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T11:33:32\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMax\": 0,\n \"leader\": false,\n \"follower\": false,\n \"created\": \"2018-05-14T11:33:52\",\n \"connectionsCount\": 0,\n \"mode\": null,\n \"latencyAvg\": 0,\n \"latencyMin\": 0,\n \"port\": 2000,\n \"outstandingCount\": 0,\n \"host\": \"localhost\",\n \"nodeCount\": 0,\n \"ok\": false\n }\n] Historical Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMinAvg\": 0,\n \"connectionsCountTrend\": 0,\n \"nodeCountTrend\": 0,\n \"latencyMinTrend\": 0,\n \"outstandingCountTrend\": 0,\n \"latencyAvg\": 0,\n \"latencyMaxTrend\": 0,\n \"port\": 2000,\n \"isOkCount\": 0,\n \"host\": \"localhost\",\n \"outstandingCountAvg\": 0,\n \"connectionsCountAvg\": 0,\n \"leaderCount\": 0,\n \"followerCount\": 0,\n \"latencyAvgTrend\": 0,\n \"latencyMaxAvg\": 0,\n \"nodeCountAvg\": 0,\n \"isNotOkCount\": 5\n }\n] Kafka Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T11:55:24\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST {\n \"consumerGroups\": [\n {\n \"groupName\": \"0_1790010567376612\",\n \"consumerTopicList\": [\n {\n \"consumerHost\": \"172.16.1.111\",\n \"clientId\": \"unravel_diag_meta\",\n \"lag\": 0,\n \"currentOffset\": 0,\n \"partitionId\": 0,\n \"consumerId\": \"unravel_diag_meta-1971faac-1d57-462b-b73e-c45e9c3cee52\",\n \"topicName\": \"meta\",\n \"logEndOffset\": 0\n }\n ]\n }\n ],\n \"created\": \"2018-05-14T12:06:38\",\n \"kafkaRunning\": true,\n \"topicList\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"meta\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ],\n \"topicsWithoutConsumer\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ]\n} Historical Data http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.kafka.history.size Elastic Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T12:15:02\",\n \"lastUpdateSuccessful\": false,\n \"errorMessage\": \"Cannot retrieve ElasticSearch data: Cannot get ElasticSearch data. Address: sako1:4171\",\n \"initialized\": true\n} RecentData http:\/\/ MONITOR_HOST {\n \"running\": true,\n \"nodes\": [\n {\n \"indices\": {\n \"search\": {\n \"fetchTimeInMillis\": 0,\n \"queryTimeInMillis\": 0,\n \"scrollTimeInMillis\": 0\n },\n \"docs\": {\n \"deleted\": 0,\n \"count\": 0\n },\n \"indexing\": {\n \"noopUpdateTotal\": 0,\n \"indexTimeInMillis\": 0,\n \"throttleTimeInMillis\": 0,\n \"indexCurrent\": 0,\n \"deleteTimeInMillis\": 0,\n \"deleteCurrent\": 0,\n \"indexTotal\": 0,\n \"indexFailed\": 0,\n \"deleteTotal\": 0,\n \"throttled\": false\n },\n \"get\": {\n \"missingTimeInMillis\": 0,\n \"existsTimeInMillis\": 0,\n \"timeInMillis\": 0\n },\n \"store\": {\n \"sizeInBytes\": 0,\n \"throttleTimeInMillis\": 0\n }\n },\n \"roles\": [\n \"master\",\n \"data\",\n \"ingest\"\n ],\n \"name\": \"unravel_s_1\",\n \"timestamp\": 1526327791715\n }\n ],\n \"port\": 4171,\n \"created\": \"2018-05-14T19:56:33\",\n \"host\": \"sako1\",\n \"clusterHealth\": {\n \"activeShardsPercentAsNumber\": 100,\n \"numberOfPendingTasks\": 0,\n \"numberOfInFlightFetch\": 0,\n \"timedOut\": false,\n \"activePrimaryShards\": 0,\n \"unassignedShards\": 0,\n \"numberOfFailedNodes\": 0,\n \"numberOfNodes\": 1,\n \"taskMaxWaitingInQueueMillis\": 0,\n \"initializingShards\": 0,\n \"numberOfDataNodes\": 1,\n \"relocatingShards\": 0,\n \"clusterName\": \"unravel18679\",\n \"activeShards\": 0,\n \"delayedUnassignedShards\": 0,\n \"numberOfSuccessfulNodes\": 1,\n \"status\": \"green\"\n }\n} Historica lData http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.elastic.history.size " }, 
{ "title" : "", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ ", 
"snippet" : "\/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact support@unraveldata.com The Set By User Req Yes is blank if there is a default value Unit bool: CSL ms min ns sec path percent...", 
"body" : "\/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact support@unraveldata.com The Set By User Req Yes is blank if there is a default value Unit bool: CSL ms min ns sec path percent " }, 
{ "title" : "Required Properties for Unravel", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-RequiredPropertiesforUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Required Properties for Unravel", 
"snippet" : "Property Description Set By User Unit Default prepend username Unravel database user Required string - password password for unravel.jdbc.username Required string - url MySQL or mariadb jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod prepend hive.hdfs....", 
"body" : " Property Description Set By User Unit Default prepend username Unravel database user Required string - password password for unravel.jdbc.username Required string - url MySQL or mariadb jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod prepend hive.hdfs.dir \/user\/unravel\/HOOK_RESULT_DIR Required customer.organization Optional - tmpdir path \/srv\/unravel\/tmp login.admins Admins who can write in the Unravel UI, e.g., update\/add auto actions. admin login.admins.readonly Admins who only have read-only. These have the same access as a read\/write Admin except in read-only mode. CLS zk.quorum CLS 127.0.0.1:4181,127.0.0.1:4182,127.0.0.1:4183 es.cluster Unravel elastic search cluster name, e.g., unravel21650 " }, 
{ "title" : "General properties for all platforms", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Generalpropertiesforallplatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ General properties for all platforms", 
"snippet" : "Property Description Set By User Unit Default prepend job.collector.done.log.base HDFS path to \"done\" directory of MR logs path \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done spark.eventlog.location Spark event logs direcotry path maprfs:\/\/\/apps\/spark job.collector.log.aggregation.base An HDFS path ...", 
"body" : " Property Description Set By User Unit Default prepend job.collector.done.log.base HDFS path to \"done\" directory of MR logs path \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done spark.eventlog.location Spark event logs direcotry path maprfs:\/\/\/apps\/spark job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process path \/tmp\/logs\/*\/logs\/ " }, 
{ "title" : "Unravel UI LOGIN Properties", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-UnravelUILOGINProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Unravel UI LOGIN Properties", 
"snippet" : "Property Description Set By User Unit Default prepend login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml Yes path - kerberos.principal Spark event logs direcory path kerberos.keytab.path An HDFS path that helps locate MR job logs to process path HIVE ldap prepend url ...", 
"body" : " Property Description Set By User Unit Default prepend login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml Yes path - kerberos.principal Spark event logs direcory path kerberos.keytab.path An HDFS path that helps locate MR job logs to process path HIVE ldap prepend url Domain guidKey =uid baseDN =dc=unraveldata,dc=com customLDAPQuery groupFilter groupDNPattern userDNPattern userFilter groupMembershipKey groupClassKey SAML ldP prepend login.saml.config Location of saml configuration file. See saml.json path \/usr\/local\/unravel\/etc\/saml.json " }, 
{ "title" : "Hive Metastore Access", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-HiveMetastoreAccess", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property Definition Set By User Default prepend ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.mysql....", 
"body" : "Required for Data Insights tab to populate its information correctly. Property Definition Set By User Default prepend ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.mysql.jdbc.Driver Req ConnectionPassword Password used to access the data store. Req - ConnectionUserName Username used to access the data store. Req - ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: {DB_Driver}r {HOST} : {PORT} Examples: postgresql: jdbc:postgresql:\/\/congo21.unraveldata.com:7432\/hive Req - " }, 
{ "title" : "JDBC Configurations", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-JDBCConfigurations", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ JDBC Configurations", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Set By User Unit Default prepend acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from t...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Set By User Unit Default prepend acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. count 30 acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. ms 1000 .breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false bool False maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. sec 0 maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. sec 0 maxpoolsize The maximum connections in the connection pool. count 5 OPTIONAL DATA PAGE prepend db.c3p0.idleconnectiontestperiod Yes 0 databasePattern Yes string dname * print.metastore.stats Yes bool False " }, 
{ "title" : " Unravel EMAIL Properties", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-UnravelEMAILProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/  Unravel EMAIL Properties", 
"snippet" : "Property Set By User Set By User Unit Default prepend monitoring.alert.email.enabled Enables email alerts. true: enables alerts false: disables bool true report.user.email.domain Default email domain used for email alerts. localhost.local Opt string - login.admins Comma separated list of email recip...", 
"body" : " Property Set By User Set By User Unit Default prepend monitoring.alert.email.enabled Enables email alerts. true: enables alerts false: disables bool true report.user.email.domain Default email domain used for email alerts. localhost.local Opt string - login.admins Comma separated list of email recipients. Recipient can be defined by complete email address or email name with report.user.email.domain E.g., admin,support@ unraveldata.com Req string - prepend mail.smtp.from Used for email \"from\" and \"reply-to\" headers Req string - mail.smtp2.from Used for email \"from\" and \"reply-to\" headers Opt string - mail.smtp.port Port number 25 mail.smtp.auth Enable\/ SMTP authentication. Note mail.smtp.pw Req bool false mail.smtp.starttls.enable Use start-TLS. bool false mail.smtp.ssl.enable Use SSL right from the start. bool false mail.smtp.user Username for SMTP authentication Note Opt string - mail.smtp.pw Password for SMTP authentication Note Opt string - mail.smtp.host Host for SMTP server string l mail.smtp.localhost A domain name for apparent sender; must have at least one dot (e.g. organization.com) string localhost.local mail.smtp.debug Enable debug mode. bool false " }, 
{ "title" : "Airflow", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Airflow", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Airflow", 
"snippet" : "Property Description Set By User Unit Default prepend: available Notes if airflow is currently available false: not available true: available yes bool - server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ yes url - protocol Type of connection, i.e., https or http https login.na...", 
"body" : " Property Description Set By User Unit Default prepend: available Notes if airflow is currently available false: not available true: available yes bool - server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ yes url - protocol Type of connection, i.e., https or http https login.name Airflow login username. yes string - login.password Password for login username. yes string - status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow web UI. Default unlimited. bytes 0 no prepend airflow.look.back.num.days Date range for workflows, specified in days to look back, i.e., -5 is past 5 days, count -5 " }, 
{ "title" : "Auto Action", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-AutoAction", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Auto Action", 
"snippet" : "Property Definition Set By User Unit Default prepend auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. True: enables alerts False: disables alerts bool false auto.action.default.snooze.period.ms The time repeated violations are be ignor...", 
"body" : " Property Definition Set By User Unit Default prepend auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. True: enables alerts False: disables alerts bool false auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app 0: snooze is turned off > 0: no upper bound ms 1 hour (3,600,000 ms) " }, 
{ "title" : "HBASE", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-HBASE", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ HBASE", 
"snippet" : "AMBARI and CDH Property Definition Set By User Unit Default Required prepend source.type Source of metrics. Supported values - AMBARI & CDH. Yes rest.url Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443) Format: http[s]:\/\/your....", 
"body" : "AMBARI and CDH Property Definition Set By User Unit Default Required prepend source.type Source of metrics. Supported values - AMBARI & CDH. Yes rest.url Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443) Format: http[s]:\/\/your.ambari.server[:port]\" Example: http:\/\/your.ambari.server http:\/\/your.ambari.serve r : - rest.user Username for rest api Yes CSL rest.pwd Password for rest api Yes CSL clusters Cluster names to monitor Yes CSL Optional NOTE: prepend rest.ssl.enabled Yes bool master.port Port for master server web json jmx end point. For AMBARI: 16010 For CDH: 60010 Yes number regionserver.port port for region server web json jmx end point. For AMBARI: 16030 For CDH: 60030 Yes number metric.poll.interval Polling interval in minutes Yes min 5 http.conn.timeout Polling connection timeout in seconds sec 5 http.read.timeout Polling read timeout in seconds Yes sec 10 http.poll.parallelism Polling parallelism, no. of cores Yes number 2 alert.average.threshold Threshold factor above average value for alerts alerts Yes seco 1.2 (120%) " }, 
{ "title" : "Impala", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Impala", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Impala", 
"snippet" : "Property Definition Set By User Unit Default prepend data.source cloudera.manager.url yes path cloudera.manager.port yes number cloudera.manager.username yes string cloudera.manager.password yes string cloudera.manager.cluster.name.list List of clusters yes CSL Impalad as the Data Source prepend dat...", 
"body" : " Property Definition Set By User Unit Default prepend data.source cloudera.manager.url yes path cloudera.manager.port yes number cloudera.manager.username yes string cloudera.manager.password yes string cloudera.manager.cluster.name.list List of clusters yes CSL Impalad as the Data Source prepend data.source Can be cm or impalad yes impalad impalad.nodes Node list in the form of IP Address:Port yes CSL Insights - no need to set prepend cloudera.manager.impala.num.queries.limit Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. count 1000 cloudera.manager.impala.poll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 The following properties should be fine and don't need to be changed. impala.ddl bool false cloudera.manager.impala.look.back.days count -5 hitdoc.impala.operator.info 20480 impala.qa bool false impala.events.stalestats.threshold.bytes bytes 1000 impala.events.stalestats.ratio percent 0.2 impala.events.longop.time.millis ms 2000 impala.events.longop.ratio percent 0.2 impala.events.cost.diff.bytes bytes 500000000 impala.events.skew.time.millis ms 500000000 impala.events.skew percent 1.5 " }, 
{ "title" : "KAFKA Monitoring", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-KAFKAMonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ KAFKA Monitoring", 
"snippet" : "Property Definition Set By User Unit Example prepend clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Yes CSL c1, c2 {clusterName}.bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluste...", 
"body" : " Property Definition Set By User Unit Example prepend clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Yes CSL c1, c2 {clusterName}.bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. Yes CSL localhost:9092,localhost:9093 {clusterName}.jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. Yes CSL kafka-node1,kafka-node2 {clusterName}.jmx.{kafka-node1}..host Yes localhost To locate Kafka and JMX ports: Cloudera Manager Alternatively, you may lookup up that information in the broker nodes of Zookeeper CLI. HDP JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template {clusterName}.jmx.{kafka-node1}.port Yes number 5005 {clusterName}.jmx.{kafka-node2}..port Yes number 2010 servers Yes CSL kafka-test1,kafka-test2 insight.interval_min min 15 insight.sw_size 30 insight.num_ignored_intervals 2 insight.lag_threshold 100 prepend check.interval sec 30 ignore.topics comma separated list of topics CSL __consumer_offsets, connect-configs, connect-offsets, connect-status history.size count 5 " }, 
{ "title" : "Monitoring", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Monitoring", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Monitoring", 
"snippet" : "Property Description Set By User Unit Default prepend JavaScript Rules js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 js_rules.cool.off.period Alert action cool-off period. 1800 File System Related Rules File system related ...", 
"body" : " Property Description Set By User Unit Default prepend JavaScript Rules js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaulation. 60 js_rules.cool.off.period Alert action cool-off period. 1800 File System Related Rules File system related rules (disk usage) are evaluated separately. fs_rules.eval.interval Frequency, in seconds, to evaluate filesystem specific monitoring rules. 0: disables evaluation. 60 DB Status Monitoring db.status.check.interval Frequency, in seconds, database basic status is queried 0: disables evaluation. 30 DB Performance Monitoring db.performance.check.interval Frequency, in seconds, database performance metrics are gathered. 0: disables evaluation. 30 db.performance.query Default query: select count(*) from (select 1 from blackboards limit 1000) b see note Zookeeper Monitoring zookeeper.check.interval Frequency, in seconds, Zookeeper metrics should be queried. 0: disables monitoring. 30 zookeeper.history.size How many data sets consider in computing average values (for historical data). 5 Kafka Monitoring kafka.check.interval Frequency, in seconds,Kafka metrics should be queried. 0: disables monitoring. 30 kafka.ignore.topics Zookeeper topics to be ignored during Kafka monitoring. Topics can be ignored from different reasons, e.g. all internal topics are ignored. __consumer_offsets, connect-configs, connect-offsets, connect-status kafka.history.size Number of data sets stored in the memory and used as historical data. 5 ElasticSearch Monitoring elastic.check.interval Frequency, in seconds, ElasticSearch metrics should be queried. 0: disables monitoring. 30 elastic.history.size Number of data sets stored in the memory and used as historical data. 5 " }, 
{ "title" : "Ondemand Configurations for Reports", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-OndemandConfigurationsforReports", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Ondemand Configurations for Reports", 
"snippet" : "Property Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server - Celery Configurations com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related para...", 
"body" : " Property Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server - Celery Configurations com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: sqla+mysql - unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: db+mysql+pymysql - HiveServer2 Configurations prepend: host FQDN or IP-Address of the HiveServer2 instance port Port for the HiveServer2 instance number 10000 authentication \"KERBEROS\" or \"LDAP\" or \"CUSTOM\" - - kerberos.service.name Used with unravel.hive.server2.authentication='KERBEROS' only. Example value is 'hive' (note this is not the fully-qualified form of hive\/$fqdn@$realm) string - password Use with unravel.hive.server2.authentication='LDAP' or 'CUSTOM' only string - thrift.transport or \"TTransportBase\" for custom advanced usage - prepend: kerberos.principal Required when unravel.hive.server2.authentication='KERBEROS' only Eg., user1@xyz.com string - kerberos.keytab.path Required only when unravel.hive.server2.authentication='KERBEROS' This keytab file will be used to init and renew Kerberos Tickets path - Sessions prepend: python.enabled Enables On-demand Sessions features tab in the UI true: Sessions enabled False: Sessions disabled True yarn.timeline-service.webapp.address Host name or address of the ATS - Small Files and File Reports NOTE: unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small File and File Reports. When set to true, you should set com.unraveldata.ngui.sfhivetable.schedule.enabled=false to prevent unnecessary calls. True: disables the functionality in the Backend and UI. False: enables the ability to generate the reports. bool True com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to fetch fsimage from the cluster. When unravel.python.reporting.files.disable is set to True, this should be set to False to prevent unnecessary calls. True: Smalls Files feature is enabled False: Small Files disabled. bool True com.unraveldata.ngui.sfhivetable.schedule.interval Controls the frequency Unravel fetches fsimage from the cluster. hour 23h Small Files NOTE: unravel.python.reporting.smallfiles.hadoop.heap.in.mb Controls the JVM heap memory size of a hadoop command in MB - File Reports NOTE: prepend: huge_files_threshold_size file size >= threshold_size bytes 100GB huge_files_min_files number of files in directory >= min_files count 1 huge_files_top_n_dirs maximum number of directories to display count 10 medium_files_max_threshold_size file size <= max_threshold_size bytes 10GB medium_files_min_threshold_size file size >= min_threshold_size bytes 5GB medium_files_min_files number of files in directory >= min_files count 5 medium_files_top_n_dirs maximum number of directories to display count 20 tiny_files_threshold_size file size <= threshold_size bytes 100KB tiny_files_min_files maximum number of directories to display count 10 tiny_files_top_n_dirs maximum number of directories to display count 30 empty_files_min_files number of files in directory >= min_files count 10 empty_files_top_n_dirs maximum number of directories to display count 03 " }, 
{ "title" : "OOZIE", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-OOZIE", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ OOZIE", 
"snippet" : "Property Description Set By User Unit Default prepend: server.url The Oozie server URL to be monitored by Unravel Req path server.username Yes string server.password Yes string log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x w...", 
"body" : " Property Description Set By User Unit Default prepend: server.url The Oozie server URL to be monitored by Unravel Req path server.username Yes string server.password Yes string log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x will be trimmed from the beginning and only last x characters are kept. count 1000000 prepend disable Whether to disable bringing in Oozie workflows into Unravel. The underlying jobs will not be affected. bool false fetch.num Number of workflows to pull in each API call. count 100 fetch.interval.sec sec 120 retry.sec sec 600 " }, 
{ "title" : "RBAC", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-RBAC", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ RBAC", 
"snippet" : "Property Description Set By User Unit Default prepend: enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName tagcmd string prepend {Mode} c...", 
"body" : " Property Description Set By User Unit Default prepend: enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName tagcmd string prepend {Mode} com.unraveldata.login.mode here login.admins. {Mode} Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - login.admins.readonly. {Mode} Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - rbac. {Mode} A comma separated list of the prefix of LDAP\/saml group to be used as the PROJECT Mode - CLS - rbac. {Mode} {Tag} Defines regular expression used to parse LDAP\/saml groups for generating the TENANTs PROJECT. Value = Tag-(REGEX) PROJECT com.unraveldata.rbac.Mode.tags. Note: PROJECT - CLS - " }, 
{ "title" : "saml.json", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-samljson", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ saml.json", 
"snippet" : "Unravel supports saml2.0. These properties are located in \/usr\/local\/unravel\/etc\/saml.json Property Description Required Example Values entryPoint Identity provider entrypoint, It is required to be spec-compliant when the request is signed. Yes \" http:\/\/congo29.unraveldata.com:9080\/simplesaml\/saml2\/...", 
"body" : "Unravel supports saml2.0. These properties are located in \/usr\/local\/unravel\/etc\/saml.json Property Description Required Example Values entryPoint Identity provider entrypoint, It is required to be spec-compliant when the request is signed. Yes \" http:\/\/congo29.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php issuer Issuer string to supply to identity provider( Environment name). Should match the name configured in Idp Yes \"Congo24\", \"Localhost\" , cert IDP's public signing certificate. Yes Idp Cert String unravel_mapping Mapping saml attributes to Unravel attributes. Specific to unravel Integration. Yes { \"username\":\"userid\", \"groups\":\"ds_groups\" } logoutUrl Base address to call with logout requests (default:entryPoint) No \" http:\/\/congo29.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php privateCert Unravel private cert string to sign Auth requests No Unravel cert string " }, 
{ "title" : "Sensors", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Sensors", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Sensors", 
"snippet" : "All Sensor properties are set via JVM arguments. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend client.rest.queue The queue length for outgoing REST HTTP requests 20000...", 
"body" : "All Sensor properties are set via JVM arguments. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend client.rest.queue The queue length for outgoing REST HTTP requests 20000 client.rest.retryfail Cool-down period after unsuccessful attempt to make REST HTTP request in nanoseconds ns 30 seconds client.rest.conn.timeout.ms REST HTTP request timeout in milliseconds ms 100 client.rest.shutdown.ms Maximum time to wait for orderly shutdown of the REST client (if exceeded some messages still in the queue will be lost) ms 10 client.rest.dns.ttl The period to refresh the DNS info in milliseconds - IP is pre-resolved and kept till the next refresh if no failures are observed ms 6 hours client.rest.priority.retries Certain critical messages have priority flag and their transmission will be reattempted this many times 5 no prepend unravel.server.hostport Unravel server host:port information - Resource Usage Sensor Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend agent.metrics.enabled_keys Comma separated list of metric type names which are enabled for collection availableMemory,cpuUtilization, processCpuLoad,systemCpuLoad, maxHeap,usedHeap,vmRss,gcLoad no prepend unravel.metrics.factor Sampling period scale down factor 1 Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . no prepend metricsCaptureFilter Format allow specifying single ordinals for component IDs as well as ranges and enumerations - e.g.,metricsCaptureFilter=1,2,5-10,turns on metrics collection for components 1, 2, 5 to 10 0-1500 Spark Sensor Property Definition Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend enableLiveUpdates Enable live updates for Spark apps boolean false enableCachingInfo Enable tracking caching info for Spark apps boolean false enableSampling Enable data sampling between operators for Spark apps boolean false Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . prepend clusterID The cluster ID - currently only used in Spark " }, 
{ "title" : "Tagging", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Tagging", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Tagging", 
"snippet" : "Property Description Set By User Unit Default prepend: tagging.script.enabled boolean true app.tagging.script.method.name get_tags app.tagging.script.path string \/tmp\/tag_app.py auto.action.default.snooze.period.ms milliseconds 0 Workflow prepend name utctimestamp timestamp in human readable form yy...", 
"body" : " Property Description Set By User Unit Default prepend: tagging.script.enabled boolean true app.tagging.script.method.name get_tags app.tagging.script.path string \/tmp\/tag_app.py auto.action.default.snooze.period.ms milliseconds 0 Workflow prepend name utctimestamp timestamp in human readable form yyyymmdd T Z " }, 
{ "title" : "Tez", 
"url" : "unravel-4-4/advanced-topics/section-d3e38200.html#UUID-d4a40114-55b1-e458-5601-9ac23c70aadb_id_UnravelProperties-Tez", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/  \/ Tez", 
"snippet" : "Property Description Set By User Unit Default prepend: yarn.timeline-service.webapp.address boolean true yarn.timeline-service.port 8188 tez.app.ats.poll.timeout.millis Airflow milliseconds tez.app.load.interval.period.seconds seconds prepend username password...", 
"body" : " Property Description Set By User Unit Default prepend: yarn.timeline-service.webapp.address boolean true yarn.timeline-service.port 8188 tez.app.ats.poll.timeout.millis Airflow milliseconds tez.app.load.interval.period.seconds seconds prepend username password " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Uninstalling Unravel Server Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel...", 
"body" : " Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Uninstalling Unravel Server Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel " }, 
{ "title" : "Installing Sensors", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/installing-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors", 
"snippet" : "Individual Applications Submitted Through spark-submit Individual Hive Queries...", 
"body" : " Individual Applications Submitted Through spark-submit Individual Hive Queries " }, 
{ "title" : "Individual Applications Submitted Through spark-submit", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/installing-sensors/individual-applications-submitted-through-spark-submit.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP 1. Obtain the Sensor T...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST_IP http:\/\/UNRAVEL_HOST_IP:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Within this directory, locate the sensor file: unravel-agent-pack-bin.zip 2. Run the Sensor to Intercept Spark Apps Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}3000\/hh\/unravel-agent-pack-bin.zip\n# cd {UNRAVEL_SENSOR_PATH}\n# hdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\n# set UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, substitute your local values for: (UNRAVEL_SENSOR_PATH} unravel-agent-pack-bin.zip (UNRAVEL_SENSOR_PATH} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNRAVEL_SENSOR_PATH={UNRAVEL_SENSOR_PATH}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option B: If You Run Spark Apps in yarn-client Mode To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST If you use multiple hosts as clients, on each client # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip When using yarn-client mode, you need to use the --driver-java-options command line option in your default properties file, due to the timing of driver JVM start-up; see the example below. Define spark.executor.extraJavaOptions spark-submit # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip To use the example below, substitute your local values for: (UNZIPPED_ARCHIVE_DEST} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNZIPPED_ARCHIVE_DEST={UNZIPPED_ARCHIVE_DEST}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --driver-java-options \"-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs=spark-$SPARK_VERSION\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR " }, 
{ "title" : "Individual Hive Queries", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/installing-sensors/individual-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard Map...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel # sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel\n# sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Upgrading the Unravel Server and Sensors", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/upgrading-the-unravel-server-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Upgrading the Unravel Server and Sensors", 
"snippet" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unr...", 
"body" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Lines beginning with '\/\/' are comments. Copy the new RPM to each Unravel host. Stop each host simultaneously \/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously \/\/ host1\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host2\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host3\n# sudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties You must enter add license key to unravel.properties After all the RPM upgrades finish, restart Unravel Server on each host simultaneously \/\/ host1\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host2\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host23\n# sudo \/etc\/init.d\/unravel_all.sh start Complete any deployment-specific upgrade steps. Unravel Sensor \n HIGHLIGHTED \n UNRAVEL_HOST_IP \n SPARK_VERSION _X.Y.Z \n HIVE_VERSION_X.Y.Z Upgrade sensors on CDH cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Check you current sensor version. Log in to the Cloudera Manager and click the parcel ( Check for New Parcels UNRAVEL_SENSOR Click the Download Distribute When activating the new sensors, you will be notified that Hive and Spark services must be restarted. Once new sensor activation is completed, the old version is automatically disabled. Upgrade sensors on HDP cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar Upgrade sensors on MAPR cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Uploading Spark Programs to Unravel", 
"url" : "unravel-4-4/advanced-topics/unravel-servers-and-sensors/uploading-spark-programs-to-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading Spark Programs to Unravel", 
"snippet" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their...", 
"body" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: \n HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their location on the spark-submit Example: In yarn-client mode local \n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {Comma separated list of files} \\\n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR The default value of spark.unravel.program.dir Example: In yarn-cluster mode \n --files {comma-separated-list-of-source-files} spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_SOURCE_FILE_DIRECTORY}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {comma-separated-list-of-source-files} \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option 2: Upload a Zip Archive Package all relevant source files into a zip archive. It's advisable to keep the archive small by including only the relevant driver source files. Example: In yarn-client mode local \n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Example: In yarn-cluster mode \n --files $SRC_ZIP_PATH --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $SRC_ZIP_PATH \\\n --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Unravel searches for source files in this order: \n spark.unravel.program.dir Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Applications Program " }, 
{ "title" : "Workflows", 
"url" : "unravel-4-4/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Workflows", 
"snippet" : "Tagging Workflows Tagging a Hive on Tez Query Monitoring Oozie Workflows Monitoring Airflow Workflows...", 
"body" : " Tagging Workflows Tagging a Hive on Tez Query Monitoring Oozie Workflows Monitoring Airflow Workflows " }, 
{ "title" : "Tagging Workflows", 
"url" : "unravel-4-4/advanced-topics/workflows/tagging-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Workflows \/ Tagging Workflows", 
"snippet" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: unravel.workflow.name TenantName-ProjectName-WorkflowName unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank space...", 
"body" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: \n \n unravel.workflow.name TenantName-ProjectName-WorkflowName \n \n unravel.workflow.utctimestamp \n yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: \n SET unravel.workflow.name=\"ETL-Workflow; \n SET unravel.workflow.name=ETL-Workflow; Different runs of the same the same unravel.workflow.name \n different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name Hive Query Example This is a Hive query that was marked as part of the Financial-Tenant-ETL-Workflow \n \n \n \n SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; SET unravel.workflow.utctimestamp=20160201T000000Z; SELECT foo FROM table WHERE … {Rest of Hive Query text goes here} Easy Recipes for Tagging Workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Export the workflow name. export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow Export the UTC timestamp for this run of the workflow. Here, we use bash's date export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Then follow the appropriate instructions: \n Hive on MR query \n Hive on Tez query \n Sqoop job \n Direct MapReduce job \n Spark job \n Pig job \n Impala Job How to Tag a Hive on MR Query Using SET Commands in Hive # hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=${env:WORKFLOW_NAME};\nSET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP};\nselect count(1) from lineitem; How to Tag a Sqoop Job Using –D Command Line Parameters # sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes: https:\/\/issues.apache.org\/jira\/browse\/SQOOP-3061 How to Tag a Direct MapReduce Job Using –D Command Line Parameters # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input \/tmp\/soumitra\/data\/small -output \/tmp\/soumitra\/outsmoke How to Tag a Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name # spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster How to Tag a Pig Job Using –param and SET Commands # pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME;\nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP;\n\nlines = LOAD '\/tmp\/soumitra\/data\/small' using PigStorage('|') AS (line:chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount; How to Tag an Impala Job Using SET Commands # impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql: SET DEBUG_ACTION=\"|unravel.workflow.name::${var:workflowname}|unravel.workflow.utctimestamp::${var:utctimestamp}|\";\nselect * from usstates; Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications | Workflows " }, 
{ "title" : "Tagging a Hive on Tez Query", 
"url" : "unravel-4-4/advanced-topics/workflows/tagging-a-hive-on-tez-query.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Workflows \/ Tagging a Hive on Tez Query", 
"snippet" : "For general information see Tagging Workflows The following properties must be set in\/usr\/local\/unravel\/etc\/unravel.properties. You should adjust the script path and method name parameters (highlighted in red) according to your cluster setup. com.unraveldata.tagging.script.enabled=true com.unravelda...", 
"body" : "For general information see Tagging Workflows The following properties must be set in\/usr\/local\/unravel\/etc\/unravel.properties. You should adjust the script path and method name parameters (highlighted in red) according to your cluster setup. com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path=\/usr\/local\/unravel\/etc\/tag_app.py com.unraveldata.app.tagging.script.method.name=get_tags You can create tagged workflows for tez applications in four (4) ways. 1. Use --hiveconf via hive command. Enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql. set hive.execution.engine=tez; select count(*) from my_test_table; 2.Use theglobal python script for application tagging. Assuming the global script is \/tmp\/tag_app.py, 3.Use--hiveconf via beeline command. Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000' --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n 4.Use thetez.sql script, then run beeline. You must define these the two (2) workflow tags in tez.sql: set hive.execution.engine=tez; set unravel.workflow.name=my_tez_workflow; set unravel.workflow.utctimestamp=20180801T000001Z; select count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000'-f tez.sql\n " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-4/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url UNRAVEL_HOST oozie.server.url UNRAVEL_HOST Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows....", 
"body" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url UNRAVEL_HOST oozie.server.url UNRAVEL_HOST Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows. " }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-4/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that r...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 HIGHLIGHTED Airflow Web UIAccess Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following properties. If you can't find them, add them. Https For Airflow Web UIAccess Set the following three (3) properties. Replace AIRFLOW_WEB_URL http:\/\/ com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true Https For Airflow Web UIAccess Set the following four (4) properties. Replace AIRFLOW_WEB_URL https:\/\/ Replace AIRFLOW_WEB_UI_username AIRFLOW_WEB_UI_password com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true com.unraveldata.airflow.login.name=AIRFLOW_WEB_UI_username com.unraveldata.airflow.login.password= AIRFLOW_WEB_UI_password Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Range By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following property. If you can't find it, add it.Note there's a \"-\" (minus sign) in the value. airflow.look.back.num.days=- X Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Below is a sample script, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add Hooks for Unravel Instrumentation. This script below, example-hdp-client.sh spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application only spark-defaults.conf This script can be invoked to submit an Airflow Spark application via spark-submit PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call Configuration Properties See here " }, 
{ "title" : "How to Write Jolokia JMX MBean", 
"url" : "unravel-4-4/advanced-topics/how-to-write-jolokia-jmx-mbean.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ How to Write Jolokia JMX MBean", 
"snippet" : "To introduce new Jolokia JMX MBean you must: Write the interface annotated with @MXBean. While not required, it is good practice for the name to have the suffix MXBean. Write the class to implement the interface. The class must be annotated using @JsonMBean, @Singleton and @AutoService(JolokiaMBean....", 
"body" : "To introduce new Jolokia JMX MBean you must: Write the interface annotated with @MXBean. While not required, it is good practice for the name to have the suffix MXBean. Write the class to implement the interface. The class must be annotated using @JsonMBean, @Singleton and @AutoService(JolokiaMBean.class) annotations. MBean can be injected by google Guice and used by unravel daemon (or by other java class). MBean must be thread safe unravel daemon (typically for write access), and Jolokia agent (typically for read access). Jolokia MBean is singleton Jolokia MBean interface has one mandatory method: getName() which must return an unique MBean ObjectName Jolokia MBean can contain also operations. Operations are methods that can be called remotely on a MBean. They may: trigger some action on an unravel daemon, have any number of parameters, and can return any supported type. Jolokia supports following MBean attribute types: primitive types and their object equivalents, List, Set, and Map types, POJO type composed of types mentioned above and which can be nested, and objects of other types have to be converted to supported types, e.g. to String. Use the helper class com.unraveldata.jmx.Converter " }, 
{ "title" : "Example", 
"url" : "unravel-4-4/advanced-topics/how-to-write-jolokia-jmx-mbean.html#UUID-760444e4-4007-0d9e-3483-8a504db7348c_id_HowtoWriteJolokiaJMXMBean-Example", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Example", 
"snippet" : "Below we create and then use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean public interface DbStatusMXBean extends JolokiaMBean { boolean isConnectionOk(); } Class DbStatusMBean @JsonMBean @Singleton @AutoService(JolokiaMBean.class) public class DbStatusMBean implements DbStatusMXBean ...", 
"body" : "Below we create and then use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean\npublic interface DbStatusMXBean extends JolokiaMBean {\n\n boolean isConnectionOk();\n} Class DbStatusMBean @JsonMBean\n@Singleton\n@AutoService(JolokiaMBean.class)\npublic class DbStatusMBean implements DbStatusMXBean {\n private static final String JMX_NAME = \"com.unraveldata:type=Monitoring,group=Database,name=DbStatus\";\n\n private boolean isConnectionOk;\n private MBeanStatus mBeanStatus;\n\n public synchronized void setConnectionOk(boolean connectionOk) {\n this.isConnectionOk = connectionOk;\n this.mBeanStatus = new MBeanStatus(LocalDateTime.now());\n }\n\n @Override\n public synchronized boolean isConnectionOk() {\n return isConnectionOk;\n }\n\n @Override\n public synchronized MBeanStatus getMBeanStatus() {\n return mBeanStatus;\n }\n\n @Override\n public String getName() {\n return JMX_NAME;\n }\n} Usage DbStatusMonitor @Singleton\npublic class DbStatusMonitor {\n\n private final DbStatusMBean mBean;\n\n @Inject\n public DbStatusMonitor(DbStatusMBean mBean) {\n this.mBean = mBean;\n }\n\n void execute(Connection sqlConnection) {\n boolean isConnectionOk = testConnection();\n mBean.setConnectionOk(isConnectionOk);\n }\n} " }, 
{ "title" : "Configure JVM Sensor", 
"url" : "unravel-4-4/advanced-topics/configure-jvm-sensor.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configure JVM Sensor", 
"snippet" : "HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)...", 
"body" : " HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) " }, 
{ "title" : "HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"url" : "unravel-4-4/advanced-topics/configure-jvm-sensor/hdp-enable-jvm-sensor-cluster-wide-for-mapreduce2--mr-.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configure JVM Sensor \/ HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"snippet" : "UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2, next click Configs, go to the Advanced tab, and select\/click Advancedmapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace...", 
"body" : " UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2, next click Configs, go to the Advanced tab, and select\/click Advancedmapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 On the top notification banner, click Save In AWU, on the left-hand side, click MapReduce2, next click Configs, go to the Advanced tab, and select\/click Custom mapred-site. Inside Custom mapred-site, click Add Property for MR JVM sensor as follows and use Bulk property add mode. On the top notification banner, click Save. You can manually edit mapred-site.xml without using AWU. The mapred-site.xml \/etc\/hadoop\/conf\/ -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Propagate Unravel resource metrics sensor jar onto all the servers in the cluster. If you have already run the unravel_hdp_setup.sh ssh to the Unravel gateway host server and use guided steps to unzip the Unravel MR jars. With root or sudo access, change directory to \/usr\/local\/unravel-agent curl # cd \/usr\/local\/unravel-agent\n# curl http:\/\/localhost:3000\/hh\/unravel-agent-pack-bin.zip -o unravel-agent-pack-bin.zip\n# unzip -d jars unravel-agent-pack-bin.zip Ensure you have already installed unzip curl 3000 Tar up the \/usr\/local\/unravel-agent # cd \/usr\/local\/\n# tar -cvf unravel-agent.tar .\/unravel-agent\n Copy the unravel-agent.tar untar \/usr\/local untar unravel-agent Restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. " }, 
{ "title" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"url" : "unravel-4-4/advanced-topics/configure-jvm-sensor/cdh-enable-jvm-sensor-cluster-wide-for-mapreduce--mr-.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Advanced Topics \/ Configure JVM Sensor \/ CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"snippet" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor Parcel on CDH+CM In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for ApplicationMaster Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_...", 
"body" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor Parcel on CDH+CM In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for ApplicationMaster Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property><name>mapreduce.task.profile<\/name><value>true<\/value><\/property> <property><name>mapreduce.task.profile.maps<\/name><value>0-5<\/value><\/property> <property><name>mapreduce.task.profile.reduces<\/name><value>0-5<\/value><\/property>\n\/\/ this is one line\n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property>3 Save changes. Deploy client configuration by clicking the deploy glyph () or by using the Actions Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services Monitor the situation and you should see in Unravel UI a Resource Usage Application " }, 
{ "title" : "Appendices", 
"url" : "unravel-4-4/appendices.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices", 
"snippet" : "Server Daemon Reference HBASE Alerts and Metrics...", 
"body" : " Server Daemon Reference HBASE Alerts and Metrics " }, 
{ "title" : "Server Daemon Reference", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html#UUID-5c67ef0d-fbed-2958-8f62-d73c128ea80e_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP daemon unravel_ew_N Event Worker unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP daemon unravel_ew_N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_hostN Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2_N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma_N Metrics Analyzer unravel_ngui aNGular web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s_N Elasticsearch unravel_sw_N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor \\ Impala unravel_zk_N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html#UUID-5c67ef0d-fbed-2958-8f62-d73c128ea80e_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value general unravel properties com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. ...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value general unravel properties com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp hdfs properties com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) 30 jdbc properties unravel.jdbc.username MySQL (embedded or external) username for db unravel unravel.jdbc.password MySQL (embedded or external) password for db random generated for bundled MySQL unravel.jdbc.url This is JDBC URL without username and password jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prodc kafka com.unraveldata.kafka.broker_list embedded Kafka 127.0.0.1:4091 mapreduce com.unraveldata.longest.job.duration.days Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) 2 oozie com.unraveldata.oozie.fetch.interval.sec seconds between intervals for fetching Oozie workflow status 120 com.unraveldata.oozie.fetch.num Max number of jobs to fetch during an interval 100 oozie.server.url URL for accessing Oozie to track workflows http:\/\/localhost:11000\/oozie Zookeeper com.unraveldata.zk.quorum embedded Zookeeper ensemble in form host1:port1,host2:port2, … 127.0.0.1:4181 " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html#UUID-5c67ef0d-fbed-2958-8f62-d73c128ea80e_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to jvm to override other settings unset HADOOP_CONF_DIR The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml as discovered by running \"hadoop fs -ls \" UNRAVEL_DATA_DIR A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data, zk_data tmp_data com.unraveldata.tmpdir \/srv\/unravel UNRAVEL_LISTEN_PORT The Web UI port on the primary or standalone Unravel installation ( service unravel_ngui 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable Root Environment Settings", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html#UUID-5c67ef0d-fbed-2958-8f62-d73c128ea80e_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference \/ Adjustable Root Environment Settings", 
"snippet" : "The optional file \/etc\/unravel_ctl Env Variable Default if not set Description RUN_AS unravel The \/etc\/init.d\/unravel_* USE_GROUP unravel The primary group membership of the user that runs the daemons...", 
"body" : "The optional file \/etc\/unravel_ctl Env Variable Default if not set Description RUN_AS unravel The \/etc\/init.d\/unravel_* USE_GROUP unravel The primary group membership of the user that runs the daemons " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-4/appendices/server-daemon-reference.html#UUID-5c67ef0d-fbed-2958-8f62-d73c128ea80e_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and ...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~3.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. " }, 
{ "title" : "HBASE Alerts and Metrics", 
"url" : "unravel-4-4/appendices/hbase-alerts-and-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ HBASE Alerts and Metrics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Alerts", 
"url" : "unravel-4-4/appendices/hbase-alerts-and-metrics.html#UUID-c612f700-23e8-f1a4-26a9-f4f2ce81bc5d_id_HBASEAlertsandMetrics-Alerts", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ HBASE Alerts and Metrics \/ Alerts", 
"snippet" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offl...", 
"body" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region in transition beyond threshold period. If a region server is dead, this is common. If not run \"hbase hbck\" to see if your HBase cluster has corruptions. Server availability Dead region servers Check region server logs for more information Performance Region servers with reads > 20% of avg Region server hotspotting. Split regions or randomize the keys. Region servers with writes > 20% of avg Region server hotspotting. Split regions or randomize the keys. Regions within a table with reads > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a table with writes > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a regionserver with reads > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Regions within a regionserver with writes > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Load, osload > 20% of avg Check for compactions, regions in transition and server logs Balancer not running Enable Blancer # of compactions and length of compaction Disable periodic automatic major compactions by setting - hbase.hregion.majorcompaction to 0 Storage Regionservers with storage (storefilesie sum) > 20% of avg Split or randomize the keys Regions within a table with storage (storefilesie sum) > 20% of avg for that table Split or randomize the keys Temporal e.g. requests > 20% higher for the last 1 hour as compared to the prior 3 hours (just an example) Check master and region server alerts or environment issues which could be slowing down the read\/write " }, 
{ "title" : "HBase Cluster Metrics", 
"url" : "unravel-4-4/appendices/hbase-alerts-and-metrics.html#UUID-c612f700-23e8-f1a4-26a9-f4f2ce81bc5d_id_HBASEAlertsandMetrics-HBaseClusterMetrics", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Appendices \/ HBASE Alerts and Metrics \/ HBase Cluster Metrics", 
"snippet" : "Master\/Cluster Metrics JMX Metrics Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseco...", 
"body" : "Master\/Cluster Metrics JMX Metrics Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseconds numDeadRegionServers Number of dead Region Servers count numRegionServers Number of live Region Servers count ritCount The number of regions in transition count ritCountOverThreshold The number of regions that have been in transition longer than a threshold time seconds ritOldestAge The age of the longest region in transition, in milliseconds millliseconds OS Metrics (Ambari Only) OS Metrics Description Unit jvm_* jvm metrics number rpc_* rpc metrics number Region Server Metrics JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local DataNode, 0-100 percentage readRequestCount The number of read requests received count regionCount The number of regions hosted by the regionserver count slowOPCount The number of operations we thought were slow. OP: delete, get, put, increment, append count storeFileSize Aggregate size of the store files on disk bytes writeRequestCount The number of write requests received count OS Metrics (Ambari Only) OS Metrics Description Unit cpu_user cpu percentage disk.disk_free Amount of free disk space bytes disk.write_bps Number of bytes written per second to disk. bytes per second disk.read_bps Number of bytes read per second to disk. bytes per second load.load_one load number memory.mem_free Percentage of free memory. percentage network.bytes_in Total number incoming bytes to network. bytes network.bytes_out Total number outgoing bytes to network. bytes Table\/Region Metrics Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served bytes readRequestCount Number of read requests this region server has answered count writeRequestCount Number of mutation requests this region server has answered count " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-4/troubleshooting.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Troubleshooting", 
"snippet" : "Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround:...", 
"body" : " Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround: " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "unravel-4-4/troubleshooting/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh Email the bundle to the Unravel support team. " }, 
{ "title" : "Release Notes", 
"url" : "unravel-4-4/release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes", 
"snippet" : "v4.4 v4.4.1 v4.4.1 - Configuration Properties v4.4.1.1 v4.4.2.0 v4.4.2.1 v4.4 - Configuration Properties v4.4.x - Upgrade Instructions...", 
"body" : " v4.4 v4.4.1 v4.4.1 - Configuration Properties v4.4.1.1 v4.4.2.0 v4.4.2.1 v4.4 - Configuration Properties v4.4.x - Upgrade Instructions " }, 
{ "title" : "v4.4", 
"url" : "unravel-4-4/release-notes/v4-4.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Software Version", 
"snippet" : "Release Date: 09\/17\/2018 for 4.4 For details on downloading updates see here 4.4.0.1 Hotfix...", 
"body" : "Release Date: 09\/17\/2018 for 4.4 For details on downloading updates see here 4.4.0.1 Hotfix " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1....", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1. " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Unravel Sensor Upgrade", 
"snippet" : "Not Applicable for Fresh Installation...", 
"body" : " Not Applicable for Fresh Installation " }, 
{ "title" : "New Features", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ New Features", 
"snippet" : "Reports Multiple reports have been added to enable to you manage and tune your cluster and it's resources. You must have OnDemand installed and use MySQL to use these features. You can schedule reports All reports are archived Cluster Optimization Forecasting Small Files Impala Insights have been gr...", 
"body" : " Reports Multiple reports have been added to enable to you manage and tune your cluster and it's resources. You must have OnDemand installed and use MySQL to use these features. You can schedule reports All reports are archived Cluster Optimization Forecasting Small Files Impala Insights have been greatly enhanced. Auto actions templates for long running Impala queries and rogue Impala HDFS RW have been added. HBase Cluster Sessions PostgreSQL 10.2.1 is now the default database embedded in the RPM " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Improved detection of unresponsive YARN applications. (PLATFORM-659) Bug Fixes Customer Fixes Failed jobs are only shown if Application ID is available (CUSTOMER-192) Cluster Workload Report populates correctly. (CUSTOMER-209, CUSTOMER-211) Reduced unravel_ngui daemon startup time. (CUS...", 
"body" : "Improvements Improved detection of unresponsive YARN applications. (PLATFORM-659) Bug Fixes Customer Fixes Failed jobs are only shown if Application ID is available (CUSTOMER-192) Cluster Workload Report populates correctly. (CUSTOMER-209, CUSTOMER-211) Reduced unravel_ngui daemon startup time. (CUSTOMER-244) Enabled \"search\" by app name field on Applications page tabs. (CUSTOMER-263) Removed unnecessary fields from from MapReduce Timeline view. (CUSTOMER-279) Tez tagged workflows are now working correctly. (CUSTOMER-272, CUSTOMER-282) Kafka Added the ability to show more then one active broker at a time (CUSTOMER-93) Need the ability to show Kafka graphs aggregated at cluster level not per broker (UIX-660) com.sun.management.jmxremote.port is disabled for all unravel processes due to JMX vunerability concerns. (PLATFORM-382) Spark Spark UI Navigational issues are fixed (USPARK-122,USPARK-127) Spark Execution DAG arrow direction is fixed (USPARK-120) Spark Event will be shown only if Events are available (USPARK-124) Tez Various improvements in Tez pipeline efficiency (TEZLLAP-110) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Known Issues", 
"snippet" : "YARN applications might be reported as RUNNING even though they are still waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable (SESS-60) AutoActions are not t...", 
"body" : " YARN applications might be reported as RUNNING even though they are still waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable (SESS-60) AutoActions are not triggering for long running Hive apps with Tez as Execution Engine (PLATFORM-643) " }, 
{ "title" : "Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-NewConfConfigurationProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ Configuration Properties", 
"snippet" : "See here Software Upgrade Support New install only; upgrades are not supported....", 
"body" : "See here Software Upgrade Support New install only; upgrades are not supported. " }, 
{ "title" : "4.4.0.1 Hotfix", 
"url" : "unravel-4-4/release-notes/v4-4.html#UUID-baa5fa2c-6e38-acd8-d647-a1999ac0603c_id_v44-4401Hotfix", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4 \/ 4.4.0.1 Hotfix", 
"snippet" : "Release Date: 10\/11\/2018 This hotfix addresses the below issues. Issue ID Component Description CUSTOMER-352 \/ PLATFORM-736 Tez Tez applications don't show up in Unravel UI. TEZLLAP-189 Tez Event Worker will crash when \"com.unraveldata.yarn.timeline-service.webapp.address\" is not configured. CUSTOME...", 
"body" : "Release Date: 10\/11\/2018 This hotfix addresses the below issues. Issue ID Component Description CUSTOMER-352 \/ PLATFORM-736 Tez Tez applications don't show up in Unravel UI. TEZLLAP-189 Tez Event Worker will crash when \"com.unraveldata.yarn.timeline-service.webapp.address\" is not configured. CUSTOMER-358 LDAP LDAP Authentication fails when hive.server2.authentication.ldap.groupFilter is configured. SENSOR-73 Hive Sensors Hive apps don't show up due to differences in API return values in FileSinkDesc API Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.4.1", 
"url" : "unravel-4-4/release-notes/v4-4-1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Software Version", 
"snippet" : "Release Date: 10\/26\/2018 For details on downloading updates see here...", 
"body" : "Release Date: 10\/26\/2018 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3.x Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3.x Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1....", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13 & 5.14 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1. " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Unravel Sensor Upgrade", 
"snippet" : "Yes...", 
"body" : " Yes " }, 
{ "title" : "New Features", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ New Features", 
"snippet" : "Reports Multiple reports have been added to enable to you manage and tune your cluster and it's resources. In order to use these reports, you must have OnDemand installed and use MySQL. Added in 4.4.0 You can schedule reports All report, unless noted, are archived Cluster Optimization Forecasting Sm...", 
"body" : " Reports Multiple reports have been added to enable to you manage and tune your cluster and it's resources. In order to use these reports, you must have OnDemand installed and use MySQL. Added in 4.4.0 You can schedule reports All report, unless noted, are archived Cluster Optimization Forecasting Small Files Added in 4.4.1 Top Hive X Queue Analysis File Reports Impala Insights Auto actions templates for long running Impala queries and rogue Impala HDFS RW have been added. HBase Cluster Support for HBase Cluster has been added. Sessions Provides the ability to tune\/auto tune your applications. PostgreSQL 10.2.1 is now the default database embedded in the RPM. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements More then one active broker can now be displayed. (CUSTOMER-93) Time picker for Kafka cluster now offers options for 1, 2, 6, and 12 hours. (CUSTOMER-94) Support added for EC2 instance profiles tied to IAM roles for AWS authentication. (CUSTOMER-258) Added app status \"WAITING\". (CUSTOME...", 
"body" : "Improvements More then one active broker can now be displayed. (CUSTOMER-93) Time picker for Kafka cluster now offers options for 1, 2, 6, and 12 hours. (CUSTOMER-94) Support added for EC2 instance profiles tied to IAM roles for AWS authentication. (CUSTOMER-258) Added app status \"WAITING\". (CUSTOMER-278) Workflow instances are now configurable for past and future runs. (CUSTOMER-291) Workload heat map report contains more detailed information, including ability to drill down to specific apps, users, etc. (CUSTOMER-215) REST API Added REST API functionality for data and application, cluster information, and chargeback. (CUSTOMER-208) Added HTTPS support for API endpoints. (CUSTOMER-226) Auto Actions Value being typed is displayed correctly when creating auto actions. (CUSTOMER-76) All metric values found in auto actions panel on application level are displayed. (CUSTOMER-207) Added ability to be able to import \/ export auto actions (CUSTOMER-330) Support for JSON copy as an action is available from auto action list page. (CUSTOMER-344) Units for auto action metrics are now shown in the UI. (Platform-667) Enabled MySQL database partitioning for huge tables. (PLATfORM-306) Bug Fixes Customer Fixes Security and vulnerability fixes. (CUSTOMER-262) Hive I\/O metrics shown in Application tab summary and Table tab for HDP running Hive on Tez. (CUSTOMER-298) LDAP Authentication no longer ails when hive.server2.authentication.ldap.groupFilter Unique Stage IDs are shown in Execution tab for Spark. (CUSTOMER-236) Tez Tez recommendation improvements and new events. (TEZLLAP-75, TEZLLAP-111) Application Timeline Server polling has been improved to ensure DAG data gets captured. (TEZLLAP-53) RM diagnostic logs are now available if a Tez application or DAG fails. (TEZLLAP-128) Cluster ID is now displayed for Tez applications. (TEZLLAP-31) Queue name is now displayed for Tez applications. (TEZLLAP-56) An application with a successful run now has correct data I\/O displayed. (TEZLLAP-161) A Hive on Tez application now has correct data I\/O displayed. (TEZLLAP-179) A Hive on Tez application now has the correct status if the corresponding Tez app has finished. (TEZLLAP-170) A Tez application now has the correct status if one of the DAGs has failed. (TEZLLAP-171) A link from a Tez DAG to the parent Tez application has been added. (TEZLLAP-81) A NumberFormatException is no longer raised when a Tez DAG gets killed. (TEZLLAP-176) Hive on Tez app-Navigation type for DAG now shows correct number of events. (UIX-1314) Hive Sensor Hive apps now show up when there are differences in API return values in FileSinkDesc API. (SENSOR-73) HiveHook support for Tez captures changed config values. (SENSOR-70) Added HiveHook support for Hive >=1.2.0. (SENSOR-72) Impala Impala apps loading correctly. (IMPALA-139, PLATFORM-736) Spark Spark UI now shows information on multiple attempts. (UIX-259) Kafka All topic consumer croups are shown in the Kafka UI. (UIX-1296) Airflow Specialcharacters in Airflow Workflow ID generation are fixed (PLATFORM-700) Workflow compare graph timestamp issue fixed. (PLATFORM-703) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Known Issues", 
"snippet" : "YARN applications might be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not trigg...", 
"body" : " YARN applications might be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not triggering for long running Hive apps with Tez as Execution Engine. (PLATFORM-643) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) Airflow Monitoring does not work consistently. (CUSTOMER-304) DFS Forecasting Report has the wrong AMS URL (REPORT-246) " }, 
{ "title" : "Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4-1.html#UUID-ae52c669-8b90-8540-c0ae-526a9b798970_id_v441-NewConfConfigurationProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ Configuration Properties", 
"snippet" : "See here Please write to us at support@unraveldata.com...", 
"body" : " See here Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.4.1 - Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4-1/v4-4-1---configuration-properties.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1 \/ v4.4.1 - Configuration Properties", 
"snippet" : "NewReport Files Reports & Small Files Report Property Description Unit Default NOTE: unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small File and File Reports. True: disables the functionality in the Backend and UI. False: enables the ability to generate the ...", 
"body" : "NewReport Files Reports & Small Files Report Property Description Unit Default NOTE: unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small File and File Reports. True: disables the functionality in the Backend and UI. False: enables the ability to generate the report. boolean false File Report Property Description Unit Default NOTE: prepend: huge_files_threshold_size file size >= threshold_size bytes 100GB huge_files_min_files number of files in directory >= min_files count 1 huge_files_top_n_dirs maximum number of directories to display count 10 medium_files_max_threshold_size file size <= max_threshold_size bytes 10GB medium_files_min_threshold_size file size >= min_threshold_size bytes 5GB medium_files_min_files number of files in directory >= min_files count 5 medium_files_top_n_dirs maximum number of directories to display count 20 tiny_files_threshold_size file size <= threshold_size bytes 100KB tiny_files_min_files maximum number of directories to display count 10 tiny_files_top_n_dirs maximum number of directories to display count 30 empty_files_min_files number of files in directory >= min_files count 10 empty_files_top_n_dirs maximum number of directories to display count 30 Workflow Property Description Unit Default prepend num.past.instances count 1 num.future.instances count 10 Spark Property Description Unit Default prepend spark.master This property is used if spark.master string yarn Yarn Property Description Unit Default prepend attempt.log.max.containers If number of containers of an YARN app > this value, log is not loaded and and getSizeOfPartition API is not called to check size of log directory. count 500 min.failed.job.duration.for.attempt.log Load YARN log for failed\/killed apps only when duration in seconds > this property, similar to com.unraveldata.min.job.duration.for.attempt.log for successful apps count 0 Deprecated None " }, 
{ "title" : "v4.4.1.1", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Software Version", 
"snippet" : "Release Date: 11\/16\/2018 For details on downloading updates see here...", 
"body" : "Release Date: 11\/16\/2018 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3.x Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3.x Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1....", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1. " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Unravel Sensor Upgrade", 
"snippet" : "No...", 
"body" : " No " }, 
{ "title" : "New Features", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ New Features", 
"snippet" : "Support for CDH 5.15...", 
"body" : " Support for CDH 5.15 " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Recommendations\/Insights for long running MR jobs. (CUSTOMER-340) Support to store external mysql Jars in case of external mysql. (CUSTOMER-415) Improvements in loading snappy compressed spark event log files. (USPARK-164) Bug Fixes TOP X Fixed JSON parsing error for SQL Queries. (REPOR...", 
"body" : "Improvements Recommendations\/Insights for long running MR jobs. (CUSTOMER-340) Support to store external mysql Jars in case of external mysql. (CUSTOMER-415) Improvements in loading snappy compressed spark event log files. (USPARK-164) Bug Fixes TOP X Fixed JSON parsing error for SQL Queries. (REPORT-249) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Known Issues", 
"snippet" : "Airflow Monitoring does not work consistently. (CUSTOMER-304) YARN applications might be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earl...", 
"body" : " Airflow Monitoring does not work consistently. (CUSTOMER-304) YARN applications might be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not triggering for long running Hive apps with Tez as Execution Engine. (PLATFORM-643) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) " }, 
{ "title" : "Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4-1-1.html#UUID-d63275a1-78cc-af8b-c8be-34c4a8e9a115_id_v4411-NewConfConfigurationProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.1.1 \/ Configuration Properties", 
"snippet" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled - enables small and file reports. See here Please write to us at support@unraveldata.com...", 
"body" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled - enables small and file reports. See here Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.4.2.0", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Software Version", 
"snippet" : "Release Date: 11\/30\/2018 For details on downloading updates see here...", 
"body" : "Release Date: 11\/30\/2018 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3.x Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3.x Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1....", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1. " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Unravel Sensor Upgrade", 
"snippet" : "No...", 
"body" : " No " }, 
{ "title" : "New Features", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ New Features", 
"snippet" : "Support for Security Assertion Markup Language (SAML) 2.0 for Login and RBAC. (PLATFORM-824)...", 
"body" : " Support for Security Assertion Markup Language (SAML) 2.0 for Login and RBAC. (PLATFORM-824) " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements db_access.sh can now connect to DB using an encrypted password. (CUSTOMER-465) HTTPS Support for HBASE. (HBASE-78) Support for Hive2 action in Oozie workflow for Hive on Tez. (PLATFORM-820) Bug Fixes Auto action policy now triggers only specified workflows. (CUSTOMER-400) Impala events ...", 
"body" : "Improvements db_access.sh can now connect to DB using an encrypted password. (CUSTOMER-465) HTTPS Support for HBASE. (HBASE-78) Support for Hive2 action in Oozie workflow for Hive on Tez. (PLATFORM-820) Bug Fixes Auto action policy now triggers only specified workflows. (CUSTOMER-400) Impala events are shown in Operations page for Inefficient Applications. (PLATFORM-909) Oozie Wofkflow Hive on Tez workflow flows issues fixed. (PLATFORM-777) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Known Issues", 
"snippet" : "YARN applications may be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not trigger...", 
"body" : " YARN applications may be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not triggering for long running Hive apps with Tez as Execution Engine. (PLATFORM-643) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) " }, 
{ "title" : "Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4-2-0.html#UUID-1277a728-eb59-0058-ffb2-6d0e31cddc5d_id_v4420-NewConfConfigurationProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.0 \/ Configuration Properties", 
"snippet" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled: enables small files and file reports. See here Please write to us at support@unraveldata.com...", 
"body" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled: enables small files and file reports. See here Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.4.2.1", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Software Version", 
"snippet" : "Release Date: 1\/09\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 1\/09\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-SoftwareUpgradeSupport", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3.x Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3.x Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1....", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 5.12, 5.13, 5.14, 5.15 (incl. Spark 2.3.x) with Kerberos. HDP: On-premise HDP 2.6.3.0 with Kerberos & spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1. " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Unravel Sensor Upgrade", 
"snippet" : "No...", 
"body" : " No " }, 
{ "title" : "New Features", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-NewFeaturesNewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements None Bug Fixes Customer Unravel using global resources to make connections to internet (CUSTOMER-449)...", 
"body" : "Improvements None Bug Fixes Customer Unravel using global resources to make connections to internet (CUSTOMER-449) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-KnownKnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Known Issues", 
"snippet" : "YARN applications may be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not trigger...", 
"body" : " YARN applications may be reported as RUNNING even though they're waiting to be scheduled. (PLATFORM-715) Global search returns multiple results for single Hive Query ID. (UIX-1224) CMP API issue picks up data ~4 days later than the earliest data point viewable. (SESS-60) Auto actions are not triggering for long running Hive apps with Tez as Execution Engine. (PLATFORM-643) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) " }, 
{ "title" : "Configuration Properties", 
"url" : "unravel-4-4/release-notes/v4-4-2-1.html#UUID-cdd6a268-9e1b-c567-f9d8-500a42521fad_id_v4421-NewConfConfigurationProperties", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.2.1 \/ Configuration Properties", 
"snippet" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled: enables small files and file reports. See here Please write to us at support@unraveldata.com...", 
"body" : "New com.unraveldata.ngui.sfhivetable.schedule.enabled: enables small files and file reports. See here Please write to us at support@unraveldata.com " }, 
{ "title" : "New", 
"url" : "unravel-4-4/release-notes/new.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Timezone of HDFS servers", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-TimezoneofHDFSservers", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ Timezone of HDFS servers", 
"snippet" : "Property Description com.unraveldata.hdfs.timezone If timezone is not set then error message is logged and UTC timezone is used as fallback option....", 
"body" : " Property Description com.unraveldata.hdfs.timezone If timezone is not set then error message is logged and UTC timezone is used as fallback option. " }, 
{ "title" : "JCTopic Sensor", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-JCTopicSensor", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ JCTopic Sensor", 
"snippet" : "Property Description Default com.unraveldata.jc.topic.hdfs_ls.delay.millis Number of milliseconds to wait between calls of 'hdfs -ls' commands. Set 0 for no wait at all. 1000 com.unraveldata.jc.topic.rm_based_log_fetcher.delay.secs Number of seconds wait between two runs of RM based log name fetcher...", 
"body" : " Property Description Default com.unraveldata.jc.topic.hdfs_ls.delay.millis Number of milliseconds to wait between calls of 'hdfs -ls' commands. Set 0 for no wait at all. 1000 com.unraveldata.jc.topic.rm_based_log_fetcher.delay.secs Number of seconds wait between two runs of RM based log name fetcher.Value unit is second. Set 0 to disable RM based fetcher. 30 " }, 
{ "title" : "Impala", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-Impala", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ Impala", 
"snippet" : "Property Description Default com.unraveldata.cloudera.manager.impala.poll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. 60000 com.unraveldata.cloudera.manager.impala.num.queries.limit Maximum number of queries that can be returned by a singl...", 
"body" : " Property Description Default com.unraveldata.cloudera.manager.impala.poll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. 60000 com.unraveldata.cloudera.manager.impala.num.queries.limit Maximum number of queries that can be returned by a single REST API call to the Cloudera Manager API. 1000 " }, 
{ "title" : "Disk Monitoring", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-DiskMonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ Disk Monitoring", 
"snippet" : "com.unraveldata.monitoring.js_rules.eval.interval com.unraveldata.monitoring.js_rules.cool.off.period com.unraveldata.monitoring.fs_rules.eval.interval com.unraveldata.monitoring.db.status.check.interval com.unraveldata.monitoring.db.performance.check.interval com.unraveldata.monitoring.db.performan...", 
"body" : "com.unraveldata.monitoring.js_rules.eval.interval com.unraveldata.monitoring.js_rules.cool.off.period com.unraveldata.monitoring.fs_rules.eval.interval com.unraveldata.monitoring.db.status.check.interval com.unraveldata.monitoring.db.performance.check.interval com.unraveldata.monitoring.db.performance.query com.unraveldata.monitoring.zookeeper.check.interval com.unraveldata.monitoring.zookeeper.history.size com.unraveldata.monitoring.kafka.check.interval com.unraveldata.monitoring.kafka.ignore.topics com.unraveldata.monitoring.kafka.history.size com.unraveldata.monitoring.elastic.check.interval com.unraveldata.monitoring.elastic.history.size Properties are described in detail here Disk Monitoring " }, 
{ "title" : "Ondemand Configurations", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-OndemandConfigurations", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ Ondemand Configurations", 
"snippet" : "Property Description Required Values unravel.server.ip FQDN or IP-Address of Unravel Server Yes FQDN or IP-Address of Unravel Server Celery Configurations unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel proper...", 
"body" : " Property Description Required Values unravel.server.ip FQDN or IP-Address of Unravel Server Yes FQDN or IP-Address of Unravel Server Celery Configurations unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: sqla+mysql No sqla+mysql:\/\/$DB_CONNECTION_STRING unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: db+mysql+pymysql No db+mysql+pymysql:\/\/$DB_CONNECTION_STRING com.unraveldata.ngui.proxy.celery Yes http:\/\/localhost:5000 HiveServer2 Configurations unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance Yes FQDN or IP-Address of the HiveServer2 instance unravel.hive.server2.port Port for the HiveServer2 instance No Default: 10000 unravel.hive.server2.authentication No Default: NONE or \"KERBEROS\" or \"LDAP\" or \"CUSTOM\" unravel.hive.server2.kerberos.service.name Used with unravel.hive.server2.authentication='KERBEROS' only. Example value is 'hive' (note this is not the fully-qualified form of hive\/$fqdn@$realm) No Default: NONE or any name string com.unraveldata.kerberos.principal Required when unravel.hive.server2.authentication='KERBEROS' only Eg., user1@xyz.com No User Name string com.unraveldata.kerberos.keytab.path Required when unravel.hive.server2.authentication='KERBEROS' only This keytab file will be used to init and renew Kerberos Tickets No Absolute Path string to keytab file for the user unravel.hive.server2.password Use with unravel.hive.server2.authentication='LDAP' or 'CUSTOM' only No Default: NONE or any password string unravel.hive.server2.thrift.transport No Default: NONE or \"TTransportBase\" for custom advanced usage Small Files Configurations unravel.python.reporting.smallfiles.hadoop.heap.in.mb Controls the JVM heap memory size of a hadoop command in MB No Default: NONE or any number com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to fetch fsimage from the cluster. If false, the Small Files feature will be turned off. No Default: true or false com.unraveldata.ngui.sfhivetable.schedule.interval In Unit of Minute, controls the frequency Unravel fetches fsimage from the cluster. Example value: 10 Recommend value: default, or 10080 (1 week) No Default: 1440 Sessions Configurations com.unraveldata.python.enabled Enables On-demand Sessions features tab in the UI Yes Default: true or false com.unraveldata.yarn.timeline-service.webapp.address Host name or address of the ATS No Host name or address of the ATS com.unraveldata.yarn.timeline-service.port Host name or address of the ATS No Host name or address of the ATS " }, 
{ "title" : "Deprecated", 
"url" : "unravel-4-4/release-notes/new.html#UUID-9db2f3b7-0c73-9e44-ff43-0afc79e2b008_id_v44-ConfigurationProperties-Deprecated", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ New \/ Deprecated", 
"snippet" : "com.unraveldata.cloudera.manager.version com.unraveldata.cloudera.manager.cluster.name.list com.unraveldata.cloudera.manager.service.impala.name com.unraveldata.filesystem.volumes.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark...", 
"body" : "com.unraveldata.cloudera.manager.version com.unraveldata.cloudera.manager.cluster.name.list com.unraveldata.cloudera.manager.service.impala.name com.unraveldata.filesystem.volumes.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark " }, 
{ "title" : "v4.4.x - Upgrade Instructions", 
"url" : "unravel-4-4/release-notes/v4-4-x---upgrade-instructions.html", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.x - Upgrade Instructions", 
"snippet" : "PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL either as an internal or extern...", 
"body" : " PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL either as an internal or external database 4.4.x you might be using with PostgreSQL which was bundled " }, 
{ "title" : "The following upgrade options arenotsupported:", 
"url" : "unravel-4-4/release-notes/v4-4-x---upgrade-instructions.html#UUID-d91fcb79-eb60-f0ba-99ca-eec4c151930b_id_v44x-UpgradeInstructions-Thefollowingupgradeoptionsarenotsupported", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.x - Upgrade Instructions \/ The following upgrade options arenotsupported:", 
"snippet" : "Upgrade from 4.3.x embedded MySQL to 4.4.1 using embedded PostgreSQL external MySQL to 4.4.1 using embedded PostgreSQL Upgrade from 4.4.0.x embedded PostgreSQL to 4.4.1 using external MySQL external MySQL to 4.4.1 using embedded PostgreSQL...", 
"body" : " Upgrade from 4.3.x embedded MySQL to 4.4.1 using embedded PostgreSQL external MySQL to 4.4.1 using embedded PostgreSQL Upgrade from 4.4.0.x embedded PostgreSQL to 4.4.1 using external MySQL external MySQL to 4.4.1 using embedded PostgreSQL " }, 
{ "title" : "Supported Installs\/Upgrade", 
"url" : "unravel-4-4/release-notes/v4-4-x---upgrade-instructions.html#UUID-d91fcb79-eb60-f0ba-99ca-eec4c151930b_id_v44x-UpgradeInstructions-SupportedInstallsUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.4 \/ Release Notes \/ v4.4.x - Upgrade Instructions \/ Supported Installs\/Upgrade", 
"snippet" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel...", 
"body" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel for MySQL Upgrade from 4.4.x If your database is MySQL If the MySQL data has not been migrated to the new partitioned tables, then follow all the steps in Upgrade from 4.3.1.X If MySQL data is already migrated to partitions, then follow all the steps in Upgrade from 4.3.1.X MySql Partitioning and Data Migration To check if the tables are already partitioned, execute the following command: # echo \"show table status like '%blackboards%'\" | \/usr\/local\/unravel\/install_bin\/db_access.sh If your database is PostgreSQL Install RPM Upgrade from 4.3.1.x embedded\/external MySQL to 4.4.1 using external MySQL Unravel 4.4.1.0 uses partitioned MySQL tables to manage the disk space and you must prepare for the upgrade. Prepare for upgrade Check amount of disk space used by MySQL (Space_Used) via the CLI with the command Embedded MySQL # du -sh \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs du -sh Check amount of disk space available in the database partition (Space_Available) via the CLI with the command Embedded MySQL # df -h \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs df -h If Space_Used > Space_Available then follow instructions to move MySQL Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ # mkdir -p \/usr\/local\/unravel\/share\/java\n# sudo cp \/usr\/local\/unravel\/dlib\/mybatis\/*.jar \/usr\/local\/unravel\/share\/java\n# ls -lt \/usr\/local\/unravel\/share\/java Check if the ownership of \/usr\/local\/unravel\/share\/java directory and its contents are accessible by the same user that Unravel server daemons are running as. If not, please run the following command replacing { run_as_user run_as_group switch_to_user # sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/share Upgrade Download RPM Install RPM Complete the instructions at MySql Partitioning and Data Migration Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/dlib\/mybatis\/ # sudo cp \/usr\/local\/unravel\/share\/java\/*.jar \/usr\/local\/unravel\/dlib\/mybatis\/ \n# ls -lt \/usr\/local\/unravel\/dlib\/mybatis\/ Upgrade from 4.2.x 4.2.x (embedded MySQL) -> 4.4.1 (external MySQL) 4.2.x (external MySQL) -> 4.4.1 (external MySQL) Upgrade to 4.3.1.7 then follow the instructions above " }, 
{ "title" : "", 
"url" : "unravel-4-3.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.3 •...", 
"body" : " Unravel 4.3 • " }, 
{ "title" : "Overview", 
"url" : "unravel-4-3/overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "unravel-4-3/overview.html#UUID-80f60195-fedf-e736-01fb-614b2f54d654_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less....", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "unravel-4-3/overview.html#UUID-80f60195-fedf-e736-01fb-614b2f54d654_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "unravel-4-3/overview.html#UUID-80f60195-fedf-e736-01fb-614b2f54d654_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Unravel-Architecture-Diag43.jpg...", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. \n Unravel-Architecture-Diag43.jpg " }, 
{ "title" : "Installation Guides", 
"url" : "unravel-4-3/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides", 
"snippet" : "Cloudera Distribution Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3: Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP MapR Step 1: Install Unra...", 
"body" : " Cloudera Distribution Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3: Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP MapR Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Amazon Elastic MapReduce (EMR) Step 1: Provision and Configure an Unravel EC2 Instance Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision and Configure an Unravel EC2 Instance Manually Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Step 3: Set Up AWS RDS for Unravel DB (Optional) Step 4: VPC Peering for Unravel EC2 Node (Optional) Reconnecting to Your EMR Cluster Deleting the Unravel EC2 Instance Using Unravel to Monitor Jobs in Your EMR Cluster Azure HDInsight Clusters Installation Guide for Unravel VM Step 1: Install Unravel Server for Azure HDinsight Cluster Step 2: Use Script Action to Configure HDinsight Cluster for Unravel Step 3: Updating Unravel Installation Step 4: ARM template for Unravel (Optional) Unravel HDinsight app " }, 
{ "title" : "Cloudera Distribution", 
"url" : "unravel-4-3/install/install-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Cloudera Distribution", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-3/install/install-cdh.html#UUID-ae536195-c377-d888-442c-989cf0fd32df_id_ClouderaDistribution-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Cloudera Distribution \/ Ordered Steps", 
"snippet" : "Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3: Enable Impala APM...", 
"body" : " Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3: Enable Impala APM " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-3/install/install-cdh/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Cloudera Distribution \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"url" : "unravel-4-3/install/install-cdh/install-cdh-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Cloudera Distribution \/ Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"snippet" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job...", 
"body" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. These instructions apply to Unravel Sensor 4.3 Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM Obtain and distribute the parcel from Unravel Server. Put the Hive Hook JAR in AUX_CLASSPATH Configure the gateway automatic deployment of Hive instrumentation. Configure the gateway automatic deployment of Spark instrumentation. \n HIGHLIGHTED When Active Directory Kerberos is used, UNRAVEL_HOST_IP To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 1. Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, you can deactivate it now. Then click Activate 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh no subsitutions HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is Enabled: Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT {ROLE} # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE {ROLE} 3. Configure the Gateway Automatic Deployment of Hive Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways In Cloudera Manager (CM): Go to Hive service. Select the Configuration Search for hive-site.xml Add the xml snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks \n com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger, com.unraveldata.dataflow.hive.hook.HivePostHook \n IMPORTANT! However, the \"HiveServer2 Advanced Configuration Snippet for hive-site.xml\" should have all 3 classes: 2 from Navigator and 1 from Unravel. Add the xml snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Save the changes with optional comment \"Unravel snippet in hive-site.xml \" Perform action Deploy Hive Client Configuration ) or by using the Actions Restart the Hive service. (Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. ) Again, monitor the situation to see if all Hive queries are failing with a class not found or permission problems. If they are failing hive-site.xml Troubleshooting Check Unravel Web UI If queries are running fine and appearing in Unravel Web UI, then you are done. 4. Configure the Gateway Automatic Deployment of Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \" spark-defaults In the Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use the fully qualified DNS or logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) \n spark-conf\/ UNRAVEL_HOST_IP SPARK_VERSION \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark- \n \n 2.2 spark- \n 2.3 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true 5. Save changes. 6. Deploy client configuration by clicking the deploy glyph ( Actions Monitor the situation to see if all Spark queries are failing with a class not found or permission problems. If they are failing spark-defaults.conf 5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Set in Cloudera Manager In Cloudera Manager (CM): Go to YARN Select the Configuration Search for ApplicationMaster Java Opts Base ). Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043 Search for \n MapReduce Client Advanced Configuration Snippet (Safety Valve) for mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property><name>mapreduce.task.profile<\/name><value>true<\/value><\/property>\n<property><name>mapreduce.task.profile.maps<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.reduces<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> 6. Save changes. 7. Deploy client configuration by clicking the deploy glyph ( Actions 8. Cloudera Manager will specify a restart which is not necessary to effect these changes. (click Restart Stale Services Monitor the situation and you should see in Unravel UI a Resource Usage tab showing you mappers and reducers when you view the Application page for any completed MRjob. Restart is important for MR sensor to be picked up by queries submitted via Hiveserver2. 6. (Optional) Advanced Configuration Configuration for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Integrating LDAP Authentication for Unravel Web UI Troubleshooting \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR Verify that unravel \/user\/unravel\/ \n \n \n ClassNotFound com.unraveldata.dataflow.hive.hook.HivePreHook \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST References \n {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ \n DeployGlyph.png \n package.png " }, 
{ "title" : "Step 3: Enable Impala APM", 
"url" : "unravel-4-3/install/install-cdh/install-cdh-part3-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Cloudera Distribution \/ Step 3: Enable Impala APM", 
"snippet" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM By default, the ImpalaSensor task is enabled. ...", 
"body" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM By default, the ImpalaSensor task is enabled. If user wishes to disable ImpalaSensor, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw Workflow Summary If you want Unravel Server to retrieve Impala query data from ClouderaManager, start at Using CM as the Data Source If you want Unravel Server to retrieve Impala query data from Impala daemons, start at Using Impalad as the Data Source Using CM as the Data Source In order to this you need to add com.unraveldata.data.source=cm \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.cloudera.manager.url CM internal URL. Must start with http:\/\/ com.unraveldata.cloudera.manager.port (Optional) CM port number. You only need to specify this if your Cloudera Manager is not on port 7180. com.unraveldata.cloudera.manager.username CM username com.unraveldata.cloudera.manager.password CM password com.unraveldata.cloudera.manager.version (Optional) CM version. You only need to specify this if your Cloudera Manager is earlier than 5.8.0. Note com.unraveldata.cloudera.manager.cluster.name.list A comma-separated list of cluster names com.unraveldata.cloudera.manager.service.impala.name (Optional) You only need to specify this if the Impala service name is not defaulted as \" impala For example: com.unraveldata.data.source=cm\ncom.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret\ncom.unraveldata.cloudera.manager.port=9997\ncom.unraveldata.cloudera.manager.username=mycmname\ncom.unraveldata.cloudera.manager.password=mycmpassword\ncom.unraveldata.cloudera.manager.version=5_7_0\ncom.unraveldata.cloudera.manager.cluster.name.list=cluster1,cluster2,cluster5\ncom.unraveldata.cloudera.manager.service.impala.name=myimpalaservicename Hints: To find out the cluster name: {cm_url}:{cm_port}\/api\/v13\/clusters\/ To find out the service name: {cm_url}:{cm_port}\/api\/v13\/clusters\/{cluster_name}\/services\/ Substitute your particular values for bracketed ClouderaManager (CM) properties, i.e., {cm_port} Using Impalad as the Data Source Use this option if you want to import data from impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html CM as the data source In order to use impalad as the data source you need to add com.unraveldata.data.source=impalad \/usr\/local\/unravel\/etc\/unravel.properties impalad Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port For example: com.unraveldata.data.source=impalad\ncom.unraveldata.impalad.nodes=IP:port,IP:port,IP:port References https:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Hortonworks Data Platform (HDP)", 
"url" : "unravel-4-3/install/install-hdp.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Hortonworks Data Platform (HDP)", 
"snippet" : "This guide is compatible with: On-premises HDP up to v2.6.3 with Kerberos + spnego enabled...", 
"body" : " This guide is compatible with: On-premises HDP up to v2.6.3 with Kerberos + spnego enabled " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-3/install/install-hdp.html#UUID-b7a29487-0af2-ff9c-99f2-9ca5594e1b15_id_HortonworksDataPlatformHDP-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Ordered Steps", 
"snippet" : "Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : " Step 1: Install Unravel Server on HDP Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "unravel-4-3/install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host mach...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. \n Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility On-premises HDP up to v2.6.3 with Kerberos + spnego enabled Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache version equivalent) Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 \n libaio.x86_64 \n SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway You need to register edge node to Ambari LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please \n do not Access Permissions If Kerberos is in use, a keytab for principal is required for access to: Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Application Timeline Server (ATS) read-only The RUN_AS user must be included in yarn.admin.acl and tez.am the cluster uses kerberos for authentication or the resource manager has been configured in high availability mode, Network The following ports must be open on Unravel Server's host machine(s): \n \n \n Port(s) \n Direction \n Description \n \n 3000 \n Both \n Traffic to and from Unravel UI \n \n HDFS ports \n In \n Traffic from Hadoop cluster to Unravel Server(s) \n \n Hive Metadata DB port \n Out \n For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting \n \n 4043 \n In \n UDP and TCP traffic from the entire cluster to Unravel Server(s) \n \n 8032 \n Out \n Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) \n \n 8188 \n Out \n Traffic from Unravel Server(s) to the ATS server(s) \n \n 3316, 4020, 4091, 4176, 4181-4189 \n Both \n Localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) \n \n 11000 \n Out \n For Oozie only. Traffic from Unravel Server(s) to the Oozie server Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access. For HDP, use Ambari Web UI to create a Gateway node configuration. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh Done The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n Company_and_org \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n 26 \n \n \n com.unraveldata.job.collector.done.log.base \n Only modifiable through Unravel Web UI's configuration wizard. \n \/mr-history\/done \n \n \n com.unraveldata.job.collector.log.aggregation.base \n Only modifiable through Unravel Web UI's configuration wizard. \n \/app-logs\/*\/logs\/ \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n \n admin \n \n \n com.unraveldata.s3.batch.monitoring.interval.sec \n \n Optional \n \n 120 \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n hdfs:\/\/\/user\/spark\/applicationHistory\/,\nhdfs:\/\/\/user\/spark\/sparkApplicationHistory\/,\nhdfs:\/\/\/user\/spark\/spark2ApplicationHistory\/ \n \n \n yarn.resourcemanager.webapp.address \n YARN resource manager web address URL \n http:\/\/example.localdomain:8088 \n \n \n oozie.server.url \n Oozie URL \n http:\/\/example.localdomain:11000\/oozie If Kerberos is enabled add authentication for HDFS. Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using properties klist -kt KEYTAB_FILE . Run Unravel Daemons with Custom User If Ranger is enabled add these permissions. Define your own alt principal with narrow privileges. The alt principal can be unravel rpm X \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/spark-history \n Your alt principal \n read+execute \n Spark event log \n \n hdfs:\/\/spark2-history \n Your alt principal \n read+execute \n Spark2 event log \n \n \n hdfs:\/\/mr-history\/done \n Your alt principal \n read+execute \n MapReduce logs \n \n \n hdfs:\/\/app-logs \n Your alt principal \n read+execute \n YARN aggregation folder \n \n \n hdfs:\/\/apps\/hive\/warehouse (default value of hive.metastore.warehouse.dir) \n Your alt principal \n read+execute \n Obtain table partition sizes \n \n Hive Metastore database GRANT \n hive \n read+execute \n Hive table information Disable Impala Sensor Impala is not officially supported on HDP clusters therefore you should disable the ImpalaSensor by modifying the \/usr\/local\/unravel\/etc\/unravel.properties Open unravel.properties com.unraveldata.sensor.tasks.disabled com.unraveldata.sensor.tasks.disabled=iw Convert Your Unravel Installation to HDP Run the following commands on Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after later RPM upgrades; it does not need to be done each time. Switch User Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/{UNRAVEL_HOST_IP} admin\" \"unraveldata For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2 (Optional): Enable Additional Data Collection \/ Instrumentation for HDP \n signin.png " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "unravel-4-3/install/install-hdp/install-hdp-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us HIGHLIGHTED UNRAVEL_HOST_IP Convert Your Unravel Installation to HDP. # sudo \/etc\/init.d\/unravel_all.sh stop # sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change remains after RPM upgrades. Upda...", 
"body" : "Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us \n HIGHLIGHTED \n UNRAVEL_HOST_IP Convert Your Unravel Installation to HDP. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change remains after RPM upgrades. Update Site-Specific HDP Properties in unravel.properties. Add these properties to\/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n \n com.unraveldata.yarn.timeline-service.webapp.address \n The http address of the Timeline service web application \n \n http:\/\/localhost \n \n \n com.unraveldata.yarn.timeline-service.port \n Timeline service port \n 8188 For example, com.unraveldata.yarn.timeline-service.webapp.address=http:\/\/172.16.1.101\ncom.unraveldata.yarn.timeline-service.port=8188 Open \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh If you are using Spark1 and Spark2 you must com.unraveldata.spark.eventlog.location com.unraveldata.spark.eventlog.location =hdfs:\/\/\/spark1-history\/, \/\/ Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/\n \n\/\/ Add Hive Metastore database information for Unravel Hive Config \njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/{hostname}:3306\/{database_name} \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName={HiveMetastoreUserName} \njavax.jdo.option.ConnectionPassword={HiveMetastorePassword} Log into Ambari Web UI (AWU) to verify the above properties have been set correctly in unravel.properties On the left-hand side of AWU's dashboard, click MapReduce2 Configs Advanced Advanced mapred-site Verify com.unraveldata.job.collector.done.log.base mapreduce.jobhistory.done-dir. On the left-hand side of AWU's dashboard, click YARN Configs Advanced Node Manager Verify com.unraveldata.job.collector.done.log. aggregation.base yarn.nodemanager.remote-app=log-dir On the left-hand side of AWU's dashboard, click Hive Configs Advanced Hive Metastore Verify javax.jdo.option.ConnectionURL Database Host Database URL Verify javax.jdo.option.ConnectionDriverName JDBC Driver Class Verify javax.jdo.option.ConnectionUserName Database Username Start Unravel Server. Note: Unravel must be up for the next step to complete. # sudo \/etc\/init.d\/unravel_all.sh start Install Unravel Hive Hook and Spark Sensor Onto HDP Servers Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows (substitute correct fully qualified host name or IP address for UNRAVEL_HOST_IP Login as root and ensure wget # yum install -y wget From Unravel server (eg. edge node) run on each server that will use instrumentation. Be sure to substitute valid values. \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 \n \n 2.2.0 \n HIVE_VERSION - must be a Hive version that Unravel Server supports: either 1.2.0 or 0.13.0. Be sure to use either of these values in exactly this format. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --hive-version {HIVE_VERSION} --spark-version {SPARK_VERSION} Files are installed locally on the edge node under: \n \/usr\/local\/unravel_client (Hive hook jar) \n \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor jars) \n Manually copy host2 Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU. Completion of this step requires a restart of all affected Hive services in Ambari UI. In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook \ncom.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR \ncom.unraveldata.hive.hook.tcp=true \ncom.unraveldata.host={add unravel gateway internal IP hostname} \n \n\/\/ Find below properties as it may already exists, concatenate it with a comma & no spaces\nhive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook \nhive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook \nhive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook If LLAP is enabled copy the above settings in Custom hive-interactive-site Manual edit hive-site.xml (no AWU) \n hive-site.xml \/etc\/hive\/conf\/ Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar If LLAP is enabled copy above line of code into Advanced hive-interactive-env You can manually edit hive-env.sh without using AWU. The hive-env.sh \/etc\/hive\/conf\/ Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env export HADOOP_CLASSPATH export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar Optionally for Tez, Enable Unravel Tez Instrumentation on all of HDP's Services in the Cluster, Completion of this step requires a restart of all affected Hive services in Ambari UI. Confirm that hive-execution.engine is set to tez. set hive.execution.engine=tez; Using the Ambari Web UI (AWU), configure the Btrace agent for Tez: Append -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 \n tez.am tez.task.launch.cmd-opts Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment we will need to modify \n tez.am Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster Completion of this step requires a restart of all affected Spark services in Ambari UI. Be sure to substitute valid values.for UNRAVEL_HOST_IPX SPARK_VERSION_X.Y. \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark- \n \n 2.2 spark- \n 2.3 Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043\nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs=spark-{SPARK_VERSION_X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs=spark-{SPARK_VERSION_X.Y} \nspark.eventLog.enabled=true You can manually edit spark-defaults.conf without using AWU. The default location for spark-defaults.conf \/usr\/hdp\/current\/SPARK_VERSION_X.Y The cluster only has one spark 1.X version: \/usr\/hdp\/current\/spark-client\/conf For spark 2.X version: \/usr\/hdp\/current\/spark2-client\/conf Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide Completion of this step will require a restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced Search for MR AppMaster Java Heap Size \n current.yarn.app.mapreduce.am -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043 On the top notification banner, click Save In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site: Inside Custom mapred-site Add Property Bulk property add mode On the top notification banner, click Save mapreduce.task.profile=true\nmapreduce.task.profile.maps=0-5\nmapreduce.task.profile.reduces=0-5\nmapreduce.task.profile.params=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP You can manually edit mapred-site.xml without using AWU. The mapred-site.xml \/etc\/hadoop\/conf\/ Optionally for YARN If yarn.acl.enable=true \n yarn.acl.enable=false, \n yarn.admin.acl=userName Confirm that Unravel Web UI Shows Tez Data Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Unravel Web UI may take a few seconds to load Tez data. \n bulk.png \n hdp_part2_image_002.png \n hdp_part2_image_001.png \n image2017-10-25_8-1-4.png " }, 
{ "title" : "MapR", 
"url" : "unravel-4-3/install/install-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ MapR", 
"snippet" : "This guide is compatible with MapR 6.0.0 with MapR Expansion Packs 4.1.1...", 
"body" : " This guide is compatible with MapR 6.0.0 with MapR Expansion Packs 4.1.1 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-3/install/install-mapr.html#UUID-3422c586-93b9-8fe6-abcb-99c6699bfe68_id_MapR-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ MapR \/ Ordered Steps", 
"snippet" : "Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : " Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Step 1: Install Unravel Server on MapR", 
"url" : "unravel-4-3/install/install-mapr/install-mapr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2 on the MapR converged data platform. These instructions apply to Unravel Server 4.2. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (bas...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2 on the MapR converged data platform. These instructions apply to Unravel Server 4.2. \n Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 6.0.0 with MapR Expansion Packs 4.1.1 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache ver. equiv.) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 \n libaio.x86_64 \n SELINUX=permissive \/etc\/selinux\/config \n Elasticsearch needs number of File Descriptors to be at least 65,536. https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/file-descriptors.html HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) On Unravel Edge-node server, please \n do not Access Permissions If Kerberos is in use, a keytab for principal is required for access to: Access to YARN’s “done dir” in HDFS Access to YARN’s log aggregation directory in HDFS Access to Spark event log directory in HDFS Access to file sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network The following ports must be open on Unravel Server's host machine(s): \n \n \n Port(s) \n Direction \n Description \n \n 3000 or 4020 \n Both \n Traffic to and from Unravel UI \n \n HDFS ports \n In \n Traffic from Hadoop cluster to Unravel Server(s) \n \n Hive Metadata DB port \n Out \n For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting \n \n 4043 \n In \n UDP and TCP traffic from the entire cluster to Unravel Server(s) \n \n 8032 \n Out \n Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) \n \n 8188 \n Out \n Traffic from Unravel Server(s) to the ATS server(s) \n \n 3316, 4020, 4091, 4176, 4181-4189 \n Both \n Localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) \n \n 11000 \n Out \n For Oozie only. Traffic from Unravel Server(s) to the Oozie server \n HIGHLIGHTED \n UNRAVEL_HOST_IP Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access If you do not already have a gateway\/edge\/client host provisioned for Unravel server, follow these steps which are needed to enable the hadoop fs For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands on Unravel Server as root NAME, CLDB_LIST,HISTORY_SERVER. # sudo yum install mapr-client.x86_64\n# sudo \/opt\/mapr\/server\/configure.sh -N {NAME} -c -C {CLDB_LIST} -HS {HISTORY_SERVER}\n# sudo yum install mapr-hive.noarch\n# sudo yum install mapr-spark.noarch Configure the Host Before Run the following commands on Unravel Server as root # sudo useradd -g mapr unravel\n# hadoop fs -mkdir \/user\/unravel\n# hadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr \/etc\/unravel_ctl Check available RAM to ensure availability: # free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 (only change this setting on the Unravel gateway\/client machine). And the restart mfs. Install the Unravel Server RPM on the Host Machine. Get the Unravel Server RPM See Download Unravel Software Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv and symlink after installing). Do it before the first install if there is insufficient space on the target paths \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fizup.sh await_fizup.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL Do Host-Specific Post-Installation Actions. Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable additional daemons at this point, see Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties \n \n \n Property \n Description \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n Company_and_org \n \n com.unraveldata.tmpdir \n \n Optional \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n 26 \n \n \n com.unraveldata.hive.hook.topic.num.threads \n \n Optional N N ThousandJobsPerDay \n 1 \n \n \n com.unraveldata.job.collector.done.log.base \n HDFS path to \"done\" directory of MR logs \n \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done \n \n \n com.unraveldata.job.collector.log.aggregation.base \n An HDFS path that helps locate MR job logs to process \n \/tmp\/logs\/*\/logs\/ \n \n \n com.unraveldata.login.admins \n Defines the usernames that can access Unravel Web UI's admin pages. Default is admin \n admin \n \n \n com.unraveldata.spark.eventlog.location \n Where to find Spark event logs \n maprfs:\/\/\/apps\/spark \n \n \n yarn.resourcemanager.webapp.address \n Resource Manager web app address \n http:\/\/example.localdomain:8088 \n \n \n yarn.resourcemanager.webapp.username \n Resource Manager username to login \n \n \n yarn.resourcemanager.webapp.password \n Resource Manager password to login \n \n \n https.protocols \n Enable https access to Resource Manager \n TLSv1.2 \n \n \n javax.jdo.option.ConnectionURL \n A JDBC connection URL \n jdbc:mysql:\/\/example.localdomain:3306\/hive OR \n \n \n javax.jdo.option.ConnectionDriverName \n JDBC driver \n com.mysql.jdbc.Driver OR \n \n \n javax.jdo.option.ConnectionUserName \n Hive metastore user name \n hiveuser \n \n \n javax.jdo.option.ConnectionPassword \n Hive metastore password \n \n \n com.unraveldata.metastore.databasePattern \n \n Optional \n s*|t*|d* \n \n \n oozie.server.url \n Oozie URL \n http:\/\/example.localdomain:11000\/oozie If Kerberos is enabled add authentication for HDFS. Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can find and verify the principal the keytab by # klist -kt KEYTAB_FILE\n The keytab file should have chmod bits 500 and be owned by unravel Run Unravel Daemons with Custom User If Sentry is enabled add these permission. Define your own alt principal with narrow privileges. The alt principal can be admin unravel rpm X \n \n \n Resource \n Principal \n Access \n Purpose \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n Your alt principal \n read \n Spark event log \n \n \n hdfs:\/\/usr\/history\/done \n Your alt principal \n read \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n Your alt principal \n read \n YARN aggregation folder \n \n \n hdfs:\/\/user\/hive\/warehouse \n Your alt principal \n read \n Obtain table partition sizes \n \n \n Hive Metastore access \n hive \n read \n Hive table information Switch User Depending on your cluster security configuration, you will need to run the \n switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. Log into Unravel Web UI. Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} admin unraveldata For the free trial version, use the Chrome web browser. Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. Check Unravel Web UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel Web UI, see the User Guide (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the Advanced Topics \n signin.png " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-3/install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unra...", 
"body" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. Substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION _X.Y.Z Enable Additional Instrumentation on Unravel Server's Host Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client nodes that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Before unravel_mapr_setup.sh wget unzip hive spark-shell Run the shell script unravel_mapr_setup.sh host1 # sudo \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Hive hook jar is installed under: \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2) are made to: \/opt\/mapr\/spark\/spark-2.0.1\/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-env.sh Once the files are present on edge host where Unravel rpm is installed, you can tar these changes\/additions up and put on other hosts, if that is more convenient than running the script. In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server: someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh Confirm and Adjust the Settings in yarn-site.xml. Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> Enable Additional Instrumentation on Other Hosts in the Cluster To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml Run the shell script unravel_mapr_setup.sh Copy the newly edited yarn-site.xml Do a rolling-restart of HiveServer2. Enable Instrumentation manually by updating the following files: hive-site.xml hive-env.sh spark-defaults.conf hadoop-env.sh mapred-site.xml Once the files are updated on edge host where Unravel rpm is installed, you can scp Be sure to substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_X.Y.Z HIVE_VERSION _X.Y.Z Update hive-site.xml Copy the content in \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-site.xml {UNRAVEL_HOST_IP}) <property>\n <name>com.unraveldata.host<\/name>\n <value>{UNRAVEL_HOST_IP}<\/value>\n <description>Unravel hive-hook processing host<\/description>\n<\/property>\n\n<property>\n <name>com.unraveldata.hive.hook.tcp<\/name>\n <value>true<\/value>\n<\/property>\n\n<property>\n <name>com.unraveldata.hive.hdfs.dir<\/name>\n <value>\/user\/unravel\/HOOK_RESULT_DIR<\/value>\n <description>destination for hive-hook, Unravel log processing<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.driver.run.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HiveDriverHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.pre.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HivePreHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.post.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HivePostHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<property>\n <name>hive.exec.failure.hooks<\/name>\n <value>com.unraveldata.dataflow.hive.hook.HiveFailHook<\/value>\n <description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n\n<\/configuration> Update hive-env.sh In \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-env.sh ({HIVE_VERSION_X.Y.Z}). export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-{HIVE_VERSION X.Y.Z}-hook.jar\nexport HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}:\/usr\/local\/unravel_client Update spark-defaults.conf In \/opt\/mapr\/spark\/spark-SPARK_VERSION_X.Y.Z\/conf\/ spark-defaults.conf {UNRAVEL_HOST_IP} {SPARK_VERSION_X.Y.Z} spark.unravel.server.hostport {UNRAVEL_HOST_IP}:4043\nspark.eventLog.dir maprfs:\/\/\/apps\/spark\nspark.history.fs.logDirectory maprfs:\/\/\/apps\/spark\nspark.driver.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=driver\nspark.executor.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=executor Update hadoop-env.sh In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh ({HIVE_VERSION_X.Y.Z}). export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-{HIVE_VERSION_X.Y.Z}.0-hook.jar Update mapred-site.xml. In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/mapred-site.xml {UNRAVEL_HOST_IP} <property>\n <name>mapreduce.task.profile<\/name>\n <value>true<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.maps<\/name>\n <value>0-5<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.reduces<\/name>\n <value>0-5<\/value>\n<\/property>\n\n<property>\n <name>mapreduce.task.profile.params<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value>\n<\/property>\n\n<property>\n <name>yarn.app.mapreduce.am.command-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=172.36.1.126:4043<\/value>\n<\/property>\n\n Make sure the original value of yarn.app.mapreduce.am " }, 
{ "title" : "Amazon Elastic MapReduce (EMR)", 
"url" : "unravel-4-3/install/install-emr.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR)", 
"snippet" : "This guide is compatible with Amazon EMR 5.x Amazon EMR 5.14 (preferred) Step 1: Provision and Configure an Unravel EC2 Instance Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision ...", 
"body" : " This guide is compatible with Amazon EMR 5.x Amazon EMR 5.14 (preferred) Step 1: Provision and Configure an Unravel EC2 Instance Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision and Configure an Unravel EC2 Instance Manually Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Step 3: Set Up AWS RDS for Unravel DB (Optional) Step 4: VPC Peering for Unravel EC2 Node (Optional) Reconnecting to Your EMR Cluster Deleting the Unravel EC2 Instance Using Unravel to Monitor Jobs in Your EMR Cluster " }, 
{ "title" : "Step 1: Provision and Configure an Unravel EC2 Instance", 
"url" : "unravel-4-3/install/install-emr/install-emr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance", 
"snippet" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR deployment. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Im...", 
"body" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR deployment. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision and Configure an Unravel EC2 Instance Manually " }, 
{ "title" : "Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"url" : "unravel-4-3/install/install-emr/install-emr-part1/install-emr-part1-option1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"snippet" : "Introduction This topic explains how to provision an EC2 instance with Unravel Server 4.3.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.14.1 EMR Support Matrix Hadoop 2.8.3 ...", 
"body" : "Introduction This topic explains how to provision an EC2 instance with Unravel Server 4.3.X preinstalled on it, using our CloudFormation template. Requirements Checklist \n \n \n Platform Compatibility \n EC2 Instance Type \n Security Group \/ IAM Role \n AWS Permissions \n \n AWS EMR 5.14.1 EMR Support Matrix Hadoop 2.8.3 Hive 2.3.2 Hue 4.1.0 Spark 2.3.0 Zeppelin 0.7.3 Tez 0.8.4 Livy 0.4.0 Pig 0.17.0 Oozie 4.3.0 \n \n Minimum \n Maximum \n Recommended \n Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Allowed port for inbound access to EMR master node: Port 8020 (Name node access for spark event log) Allowed port for inbound access to EMR core node: Port 50010 (Data node access for spark event log) Port 50020 \n AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings \n \n \n Name \n Default Value \n Description \n \n Stack name \n The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date \n \n InstanceType \n r4.4xlarge \n Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge \n \n KeyName \n mysshkey \n EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. \n \n TrustedSshIPBlock \n 10.10.0.0\/16 \n Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). \n \n UnravelInstanceCount \n 1 \n Leave this as 1. \n \n UnravelVPCBlock \n 10.10.0.0\/16 \n Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). \n \n Zone \n us-east-1a \n Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our CloudFormation Template Download our CloudFormation template from the following public S3 bucket into your local \/tmp folder: https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_aws_marketspace_01.json Use AWS CLI or the AWS Marketplace console to create the Unravel EC2 instance. \n From the AWS CLI run the aws cloudformation Syntax: \n aws cloudformation create-stack --stack-name UnravelEC2 \\ --template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ --parameters ParameterKey=Zone,ParameterValue=AWS_ZONE_NAME \\ ParameterKey=InstanceType,ParameterValue=AWS_INSTANCE_TYPE \\ ParameterKey=KeyName,ParameterValue=my_ssh_key \\ ParameterKey=UnravelVPCBlock,ParameterValue=AWS_VPC_BLOCK \\ ParameterKey=TrustedSshIPBlock, ParameterValue=AWS_SSH_BLOCK \\ --capabilities CAPABILITY_IAM | tee UnravelEC2-stack.json Example: \n aws cloudformation create-stack --stack-name UnravelEC2number \\ --template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ --parameters ParameterKey=Zone,ParameterValue=us-east-1d \\ ParameterKey=InstanceType,ParameterValue=r4.2xlarge \\ ParameterKey=KeyName,ParameterValue=topcat \\ ParameterKey=UnravelVPCBlock,ParameterValue=10.12.0.0\/16 \\ ParameterKey=TrustedSshIPBlock, ParameterValue=0.0.0.0\/0 \\ --capabilities CAPABILITY_IAM | tee UnravelEC2-stack.json \n From AWS Marketplace https:\/\/aws.amazon.com\/marketplace Type \"unravel\" in the search box: In the search results, select Unravel APM for EMR Continue to Subscribe Click Continue to Configuration On the Configure this software \n Fulfillment Option \n Software Version \n Region Click Continue to Launch On the Launch this software Usage Instructions In the Choose Action Launch CloudFormation Click Launch You are now on the CloudFormation portal ( https:\/\/console.aws.amazon.com\/cloudformation\/home....). In the Choose a template section, select Specify an Amazon S3 template URL Next You are now on the Specify Details On the Specify Details settings for this EC2 instance Next On the Options Next rom AWS Marketplace ( https:\/\/aws.amazon.com\/marketplace On the Review Create The deployed application together with its virtual resources is called a CloudFormation \"stack\" and is named UnravelEC2number If you don't see your new stack in the list, wait 30-60 seconds and refresh the page. When the status of the stack changes to CREATE_COMPLETE, select it and expand its Outputs section: From the AWS CloudFormation portal, click CloudFormation → UnravelEC2number → Outputs. Confirm that you see four keys listed in this section:. \n UnravelBackupS3 \n UnravelAutoScalingGroupId \n VPC \n SecurityGroup For example: Get the public IP address and private IP address of this EC2 instance. You can do this through aws CLI or through the EC2 console, ( https:\/\/console.aws.amazon.com\/ec2\/ To use aws CLI, run the ec2 describe-instances UnravelAutoScalingGroupId Outputs \n AUTOSCALING_GROUP=UnravelAutoScalingGroupId_value_from_Outputs_tab \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PublicIpAddress |head -1 \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PrivateIpAddress |head -1 To use the EC2 console, highlight this EC2 instance and look at its IP addresses in its Description For example: Log into Unravel Web UI Navigate to URL http:\/\/EC2_Public_IP:3000 : admin unraveldata. Congratulations! Unravel Server is up and running. You can proceed to connect to your existing or new EMR clusters \n 2019_01_10_15_55_03_EC2_Management_Console.png \n 2018-12-13 14_21_46-Create A New Stack.png \n signin.png \n 2018-12-13 17_34_11-EC2 Management Console.png \n 2018-12-13 17_31_14-EC2 Management Console.png \n 2018-12-13 17_25_55-EC2 Management Console.png \n 2018-12-10 12_50_27-Create A New Stack.png \n 2018-12-11 12_30_45-Define key pair and launch.png \n 2018-12-04 15_44_27-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-11 12_20_12-EC2 Management Console.png \n 2018-12-10 12_56_18-CloudFormation Management Console.png \n 2018-12-04 16_08_14-Create A New Stack.png \n 2018-12-04 16_07_17-Create A New Stack.png \n 2018-12-04 15_57_11-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_56_02-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_47_08-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_45_34-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-06 14_46_50-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-06 12_41_29-AWS Marketplace_ Search Results.png \n image2018-12-5_15-39-36.png \n image2018-12-5_15-37-14.png " }, 
{ "title" : "Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"url" : "unravel-4-3/install/install-emr/install-emr-part1/install-emr-part1-option2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"snippet" : "Introduction This topic explains how to provision an EC2 instance with Unravel Server 4.3.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.14.1 EMR Support Matrix Hadoop 2.8...", 
"body" : "Introduction This topic explains how to provision an EC2 instance with Unravel Server 4.3.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist \n \n \n Platform Compatibility \n EC2 Instance Type \n Security Group \/ IAM Role \n AWS Permissions \n \n AWS EMR 5.14.1 EMR Support Matrix Hadoop 2.8.3 Hive 2.3.2 Hue 4.1.0 Spark 2.3.0 Zeppelin 0.7.3 Tez 0.8.4 Livy 0.4.0 Pig 0.17.0 Oozie 4.3.0 \n \n Minimum \n Maximum \n Recommended \n Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. \n AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings \n \n \n Name \n Default Value \n Description \n \n Stack name \n The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date \n \n InstanceType \n r4.4xlarge \n Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge \n \n KeyName \n mysshkey \n EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. \n \n TrustedSshIPBlock \n 10.10.0.0\/16 \n Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). \n \n UnravelInstanceCount \n 1 \n Leave this as 1. \n \n UnravelVPCBlock \n 10.10.0.0\/16 \n Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). \n \n Zone \n us-east-1a \n Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our Amazon Machine Image (AMI) Navigate to the EC2 console ( https:\/\/console.aws.amazon.com\/ec2\/ From the menu on the left, select IMAGES AMIs In search box pull-down menu, select Public images Search for ami-08d8b2a645bdc7482 Select this AMI and click Launch On the Choose an Instance Type R4.2xlarge Next: Configure Instance Details (Optional) Modify the configuration of the EC2 instance: On the Configure Instance Details only \n Network \n Subnet \n IAM role Click Next: Add Storage (Optional) Increase the storage capacity of the EC2 instance to a maximum of 500GiB, depending on the number of clusters, the number of jobs running on those clusters, and whether you plan to enable debug logging. Click Next: Add Tags (Optional) Add tags. Click Next: Configure Security Group On the Configure Security Group \n \n \n Rule Type \n Protocol \n Port(s) \n Source \n \n Inbound: SSH \n TCP \n 22 \n Your trusted CIDR for SSH access \n \n Inbound: Custom TCP \n TCP \n 3000 \n 0.0.0.0\/0 or EMR security group ID or EMR subnet IP block \n \n Inbound: Custom TCP \n TCP \n 4043 \n EMR security group ID or EMR subnet IP block \n \n Outbound: All traffic \n All \n All \n 0.0.0.0\/0 \n \n Security policy required for all nodes in the EMR cluster (master, core, and task nodes) \n All \n All \n Unravel EC2 node security group ID or Unravel EC2 node private IP address Security Reminder We do not recommend making Unravel Server UI accessible on the public Internet. Click Review and Launch Review your settings and click Launch Enter the name of your key pair file and click Launch Instances Verify that you see the following notice: Click the instance. Find the public IP address of the instance in its Description Log into Unravel Web UI Using the public IP address you found in the step above, open a web browser window and navigate to URL http:\/\/EC2_Public_IP:3000 : admin unraveldata. connect to your existing or new EMR clusters \n 2019_01_10_17_37_22_EC2_Management_Console.png \n 2019_01_10_17_39_56_EC2_Management_Console.png \n 2018-12-13 17_31_14-EC2 Management Console.png \n 2019-01-10 17_09_33-EC2 Management Console.png \n signin.png \n 2018-12-13 17_34_11-EC2 Management Console.png \n 2018-12-11 12_30_45-Define key pair and launch.png \n 2018-12-13 17_25_55-EC2 Management Console.png \n 2018-12-11 12_20_12-EC2 Management Console.png \n image2018-12-5_15-39-36.png \n 2018-12-10 12_56_18-CloudFormation Management Console.png \n 2018-12-10 12_50_27-Create A New Stack.png \n 2018-12-04 15_57_11-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_56_02-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_47_08-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-04 15_45_34-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-06 14_46_50-AWS Marketplace_ Unravel APM for EMR.png \n 2018-12-06 12_41_29-AWS Marketplace_ Search Results.png " }, 
{ "title" : "Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"url" : "unravel-4-3/install/install-emr/install-emr-part1/install-emr-part1-option3.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.3.X on an AWS EC2 instance used to monitor existing or newly created EMR clusters. For instructions corresponding to older versions of Unravel Server, contact Unravel Support Workflow Summary Setup the EC2 instance for unravel installat...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.3.X on an AWS EC2 instance used to monitor existing or newly created EMR clusters. For instructions corresponding to older versions of Unravel Server, contact Unravel Support \n Workflow Summary Setup the EC2 instance for unravel installation. Install the Unravel EMR RPM on the created EC2 instance. Log into Unravel Web UI. Requirements Checklist \n \n \n Platform Compatibility \n Base OS for EC2 \n EC2 instance type & size \n Security Group \/ IAM role \n \n \n AWS EMR 5.x (5.14 preferred) \n EMR Support Matrix \n Hive 0.9.x - 2.3.2 \n Spark 1.3.x, 1.6.x - 2.3.x \n \n Base OS Redhat\/CentOS 6.4 - 7.4 \n Recommended Centos 7.4 AMI \n e.g. ami-02e98f78 (in us-east-1) \n \n \n Minimum \n \n Recommended \n Virtualization type: HVM \n Root device type: EBS – 100 - 500GB \n \n Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 \n Unravel EC2 node can access EMR cluster all ports \n Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Create an EC2 instance. Provision an EC2 Instance Instance Type: r4.2xlarge; recommended r4.4xlarge; the instance type: HVM with EBS OS: centos7.4 (ami-02e98f78) Minimum root disk space is 100GB. Recommendation: 200GB or above with \"Provisioned IOPS SSD (IO1)\" storage. The EC2 instance must Create a S3 ReadAccess only IAM role and assign it to Unravel EC2 node to READ the archive logs on the S3 bucket configured for the EMR cluster. For instance, create an IAM role that contains the policy that only READ the specific S3 bucket used on EMR cluster. Then create a EC2 instance profile and add the IAM role to it. See Creating IAM Role Create Instance Profile CLI Security Group or policy required for Unravel EC2 node: Unravel EC2 node works with multiple EMR clusters, including existing and newly created clusters. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node. You must create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address put the member of security group used on EMR cluster in this rule. Below is an example of security group used for Unravel EC2 node. \n Inbound rule \n \n \n Type \n Protocol \n Port Range \n Source \n \n All traffic \n All \n All \n SG ID of this group or Subnet IP block (e.g. 10.10.0.0\/16) \n \n SSH \n TCP \n 22 \n 0.0.0.0\/0 or trusted public IP for ssh access \n \n Custom TCP Rule \n TCP \n 3000 \n SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection \n \n Custom TCP Rule \n TCP \n 4043 \n SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection \n Outbound rule \n \n \n Type \n Protocol \n Port Range \n Source \n \n All traffic \n All \n All \n 0.0.0.0\/0 The Unravel EC2 node should have all TCP access to the EMR cluster (master or slave) nodes. You can grant access by inserting a security policy into both SG (security group) of EMR master and slave with (All TCP, All port range) and the source is the SG ID of the unravel VM. (see screen capture below) If it's not possible to Unravel EC2 access All traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Configure the EC2 node at first login. Disable selinux # sudo setenforce Permissive\n Edit \/etc\/selinux\/config SELINUX=permissive # vi \/etc\/selinux\/config\n Install libaio.x86_64 and lzop.x86_64. # sudo yum install -y libaio.x86_64\n# sudo yum install -y lzop.x86_64 Start ntpd # sudo service ntpd start\n# sudo ntpq -p Create a new user hadoop # sudo useradd hadoop In a PoC or evaluation, the minimal root disk size 100GB should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend to set minimal root disk from 300 - 500GB \"Provisioned IOPS\" EBS volume with 3000 IOPS. For production unravel use case, 200GB root disk Provisioned IOPS EBS and RDS are recommended . Install the Unravel EMR rpm on the created EC2 instance. Get the Unravel Server RPM; see Download Unravel Software Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. # sudo rpm -U unravel-4.3.1.*-EMR-latest.rpm \n Run the specified await_fixups.sh await_fixups.sh Done\" # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n# \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Edit the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem=false For monitoring EMR spark service, add the following properties in the unravel.properties com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel ( \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used; you can switch to use AWS RDS Security Remind We do not recommend making Unravel Server UI accessible on the public Internet. Log into Unravel Web UI. Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Create a ssh # ssh -i ssh_key.pem centos@{UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 From your workstation browser, navigate to URL http:\/\/127.0.0.1:3000 : admin unraveldata. connect to your existing or new EMR clusters \n image2019-1-24_13-2-57.png \n image2018-8-23_15-27-3.png \n image2018-8-23_15-26-1.png \n signin.png " }, 
{ "title" : "Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"url" : "unravel-4-3/install/install-emr/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"snippet" : "Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traff...", 
"body" : "Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes allow all traffic from the Unravel node . Both EMR cluster and Unravel node are created in same VPC, same subnet; and the security group allows all traffic from the same subnet. For existing EMR cluster connection located on a different VPC, you must configure VPC peering, route table creation, and security policy update Network ACL on VPC allows all traffic. Connect the Unravel EC2 Instance to a New EMR Cluster To connect the Unravel EC2 instance to a new EMR cluster, follow the steps below to run the Unravel EMR bootstrap script on all nodes in the cluster. The bootstrap script does the following: On the master node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf Updates \/etc\/hadoop\/conf\/mapred-site.xml Updates \/etc\/hadoop\/conf\/yarn-site.xml If TEZ is installed, it updates \/etc\/tez\/conf\/tez-site.xml Installs and starts the unravel_es daemon \/usr\/local\/unravel_es Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs the Hive hook sensor in \/usr\/lib\/hive\/lib\/ On all other nodes: Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs Hive sensors in \/usr\/lib\/hive\/lib Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py # curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the Unravel EMR bootstrap script unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce # aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce In the AWS console, select the EMR service and click Create cluster In the Create Cluster - Quick Options Go to advanced options . Select Step 1: Software and Steps emr-5.14.0 Release Next. Select Step 2: Hardware Set Network EC2 Subnet If you created the Unravel EC2 node from our CloudFormation template, then a new VPC was generated, named Unravel_VPC. This VPC comes with one configured subnet, and by default has a CIDR \/ network address block of 10.10.0.0\/16 (but you might have changed this during stack creation). If you created the Unravel EC2 node from our Amazon Machine Image (AMI), you must create the EMR cluster on the same VPC and same subnet as the Unravel EC2 node. Modify the instance type and enter the desired instance count for core (slave) node(s). Click \n Next. Select Step 3: General Cluster Settings Cluster name S3 folder Add bootstrap action Custom action Configure and add For details on how to set up your EMR cluster, see https:\/\/docs.aws.amazon.com\/emr\/latest\/ManagementGuide\/emr-gs-launch-sample-cluster.html In the Add Bootstrap Action Script location step 2 Optional arguments Add \n \n \n Sample script location \n \n s3:\/\/aws-logs-account_number-region\/elasticmapreduce \n \n Optional arguments (mandatory here) \n \n --unravel-server UNRAVEL_EC2_IP --bootstrap In the Bootstrap Actions Nex Advanced Options Select Step 4: Security Choose the EC2 key pair Select the EC2 security groups In this example, the security group picked for both Maste Core & Task You must choose the security group that includes the Unravel EC2 instance, otherwise bootstrap will fail. Click Create cluster If everything was entered correctly, your new EMR cluster should finish the bootstrap process and be in the Waiting Once your new EMR cluster is up and running, you can run some jobs and log into the Unravel EC2 node's web UI to see the metrics collected by the Unravel node. Connect the Unravel EC2 Instance to an Existing EMR Cluster To connect the Unravel EC2 instance to an existing EMR cluster, follow the steps below to run the Unravel EMR Ansible playbook either or Substitute your local values for text in {red brackets} Whenever you upgrade Unravel Server, repeat the steps below to upgrade Unravel Sensors as well. Prerequisites Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 <key> Option 1: Run Our Ansible Playbook on the EMR Master Node Download unravel-emr-ansible.zip $ curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip --output unravel-emr-ansible.zip\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 11708 100 11708 0 0 66541 0 --:--:-- --:--:-- --:--:-- 66902 Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --ssh-key path \n --unravel-host hostname \n --cluster-name displayname $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible on the EMR master node: $ sudo pip install ansible (Optional) Determine what directory Ansible was installed in, and add that directory to the $PATH variable in ~\/.bashrc $ which ansible\n\/usr\/local\/bin\/ansible In ~\/.bashrc export PATH=\/usr\/local\/bin\/:$PATH Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Option 2: Run Our Ansible Playbook on Your Personal Workstation (Mac or Linux Only) Set up AWS CLI. For instructions, see https:\/\/aws.amazon.com\/cli\/ Make sure AWS CLI has permission to list EMR clusters: $ aws emr list-instances --cluster-id {cluster id} Download unravel-emr-ansible.zip $ wget https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --cluster-id string \n --region string \n --inventory path \n --ssh-key path \n --ssh-user string hadoop \n --unravel-host hostname \n --cluster-name displayname \n --use-public $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible: $ sudo pip install ansible Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Sanity Check After you connect the Unravel EC2 instance to your EMR cluster, run some jobs on the EMR cluster and monitor the information displayed in Unravel UI ( http:\/\/unravel_ec2_node_public_IP:3000) Troubleshooting Check Ansible playbook logs in \/tmp\/unravel\/unravel_sensor_ansible.log If the EMR cluster is created in a different VPC, see Step 4 for configuring VPC peering connection for an Unravel node. \n image2018-9-14_17-49-36.png \n image2018-9-14_17-42-53.png \n image2018-9-14_17-40-16.png \n image2018-9-14_17-36-59.png \n image2018-9-14_17-14-37.png \n image2018-9-14_16-59-36.png \n image2018-9-14_16-57-32.png \n image2018-8-24_18-55-52.png \n image2018-8-24_18-38-12.png \n image2018-8-24_18-31-8.png \n image2018-8-24_18-24-38.png \n image2018-8-24_17-46-45.png \n image2018-8-24_17-34-16.png \n image2018-8-24_17-0-48.png \n image2018-8-24_16-56-43.png \n image2018-8-24_16-55-50.png \n image2018-8-24_16-53-24.png \n image2018-8-24_16-49-35.png " }, 
{ "title" : "Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"url" : "unravel-4-3/install/install-emr/step-3--set-up-aws-rds-for-unravel-db--optional-.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"snippet" : "Introduction Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow ...", 
"body" : "Introduction Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group. Create new DB subnet group Create new DB parameter group Setup MySQL RDS in AWS Create Unravel RDS instance In AWS portal → RDS, click Create database Choose MySQL Next Choose Production - MySQL Change the following properties, leave all others as the default. \n License model \n \n DB engine version \n \n DB instance class \n \n Multi-AZ deployment \n \n Storage type \n \n Allocated storage \n \n Provisioned IOPS Create a new DB instance and Master user and password. Click Next \n DB Instance identifier \n Master username \n Master password In the Advanced Settings . \n \n Network & Security Settings \n Virtual Private Cloud \n Subnet group \n Public accessibility \n \n \n Availability zone \n VPC security group Create a new DB subnet group in advance. It is required for Multi-AZ deployment. The VPC should at least contains two subnets in at least two Availability zones in a given region. For further information please check AWS documentation. Create a new DB parameter group in advance, and this group is based on mysql 5.5. Alter the parameters base on the custom db parameters. \n Screen shot of DB Parameter Group \n Custom DB Parameters key_buffer_size = 268435456\nmax_allowed_packet = 33554432\ntable_open_cache = 256\nread_buffer_size = 262144\nread_rnd_buffer_size = 4194304\nmax_connect_errors=2000000000\nnet-read-timeout = 300\nnet-write-timeout = 600\nopen_files_limit=9000\ninnodb_open_files=9000\ncharacter_set_server=utf8\ncollation_server = utf8_unicode_ci\ninnodb_autoextend_increment=100\ninnodb_additional_mem_pool_size = 20971520\ninnodb_log_file_size = 134217728\ninnodb_log_buffer_size = 33554432\ninnodb_flush_log_at_trx_commit = 2\ninnodb_lock_wait_timeout = 50 \n \n \n Database Options Settings \n Database name \n Port \n DB parameter group For other RDS options such as Encryption Backup Monitoring Maintenance Create database Connecting unravel node to the unravel RDS instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. Locate the MySQL database endpoint in the RDS dashboard. Look for the security group used for unravel RDS instance from RDS dashboard. Edit the inbound rule of the security group. Add new rule allow either from unravel node's Security Group, or subnet IP block which unravel node located. The SG or IP block works provided the RDS instance is located on the same region as the VPC. Verify the myql connection from unravel node. # \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Verify If database \"unravel_mysql_prod \" is created if not create one. # CREATE DATABASE IF NOT EXISTS unravel_mysql_prod; Create unravel db schema in RDS unravel database Stop the Unravel server. # sudo \/etc\/init.d\/unravel_all.sh stop Configure the following properties in unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Locate and modify the properties below so that they reflect your particular values. If the property isn't found, add it. Use the actual values you set in the above steps, here here encrypted unravel.jdbc.username=unravel\nunravel.jdbc.password={unraveldata}\nunravel.jdbc.url=jdbc:mysql:\/\/unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod\n Ensure the schema is up to date using the schema upgrade utility provided by Unravel server. The script step connects to the database and applies schema deltas in-order until the schema is up to date. The success or failure of the update is noted. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user admin # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Start unravel daemon Disable the bundled db on the Unravel server. Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command produces an error that can be ignored. # sudo chkconfig unravel_db off\n# sudo chkconfig unravel_pg off Start the Unravel server. # sudo \/etc\/init.d\/unravel_all.sh start \n image2018-9-11_14-25-13.png \n image2018-9-11_14-17-18.png \n image11.jpg \n image2018-9-12_18-49-0.png \n image2018-9-12_18-48-37.png \n image2018-9-12_18-47-32.png \n image2018-9-11_17-57-29.png \n image2018-9-11_16-47-2.png \n image2018-9-11_15-32-42.png \n image2018-9-11_15-7-20.png \n image2018-9-11_14-57-15.png \n image2018-9-11_14-49-23.png \n image2018-9-11_14-44-28.png \n image2018-9-11_14-3-44.png \n image2018-9-11_14-1-12.png \n image2018-9-11_14-0-3.png \n signin.png " }, 
{ "title" : "Step 4: VPC Peering for Unravel EC2 Node (Optional)", 
"url" : "unravel-4-3/install/install-emr/step-4--vpc-peering-for-unravel-ec2-node--optional-.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 4: VPC Peering for Unravel EC2 Node (Optional)", 
"snippet" : "Follow these steps only Introduction This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assu...", 
"body" : " Follow these steps only Introduction This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. \n Assumptions The VPC where Unravel EC2 located is in the same region where the EMR cluster located (e.g., us-east-1). The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in EMR cluster. Network ACL on both VPC for Unravel EC2 and EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. In the following steps, we have both Unravel EC2 node and EMR cluster located in us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. \n \n \n Resources \n Internal IP Address \n Subnet ID \n Subnet IP Block \n VPC ID (Name) \n IP block in VPC \n Security Group ID (Name) \n \n Unravel EC2 node \n 10.10.0.7 \n subnet-03b82c56b2c26dbd1 \n 10.10.0.0\/24 \n vpc-0b0e17b01c4a3b54a (Unravel_VPC) \n 10.10.0.0\/16 \n sg-0e0a03084398287c9 (Unravel-EC2_SG) \n \n EMR Cluster Master node \n 10.11.0.53 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) \n \n EMR Cluster Core nodes \n 10.11.0.76 10.11.0.130 \n subnet-0294cc17a42a9acfd \n 10.11.0.0\/24 \n vpc-c3d079a4 (VPC_for_VPC Peering) \n 10.11.0.0\/16 \n sg-0a73c3aea9340ae49 (EMR_VPC_SG) Create VPC Peering in VPC Dashboard From the AWS console → VPC services → Peering Connections Create Peering Connection VPC (Requester) VPC (Accepter) Create Peering Connection. A success message should appear in the screen. Click OK Accept the VPC Peering Request In the VPC Dashboard Pending Acceptance Action Accept Request Click Yes Accept Close Create Routes Between Peered VPC Go to VPC Dashboard → Route Tables Edit Add another route Find the Unravel_VPC route table. Enter the IP block of EMR VPC, e.g., 10.11.0.0\/16 In the Destination Target Save. Find the Test_EMR_VPC route table. Set the Destination Target Save. pcx-0a57a978ef9a525e2 Target Save Update Security Groups Go to VPC Dashboard → Security Group Add another rule Type ALL traffic Protocol ALL. Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block, e.g., 10.11.0.0\/16 in the Source Save. Locate the security group used on EMR cluster node. Enter the Unravel VPC IP block, e.g., 10.10.0.0\/16 in the Source Save Verify Connection Between Unravel EC2 node and EMR Master Node \n ssh ALL Traffic Then on Unravel EC2 node, telnet On the EMR master node, telnet . If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network acl, security groups, and route tables used on both VPC. \n See unsupported VPC peering configurations on AWS documentation. \n image2018-9-14_15-51-49.png \n image2018-9-14_15-41-49.png \n image2018-9-14_15-40-24.png \n image2018-9-14_15-38-50.png \n image2018-9-14_15-34-33.png \n image2018-9-14_15-29-1.png \n image2018-9-14_15-21-53.png \n image2018-9-14_15-11-26.png \n image2018-9-14_15-5-57.png \n aws-emr-create-peer-accept.png \n image2018-9-14_15-0-54.png \n image2018-9-14_14-59-11.png \n image2018-9-14_14-58-41.png \n image2018-9-13_16-39-56.png \n image2018-9-13_16-38-20.png \n image2018-9-13_16-37-34.png \n image2018-9-13_15-51-15.png \n image2018-9-13_15-33-48.png \n image2018-9-13_15-29-39.png \n image2018-9-13_15-28-39.png \n worddav17bd11fa4b44a52a180fec44ac2a622c.png \n FileFolder.png \n WX20180625-104211.png " }, 
{ "title" : "Reconnecting to Your EMR Cluster", 
"url" : "unravel-4-3/install/install-emr/reconnecting-to-your-emr-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Reconnecting to Your EMR Cluster", 
"snippet" : "If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of this instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup sav...", 
"body" : "If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of this instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup saved in the S3 bucket. In this case, your existing EMR clusters just need to be reconnected to the newly created Unravel EC2 node as described in Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster " }, 
{ "title" : "Deleting the Unravel EC2 Instance", 
"url" : "unravel-4-3/install/install-emr/deleting-the-unravel-ec2-instance.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Deleting the Unravel EC2 Instance", 
"snippet" : "If you're done with the Unravel EC2 instance, you can delete it as follows. From your EC2 console ( https:\/\/console.aws.amazon.com\/cloudformation\/ In the Actions Delete Stack Click Yes, Delete Monitor the Status...", 
"body" : "If you're done with the Unravel EC2 instance, you can delete it as follows. From your EC2 console ( https:\/\/console.aws.amazon.com\/cloudformation\/ In the Actions Delete Stack Click Yes, Delete Monitor the Status " }, 
{ "title" : "Using Unravel to Monitor Jobs in Your EMR Cluster", 
"url" : "unravel-4-3/install/install-emr/using-unravel-to-monitor-jobs-in-your-emr-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Using Unravel to Monitor Jobs in Your EMR Cluster", 
"snippet" : "This is an excerpt of the Unravel 4.3 User Guide Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 node. For example: ssh -i ssh_key.pem centos@Unravel_node_public_IP -L 3000:127.0.0.1:3000 Start your browser from your workstation and navigate to http:...", 
"body" : " This is an excerpt of the Unravel 4.3 User Guide Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 node. For example: ssh -i ssh_key.pem centos@Unravel_node_public_IP -L 3000:127.0.0.1:3000 Start your browser from your workstation and navigate to http:\/\/127.0.0.1:3000 Log in with username admin unraveldata The OPERATIONS Trial versions include a message in the top menu bar about the trial license and the number of days remaining until the trial expires. Please contact sales@unraveldata.com Run some jobs from the EMR master node. The EMR master node has some sample MapReduce and Spark jobs on it. Run these jobs to verify that the Unravel EC2 node is collecting data from the EMR cluster. Your usage may vary depending on what applications you installed on your cluster. \n Sample MapReduce (MR) Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this MapReduce (MR) \"Pi\" job: sudo -u hdfs hadoop jar \/usr\/lib\/hadoop-mapreduce\/hadoop-mapreduce-examples.jar pi 100 100 When the MR job finishes, check Unravel UI: You should see one successful application labelled MR To see details about the MR job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. This notification might take a few minutes to appear. Explore other metrics about this job by clicking the tabs within the job details screen. \n Sample Spark Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this Spark \"Pi\" job: sudo -u hdfs spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/lib\/spark\/examples\/jars\/spark-examples.jar 1000 When the Spark job finishes, check Unravel UI: You should see one successful application labelled SPARK To see details about the Spark job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. This notification might take a few minutes to appear. Explore other metrics about this job by clicking the tabs within the job details screen. \n apm_for_emr_09.png \n apm_for_emr_07.png \n apm_for_emr_06.png \n apm_for_emr_05.png \n apm_for_emr_04.png \n apm_for_emr_03.png \n apm_for_emr_02.png \n apm_for_emr_01.png " }, 
{ "title" : "Azure HDInsight Clusters", 
"url" : "unravel-4-3/install/install-hdi.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters", 
"snippet" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster (Not yet available for...", 
"body" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster (Not yet available for Unravel 4.3) Unravel VM vs Unravel App: To monitor multiple HDInsight clusters on the same virtual network, Unravel VM is a preferable way. On the other hand, to quickly try out Unravel for your on demand HDInsight cluster, you can add the Unravel app during the cluster creation or add the app anytime after a cluster is created. Unravel app resides on the edge node of the target HDInsight cluster, and it can monitor one cluster's activities. " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "unravel-4-3/install/install-hdi.html#UUID-b9b31976-d92a-aee5-2e2f-f86472dd4267_id_AzureHDInsightClusters-InstallationGuideforUnravelVM", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to Configure HDinsight Cluster for Unravel Page: Step 3: Updating Unravel Installation Page: Step 4: ARM template for Unravel (Optional)...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to Configure HDinsight Cluster for Unravel Page: Step 3: Updating Unravel Installation Page: Step 4: ARM template for Unravel (Optional) " }, 
{ "title" : "Installation Guide for Unravel App", 
"url" : "unravel-4-3/install/install-hdi.html#UUID-b9b31976-d92a-aee5-2e2f-f86472dd4267_id_AzureHDInsightClusters-InstallationGuideforUnravelApp", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel App", 
"snippet" : "Page: Unravel HDinsight app...", 
"body" : " Page: Unravel HDinsight app " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "unravel-4-3/install/install-hdi/installation-guide-for-unravel-vm.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to Configure HDinsight Cluster for Unravel Page: Step 3: Updating Unravel Installation Page: Step 4: ARM template for Unravel (Optional)...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to Configure HDinsight Cluster for Unravel Page: Step 3: Updating Unravel Installation Page: Step 4: ARM template for Unravel (Optional) " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"url" : "unravel-4-3/install/install-hdi/installation-guide-for-unravel-vm/install-hdi-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM \/ Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"snippet" : "Introduction Unravel server can be deployed in Azure environment to monitor multiple HDInsight clusters and the following steps explain how to deploy Unravel Server 4.3.1.7 on an Azure VM. Requirements Checklist Azure HDI cluster Compatibility Hadoop 2.7.3 Spark 1.6.3 Spark 2.1.0 Spark 2.2.0 Spark 2...", 
"body" : "Introduction Unravel server can be deployed in Azure environment to monitor multiple HDInsight clusters and the following steps explain how to deploy Unravel Server 4.3.1.7 on an Azure VM. Requirements Checklist \n \n \n Azure HDI cluster Compatibility \n Hadoop 2.7.3 \n Spark 1.6.3 \n Spark 2.1.0 \n Spark 2.2.0 \n Spark 2.3.0 \n Kafka 0.10.0 \n \n Unravel VM OS \n RHEL or Centos 7.2 - 7.6 \n \n Min requirement \n 64 GB (Min VM type suggested Standard_E8s_v3) \n \n Disk requirement \n min 100GB for \/srv \/srv \n \n Network requirement \n unravel VM should be located in the same VNET and VSNET of the HDI cluster \n \n Security requirement \n allows inbound ssh to the unravel VM allows outbound Internet access and all traffic within the subnet (VSNET). allows TCP port 3000 and 4043 to unravel VM from HDI cluster \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Setup Unravel VM Log into https:\/\/portal.azure.com Create a virtual machine in Azure Portal, assign the Resource group, VNET, VSNET same as the HDI cluster located. Create a new disk for unravel VM Configure networking and assign security group, ensure VNET & VSNET are same for HDI cluster Sample Security Group for Unravel VM and HDI cluster Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the head node of each HDInsight cluster to Unravel Server. Add inbound security policy allow ssh access from trusted external network and public 443 access to both the unravel node and EMR cluster The default security policy already allow all access within the VNET. Default rule start with 6500. Rest of configuration in Management Guest config Tag Once the unravel VM is available, then ssh to the VM and configure the environment as follows: Disable selinux, and persists after reboot. sudo setenforce 0\nsudo \/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config Install libaio, lzop, and ntp sudo yum -y install libaio lzop ntp Disable the local Firewall. sudo systemctl disable firewalld\nsudo systemctl stop firewalld\nsudo iptables -F\nsudo iptables -L Prepare the 2nd disk (e.g. \/dev\/sdc ) with 500GB size that configured previously on Azure portal; use \"fdisk -l \" to check any 500GB disk without partition. (This step requires root privilege) sudo su -\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/sdc\n\/usr\/sbin\/mkfs -t ext4 \/dev\/sdc1\nmkdir -p \/srv\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc1 | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\nmkdir -p \/srv\/local\/unravel\nchmod -R 755 \/srv\/local\nln -s \/srv\/local\/unravel \/usr\/local\/unravel\nchmod 755 \/usr\/local\/unravel Create hdfs user and hadoop group in Unravel VM sudo useradd hdfs\nsudo groupadd hadoop\nsudo usermod -a -G hadoop hdfs 4. Install the Unravel Server RPM on the VM Get the Unravel Server RPM (v4.3.1.7) Download Unravel software for Azure curl https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/Azure\/unravel-4.3.1.7-Azure-latest.rpm -o \/tmp\/unravel-4.3.1.7-Azure-latest.rpm\nsudo rpm -Uvh \/tmp\/unravel-4.3.1.7-Azure-latest.rpm\nsudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\necho \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop Run the specified await_fixups.sh await_fixups.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh \n Grant Access to Unravel Server By Default Public IP should be assigned to the Unravel VM node. Create a security policy that allows ssh It is recommended to use sshkey for your ssh access to the Unravel VM. Security Remind Do not make Unravel Server UI TCP port 3000 accessible on the public Internet . Azure Blob storage access For default HDI cluster is using blob storage container, and you need to update \/usr\/local\/unravel\/etc\/unravel.properties file manually to contain the blob storage account name\" and access keys. Below is the example for this configuration in unravel.properties, change the STORAGE_ACCOUNT_NAME and keys for your own settings. The storage-account-name-1 storage-account-name-2 # Adding Blob Storage Account information, Update and uncomment following lines (required for blob storage HDI cluster) com.unraveldata.hdinsight.storage-account-name-1= fs.azure.account.key. STORAGE_ACCOUNT_NAME .blob.core.windows.net com.unraveldata.hdinsight.primary-access-key=h7SNeVuxdNTJ2ksVBgAERWczwi65Ik2kJnbKKRSMU7mrCp37k2zO\/AyVEzZuJkAqXLrenQBj1UfH7KMhKB1SyQ== com.unraveldata.hdinsight.storage-account-name-2= fs.azure.account.key.STORAGE_ACCOUNT_NAME.blob.core.windows.net com.unraveldata.hdinsight.secondary-access-key=JG\/sICi27GZFy5T3omcSYFL4WRCK2aiqSPQZV7fTB6ZMuUroLcCGQZfQevHSYiLjHk\/kaRgtTDHbYQ5naHOHpA== Find blob storage account name and keys Your Azure blob storage account name and keys can be found under Home → Storage accounts → select the storage account → Access keys (see below screen capture). 5. Start Unravel and log into Unravel UI The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.ext.sh unravel.properties # ssh -i {ssh_private_key.pem} sshuser@$UNRAVEL_IP \n The unravel configuration file \/usr\/local\/unravel\/etc\/unravel.properties is created during initial install and subsequent RPM upgrades will not change it because your site-specific properties are put into this file. Unravel properties table \n \n \n Property \n Description \n Required \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n optional \n \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n optional \n \n Company_and_org \n \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n mandatory and is set by installation \n \n \/srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n optional \n \n 26 \n \n \n com.unraveldata.login.admins \n Unravel UI admin \n mandatory and is set by installation \n \n admin \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n Optional for Spark when HDinsight using blob storage storage account name for the HDinsight cluster \n mandatory if using Azure blob \n \n fs.azure.account.key.STORAGE_NAME. blob.core.windows.net \n \n \n com.unraveldat \n Primary storage account key \n mandatory if using Azure blob \n \n Ondaq2aYMpJf8pCdvtFJ\/zARJLhFr4Vf94PPJvMP1EsoFzBKp \n \n \n com.unraveldat a.hdinsight \n Optional for Spark when HDinsight using blob storage Storage account name for the HDinsight cluster (same as account-name-1 \n mandatory if using Azure blob \n \n fs.azure.account.key. blob.core.windows.net \n \n \n com.unraveldat a.hdinsight \n Secondary storage account key \n mandatory if using Azure blob \n \n aL3MFZ\/5hP4k0LxA+tn5\/NM6EkM1AZkFZzKmWjgEMqe0o6F \n \n \n com.unraveldat a.adl.accountFQDN \n The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net \n mandatory if using Azure Data Lake \n \n \n datalake0001.azuredatalakestore.net \n \n \n com.unraveldat a.adl.clientld \n An application ID. An application registration has to be created in the Azure Active Directory \n mandatory if using Azure Data Lake \n \n 5d19877f-3eb5-413a-9a41-7ae8a0048cfk \n \n \n com.unraveldat a.adl.clientKey \n An application access key which can be created after registering an application \n mandatory if using Azure Data Lake \n \n 6FMzo61+cKIRPFZRxzUxiLSuWc5YEsdZzYbtU5rMyUg= \n \n \n com.unraveldat a.adl.accessTokenEndpoint \n The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal \n mandatory if using Azure Data Lake \n \n \n https:\/\/login.microsoftonline.com\/bc745a0d-f282-4e99-b95f-1ecb477a209g\/oauth2\/token n \n \n \n com.unraveldat a.adl.clientRootPath \n It is the path in the Data lake store where the target cluster has been given access. \n mandatory if using Azure Data Lake \n \n \/clusters\/CLUSTERNAME \n \n \n com.unraveldat a.ext.kafka.clusters \n Name of kafka cluster. The display name show on the Unravel UI to define kafka cluster. Other Unravel kafka properties depends on this name CLUSTERNAME \n mandatory for HDI Kafka cluster \n \n udkafka \n \n \n com.unraveldat a.ext.kafka.CLUSTERNAME.bootstrap_servers \n Kafka cluster bootstrap server and port (usually are two worker nodes) \n mandatory for HDI Kafka cluster \n \n wn0-UDKAFK:9092,wn1-UDKAFK:9092 \n \n \n com.unraveldat a.ext.kafka. \n Define kafka cluster broker servers names \n mandatory for HDI Kafka cluster \n \n broker-1,broker-2,broker-3 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 host \n mandatory for HDI Kafka cluster \n \n wn0-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 port \n mandatory for HDI Kafka cluster \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 host \n mandatory for HDI Kafka cluster \n \n wn1-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 port \n mandatory for HDI Kafka cluster \n \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 host \n mandatory for HDI Kafka cluster \n \n wn2-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 port \n mandatory for HDI Kafka cluster \n \n 9999 \n For HDinsight cluster using blob storage or Data Lake Storage you need to update the following properties. The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel properties' values in Microsoft's Azure \n \n \n \n When using blob storage update or add this properties \n \n \n When using \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n \n com.unraveldat a.adl.accountFQDN \n \n \n com.unraveldat \n \n com.unraveldat a.adl.clientld \n \n \n com.unraveldat a.hdinsight \n \n com.unraveldat a.adl.clientKey \n \n \n com.unraveldat a.hdinsight \n \n com.unraveldat a.adl.accessTokenEndpoint \n \n \n com.unraveldat a.adl.clientRootPath \n Restart Unravel daemons After editing the unravel.properties file # sudo \/etc\/init.d\/unravel_all.sh restart\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" \n Log into Unravel Web UI Create a SSH # ssh -i somefile.pem sshuser@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Congratulations! Unravel Server is up and running. Please proceed to Part 2 For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "Step 2: Use Script Action to Configure HDinsight Cluster for Unravel", 
"url" : "unravel-4-3/install/install-hdi/installation-guide-for-unravel-vm/install-hdi-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM \/ Step 2: Use Script Action to Configure HDinsight Cluster for Unravel", 
"snippet" : "Before Unravel can analysis any job run on the HDInsight cluster, Unravel agent and sensors are required to deploy on cluster nodes by Azure script action process. There are two kind of Unravel script actions scripts and the followings are the use case and specifications. For HDInsight cluster witho...", 
"body" : "Before Unravel can analysis any job run on the HDInsight cluster, Unravel agent and sensors are required to deploy on cluster nodes by Azure script action process. There are two kind of Unravel script actions scripts and the followings are the use case and specifications. For HDInsight cluster without Internet access, you can download these scripts and store them in your Azure blob storage and use the blob storage URI on the script action's Bash script URI \n \n \n Script Action description \n Download path \n Supported HDI cluster(s) \n Apply to cluster node type(S) \n \n Unravel script action for Spark, Hive, and Hadoop \n \n unravel_hdi_spark_bootstrap_3.0.sh \n Hadoop 2.7.3 Spark 2.0, 2.1, 2.3 \n Master, Worker, Edge nodes \n \n Unravel script action for Kafka \n \n unravel_hdi_kafka_bootstrap.sh \n Kafka 0.10.0 \n Master node Checks before running script action Ensure Unravel service is running on Unravel VM and port 3000 and 4043 is reachable from the Azure HDInsight cluster master node, before running the Unravel script action script 1. Run Unravel script action on existing Spark or Hadoop HDinsight cluster Login into the Azure portal and choose service HDInsight cluster Click the Script actions Submit new \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh \n \n Node type(s) \n Head, Worker, Edge (only if you have deployed edge node) \n \n Parameters \n \n --unravel-server <unravel_server_private_ip>:3000 --spark-version <spark_version> e.g. --unravel-server 10.10.1.10:3000 \n \n Persist this script action \n Persistence only applies on new Head and Worker nodes Script action script URI You can download Unravel script action scripts and upload it to your blob storage and use the blob storage URI on the above Bash script URI field. e.g. From the Azure portal, you can check if script action process is finished successfully by checking the SCRIPT ACTION HISTORY If script action process is failed, you can check the error messages from the HDinsight cluster's Ambari task which is the balloon next to the cluster name on the top menu bar run customscriptaction The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new script action script. 2. Run Unravel script action script when launching a fresh Spark or Hadoop cluster Login to Azure Portal to create a Spark 2.1 HDinsight Cluster. Select Summary Confirm configuration VNET Subnet On Advanced settings When entering Virtual Network settings you must enter the VNET and Subnet where Unravel VM is located. On the Advanced setting screen click on the arrow to the right of script actions to bring up the Submit script action screen: Select Script type Name, “Unravelscript001” The Bash script URI uses the same URL https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Check the boxes “Head”, “Worker” and \"Edge\" so custom script action applies on both Head and worker nodes. In the parameter field enter the string -- unravel-server unravel-server 10.10.1.10:3000 --spark-version 2.1.0 Script arguments are in the form of --spark-version --hive-version --spark-version 2.1.0 --hive-version 1.2.0 Click Create Azure portal checks to ensure the script is valid. Once validated you are returned to the initial screen. Click Select Return to Step 5. The script action is shown as Configured. Click Next The summary configuration is displayed on Step 6. Click Create Return to the Azure dashboard; the HDinsight cluster is displayed with the status of Deploying. The Cluster creation process takes approximately 20-25 minutes. Verify script action process from Azure portal and cluster's Ambari task screen mentioned previously. 3. Run Unravel script action on existing Kafka cluster. Login into the Azure portal and choose service HDinsight cluster If the kafka cluster has no Internet access; then download the HDinsightUtilities-v01.sh script and scp it in the kafka Head node \/tmp folder wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click the Script actions Submit new \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh \n \n Node type(s) \n Head \n \n Parameters \n \n <unravel_server_private_ip>:3000 e.g. 10.10.1.24:3000 \n \n Persist this script action \n Persistence only applies on new Head nodes Click create and script Action scripts will apply to the specified cluster node and monitor the script action result show on the SCRIPT ACTION HISTOR. If script action is failed to run, inspect Ambari tasks and look for run customscriptaction After kafka script action script completed successfully, ssh to the kafka Head node and use the content of \/tmp\/unravel\/unravel.ext.properties and append it to the \/usr\/local\/unravel\/etc\/unravel.properties file on your Unravel VM. An example content from unravel.ext.properties is shown below. #Adding Kafka properties\ncom.unraveldata.ext.kafka.clusters=<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.bootstrap_servers=wn0-<cluster_name>:9092,wn1-<cluster_name>:9092\ncom.unraveldata.ext.kafka.<cluster_name>.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.host=wn0-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.host=wn1-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.port=9999 Unravel VM is required to access to the kafka worker nodes' kfaka broker port 9092 and kafka JMX port 9999 \n image2018-11-9_10-26-26.png \n image2018-11-8_17-9-0.png \n image2018-11-8_17-5-46.png \n image005.jpg \n image2018-11-8_16-17-17.png \n image2017-11-27_3-41-44.png \n image2017-11-27_3-25-36.png \n image2017-11-27_3-48-7.png \n image2017-11-27_2-57-8.png " }, 
{ "title" : "Step 3: Updating Unravel Installation", 
"url" : "unravel-4-3/install/install-hdi/installation-guide-for-unravel-vm/step-3--updating-unravel-installation.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM \/ Step 3: Updating Unravel Installation", 
"snippet" : "From time to time Unraveldata will release new package with new features and improvement for customers to download and update their existing installations. Updating unravel VM on Azure is simple; just download the new unravel RPM package and install it. Download Unravel 4.3.1.7 RPM for Azure curl ht...", 
"body" : "From time to time Unraveldata will release new package with new features and improvement for customers to download and update their existing installations. Updating unravel VM on Azure is simple; just download the new unravel RPM package and install it. Download Unravel 4.3.1.7 RPM for Azure curl https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/Azure\/unravel-4.3.1.7-Azure-latest.rpm -o \/tmp\/unravel-4.3.1.7-Azure-latest.rpm Upgrade Unravel package on Unravel VM node rpm -Uvh \/tmp\/unravel-4.3.1.7-Azure-latest.rpm [root@RTYJK9 ~]# rpm -U \/tmp\/unravel-4.3.1.7-Azure-latest.rpm\nerror: open of \/tmp\/unravel-4.3.1.7-Azure-latest.rpm failed: No such file or directory\n[root@RTYJK9 ~]# rpm -U unravel-4.3.1.7-Azure-latest.rpm\nPre-existing Unravel detected, doing upgrade\nRedirecting to \/bin\/systemctl status ntpd.service\n? ntpd.service - Network Time Service\n Loaded: loaded (\/usr\/lib\/systemd\/system\/ntpd.service; enabled; vendor preset: disabled)\n Active: active (running) since Mon 2018-11-12 22:26:05 UTC; 2h 34min ago\n Main PID: 7785 (ntpd)\n CGroup: \/system.slice\/ntpd.service\n mq7785 \/usr\/sbin\/ntpd -u ntp:ntp -g\n\nNov 12 22:26:05 RTYJK9 ntpd[7785]: Listen normally on 3 eth0 10.10.1.74 UDP 123\nNov 12 22:26:05 RTYJK9 ntpd[7785]: Listen normally on 4 lo ::1 UDP 123\nNov 12 22:26:05 RTYJK9 ntpd[7785]: Listen normally on 5 eth0 fe80::20d:3aff:fe4d:f4a8 UDP 123\nNov 12 22:26:05 RTYJK9 ntpd[7785]: Listening on routing socket on fd #22 for interface updates\nNov 12 22:26:05 RTYJK9 ntpd[7785]: 0.0.0.0 c016 06 restart\nNov 12 22:26:05 RTYJK9 ntpd[7785]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM\nNov 12 22:26:05 RTYJK9 ntpd[7785]: 0.0.0.0 c011 01 freq_not_set\nNov 12 22:26:12 RTYJK9 ntpd[7785]: 0.0.0.0 c614 04 freq_mode\nNov 12 22:41:53 RTYJK9 ntpd[7785]: 0.0.0.0 0612 02 freq_set kernel -9.222 PPM\nNov 12 22:41:53 RTYJK9 ntpd[7785]: 0.0.0.0 0615 05 clock_sync\n Upgrade in progress (step b1)\n\n _\n | |\n _ _ _ __ _ __ __ ___ _____| |\n | | | | '_ \\| '__\/ _ \\ \\ \/ \/ _ \\ |\n | |_| | | | | | | (_| |\\ V \/ __\/ |\n \\__,_|_| |_|_| \\__,_| \\_\/ \\___|_|\n\n********************************************\n* Start Unravel Server Installation *\n********************************************\nPre-existing Unravel detected, doing upgrade (step b2)\n\n (stopping Unravel service so we can check free RAM)\n.......................................................................................................unravel_ud is stopped\n......................unravel_host1 stopped\nunravel_ja stopped\n.unravel_td stopped\n......unravel_jcse2 stopped\nunravel_cw stopped\nunravel_km stopped\n...unravel_os3 stopped\nunravel_tw stopped\n....unravel_us_1 stopped\n....unravel_lr stopped\n...unravel_ma_1 stopped\n..unravel_hhwe stopped\n...unravel_hl stopped\nunravel_sw_1 stopped\n.................unravel_ew_1 stopped\n..unravel_jcw2_1 stopped\n\netl daemons stopped\n\n............unravel_tc stopped\n.....unravel_ngui stopped\n..........unravel_s_1 stopped\nStopping unravel_k: .................. done.\n...unravel_zk_1 stopped\n...unravel_zk_2 stopped\n...unravel_zk_3 stopped\nsending shutdown command to db... could take 10min\nunravel_db stopped\npreparing for es_migrate_4.3.1.0_from42.sh ...\n PreInstall Step for Unravel version 4.3.1.7\n\n Preinstall: Using existing db, existing k, existing zk\n\nwarning: \/usr\/local\/unravel\/kafka\/config\/consumer.properties saved as \/usr\/local\/unravel\/kafka\/config\/consumer.properties.rpmsave\nwarning: \/usr\/local\/unravel\/kafka\/config\/server.properties created as \/usr\/local\/unravel\/kafka\/config\/server.properties.rpmnew\n\n running after-install of build\n\n Are any processes owned by user unravel running?\nUID PID PPID C STIME TTY TIME CMD\n\n None.\nvm.max_map_count = 262144\n\n Unravel Server from Unravel Data Systems, Inc.\n\n target user unravel:unravel\n\n Upgrade in progress for bundled db\n Upgrade is from build.swizzle 1150 to 43107027\nExisting broker.id=3709\n\/bin\/rdate\nmaking sure time is not off by large amount by explicitly syncing to time-c.timefreq.bldrdoc.gov\nRedirecting to \/bin\/systemctl stop ntpd.service\nTue Nov 13 01:02:48 UTC 2018\nrdate: [time-c.timefreq.bldrdoc.gov] Tue Nov 13 01:02:49 2018\n\nTue Nov 13 01:02:49 UTC 2018\nRedirecting to \/bin\/systemctl start ntpd.service\n\n Configuring Unravel version\n\nRed Hat or Centos detected\nusing existing db\ninstall zk_1 to run as unravel with datadir \/srv\/unravel\/zk_1_data\ninstall zk_2 to run as unravel with datadir \/srv\/unravel\/zk_2_data\ninstall zk_3 to run as unravel with datadir \/srv\/unravel\/zk_3_data\ninstalling kafka to run as unravel with datadir \/srv\/unravel\/k_data\nusing existing kafka data\nEnabling SECCOMP for syscall filtering security\nExamining \/usr\/local\/unravel\/etc\/unravel.properties\nFound: 127.0.0.1:4181,127.0.0.1:4182,127.0.0.1:4183\nupdating \/usr\/local\/unravel\/kafka\/config\/server.properties\nupdating \/usr\/local\/unravel\/kafka\/config\/consumer.properties\nupdating \/usr\/local\/unravel\/etc\/zk_1.properties\nupdating \/usr\/local\/unravel\/etc\/zk_2.properties\nupdating \/usr\/local\/unravel\/etc\/zk_3.properties\nunravel_hl not running\nunravel_ma_1 not running\nunravel_us_1 not running\nunravel_sw_1 not running\nunravel_host1 not running\nunravel_td not running\nunravel_ja not running\nunravel_ew_1 not running\nunravel_hhwe not running\nunravel_cw not running\nunravel_km not running\nunravel_jcse2 not running\nunravel_jcw2_1 not running\nunravel_lr not running\nunravel_os3 not running\nunravel_tw not running\n\netl daemons stopped\n\nunravel_tc not running\nunravel_ngui not running\nunravel_s_1 not running\nunravel_k not running\nunravel_zk_1 not running\nunravel_zk_2 not running\nunravel_zk_3 not running\nunravel_db not running\nDone clearing old state on RTYJK9, Kafka topics are now removed on this host\nStarting unravel_db: MYSQL_SAFE_LOG=\/usr\/local\/unravel\/logs\/unravel_mysql_safe.log MYSQL_ERROR_LOG=\/usr\/local\/unravel\/logs\/unravel_db.errors.log\nunravel_db started\n\n\n migrate\/upgrade db schema (run in background)\n\n...............\n\n\n NOTICE: new sensor version 1.0.66 available with this upgrade (previously was 1.0.61)\n\n********************************************\n Unravel Server Installation Finished\n Some background processing might continue for a few minutes...\n RUN THIS NEXT: \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n********************************************\n\n To stop and restart, use: \/etc\/init.d\/unravel_all.sh\n Logs owned by unravel are in \/usr\/local\/unravel\/logs\n Logs owned by root are in \/var\/log\/\n\n After you start Unravel with sudo \/etc\/init.d\/unravel_all.sh start\n Visit http:\/\/RTYJK9.ljecxtr4hjgedetsjnbphijplg.bx.internal.cloudapp.net:3000\/ for next steps (first page load can take a while)\n\n running before-remove of build 1150, op=1\n running after-remove of build 1150, op=1 Post rpm upgrade configuration # run await_fixup script; this could take up to 1-2 minutes\n\/usr\/local\/unravel\/install_bin\/await_fixups.sh\n\n# Create hdfs user and hadoop group in unravel VM\nsudo useradd hdfs\nsudo groupadd hadoop\nsudo usermod -a -G hadoop hdfs\n\n# Run switch user script\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop Restart unravel daemons \/etc\/init.d\/unravel_all.sh restart For connected spark cluster, it is required to submit the Unravel script action script run to Head, Worker and Edge nodes of the spark cluster. And for kafka cluster, there is no need to run script action after upgrading unravel package. Updating unravel installation should not affect the connected HDinsight cluster operation and can be done at any time. However some unravel rpm updates requires the sensor upgrade and this will require re-submit the unravel script action script to head, worker, and edge nodes. " }, 
{ "title" : "Step 4: ARM template for Unravel (Optional)", 
"url" : "unravel-4-3/install/install-hdi/installation-guide-for-unravel-vm/step-4--arm-template-for-unravel--optional-.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Installation Guide for Unravel VM \/ Step 4: ARM template for Unravel (Optional)", 
"snippet" : "Create Unravel VM using ARM template Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for ref...", 
"body" : "Create Unravel VM using ARM template Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. The example ARM template creates an Azure VM with HW type set to \" standard E8s V3 srv\" Update the following azuredeploy.json file to change VM OS, default is set to use Redhat Enterprise 7.4; you can change to CentOS 7.4. Update all parameters defined in azuredeploy.parameters.json Finding Unravel properties' values in Microsoft's Azure ARM Template for Unravel VM https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM\/azuredeploy.parameters.json This ARM template embedded with Azure Extension script to download and install Unravel RPM. The Azure Extension script for Unravel RPM installation. If your Azure VM network has no Internet access, you should download the extension script to your Azure blob storage and update the azuredeploy.parameters.json fileUris Extension Script https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/UBsetup\/unravel-AWS-vm-4317-setup.sh Using the ARM template and custom extension script will prepare the Unravel VM, include disk configuration and installing Unravel rpm; and you just need to ssh to the unravel node and start unravel daemons by running the command: \/etc\/init.d\/unravel_all.sh start Contents of unravel-AWS-vm-4317-setup.sh extension script. # Download unravel 4.3.1.7 rpm\ncurl https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.3.1\/Azure\/unravel-4.3.1.7b0027-1.x86_64.EMR.rpm -o unravel-4.3.1.7b0027-1.x86_64.EMR.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\nsleep 10\n\nmkdir -p \/srv\/local\/unravel\nchmod -R 755 \/srv\/local\n\nln -s \/srv\/local\/unravel \/usr\/local\/unravel\nchmod -R 755 \/usr\/local\/unravel\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.3.1.7b0027-1.x86_64.EMR.rpm\n\n\/usr\/bin\/sleep 5\n\/usr\/local\/unravel\/install_bin\/await_fixups.sh\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=unravel_license_string\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Switch user \nuseradd hdfs\ngroupadd hadoop\nusermod -a -G hadoop hdfs\n\n\/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop\n\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start Download the ARM template and parameter JSON files into your configured Azure CLI workstation. Use the Azure CLI to deploy Unravel VM using this template and parameters JSON file. # az group deployment create --name deploymentname --verbose --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Validate template before deployment # az group deployment validate --verbose --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually starting unravel daemons. # \/etc\/init.d\/unravel_all.sh start Create Spark 2.1 cluster with Unravel script action This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a spark edge node. This template will also run Unravel's Script Actions script to setup unravel sensors and configuration on header, worker and edge nodes. This ARM template use existing VNET, Subnet and Storage Account on the same resource group. You will need to update those values in parameter, variables to reflect your Azure environment. A spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use spark edge node for accessing the cluster, testing your client applications, and hosting your client applications. To use this ARM template, it is required to have Unravel VM be deployed first and update the following azuredeploy.parameters.json azuredeploy.json -- unravel-server10.10.1.29:3000--spark-version 2.1.0\" and replace 10.10.1.29 to the Unravel VM private IP address. ARM Template for HDI Spark + unravel script action https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json Parameter file for HDI Spark https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json Unravel Script Action URI for HDI Spark https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access, download the no dependency unravel_hdi_spark_bootstrap_3.0.sh Download the ARM template and parameter JSON files onto your configured Azure CLI workstation. Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.js Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Create Kafka 0.10.0 Cluster with Unravel Script Actions This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions Script Actions unravel.properties It is required to update the azuredeploy.json azuredeploy.parameter.json VNET Subn Storage Account, Cluster name Standard_D3_v2 Script Actions ARM Template for HDI Kafka 0.1 + Unravel script action https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.json Parameter file for HDI Kafka 0.1 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.parameters.json Unravel Script Action URI for HDI Kafka 0.1 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Using Script Actions via CLI It is required to update the azuredeploy.json azuredeploy.parameter.json Validate the template before deployment. # az group deployment validate --verbose --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Use Azure CLI to deploy kafka cluster using this template and parameters JSON file. # az group deployment create --name deploymentname --verbose --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json After the kafka cluster is successfully created, the Unravel kafka Script Actions \/tmp\/unravel\/unravel.ext.properties \/usr\/local\/unravel\/etc\/unravel.properties unravel.ext.properties unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 After updating unravel.properties unravel_km # \/etc\/init.d\/unravel_km restart Create edge node for HDinsight Spark cluster and apply Unravel Script Action ARM template for install edge node with Unravel Script Action only ARM Template for Edge node + Unravel script action https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json Parameter file https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json ARM template for install edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example) In this example edge node will be created and first will run a emptynode-setup.sh unravel_hdi_bootstrap.sh ARM Template for Edge node + your script(s) + Unravel script action https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json Parameter file https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires updating both azuredeploy.json azuredeploy.parameter.json " }, 
{ "title" : "Unravel HDinsight app", 
"url" : "unravel-4-3/install/install-hdi/unravel-hdinsight-app.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Installation Guides \/ Azure HDInsight Clusters \/ Unravel HDinsight app", 
"snippet" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to inst...", 
"body" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to install Unravel HDInsight app on a fresh Spark 2.1 cluster 1. Launch a Spark 2.1 Cluster Login to the Azure port. Click or choose HDInsight service. Create a new cluster; choosing Spark as the cluster type version Next. 2. Setup storage account configuration for the cluster Either create a new storage account or use existing one. Fill in the storage account information for the spark cluster. 3. Find Unravel app on search box or available app listing Enter UNRAVEL Click OK Create After you accepted the \"Terms of Use\" and \"Privacy Policy\" click Next 4. Review the summary and click Create to launch the cluster + Unravel app. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. 5. Access to Unravel app user interface After Unravel app and the spark2 cluster is successfully launched, go to HDinsight service, look for the spark2 cluster, and click on it. Click on Application In most cases, the Unravel HDInsight app user interface is in the following format https:\/\/<clusterName>-unr.apps.azurehdinsight.net\/ 6. Login to the Unravel app Start your browser and navigate to the Unravel app webpage URL https:\/\/clusterName-apps.azurehdinsight.net. The default admin login credential is admin unraveldata. 7. Unravel Dashboard When logging into Unravel, you will see the Dashboard. See the User Guide 8. Unravel daemons \n ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop 9. Licensing and support By default Unravel app doesn't contains any license keys, and runs without any issue during the inital 30 days trial period. To continue using Unravel app and technical support, contact our sales. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 10. Getting started to use Unravel Please read the Unravel User Guide Getting Started User Guide 11. Unravel API (special note for Unravel app) Unravel provides REST api to perform some operations. To try the api, click on the API tab on the dashboard An API page with avaiable api command options are displayed and explained. You can try the API by clicking \"Try it out\" → Execute buttons; it will display the corresponding curl commands for that REST api call. See below screen capture. From the Unravel user interface, trying out the api will always has \"TypeError: Failed to fetch\". Because the generated curl command is not using https. Copy the generated curl commands and modify it to include default user credential and using https protocol. ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n The api output will be in JSON format shown below and the long numeric string displayed is the epoch time {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } \n signin.png \n image2018-4-19_14-41-51.png \n image2018-4-19_15-18-50.png \n image2018-4-19_15-22-30.png \n image2018-4-19_15-23-46.png \n image2018-4-19_15-37-45.png \n image2018-4-25_11-28-58.png \n image2018-4-25_14-38-10.png \n image2018-4-25_14-41-48.png \n image2018-4-25_14-44-27.png \n image2018-4-25_14-51-2.png \n image2018-4-25_14-57-37.png \n image2018-5-2_14-44-41.png \n image2018-5-2_14-48-12.png \n image2018-5-2_15-2-9.png \n image2018-5-2_15-2-23.png " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-3/user-guide.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optim...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-3/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-3/user-guide/getting-started.html#UUID-fae3d174-ca9b-6e3b-f4b9-7fce5745ef99_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements...", 
"body" : "Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "unravel-4-3/user-guide/getting-started.html#UUID-fae3d174-ca9b-6e3b-f4b9-7fce5745ef99_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin " }, 
{ "title" : "Setting Up Access to Big Data Components", 
"url" : "unravel-4-3/user-guide/getting-started.html#UUID-fae3d174-ca9b-6e3b-f4b9-7fce5745ef99_id_GettingStarted-SettingUpAccesstoBigDataComponents", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Getting Started \/ Setting Up Access to Big Data Components", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"url" : "unravel-4-3/user-guide/getting-started.html#UUID-fae3d174-ca9b-6e3b-f4b9-7fce5745ef99_id_GettingStarted-CreatingUsersandSettingUpEmailSMTPLDAPKerberos", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Getting Started \/ Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"snippet" : "gui_admin_manage_configuration.png gui_email.png...", 
"body" : " \n gui_admin_manage_configuration.png \n gui_email.png " }, 
{ "title" : "Common UI Features", 
"url" : "unravel-4-3/user-guide/common-ui-features.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Common UI Features", 
"snippet" : "Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are enduser rest...", 
"body" : " Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are enduser restricted by Role Based Access Control, you can only view the Applications About Logout If you are unrestricted enduser or an admin, you have all the pages available with possible read\/write restrictions. The pull-down menu has Manage About Logout If your admin has disabled Support If you can configure the date range time period cluster(s) When there are multiple tabs, click on the tab to display its contents. When detailed or further information is available open section ( ) is in the upper right hand corner; click on it for further information. To expand section to the width of the entire tile click on the double arrows displayed ( ); to close it click on ( ). Clicking on the application name\/id\/workflow usually bring ups information on the appplication, fragment etc, i.e., the Spark Application Manager, table information, etc. Lists\/Tables Can be sorted by a column, i.e., start time, in ascending or descending order. The sort column highlights the arrow indicating the sort order ( Clicking on a column being used reverses the sort order. If you can chose which columns to display a plus ( ) is on the right end of the table header. Click on it to see the column headings; check\/uncheck the column heading to show\/hide the column. When applicable, the application status is color coded: Clicking on the app name\/id\/workflow usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. When applicable, the Notifications ) notes if Unravel has tuning suggestions ( ), an Auto Action alert ( ) or both ( ). When relevant there is an Auto Actions\/Events column ( ). The number of auto actions\/events (0-n) triggered is noted. When an application has a parent a link to it will appear in the GoTo ), its title bar or a more info glyph ( ). Click on it glyph display its parent APM. A block glyph ( Graphs (see Operations | Usage Details | Infrastructure Hovering over a line in a graph causes the information to be displayed in a text box ( ). When \" Applications running at mm\/dd\/yy hh:mm:ss\" Clicking Show More ( If graph can be displayed based upon Group By Tags Metric ) in the upper right hand corner. Click to print or download the information in the graph in various forms, i.e., jpg, png, csv. If you can zoom in\/out of a diagram\/execution graph the magnifiers ( , ) in the upper right hand corner. Click to return to the initial view. \n openSection.png \n TitleBarNoSupport.png \n Title-RBAC-RestrictedApps.png \n TitleBar.png \n zoomOut.png \n zoomIn.png \n returnToIntial.png \n MoreInfo-2.png \n fineTuneWAlert.png \n MenuBars.png \n GroupByPullDownClosed.png \n CloseCross.png \n ShowMore.png \n Graph-Popup.png \n block.png \n AACol.png \n AutoAlert.png \n fineTuning.png \n alert.png \n GoTo.png \n killed.png \n unknown.png \n accepted.png \n failed.png \n success.png \n DarkRunning.png \n SortPlus.png \n WriteSort.png \n Expand-Close.png \n Expand-Close-Open.png \n DashboardChartsTitleBarwAr.PNG " }, 
{ "title" : "The Operations Page", 
"url" : "unravel-4-3/user-guide/the-operations-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Operations Page", 
"snippet" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Detail Operations Dashboard Note Click here...", 
"body" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Detail Operations Dashboard Note Click here " }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-3/user-guide/the-operations-page.html#UUID-0a1a8b6a-9d59-e69c-5665-d10264538d5d_id_TheOperationsPage-DashboardDashboard", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Operations Page \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applicat...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applications Tile The line graphs display the successful, failed, and killed jobs for the time period time incremen cluster Clicking on Open Section Applications | Applications Finding Applications Running YARN Application Tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Clicking on Open Section Operations | Usage Details | Jobs here Resources Tile Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations | Usage Details | Infrastructure . below Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications | Applications Finding Applications Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. A separate entry appears for each time a particular Auto Action was triggered. In the image below, the same auto action triggered at 23:56 and 2:58. Clicking an event\/alert brings up a Cluster Resource view ( Operations | Usage Detail | Infrastructure Auto Actions Overview \n Add a New AutoAction or Alert Clear " }, 
{ "title" : "Usage Details", 
"url" : "unravel-4-3/user-guide/the-operations-page.html#UUID-0a1a8b6a-9d59-e69c-5665-d10264538d5d_id_TheOperationsPage-ChartsUsageDetails", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Operations Page \/ Usage Details", 
"snippet" : "Usage Detail has four tabs: Infrastructure Jobs Nodes Impala Usage Kafka One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details recommendations and insights By defa...", 
"body" : " Usage Detail has four tabs: Infrastructure Jobs Nodes Impala Usage Kafka One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details \n recommendations and insights By default the Usage Details tab opens showing the Infrastructure tab. For all charts, click on the menu bars ( Show more Reset Graph Infrastructure Infrastructure Tab This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, i.e., Application Type User Queue Business Tags Clicking within a chart (1) displays the applications running for that point in time. You can chose how to display the bottom two graphs by clicking on the View By Showing View Showing x Infrastructure Application Type above show more To View by use the Business Tags Showing " }, 
{ "title" : "Jobs Tab", 
"url" : "unravel-4-3/user-guide/the-operations-page.html#UUID-0a1a8b6a-9d59-e69c-5665-d10264538d5d_id_TheOperationsPage-ChartsJobsJobsTab", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Operations Page \/ Jobs Tab", 
"snippet" : "Graphs the running and accepted jobs as applicable. You can Group by Nodes Tab This chart graphs the Total Total Active Unhealthy Where: Active: Unhealthy: You can toggle the display of an item by clicking on its name. Impala Tab Graphs memory MB consumption and Query Number. The # Queries Tags Grou...", 
"body" : "Graphs the running and accepted jobs as applicable. You can Group by Nodes Tab This chart graphs the Total \n Total Active Unhealthy Where: \n Active: \n Unhealthy: You can toggle the display of an item by clicking on its name. Impala Tab Graphs memory MB consumption and Query Number. The # Queries Tags Group By Kafka Tab Lists all the configured Kafka clusters. See Kafka Application Manager Kafka Use Case Clicking the cluster name brings detailed information about the Kafka Cluster \n Kafka-ClusterView.png \n OpsUD-Infra-Zoom.png \n OpsUD-Infra-Expd.png \n OpsDash-AA-2.png \n closeCross.png \n OpsUD-ImpalaUsage.png \n OpsUD-Nodes.png \n OpsUD-Jobs.png \n SelectTags.png \n MenuBars.png \n OpsUD-Infra.png \n OpsDash-IneffAppsDrillDown.png \n OpsDash-IneffApps.png \n OpsResourcesTile.png \n OpsDash-RunningYarn.png \n OpsDash-FY-App.png \n OpsDash-FinishedYarn.png \n Operations.png \n OpsUsageHeader.png \n 1-Operation-Charts-KafkaWArrow.png " }, 
{ "title" : "The Applications Page", 
"url" : "unravel-4-3/user-guide/the-applications-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page", 
"snippet" : "The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron...", 
"body" : "The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron Unravel currently supports the the following application frameworks: Cascading\/Pig Hive (on Map-Reduce) Kafka Impala Map-Reduce Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL \n events Event Panel Examples The Applications Page has two views: \n Applications, and \n Workflows. Note Click here " }, 
{ "title" : "Applications Tab", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-ApplicationsTabApplicationsTab", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Applications Tab", 
"snippet" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filter by App type Status Tags , Queue User Cluster Duration Number of Events By time period, Unless restricted Go To...", 
"body" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filter by App type Status Tags , Queue User Cluster Duration Number of Events By time period, Unless restricted \n Go To " }, 
{ "title" : "Workflow Tab", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-WorkflowTabWorkflowTab", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Workflow Tab", 
"snippet" : "Click on the tab Application | Workflow Applications Applications above Workflow Manager Application Manager. Click on Add Workflow Add Selected Workflows Add Selected Workflows....", 
"body" : "Click on the tab Application | Workflow Applications Applications above Workflow Manager Application Manager. Click on Add Workflow Add Selected Workflows Add Selected Workflows. " }, 
{ "title" : "Typical Application Manager's Layout", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-TypicalAPMLayoutTypicalApplicationManagersLayout", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Typical Application Manager's Layout", 
"snippet" : "A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the pa...", 
"body" : " A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the parent's type. Clicking on it brings up its APM. Unravel's Intelligence Engine can provide insights into an application and may provide recommendations, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) \n Event icon No Events Event Panel Examples \n Job icon: \n Job information: \n KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation ) column notes the number of events associated with the job\/stage. Common Sub-Tabs: \n Errors: Keywords \n Conf\/Configuration \n Tags project dept group11 hr " }, 
{ "title" : "Cascading and Pig Application Managers", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-CascadingCascadingandPigApplicationManagers", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Cascading and Pig Application Managers", 
"snippet" : "The only Key Performance Indicators Events Event Panel Examples Duration Data I\/O Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Sub-Tabs The left sub-tabs: Navigation Gantt chart Tags here The right sub-tabs Task Attempts entire Attempts...", 
"body" : "The only Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Sub-Tabs The left sub-tabs: \n Navigation \n Gantt chart \n Tags here The right sub-tabs \n Task Attempts entire \n Attempts " }, 
{ "title" : "Hive Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-HiveAPMHiveApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Hive Application Manager", 
"snippet" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indi...", 
"body" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indicators \n Events Event Panel Examples Hive Event List \n Duration \n Data I\/O \n Number of YARN apps Sub-Tabs By default the Hive APM opens displaying the Navigation Query The left sub-tabs are: \n Navigation MapReduce Application Manager \n Execution Graph The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other along some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). Click on a table name to bring up the table information Close \n Gantt Chart \n Errors here \n Tags here The right sub-tabs are: \n Query window Copy Query \n Tables: here \n Task Attempts entire \n Attempts " }, 
{ "title" : "Impala Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-ImpalaAPMImpalaApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Impala Application Manager", 
"snippet" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators Events Event Panel Examples Impala Event List Duration Data I\/O Number of Fragments Number of Operators Sub-Tabs The left sub-tabs are: Fragments More L ess This window shows the F...", 
"body" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators \n Events Event Panel Examples Impala Event List \n Duration \n Data I\/O \n Number of Fragments \n Number of Operators Sub-Tabs The left sub-tabs are: \n Fragments More L ess This window shows the Fragment and it's KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking on the operator brings up the operator window. (See Operators below for more information.) You can view the Query Plan \n Instance View: \n Operators \n Scan HDFS details \n Aggregate Details \n Exchange Details \n Gannt Chart \n Query Plan \n Tags here The right sub-tabs are: \n Query Query Copy window \n Mem Usage " }, 
{ "title" : "Kafka Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-KafkaAPMKafkaApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Kafka Application Manager", 
"snippet" : "The Kafka Application Manager provides Multi-Cluster support for monitoring : Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out\/...", 
"body" : "The Kafka Application Manager provides Multi-Cluster support for monitoring : Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled \n Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators \n Bytes in\/sec \n Bytes out\/sec \n Messages in\/sec \n Total Fetch Requests per \/sec \n Number of Active Controller \n Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafka Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators \n Number of Topics \n Number of Partitions The Topic lists displays the KPIs; when details are available a more info \n The Partition View You can chose both the Partition Metric th offset Partition Details' \n Kafka Topic view The Kafka View has two tabs, Topic Detail Partition Detail \n Consumer Details' \n Kafka Topic Detail By default the Kafka Topic Detai Topic Detail \n Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is \n Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. \n Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: \n OK, \n Warning: \n Error " }, 
{ "title" : "MapReduce Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-MapReduceAPMMapReduceApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ MapReduce Application Manager", 
"snippet" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sect...", 
"body" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators \n Events Event Panel Examples MapReduce Event List \n Duration \n Data I\/O Sub-Tabs By default the MapReduce APM opens in the Graphs | Attempts \n Graphs \n Attempts \n Containers, Vcores, \n Timeline The Timeline tab is divided into two sections: a Distribution Map Reduce a bottom table which lists either the tasks by stages on servers or teh list of tasks and their associated KPIs' The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map Reduce Timeline Selected \n Metrics \n Logs: Below is an example of a log. \n Configuration: \n Resource Usage Metric Metric nly Show All \n Errors here \n Tags here " }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-SparkApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Spark Application Manager", 
"snippet" : "See Spark Application Manager page...", 
"body" : "See Spark Application Manager page " }, 
{ "title" : "Tez Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-TezAPMTezApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Tez Application Manager", 
"snippet" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). \/srv\/unravel\/log_hdfs\/unravel_us_1.log Key Performance Indicators Events Event Panel Examples Tez Event List Duration Data I\/O Sub-Tabs By default the Tez APM opens showing the N...", 
"body" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). \n \/srv\/unravel\/log_hdfs\/unravel_us_1.log Key Performance Indicators \n Events Event Panel Examples Tez Event List \n Duration \n Data I\/O Sub-Tabs By default the Tez APM opens showing the Navigation and Program Tabs. The left sub-tabs are: \n Navigation The DAG detail has six tabs: \n Query: \n Graph: \n Counter \n Vertex Timeline Wall Clock Total Run \n All Vertices \n All Task \n All Task Attempts \n Changed Configuration \n Configuration \n Tags here The right sub-tabs are: \n Program \n Graphs \n Containers, Vcores, \n Resources Resource systemCpuLoad Select series Metric Get Data " }, 
{ "title" : "Workflow Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-WorkflowAPMWorkflowManager", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Workflow Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applica...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications | Applications Key Performance Indicators \n Events Event Panel Examples Workflow Event List \n Duration \n Data I\/O \n Number of Yarn Apps Sub-Tabs The APM opens showing the Navigation Compare The left sub-tabs \n Navigation More Below the second Oozie node is shown, it is comprised of one MapReduce job and three Hive jobs. The hive jobs comprise one or more tasks, so that too can be expanded. In the example below, the second Oozienode has been expanded along with the first hive job within it. You can click on any job to see the application manager for it. In the example, below you can click on the expanded hive job to bring up the hive application manager. Similarly you can click on the mapreduce job within the hive job to go directly to it. Click on Less \n Execution Click \n Errors here \n Tags here The right sub-tabs: \n Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events above \n Task Attempts \n Attempts " }, 
{ "title" : "Event Panel Examples", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-EventPanelEventPanelExamples", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Event Panel Examples", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job Example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. \n Recommendations \n Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). \n Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . \n Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). \n Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG Example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. \n Recommendations \n Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. \n Efficiency 2: Tez DAG Resulted in one recommendation (#1). \n Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Appendices", 
"url" : "unravel-4-3/user-guide/the-applications-page.html#UUID-f67a23de-520f-6e4f-c58c-b397f19ca5a1_id_TheApplicationsPage-Appendices", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Appendices", 
"snippet" : "Resource Metrics A list of resource metrics collected by Unravel. 20180716_172.36.1.124-WorkflowTab.png AACol.png recbar.png returnToIntial.png zoomIn.png 20180430_172.36.1.110-Tez-APM.png 20180419_172.36.1.124-PigGantt.png 20180419_172.36.1.124-Pig-TaskAttempts.png 20180419_172.36.1.124-PigAPM.png ...", 
"body" : "Resource Metrics A list of resource metrics collected by Unravel. \n 20180716_172.36.1.124-WorkflowTab.png \n AACol.png \n recbar.png \n returnToIntial.png \n zoomIn.png \n 20180430_172.36.1.110-Tez-APM.png \n 20180419_172.36.1.124-PigGantt.png \n 20180419_172.36.1.124-Pig-TaskAttempts.png \n 20180419_172.36.1.124-PigAPM.png \n 20180430__172.36.1.124-Attempts.png \n 20180430__172.36.1.124-TaskAttempts.png \n zoomOut.png \n 20180430 -172.36.124-WrkFlw-ExecHover.png \n 20180430 -172.36.124-WrkFlw-NavExpanded.png \n 2018-04-30_120947_172.36.1.124-WorkFLowAPM.png \n 20180430__172.36.1.124-WrkFlw-AddedSelected.png \n Impala-InstanceView.png \n Impala-FragmentOpen-2.png \n coord.png \n Impala-FragMorewCoord.png \n Impala-APM.png \n Impala-Gannt-ShowingSectionInfo.png \n 20180420_Impala-MemoryUsagepng.png \n 20180420_MR-APM-Resource-ShowOnly.png \n 20180420_MR-APM-Conf.png \n 20180420_MR-APM-TaskLogDis.png \n 20180420_MR-APM-Logs.png \n 20180420_MR-APM-Metrics.png \n 20180420_MR-APM-Timeline-MapTimeline.png \n 20180420_MR-APM.png \n HIVE-APM-Attempts.png \n HIVE-APM-TaskAttempts.png \n Hive-Table.png \n HIVE-MR-APMs.png \n Tags-FromSpark.png \n Errors-fromMR.png \n 20180420_Hive-APM-TypLayoutExample.png \n MR-APM-KPI-Section.png \n APM-TitleBarWParent.png \n 4.3-ApplicationsExcerpt.png \n 4.3.1-Applications.png \n 4.3-ApplicationsHeader.png \n Workflow-APM.png \n MapReduce-4Rec4Eff-Eff3.png \n Tez-Dag-Eff3.png \n Tez-Dag-Eff1.png \n MapReduce-4Rec4Eff-Eff2.png \n Tez-APM-0274-DagDetail-ChangedConfiguration.png \n Tez-APM-0274-DagDetail-AllTaskAttempts.png \n Tez-APM-0274-DagDetail-Tasks.png \n Kafka-Topic-Partition.png \n Kafka-Topic-Topic.png \n Kafka-ConsumerGroupTopicDetail.png \n Kafka-Cluster-WithConsumer-KPIsOnly.png \n Impala-Operator-ScanHDFS.png \n Tez-APM-0274-DagDetail-VertexTimeline-WallwRunTime.png \n HIVE-APM-Tables.png \n Kafa-Cluster-WithConsumer-MetricGraphsOnly.png \n Tez-Dag-Rec.png \n MapReduce-4Rec4Eff-Eff1.png \n Tez-Dag-Eff2.png \n MapReduce-4Rec4Eff-Eff4-ex.png \n Impala-Operators-60px.png \n Tez-APM-0274-DagDetail-AllVertices.png \n ClusterList.png \n Impala-QueryPlan-60px.png \n Kafa-Cluster-WithConsumers-TopicListOnly.png \n Hive-APM-Exec-Popup-2-60.png \n Impala-Operator-Aggregate.png \n Tez-APM-0274-DagDetail-Graph-DetailBox.png \n Tez-APM-0274.png \n MapReduce-4Rec4Eff-Rec.png \n Tez-APM-0274-Resources.png \n Impala-Operator-Exchange.png \n Kafak-ConsumerGroup-ParitionDetail.png " }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-3/user-guide/the-applications-page/spark-application-manager.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Applications Page \/ Spark Application Manager", 
"snippet" : "Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source co...", 
"body" : "Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. \n Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source code from the stage tile, spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an application's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark driver and executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. Unravel provides insights into Spark applications and potentially tuning recommendations; see Spark Event List There are multiple Spark application types and the Spark APM information can vary by the application type. Currently Unravel distinguishes between: \n Scala, Java, and PySpark \n SQL-Query \n Streaming Regardless of the application type and how they are are submitted (e.g., from Notebooks, spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark Application Manager's Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). The title bar's right side are glyphs for adding a comment, and to minimize or close the tile if possible. If it has a link to the parent, there is an up arrow ( Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions, or insights into how to improve the application's run. When there are insights, a bar appears immediately below the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples Spark Event List The next section contains the Key Performance Indicators (KPIs) and general application information. \n Event icon No Events Event Panel Examples Spark Event List \n Application icon: application status \n Application information: \n Key Performance Indicators (KPIs): The last section, divided in half, has specific information related to the application. The sections for a specific Spark Application (e.g., Streaming) go into more detail. If the application is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Stream Common Tabs Regardless of the application type, the Application Manager view is split into two blocks and each block contains the following tabs. Left Tabs \n Errors Lists all errors associated with the application. The errors are color coded (fatal Keywords \n \n Logs Below is an excerpt of the executor-20 \n Conf Reset Right Tabs \n Program Uploading Spark Programs to Unravel. \n Task Attempts \n Graphs \n Running Containers \n Vcores \n Memory \n Running Containers \n Allocated Vcores \n Allocated Memory \n \n Resource You chose one or more series to display using the Select X Hovering only resource highlights it, clicking on it toggles the display, i.e., if currently displayed it is removed from the graph. Conversely if you click the Only Show All Below is an example of the JSON when clicking on Get Data Common Tiles Spark Job A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators \n Duration: \n Number of Stages: It has three tabs: \n \n Stages start time \n \n Gannt Chart \n \n \n Metadata Spark Stage The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators \n Duration: \n Date IO It has two tabs, by default it opens in the graph \n Graph \n \n \n Task Attempt \n \n \n \n \n Program Details Time Line Tab The Time Line tab has two sections, \n Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown, Selected Tasks Time Line The Time Line tab has two sections, \n Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown Selected Tasks Be default the Time Line Distribution Charts ShuffleMap Disk Bytes Spilled Memory Bytes Spilled Records Read. The lower section opens displaying the Time Line , \n Timeline Breakdown Scheduler Delay, Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Using the ratio of Executor Computing Time performing actual work thrashing, or waiting for scheduling. \n Selected Tasks Spark - Scala, Java, and PySpark Key Performance Indicators \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Stages Left Five Tabs \n Navigation Lists the application's jobs with their relevant KPIs: Status Start Time Duration Partitions\/Tasks Read Write # Stages Start Time The job block lists the KPIs Duration # of Stages Stages Metadata Status Start Time Duration Partitions\/Tasks Read Write Input Output Start Time Spark Stage Details \n Execution RDDs Operations Program If the program tab displays the code it is linked with the DAG. If you display the execution and program tab simultaneously, as shown below. Click on the vertex to highlight relevant code. Below we see the corresponding code for vertex 18. Below we expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call and finally the number of stages which were involved. Below RDD represented by vertices 17-16 involved two (2) stages, while 15-16 had five (5). Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. \n Gantt Chart Displays the stages using a Gantt Chart. The table is sorted on Start Time \n Errors, Log and Conf Tabs Errors Logs Conf Four Right Tabs \n Program above \n Task Attempts, Graphs and Resources Task Attempts Graphs Resources Spark - SQL-Query Key Performance Indicators \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Stages Left Five Tabs \n Navigation above \n Execution above \n Gantt Chart above \n Errors, Log and Conf Tabs Errors Logs Conf Four Right Tabs Program This tab connects all the pieces of a SQL query. The table lists all queries with significant KPI's and the top five stages, i.e., the stages with the longest duration. The lower section contains two tabs, SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. \n SQL Plan - \n Task Attempts, Graphs and Resources Task Attempts Graphs Resources Spark - Streaming Enable Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf. After the Unravel sensor has been deployed and installed on the cluster open spark-defaults.conf spark.driver.extraJavaOptions X.Y. \n javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-X.Y. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited. Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. \n Events Event Panel Examples Spark Event List \n Duration \n Data I\/O \n Number of Jobs \n Number of Stages Unlike other Spark Application Managers this has a Stream tab Stream Program Left Four Tabs \n Stream Displays the core of an Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Scheduling Delay Metric Scheduling Delay, Processing Time Total Delay. This graph is composed of two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven (7) pages. In the example below, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one (1) page, versus the seven (7) above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time Click on a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. \n Stream Batch Block Duration Processing Delay Scheduling Delay Total Delay Output Operation Input The example below is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two (2) jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click on the Job ID Spark Job The Input Tab \n Errors, Log and Conf Errors Logs Conf Right Four Tabs \n Program The program (if uploaded by the user) is shown in this tab. \n Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resources " }, 
{ "title" : "The Reports Page", 
"url" : "unravel-4-3/user-guide/the-reports-page.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Reports Page", 
"snippet" : "Unravel provides a variety of reports to help you manage your clusters. The page has two tabs. Operational Insights Data Insights The default view of Reports is Reports Operational Insights | Chargeback Note Click here...", 
"body" : "Unravel provides a variety of reports to help you manage your clusters. The page has two tabs. \n Operational Insights \n Data Insights The default view of Reports is Reports Operational Insights | Chargeback Note Click here " }, 
{ "title" : "Operational Insights", 
"url" : "unravel-4-3/user-guide/the-reports-page.html#UUID-f1abcce2-6d62-f465-45e7-f9e52e22975e_id_TheReportsPage-OperationalInsights", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Reports Page \/ Operational Insights", 
"snippet" : "Has four tabs. Chargeback Cluster Summary Cluster Compare Cluster Workload If you can specify a date range, a pull down menu appears on the right hand side of the Operational Insights title bar. By default Operational Insights Chargeback Application Type, Chargeback You can generate ChargeBack Group...", 
"body" : "Has four tabs. \n Chargeback \n Cluster Summary \n Cluster Compare \n Cluster Workload If you can specify a date range, a pull down menu appears on the right hand side of the Operational Insights title bar. By default Operational Insights Chargeback Application Type, Chargeback You can generate ChargeBack Group By Application Type, User, Queue Tags. Application Type Donut graphs showing the top results for the Group by Charge back report showing costs, sorted by the Group By choice(s), and List of Yarn applications running. Generate Charge Back Report You can set the date range and clusters to use for the report in the Operational Insights Group By Group By User dept User dept VCore\/Hour Memory MB\/Hour Update Report CSV A new charge back report is generated each time you change the Group By must Update Report Cluster Summary The Cluster Summary Applications User Queue User Applications You can sort applications on vcore or memory seconds. User Queue Cluster Compare This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use Group By User Queue Time Range Compare With Range Any deviation in metrics across the time ranges is highlighted (2). A green red Time Compare With Group By Cluster Workload Displays your cluster(s) workload. You can use to this graphs to drill down into your cluster workload by weekday, hour and by hour within a given day. For the specified time period can set the display ( View By Month Hour Day Hour Day Month Operational Insights In the following examples the workload displayed is for the past 16 days (July 1 through July 16) during which 441 were run. \n Month Every day the cluster had jobs. The color indicates how the day's load compares with the other days. The day will the least jobs is colored previous next The Hour Day Hour Day \n Hour This graph shows 72 (16%) of the jobs ran between 1 am and 2 am. \n Day This graph shows 205 (46%) of the jobs ran on Thursday. \n Hour\/Day With this graph we see: 25 (34%) of 72 jobs which ran between 1-2 am 25 (12%) of 205 which ran on Thursday " }, 
{ "title" : "Data Insights", 
"url" : "unravel-4-3/user-guide/the-reports-page.html#UUID-f1abcce2-6d62-f465-45e7-f9e52e22975e_id_TheReportsPage-DataInsightsTabDataInsights", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ The Reports Page \/ Data Insights", 
"snippet" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. See Hive Metastore Configuration It has two tabs. Overview Details Both tables and partitions have color coded labels when applicable: Hot Warm Co ld policy configuration Overview The Overvi...", 
"body" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. See Hive Metastore Configuration It has two tabs. \n Overview \n Details Both tables and partitions have color coded labels when applicable: Hot Warm Co ld policy configuration Overview The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. \n Table KPIs \n Partition KPIs The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Partition KPIs sections contain: \n # Accessed \n # Created \n Size Created \n Total Number The Table KPI's also contains: \n Accessed Queries \n Total Read IO Donut Charts These display the Current Label Distribution policy configuration Details Tab The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In this example, the second through fourth table are selected and graphed. Use the Metric Read IO, Total Users, Total Attempts, Total Size Reset Graph Read IO Table List You can Search Show All Read IO More Info Table Detail . \n Configuration Policy Download CSV Table Detail This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed: Read IO User # Apps, Attempts There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Read IO, Total Users, Total Attempts, Total Size Application Detail Application Tab Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition S ize MR jobs By default the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View Configuration This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data | Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save \n 20180716_172.36.1.126-TableList.png \n 20180716_172.36.1.124-Rep-DI-Details.png \n Configuration.png \n CurrentLabelDistribution.png \n 20180716_172.36.1.124-Rep-DI.png \n 20180716_172.36.1.110_RepOpInsightClComp.png \n 20180716_172.36.1.110_RepOpInsightOpening.png \n 20180716_172.36.1.110_RepOpInsightClChgbk.png \n 20180716_172.36.1.110_RepOpInsightClSumQ.png \n 20180716_172.36.1.110_RepOpInsightClSumUser.png \n 20180716_172.36.1.110_RepOpInsightClSumApps.png \n 20180716_172.36.1.110_RepOpInClComp.png \n 20180716__172.36.1.11-RepOpsClCompHourDay-Def.png \n 20180716__172.36.1.11-RepOpsClCompMonth-Def.png \n 20180716__172.36.1.11-RepOpsClCompDay-Def.png \n 20180716__172.36.1.11-RepOpsClCompHour-Def.png \n Wrkld-5.png \n Wrkld-1.png \n Data-TableAPM-1stTable-RetentionDetail-60px.png \n Data-TableAPM-1stTable-PartitionDetail-60px.png \n Data-TableAPM-1stTable-60px.png \n cold.png \n Warm.png \n hot.png " }, 
{ "title" : "Auto Actions", 
"url" : "unravel-4-3/user-guide/auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions", 
"snippet" : "Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Supported cluster metrics Running Auto Action Demos...", 
"body" : " Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Supported cluster metrics Running Auto Action Demos " }, 
{ "title" : "Auto Actions Overview", 
"url" : "unravel-4-3/user-guide/auto-actions/auto-actions-overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Auto Actions Overview", 
"snippet" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can b...", 
"body" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can be set to automatically kill an application or move it to a different queue. The Unravel Server processes auto actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: \n Prerequisite conditions: \n Defining conditions: \n Actions Viewing Auto Actions Select Manage | Auto Actions. The auto actions tab provides a quick way to view auto actions and quickly see their status, along with its defined actions and scope. The tab displays all defined auto actions separated into an Active and Inactive list. You enable\/disable by clicking the check box on the left. You can edit ( define new auto actions Hovering over the auto action's name gives you the description which was entered when defining the auto action. Hovering over action or scope glyph brings up its detail. For example, for the active auto action above: rule description: , email action: an email is sent to only 1 person, , and queue scope: is three queues: . The Actions Scope quicktest must Expert Rule quicktest in MR History of Runs By default all actions are off. Possible actions are: Send an Email ( ) Kill the App ( ) Move the app to another queue ( ), and Send a Http post ( ) By default the various scopes apply to all, i.e., all applications and constantly on. The scopes are: User ( ) Queue ( ), Cluster ( ), Application ( ), Time ( ), Sustained Violation: This is not shown in the auto actions list. If you have not defined a particular action or scope, i.e., it's using the default, the glyph is grey ( The history of runs contains an entry for each time the auto action was triggered. Click on the run's Link Reports Resources The Cluster View shows a time slice, ±5 minutes from when the auto action was triggered and lists all the applications running during that period. This application table is similar to the application table shown under. This application table is similar to the application table shown under Applications | Applications Notifications 'Snoozing' Auto Actions The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., the new violation is essentially noise. See Snooze Feature \n \n \n Property \n Definition \n Possible Value \n Default \n \n \n com.unraveldata.auto.action.snooze.period.ms \n The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app Value is in millliseconds. \n 0: snooze is turned off. > 0: snooze is on, there is no no upper bound. \n 1 hour (3,600,000 seconds) When you change the snooze time period all applications currently snoozed are reset. Upon next violation the application is \"snoozed\" using new snooze value. To change the snooze time On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms. com.unraveldata.auto.action.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart \n QueueScopeAll.png \n Mail-Enabled-Inactive.png \n Mail-Enabled-Active.png \n Mail-Default.png \n Kill.png \n Move.png \n App.png \n User.png \n HttpPost.png \n trash.png \n Edit.png \n alert.png \n AutoAlert.png \n finetuning.png \n finetuneWAlert.png \n 20180424_172.36.1.124-AA-Default.png \n Time.png \n Cluster.png \n ClusterViewForAA-Link.png \n AA-History.png \n 20180424_172.36.1.124-SingleAA.png \n QueuePopup.png \n emailPopup.png \n AADescriptPopup.png " }, 
{ "title" : "Creating Auto Actions", 
"url" : "unravel-4-3/user-guide/auto-actions/creating-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Creating Auto Actions", 
"snippet" : "1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edit ( 2....", 
"body" : "1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edit ( 2. To create a new auto action, click either Create from Template, Build Rule, or Expert Rule. \n Create from Template \n Build Rule \n Expert Rule 3. Using Create from Template or Build Rule to define you auto action. Whether using Create from Template Build Rule The sections are: \n Template Name - Create from Template Build Rule Expert Rule \n Auto Action Name and Description \n Ruleset \n Options \n Actions Name and Description. The name is mandatory; the description is optional. The name is used by the UI for all auto actions' displays; we recommend using a name that reflect the auto action's purpose. The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name the description is displayed. This example is from Create from Template Build Rule Ruleset At least one rule type ( User Queue, Apps \n Expert Rule \n metric type state Build Rule \n Metric See Supported Cluster Metrics metrics The comparison value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type mapreduce, yarn, tez, spark, workflow and hive. The State new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: \n Or, And Same \n \n Or \n And Same Logical Operator Same The Ruleset User Queue, Cluster App \n Expert Rule Add Queue metric type state Expert Rule must In the example below, Metric Type Queue , Apps metric comparison operato type state above , Same Or And Same Logical Operator Same Or And . Click Close trash Example of Create from Template Ruleset This template has the Ruleset metric comparison type state Metric \n Type or State metric Same'd Same Logical Operator Options Define the scope ( User Queue Cluster Application Name Time Sustained Selected options default to All, Time Sustained Violation For Build Rule When using Create from Template Queue . \n Check User Queue Application Name Only Except Only rule Except all but those specified Transform Application Name t Except Add Application except Create from Template All The Time start end click \n Sustained violation minimum maximum \n Minimum \n Maximum Actions Defines the actions to take when the auto action is triggered. Both Create from Template Build Rule Send an email Http Post Post to Slack Move App to Queue Kill App Build Rule Expert Action. Expert Mode Auto Actions and Pagerduty You can chose one of more actions. Check the text box to chose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. For Send Email Add Recipient Include Owner For HTTP post Add URL \n Post to Slack \n https:\/\/slack.com\/api\/chat.postMessage public Slack channel, private channel, or direct message\/IM channel, It provides a better integration with Slack Service and allows you to send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: \n \n test tokens \n \n custom bot user token \n \n Slack App user token \n chat:write:bot \n \n Slack App bot user token \n Move app to queue Kill App. The Move App Kill App Kill App Move App Have directly caused the rule violation, and Have allocated resources, i.e. in allocated or running states. \n \n Move App \n Kill App The Build Rule Expert Rules Examples of content for email, http post and slack Both the Email Http Post Cluster View auto action history \n Sample Email Auto Action policy \"ROGUE APPLICATION #1\" violation detected.\n\nPolicy description: \"Identify applications that are using too much of the cluster resources\"\n\nApplication \"application_1498514199803_2411\" has 1 violation:\n\n1. Sum of memory in MB allocated to containers is 1GB >= 1MB\n\nTimestamp: 07\/19\/2017 08:41:35 +0000\n\nReach Unravel server at http:\/\/localhost:3000\/\n\nSee cluster history at http:\/\/localhost:3000\/ops_dashboard\/charts\/resources?from=1499817549571&to=1499817669570&at=1499817609571&interval=1m \n Sample Slack Post 4. To create an auto action, click eitherCreate from Template, Build Rule, or Expert Rule. \n \n Create From Template Click on Create a Template above \n Resource contention \n Resource contention in cluster \n Resource contention in queu \n Rogue Identification \n Rogue user \n Rogue application \n Long Running Jobs \n Long running YARN application \n Long running Hive query \n Long running workflow \n Build Rule above \n Expert Rule Expert Rule Sample Auto Actions 5. Click on Save Auto Action Your auto action is now listed in the Manage | Auto \n SlackPost.png \n clock.png \n 20180426_172.36.1.124-SustainSelected.png \n Edit.png \n trash.png \n 20180424_172.36.1.124-AA-DefaultNoArrow.png \n 20180424_172.36.1.124-ActionsExpMovKil.png \n 20180424_172.36.1.124-ActionsExpSlck.png \n 20180424_172.36.1.124-ActionsExpMail.png \n 20180424_172.36.1.124-ActionsExpExerpt.png \n 20180424_172.36.1.124-ActionsExpHttp.png \n 20180424_1-AA-Actions.png \n 20180426_172.36.1.124-TimeSelected.png \n 20180426_172.36.1.124-AppSelected.png \n 20180426_172.36.1.124-OptNoneSel.png \n 20180424_172.36.1.124-RuleSet-QueueRuleSetWithApp.png \n 20180424_172.36.1.124-RuleSetQueueSel.png \n AA-CAT-NameHigh.png \n 2018-04-24_172.36.1.124-RulesetResContQuMB.png \n 20180424_172.36.1.124AA-Templates.png " }, 
{ "title" : "Expert Rule", 
"url" : "unravel-4-3/user-guide/auto-actions/expert-rule.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Expert Rule", 
"snippet" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clea...", 
"body" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts and capabilities, along with JSON, which is used to define the auto action. This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule In this mode you must specify Prerequisite condition s Defining conditions Actions When using Expert Rule must rules actions {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} Header: Rules Options - Actions: Defining the Rule Header You must define a header. Attributes Name Definition Possible Value Required Default Value enabled Whether the auto action is active or not. True: active\/enabled. False: inactive\/disabled True | False √ - policy_name Value defined by Unravel. AutoActions2 √ AutoActions2 policy_id Value defined by Unravel. 10 √ 10 instance_id Any unique value √ - name_by_user Any unique string. The name is used when the auto action is displayed in the UI. √ - description_by_user Description of the auto action. - created_by Value defined by Unravel. admin √ admin last_edited_by Value defined by Unravel. admin √ admin created_at Time created. Date and time is in the form of a Epoch\/Unix timestamp. √ - updated_at Time updated. Date and time is in the form of a Epoch\/Unix timestamp. √ - Sample \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: Defining conditions You must define at least one rule. Field Name Definition Possible Values Required\/ Required by Default Value scope The rule scope. app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container ,containers, multi_ containers queue, queues, multi_queue, user, users, multi_user Note √ - target Application name any valid application name when scope is by_name - metric Metric used for comparison. see supported metrics per type - comparison Comparison operator >, >=, ==, <=, < metric - value Value for comparison. The value form varies by metric. number metric - state Scope state new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * - type Job type mapreduce, yarn, tez, spark, workflow, hive - Logical Operators for Evaluating Multiple Rules Operator Condition for a Violation OR At least one rule evaluates to true. AND All rules evaluate to true. SAME All the rules evaluate to true and See Same Logical Operator A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n\n \/\/ at least one of the following\n\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \n \"state\":\"\",\n\n \"type\":\"\"\n }\n] Violation occurs when the application is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the state \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the state type \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n Rule Array Examples Note Multi_X X. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n OR Example When they are OR'ed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] AND Example When AND'ed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] SAME Example When SAME'd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the auto action is not triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However if My_App violates both rules (allocatedMB andallocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the auto action is triggered for My_App but not Your_App. OR, AND and SAME Given the same ruleset, evaluation becomes more restrictive. OR: the auto action is triggered if one or more of conditions is true. AND: the auto action is triggered if all of conditions are true. SAME: the auto action is triggered if all of the conditions are true within Options - Policy\/Scope: Prerequiste conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. Field Name Definition Conditions Required\/ Required by Possible Values Default Value X_mode where X The mode defines how the rules are applied to type X 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type X. 2 - the rules are evaluated for only X X 3 - the rules are evaluated for everything but X X By Definition. You must define at least one option\/policy. 0, 1, 2, 3, 0 X_list A list of X type Only applicable if mode is set to 2 (only) or 3 (except). if X X_ empty, single item or comma separated list. - X_transform A list of regex used to generate a list of X Only applicable if mode is set to 2 (only) or 3 (except). if X X empty, single regex or comma separated regex list - Time The daily time the Auto Action is trigger. any time period spanning less than 24 hours. - Sustained Violation Set a minimum or maximum time period for the auto action to be triggered. See here any time period less than 24 hours. - Options - Policy\/Scope Rule where X \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , Cluster \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", Queue \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", Application Name except \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action(s) to implement upon violation You do not have to define any actions, but it defeats to purpose not to. If no actions are defined, the UI keeps track of the auto action and when triggered, who triggers it. Both the prerequisite defining Field Name Definition Required\/ Required by Possible Value Default Value action The action to be taken. at least one send_email, http_post, post_in_slack, move_to_queue, kill_app - to Email recipients. send_mail if to_owner One or more recipients in a comma delimited list. - to_owner Send email to owner. send_mail if to false: do not send email true: send email false urls URLs for Http post. http_post One or more URLs in a comma delimited list. - token Token generated by slack post_in_slack Slack token - channels Slack channel. post_in_slack One or more channels in a comma delimited list - queue Queue name. move_to_queue The name of a valid queue to move the app to. - Single Action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple Actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] Example Actions There are five (5) main actions : send_email http_post post_in_slac , move_to_queue kill_app send_email, http_post, post_in_slack Creating Auto Actions send_email http_post post_in_slack Note You must take care when entering information. A specified action fails if you enter the incorrect information, i.e., bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template Build Rule to_owner \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] http_post Just like send_email \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma delimited list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] kill_app This is straight forward, but kill_app is a destructive action \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] Actions can be Ignored When in Conflict Below we specified two actions, move_to_queue kill_app kill_app move_to_queue actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n] Action(s) Fail if the Required Information is Invalid or not Specified. Below are two actions with invalid information. In send_mail http_post Operations Dashboard , ;history of runs cluster view \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] An Expert Rule Example This auto action triggers on applications using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the applications, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the application is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} Auto Actions Examples See Sample Auto Actions Build Rule " }, 
{ "title" : "Same Logical Operator", 
"url" : "unravel-4-3/user-guide/auto-actions/same-logical-operator.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Same Logical Operator", 
"snippet" : "SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[ “SAME”:[ { “scope”:”users”, “metric”:”appCount”, “operator”:”>”, “v...", 
"body" : " SAME and Example Auto Action rule designed to alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[\n “SAME”:[\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:10,\n state”:”running”\n },\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:5,\n “state”:”pending”\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and 3 pending apps user B has 7 running and 1 pending apps user C has 21 running and 11 pending apps First, the two simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has 7 → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has 3 → FALSE User B has 1 → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called “linking” of rules (see Ruleset.java). Cluster “User A”, link count = 1. User A > 10 running apps? → TRUE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Cluster “User C”, link count = 2. User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule reset for the group. Cluster “User A” has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE FALSE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster “User C”, link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User Auto Action, therefore User C triggers the Auto Action and the alert is sent and\/or the actions performed. Comparison with AND Both User A and User C would have triggered the above rule were AND SAME any AND any To achieve the same result as the above example using AND SAME each and every user ( Username AND Username " }, 
{ "title" : "Snooze Feature", 
"url" : "unravel-4-3/user-guide/auto-actions/snooze-feature.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App ...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App An auto action specifying a Kill App Move App Snooze is set the first time the app violates the rule. The auto action itself continues to run uninterrupted whether zero (0) or all apps currently covered by the auto action are snoozed. The auto action takes action for any app not snoozing If an app is still violating upon awaking, snoozed Example Rule\/Action Two apps Snooze time at 20:00 A > 1GB → email is sent + snooze set (runs until 20:30). B < 1GB → app not violating so nothing is done. at 20:10 A > 1GB → snoozing B > 1GB → email is sent + snooze set (runs until 20:40). at 20:20 A > 1GB → snoozing B > 1GB → snoozing at 20:30 A > 1GB → application wakes B > 1GB → snoozing at 20:40 A > 1GB → snoozing, B < 1GB → app wakes To change the snooze time Property Definition Possible Value Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened snoozed An auto action containing a kill move app Value is in milliseconds. 0: snooze is turned off > 0: no upper bound 1 hour (3,600,000 seconds) On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Sample Auto Actions", 
"url" : "unravel-4-3/user-guide/auto-actions/sample-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Sample Auto Actions", 
"snippet" : "Information on Demo Auto Actions can be found here 1. Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode 1.1 Alert Examples Alert if Hive query duration > 10 minutes { \"scope\": \"multi_app\", \"user_metric\": \"duration\", \"type\": \"HIVE\", \"state\": \"RU...", 
"body" : "Information on Demo Auto Actions can be found here 1. Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode 1.1 Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: 1.2Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} 2. Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. 2.1 Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. 2.2 Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n 2.3 Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n \n Alert if SLA bound query is taking longer than expected. a. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e. for example alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n 2.4 Workflow Alert if a workflow is taking longer than expected. a. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} b. Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} 2.5 User Alert for Rogue User - Any user consuming a major portion of cluster resources. a. Check for any user where the allocated vcores aggregated over all their applications is > 1000. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} 2.6. Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} 2.7 Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Alert for Rogue application - any application which is consuming a major portion of cluster resources. a. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue You can use the Rogue Application or memory Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Set Time rule condition as: Set Move app rule as: b. If any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application or memory Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n}\n or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\":\"allocated_mb\",\n \"compare\": \">\",\n \"value\": [X]\n} Set Queue rule conditions as: Set Move app action as: Related articles \n Running Auto Action Demos \n image2017-10-20_22-58-26.png \n image2017-10-20_22-34-17.png \n image2017-10-20_22-50-18.png \n image2017-10-20_22-59-21.png \n image2017-10-20_22-59-5.png \n image2017-10-20_22-40-12.png \n image2017-10-20_22-40-33.png \n image2017-10-20_22-49-6.png \n image2017-10-20_23-0-32.png \n image2017-10-20_22-59-50.png \n image2017-10-20_22-42-41.png \n image2017-10-20_22-59-40.png \n image2017-10-20_22-50-40.png \n image2017-10-20_23-0-16.png \n image2017-10-20_22-58-3.png \n image2017-10-20_22-43-6.png \n image2017-10-20_22-49-23.png \n image2017-10-20_22-55-57.png \n image2017-10-20_22-58-41.png " }, 
{ "title" : "Supported cluster metrics", 
"url" : "unravel-4-3/user-guide/auto-actions/supported-cluster-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Supported cluster metrics", 
"snippet" : "Table of Contents Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters...", 
"body" : " Table of Contents Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on all live running Monitoring is only Hive\/Workflow Metrics Metric Definition duration total time taken by the application totalDfsBytesRead total hdfs bytes read totalDfsBytesWritten total hdfs bytes written stevev mun MapReduceApplication Master Metrics MapReduce Metrics Type Metric Definition elapsedAppTime time since the application was started Map mapsCompleted number of completed maps mapsPending number of maps still to be run mapsRunning number of running maps mapsTotal total number of maps Map Attempts failedMapAttempts number of failed map attempts killedMapAttempts number of killed map attempts newMapAttempts number of new map attempts runningMapAttempts number of running map attempts Reduce reducesCompleted number of completed reduces reducesPending number of reduces still to be run reducesRunning number of running reduces reducesTotal total number of reduces Reduce Attempts failedReduceAttempts number of failed reduce attempts killedReduceAttempts number of killed reduce attempts newReduceAttempts number of new reduce attempts runningReduceAttempts number of running reduce attempts successfulReduceAttempts number of successful reduce attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduceCounters File System Counters Metric Definitions fileBytesRead mount of data read from local file system fileBytesWritten amount of data written to local file system fileReadOps number of read operations from local file system fileLargeReadOps number of read operations of large files from local file system fileWriteOps number of write operations from local file system hdfsBytesRead amount of data read from HDFS hdfsBytesWritten amount of data written to HDFS hdfsReadOps number of read operations from HDFS hdfsLargeReadOps number of read operations of large files from HDFS hdfsWriteOps number of write operations to HDFS Job Counters Type Metric Definition Map dataLocalMaps number of map tasks which were launched on the nodes containing required data mbMillisMaps total megabyte-seconds taken by all map tasks millisMaps total time spent by all map tasks slotsMillisMaps total time spent by all executing maps in occupied slots vcoresMillisMaps total vcore-seconds taken by all map tasks Reduce mbMillisReduces total megabyte-seconds taken by all reduce tasks millisReduces total time spent by all reduce tasks slotsMillisReduces total time spent by all executing reduces in occupied slots totalLaunchedReduces total number of launched reduce tasks vcoresMillisReduces total vcore-seconds taken by all reduce tasks File Input\/Output Format Counters Metric Definition bytesRead amount of data read by every tasks for every filesystem bytesWritten amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Map-Reduce Framework Counters Type Metric Definition Map failedShuffle total number of mappers which failed to undergo through shuffle phase mapInputRecords total number of records processed by all of the mappers mapOutputBytes total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes amount of (compressed) data which was actually written to disk mapOutputRecords total number of records produced by by all of the mappers mergedMapOutputs total number of mapper output files undergone through shuffle phase shuffledMaps total number of mappers which undergone through shuffle phase Reduce reduceInputGroups total number of unique keys reduceInputRecords total number of records processed by all reducers reduceOutputRecords total number of records produced by all reducers reduceShuffleBytes amount of data processed in shuffle and reduce phase Records combineInputRecords total number of records processed by combiners combineOutputRecords total number of records produced by combiners spilledRecords total number of map and reduce records that were spilled to disk Time gcTimeMillis wall time spent in Java Garbage Collection cpuMilliseconds cumulative CPU time for all tasks Memory committedHeapBytes total amount of memory available for JVM physicalMemoryBytes total physical memory used by all tasks including spilled data splitRawBytes amount of data consumed for metadata representation during splits virtualMemoryBytes total virtual memory used by all tasks Shuffle Errors Metric Definition badId total number of errors related with the interpretations of IDs from shuffle headers connection total number of established network connections ioError total number of errors related with reading and writing intermediate data wrongLength total number of errors related to compression and decompression of intermediate data wrongMap total number of errors related to duplication of the mapper output data wrongReduce total number of errors related to the attempts of shuffling data for wrong reducer Spark Metrics In addition to the metric set supported by MapReduce applications, Spark applications can be polled on: Join joinInputRowCount the total input rows of the first join of the SQL query, aggregated for all the queries that are part of the application totalJoinInputRowCount total number of input rows count for all join operators of all SQL queries that are part of the application totalJoinOutputRowCount total number of output rows count for all join operators of all SQL queries that are part of the application joinOutputRowCount the total output rows of the first join of the SQL query, aggregated for all the queries that are part of the application Partitions inputPartitions total number of input partitions for all SQL queries that are part of the application outputPartitions total number of output partitions for all SQL queries that are part of the application Records inputRecords cumulative number of input records for all SQL queries that are part of the application (collected at stage level) outputRecords cumulative number of output records for all SQL queries that are part of the application (collected at stage level) outputToInputRecordsRatio outputRecords \/ inputRecords if inputRecords > 0, else 0 YARN Resource Manager metrics allocatedMB The sum of memory in MB allocated to the application’s running containers allocatedVCores The sum of virtual cores allocated to the application’s running containers appCount total number of applications elapsedTime The elapsed time since the application started (in ms) runningContainers The number of containers currently running for the application memorySeconds The amount of memory the application has allocated (megabyte-seconds) vcoreSeconds The amount of CPU resources the application has allocated (virtual core-seconds) For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "unravel-4-3/user-guide/auto-actions/running-auto-action-demos.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Auto Actions \/ Running Auto Action Demos", 
"snippet" : "Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory...", 
"body" : " Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory. \n DEMO_PATH auto-actions-demos # ls auto-actions-demos\ndemos\/ setup\/ Go to DEMO_PATH Open .\/settings Execute the .\/setup-all script. # .\/setup-all The Auto Action rules that include time specification will be automatically adjusted to the current time period, i.e. from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Admin->Manage->Auto Actions tab. You should see all the Auto-Actions demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag, e.g., AA-Spark-1c, Map-1b, AA-Hive-1a. Executing the demos Go to DEMO_PATH For each Auto Actions rules listed in Admin->Manage->Auto Actions there is a corresponding script in the demo's directory. Each script will trigger the corresponding Auto Action demo. For example, in the UI you will see an Auto Action named AA_Spark-1c. demo-Spark-1c. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute “.\/demo- tag Cleaning up demos. Go to DEMO_PATH Run .\/clean-all script. This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 # .\/clean-all Auto Actions demos list \n \n \n Application type \n \n Use Case Type \n \n Specific Use case \n \n Auto Action \n Triggering Script \n \n Notes \n \n \n MapReduce \n Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. \n Alert if any MapReduce job allocated memory > 20GB. \n AA-MR-1a Demo-MR-1a \n Submits to “root.sla” queue. \n \n Alert if any MapReduce job allocated vcores > 10. \n AA-MR-1b Demo-MR-1b \n Submits to “root.sla” queue. \n \n Alert if any MapReduce job is running for longer than 10 minutes. \n AA-MR-1c Demo-MR-1c \n Submits to “root.sla” queue. May trigger MR-1b. \n \n Alert if a MapReduce job may affect any production SLA jobs running on a cluster. \n Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. \n AA-MR-2a Demo-MR-2a \n Will also trigger MR-1a as well. \n \n Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. \n AA-MR-2b Demo-MR-2b \n Will also trigger MR-2a as well. \n \n Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. \n Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. \n AA-MR-3a Demo-MR-3a \n Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. \n \n Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. \n AA-MR-3b Demo-MR-3b \n Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. \n \n \n Spark \n Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. \n Alert if any Spark application has allocated more than 20GB of memory. \n AA-Spark-1a Demo-Spark-1a \n \n Alert if any Spark application allocated vcores > 8. \n AA-Spark-1b Demo-Spark-1b \n \n Alert if any Spark application is running longer than 10 minutes \n AA-Spark-1c Demo-Spark-1c \n \n Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. \n Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. \n TBD \n \n \n Hive \n Alert if a Hive query duration is running longer than expected. \n Alert if a Hive query duration > 5 minutes. \n AA-Hive-1a Demo-Hive-1a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if SLA bound query is taking longer than expected. \n Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. \n AA-Hive-2a Demo-Hive-2a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. \n AA-Hive-2b Demo-Hive-2b \n Very short query. \n \n Alert if a Hive query is writing lots of data. \n Alert if a Hive query writes out more than 200MB in total. \n AA-Hive-3a Demo-Hive-3a \n \n Alert if a Hive query reads in more than 10GB in total. \n AA-Hive-3b Demo-Hive-3b \n \n Detect inefficient and “stuck” Hive queries. \n Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. \n AA-Hive-4a Demo-Hive-4a \n \n Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. \n AA-Hive-4b Demo-Hive-4b \n \n \n Workflow \n Alert if a workflow is taking longer than expected. \n Alert if any workflow is running for longer than 10 minutes, might be stuck. \n AA-WF-1a Demo-WF-1a \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. \n AA-WF-1b Demo-WF-1b \n You can Ctrl-C the query once it triggers the AA. \n \n Alert if a workflow is reading more data than expected. Related articles \n AA-Demo.png " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-3/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-3/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email \n 2017-04-05 at 11.41.20 PM.png \n 2017-04-05 at 11.43.57 PM.png \n 2017-04-05 at 11.43.07 PM.png " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-3/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. \n Screen Shot 2017-04-05 at 11.16.49 PM.png \n Screen Shot 2017-04-05 at 11.21.10 PM.png \n Screen Shot 2017-04-06 at 8.31.49 AM.png \n Screen Shot 2017-04-05 at 11.21.50 PM.png \n Screen Shot 2017-04-05 at 11.23.34 PM.png " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "unravel-4-3/user-guide/use-cases/optimizing-the-performance-of-spark-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web...", 
"body" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 \n gui_spark_app_details_after.png \n gui_spark_app_details_before.png " }, 
{ "title" : "Kafka Insights", 
"url" : "unravel-4-3/user-guide/use-cases/kafka-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, \n OK \n Lagging \n Stalled A Topic's status is set to the lowest status among it's Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Example 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Note: Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric offset, test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI’s. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. \n eye.png \n 1-Operation-Charts-KafkaWArrow.png \n 1a-lagging-Excerpt.png \n 1b-stalled-Excerpt.png \n 2-ClusterView-KuhdiddemoWArrow.png \n 3-Kafka-Topic-topic.png \n 4-TopicView-Partition-0-Offest.png \n 5-TopicView-Partition-1-ConLag.png \n MoreInfo-2.png \n 6-CGView-TopicDetail.png \n 7-CGView-Part0-Offset.png \n 8-CGView-Part1-ConLag.png " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-3/advanced-topics.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics", 
"snippet" : "Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Mu...", 
"body" : " Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Worker daemon @ Unravel Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restrict direct access to Web UI Using a Private Certificate Authority with Unravel Connecting to\/Configuration of a Kafka Stream Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags EMR Support Matrix Hive Metastore Connecting to a Hive Metastore Hive Metastore Configuration Installing MySQL or Compatible Database for Unravel Roles and Role Based Access Control Role Based Access Control (RBAC) Supported Roles Running Verification Scripts and Benchmarks Unravel APIs Use Case - Auto Actions and Pagerduty Unravel Servers and Sensors Installing Sensors Unravel Sensor for Individual Applications Submitted Through spark-submit Unravel Sensor for Individual Hive Queries Upgrading the Unravel Server and Sensors Uninstalling Unravel Server Uploading Spark Programs to Unravel Workflows Tagging Workflows Monitoring Oozie Workflows Monitoring Airflow Workflows Tagging a Hive on Tez Query " }, 
{ "title" : "Autoscaling HDInsight Spark Cluster using Unravel API", 
"url" : "unravel-4-3/advanced-topics/autoscaling-hdinsight-spark-cluster-using-unravel-api.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Autoscaling HDInsight Spark Cluster using Unravel API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Prerequisites", 
"url" : "unravel-4-3/advanced-topics/autoscaling-hdinsight-spark-cluster-using-unravel-api.html#UUID-5f93f5ff-e302-fdff-2da1-e859b709705e_id_AutoscalingHDInsightSparkClusterusingUnravelAPI-Prerequisites", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Autoscaling HDInsight Spark Cluster using Unravel API \/ Prerequisites", 
"snippet" : "Install requests using pip: # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click Link to see installation instruction After install Azure CLI 1.0 Run the following command to login # azure login Once you login to azure you should see existing HDinsigh...", 
"body" : " Install requests using pip: # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click Link to see installation instruction After install Azure CLI 1.0 Run the following command to login # azure login Once you login to azure you should see existing HDinsight clusters using this command # azure hdinsight cluster list \n \n 5. Download the customizable script from here: https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/unravel-autoscaling\/unravel_HDInsight_autoscaling.py 6. Open \n unravel_HDInsight_autoscaling.py \n \n Property \n Notes \n Example Value \n \n \n unravel_base_url \n \n http:\/\/localhost:3000\/ \n \n \n memory_threshold \n scale up\/down when memory_usage higher\/lower 80% \n 80 \n \n \n cpu_threshold \n scale up when cpu_usage higher\/lower 10% \n 10 \n \n \n min_nodes \n min worker nodes \n 4 \n \n \n max_nodes \n max worker nodes can scale up to \n 10 \n \n \n resource_group \n \n UNRAVEL01 \n \n \n cluster_name \n \n estspk2rh75 Run auto scaling script # python unravel_HDInsight_autoscaling.py Below is a screenshot ( Operations Dashboard \n image2018-5-22_10-57-43.png " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-3/advanced-topics/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster Wide Report", 
"url" : "unravel-4-3/advanced-topics/cluster-wide-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's curren...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize it's efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of these recommendations revolve around these parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "unravel-4-3/advanced-topics/cluster-wide-report.html#UUID-e7c0d6e9-9197-94e3-5816-a66e6980150f_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSe...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/ usr\/local\/unravel\/install_bin\/ClusterReport. cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz cd dbin Input.txt. # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report # su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "unravel-4-3/advanced-topics/configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations", 
"snippet" : "Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Settin...", 
"body" : " Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Worker daemon @ Unravel Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restrict direct access to Web UI Using a Private Certificate Authority with Unravel " }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Setting Up Email for Auto Ac...", 
"body" : " Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Defining a Custom Web UI Port Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Worker daemon @ Unravel " }, 
{ "title" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/configure-permission-for-unravel-daemons-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"snippet" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export RUN_AS=$user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_R...", 
"body" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export RUN_AS=$user_name \n \n HDFS folder path \n user \n access \n Purpose \n \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR \n hdfs or alternate user \n READ + WRITE \n data transfer from Hive jobs when Unravel is not up \n \n \n hdfs:\/\/user\/spark\/applicationHistory \n hdfs or alternate user \n READ \n Spark event log \n \n \n hdfs:\/\/user\/history\/done \n hdfs or alternate user \n READ \n MapReduce logs \n \n \n hdfs:\/\/tmp\/logs \n hdfs or alternate user \n READ \n YARN aggregation folder \n \n \n >hdfs:\/\/user\/hive\/warehouse \n hdfs or alternate user \n READ \n Obtain table partition sizes with \"stat\" only \n \n \n hdfs:\/\/user\/spark\/spark2applicationHistory \n hdfs or alternate user \n READ \n Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI Using the above hive metastore access information, and completed the unravel hive configuration on the UI under the manage page → Hive " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # ...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: # sudo \/etc\/init.d\/unravel_all.sh start\n " }, 
{ "title" : "Defining a Custom Web UI Port", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/defining-a-custom-web-ui-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom Web UI Port", 
"snippet" : "These instructions apply to any platform deployment. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh 18080 export NGUI_PORT=18080 Note that port numbers under 1024 are restricted to root setuid After making this change, restart the affected daemon: # sudo \/etc\/init.d\/unravel_ngui restart...", 
"body" : " These instructions apply to any platform deployment. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh 18080 export NGUI_PORT=18080 Note that port numbers under 1024 are restricted to root setuid After making this change, restart the affected daemon: # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Run Unravel Daemons with Custom User", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/run-unravel-daemons-with-custom-user.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Run Unravel Daemons with Custom User", 
"snippet" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to...", 
"body" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to Switch User Run the following command to switch running Unravel daemons to user {USER} {GROUP} # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh {USER} {GROUP} Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\" foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\" hdfs hadoop Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh \/etc\/unravel_ctl RUN_AS USE_GROUP HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL env vars are removed from \/usr\/local\/unravel\/etc\/unravel.ext.sh \/usr\/local\/unravel\/ \/srv\/unravel\/* RUN_AS: \/srv\/unravel\/tmp_hdfs\/ logs in \/srv\/unravel\/log_hdfs \/usr\/local\/unravel\/logs \/srv\/unravel\/log_hdfs the umask of the run-as the chmod bits of \/usr\/local\/unravel \/srv\/unravel " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Below the web UI label is noted in BOLD CAPITAL unravel.properties Note Changes made via the Unravel Web UI take precedence and override the corresponding settings in \/usr\/local\/unravel\/etc\/unravel.properties In Un...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Below the web UI label is noted in BOLD CAPITAL unravel.properties Note Changes made via the Unravel Web UI take precedence and override the corresponding settings in \/usr\/local\/unravel\/etc\/unravel.properties In Unravel Web UI, select the Manage Configuration Core Retention TIME SERIES RETENTION DAYS com.unraveldata.retention.max.days=30 Search properties. WEEKS TO SHOW FOR SEARCH RESULTS com.unraveldata.recent.maxSize.weeks=4 WEEKS TO SHOW FOR DEEP SEARCH RESULTS com.unraveldata.history.maxSize.weeks=5 These two properties are closely related and the following conditions must be true: 1 < com.unraveldata. recent com.unraveldata.history.maxSize.weeks In other words: (( com.unraveldata.recent.maxSize.weeks < com.unraveldata.history.maxSize.weeks) After changing any of the properties, you must restart unravel_td # sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/setting-up-email-for-auto-actions-and-collaboration.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. \n \n \n Web UI \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 25 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n false \n Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw \n \n START TLS \n mail.smtp.starttls.enable \n false \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n null \n Username for SMTP authentication \n \n USER PASSWORD \n mail.smtp.pw \n null \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n l \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone @example.com \n Use a From: \n \n LOCALHOST \n mail.smtp.localhost \n localhost.local \n A domain name for apparent sender; must have at least one dot (e.g. organization.com) \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. GMail SMTP Example These settings are for our internal use. Do not \n \n \n Config Label \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 587 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n true \n Enable SMTP authentication? \n \n START TLS \n mail.smtp.starttls.enable \n true \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n \n s \n Username for SMTP authentication \n \n USER PASSWORD \n \n mail.smtp.pw \n ******** \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n smtp.gmail.com \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone@example.com \n This sets the From header \n \n LOCALHOST \n mail.smtp.localhost \n \n e \n A domain name for apparent sender; must have at least one dot \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. Debug mode under \"Advance SMTP\" section Unravel daemons to restart after email setup Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_tc restart \n# sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: # sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n Enter password from dist.unraveldata.com You should see following output in terminal mode and if you see \"result is = null\", then, setup is correct. :\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add following properties to \/usr\/local\/unravel\/etc\/unravel.properties on Unravel Server: mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Restart daemons: # sudo \/etc\/init.d\/unravel_all.sh restart \n Unravel_email_setup.png " }, 
{ "title" : "Spark Properties for Spark Worker daemon @ Unravel", 
"url" : "unravel-4-3/advanced-topics/configurations/custom-configurations/spark-properties-for-spark-worker-daemon---unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel", 
"snippet" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an ap...", 
"body" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Definition com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:_1,_1.lz4,_1.snappy,_1.inprogress,,.lz4,.snappy, .inprogress,_2,_2.lz4,_2.snappy,_2.inprogress com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) Executor log processing Property Definition Default com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) Tagging com.unraveldata.tagging.script.enabled Property Definition Definition com.unraveldata.tagging.enabled Enables tagging functionality. true Enables tagging. false. com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.p e com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_key and aws_secret_access_key access_key\/secretKey - EMR\/HDInsight specific properties Property Definition Definition com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False true The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel properties' values in Microsoft's Azure Block storage specific properties (for HDInsight) For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property Definition Value com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Data Lake (ADL) specific data properties Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster “spk21utj02” has been given access to “\/clusters\/spk21utj02” on Data Lake store. retrieve from Microsoft Azure " }, 
{ "title" : "Security Configurations", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabl...", 
"body" : " Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Kafka Security Restrict direct access to Web UI Using a Private Certificate Authority with Unravel " }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui res...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui restart " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/adding-ssl-and-tls-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure ...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui which listens on port 3000 in Enabling TLS to Unravel Web UI Directly Secure cookies are NOT supported when using this Apache2 reverse-proxy method, see instead Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4 and support listening on port 443. Install needed packages: # sudo yum install httpd mod_ssl Note: There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf unravelhost_FQDN SSLCertificate* <VirtualHost *:80>\n ServerName unravelhost_FQDN\n Redirect permanent \/ https:\/\/unravelhost_FQDN\/\n<\/VirtualHost>\n\n<VirtualHost *:443>\n\n DocumentRoot \/var\/www\/html\n ServerName unravelhost_FQDN\n # use this if http to https errors #RequestHeader set X-FORWARDED-PROTO 'https'\n\n SSLEngine on\n SSLCertificateFile \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt\n SSLCertificateKeyFile \/etc\/certs\/wildcard_unravelhost_RSA_private.key\n SSLCertificateChainFile \/etc\/certs\/IntermediateCA.crt\n\n # set this off for reverse proxy security\n ProxyRequests Off\n # might be helpful in logs\n ProxyPreserveHost On\n ProxyPass \/ http:\/\/localhost:3000\/ connectiontimeout=180 timeout=180\n ProxyPassReverse \/ http:\/\/localhost:3000\/\n\n <Location \/>\n Order deny,allow\n Deny from all\n Allow from all\n <\/Location>\n\n<\/VirtualHost>\n Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties :port com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_tc daemon: # sudo service unravel_ngui restart Start the httpd daemon: # sudo service httpd start Visit https:\/\/unravelhost_FQDN Troubleshooting To enable verbose logging in Apache2, add these lines: LogLevel debug\n where LogLevel debug trace1 trace8 Note: Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ add this line: RequestHeader set X-FORWARDED-PROTO 'https'\n after ServerName line in the virt. host httpd config for Unravel. Then restart apache2. " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/alternate-kerberos-principal-for-cluster-access-on-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in part 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in part 2 of the installation guide. HIGHLIGHTED 1. Check HDFS Default umask For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see sub-directories and read files. In Cloudera Manager check the value of dfs.umaskmode fs.permissions.umask-mode 2. Enable ACL Inheritance In Cloudera Manager, HDFS Configuration, search for \"namenode advanced configuration snippet\", and set dfs.namenode.posix.acl.inheritance.enabled https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 3. Restart Cluster When you are ready, restart the cluster to effect a change in dfs.namenode.posix.acl.inheritance.enabled 4. Change ACL of Target HDFS Directories Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented): Set ACL for future directories. The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse\n\n Set ACL for existing directories # hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse 5. Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target user ('unravel' or 'hdfs' or 'mapr' or custom), with a valid kerberos ticket corresponding to keytab principal (substitute your values for KEYTAB_FILE PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n " }, 
{ "title" : "Configure HDFS permission for Unravel on CDH Sentry Secured Cluster", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/configure-hdfs-permission-for-unravel-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure HDFS permission for Unravel on CDH Sentry Secured Cluster", 
"snippet" : "HDFS Permission The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export...", 
"body" : "HDFS Permission The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export RUN_AS=$user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH please read the Cloudera Documentation here " }, 
{ "title" : "Configure Hive Metastore Permissions", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/configure-hive-metastore-permissions.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Hive Metastore Permissions", 
"snippet" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metast...", 
"body" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI Go to Manage Configuration \n Hive Metastore URL use the pull-down to specify the type of database mysql or postgresql, your host, port, and name of your hive database. \n Hive Metastore Driver \n Hive Metastore User Name \n Hive Metastore Password \n 20180529_172.36.1.124_ManageHive.png " }, 
{ "title" : "Disabling Browser Telemetry", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/disabling-browser-telemetry.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Browser Telemetry", 
"snippet" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must To Disable Mixpanel. On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unrave...", 
"body" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must To Disable Mixpanel. On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com. . false. com.unraveldata.do.not.track=false Restart the Unravel UI. # sudo service unravel_ngui restart To Re-enable Mixpanel. To enable Mixpanel follow the above steps but in step 2 set com.unraveldata.do.not.track true " }, 
{ "title" : "Disabling Support\/Comments Panel", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/disabling-support-comments-panel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Support\/Comments Panel", 
"snippet" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel...", 
"body" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled. false. com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. # sudo service unravel_ngui restart Your title bar should be missing the support button like below. Show\/Re-enable Support Button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled true unravel.properties Pop-up Support Box \n TitleBar-NoSupportWArrw.png \n SupportPopup.png \n TitleBar-Arw2Sup.png " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/enabling-tls-to-unravel-web-ui-directly.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. See also Defining a Custom Web UI Port HIGHLIGHTED On Un...", 
"body" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. See also Defining a Custom Web UI Port HIGHLIGHTED On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Update or add the following properties. For example, to enable ssl #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true\n#PATH TO CERT FILE\ncom.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate\n#PATH TO KEY FILE\ncom.unraveldata.ngui.ssl.key.file=\/etc\/certs\/wildcard_unravelhost_RSA_private.key\n#OPTIONAL - COMMA SEPARATED LIST OF CA FILES \ncom.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt\n#OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE\ncom.unraveldata.ngui.ssl.passphrase=test Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/{unravel.example.com}:3000 Restart Unravel web UI: # sudo service unravel_ngui restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/encrypting-passwords-in-unravel-properties-and-settings.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh: # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you ...", 
"body" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh: # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter (Return key), it will emit something like: ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/integrating-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "You configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web U If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then follow the steps below. In Unravel server 4.3.1.2, the prope...", 
"body" : "You configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web U If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then follow the steps below. In Unravel server 4.3.1.2, the property com.unraveldata.ldap.use_jndi If you used com.unraveldata.ldap.search_bind_authentication If you set the bind_dn bind_pw You must replace all text noted in red For instance if your domain was thisLocalDomain hive.server2.authentication.ldap.Domain={MYDOMAIN}becomes hive.server2.authentication.ldap.Domain =thisLocalDomain. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties For { MYDOMAIN} If SSL is in use: For ldap:\/\/LDAP_HOST ldaps:\/\/LDAP_HOST A truststore unravel\/jre\/ unravel\/etc\/unravel.ext.sh You can append a port number, if needed; for example, ldap:\/\/ldap_host:9999 Knowing your precise DN is essential for some configuration examples below. If you are uncertain about what a normal DN is ldapsearch If you use AD, the Windows domain can be found using ldapsearch userPrincipalName How To Debug: see the property com.unraveldata.ldap.verbose Active Directory AD domain This is the simplest to implement and enables the widest access to Unravel server. The web UI looks at the login name entered, if the login has no domain name appended, i.e.,thisUser@MYDOMAIN, then the @MYDOMAIN is appended before binding to LDAP. if the name contains the domain name, it assumes the user entered the domain explicitly. If it is possible to bind to the AD (LDAP) server with name@MYDOMAIN then the login is approved. Note: The login name can appear in AD or be seen in an ldapsearch userPrincipalName For { MYDOMAIN} com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.Domain={MYDOMAIN} An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern AD with base DN defined This example works by composing a DN from the entered user account name in the web UI. For a user login \"foo\" the settings below would make a DN \"uid=foo,ou=myou,dc=domain,dc=com\" and try to bind\/authenticate with that. com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldaps:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myou,dc=domain,dc=com\nhive.server2.authentication.ldap.guidKey=uid An optional hive.server2.authentication.ldap.userFilter can be combined with this to narrow who can access Unravel. See example 3 below. Change LDAP_HOST, ou=myou,dc=domain,dc=com to appropriate value for your installation. For guidKey, the default username attribute in windows AD is uid but could also be sAMAccountName . AD with user pattern Here we match users with one or more colon-separated patterns, combined with an optional inclusion filter (userFilter). This is helpful when you want to allow users from multiple OUs or base DNs. The optional inclusion filter is a secondary check on the list of users allowed to access Unravel server. In this example, if the user enters name@MYDOMAIN then the LDAP bind uses that directly, otherwise the string entered is used to compose a DN by replacing the %s seen below with the string that the user entered. The string entered in the login form can have spaces embedded and is not normally case sensitive. Multiple userDNPatterns can be defined by using a colon ':' separator. an example of multiple userDNPatterns hive.server2.authentication.ldap.userDNPattern=CN=%s,OU=Employee,DC=unraveldatalab,DC=com:CN=%s,OU=AffiliatedPeople,DC=unraveldatalab,DC=com The userFilter property is optional to further narrow down who can login to Unravel. We recommend that you first make sure your userDNPattern is working before you add this filter. Notice that spaces are significant, they are part of the name. When this property is specified, a user must get past the userDNPattern first, and then the simple login name must be also present in this list. com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.userDNPattern=CN=%s,CN=Users,DC=domain,DC=com\nhive.server2.authentication.ldap.userFilter=user1,user2,user with space,user3\n Change LDAP_HOST, CN=%s,CN=Users,DC=domain,DC=com to appropriate value for your installation, based on DN for the users to whom you want to give access. The identifier before the =%s can also be uid instead of CN in some cases. AD with group pattern Here we match users with one or more patterns, and verify group membership in order to approve login. In this example, if the user enters name@MYDOMAIN then the LDAP bind uses that directly, otherwise the string entered is used to compose a DN by replacing the %s seen below with the string that the user entered and the result is used for the bind. The string entered in the login form can have spaces embedded and is not normally case sensitive. Multiple groupDNPatterns can be defined by using a colon ':' separator. The template text below for groupDNPattern, would correspond to a group DN like if the groupFilter clause contained mygroup memberOf=CN=mygroup,OU=myou,DC=subdomain,DC=domain,DC=com Spaces (' ') are significant in the Unravel web login form. Use values relevant to your installation for {LDAP_HOST},{%s}, {myou}, {subdomain}, and {com} {%s} uid {UNRAVEL_GROUP} com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.groupDNPattern=CN={%s},OU={myou},DC={subdomain},DC={domain},DC={com}\nhive.server2.authentication.ldap.groupFilter=<UNRAVEL_GROUP>\nhive.server2.authentication.ldap.groupMembershipKey=member\nhive.server2.authentication.ldap.groupClassKey=group\n For Open LDAP LDAP Example 1 Use values relevant to your installation for {LDAP_HOST},{myunit}, {example}, and {com}. uid An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/{LDAP_HOST}\nhive.server2.authentication.ldap.baseDN=ou={myunit},dc={example},dc={com}\nhive.server2.authentication.ldap.guidKey={uid} LDAP example 2 Below we use a typical DN to be uid=%s,ou=myunit,dc=example,dc=com %s cn uid Multiple userDNPatterns can be defined by using a colon ':' separator. An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Optional Master Bind Account When user accounts cannot do an LDAP search, you can add the properties below to specify a search account. This can be combined with any of the examples above. com.unraveldata.ldap.bind_dn=CN=unravel,ou=myunit,dc=example,dc=com\ncom.unraveldata.ldap.bind_pw=bigsecret The password can be encrypted Web UI Login Syntax In most cases, people want to enter a simple login account name in the Web UI login page of Unravel server and the instructions here reflect that. However, if name@windowsDomain is entered for values that are valid for your site, that will be used instead of appending a suffix in the AD example 1 case above. The name field of the Web UI allows spaces to be entered. That way a name like \"john smith\" can be entered and matched as a CN in a DN. 2. Restart unravel_ngui Restart unravel_ngui # sudo \/etc\/init.d\/unravel_ngui restart Advanced Properties and Details Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The Unravel login process is described next. Property names are shortened for readability, but full names should be used. The SIMPLE LDAP authentication mechanism is used. Authentication Process For Unravel server 4.3.1.2 or later If a bind_dn and bind_pw are set, then authentication starts out by binding with the given account if the bind works, then the verbose log will show \"Connected using bindDN\" an LDAP search is for the user attempting login by matching inetOrgPerson AND either sAMAccountName OR guidKey attribute matches the simple login name. If guidKey is not set, then \"uid\" is default. if an entry is found, the userPassword is fetched and compared to the entered password. On success, \"pw is a match\" is logged when verbose. This method only works with un-hashed passwords which are often not available. if password for the LDAP server is unavailable or the compare fails, then another bind is attempted with a DN composed by the entered name in the login Web UI and the baseDN (or each userDNPattern supplied) using the guidKey if the bind or password matches for candidate login user succeeds, then login is approved pending further qualifications for authorization in next part. The bind made with the bind_dn will be used during authorization for searches. if Windows domain is set, bind as username + at sign + Windows domain, using the given password. verbose log will show Connecting Connected using principal bind with a DN composed by the entered name and the baseDN (or each userDNPattern supplied) using the guidKey is attempted if the bind succeeds, \"Connected using DN\" is logged when verbose. If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds Authorization Steps These extra authorization steps are optional to further narrow who can access Unravel server. The searches will be done using a bindDN account, if specified, otherwise the user account bind is used. If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful if there are no results from the custom query, then login fails each search result will be matched with the simple name entered in the Web UI login user or group filters will be ignored if custom query is used if a user filter is specified, it is matched with the simple name entered in the Web UI login no additional LDAP search is done in this case this allows access to Unravel to be controlled with an explicit list in Unravel. If a group pattern or filter is specified, it is checked a query is made to find the groups to which a user belongs the user membership list is scanned and if one of the groups is in the specified list of allowed groups, then this authorization step succeeds. verbose log will show results list and the match arguments in effect by logging \"Checking group\" Property Description Example Value hive.server2.authentication.ldap.baseDN OR com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey OR com.unraveldata.ldap.user_name_attr LDAP attribute name whose values are unique in this LDAP server. Default is \"uid\"; not used when custom query is specified. uid or CN or sAMAccountName hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). hiveuser,impalauser,hiveadmin,hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member, uniqueMember or memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group, groupOfNames or groupOfUniqueNames hive.server2.authentication.ldap.url OR com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. ldap:\/\/host ldaps:\/\/host ldap:\/\/host:9999 ldaps:\/\/host1:9999 ldaps:\/\/host2:9999 com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_ngui.log Can be true or false or not set; default is false " }, 
{ "title" : "Kafka Security", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/kafka-security.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Kafka Security", 
"snippet" : "Table of Contents You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and...", 
"body" : " Table of Contents You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For Single Kafka clients 1. Create a file named consumerConfig.properties. Add the following properties and copy\/move the file to \/usr\/local\/unravel\/etc. You can locate youSSL + Kerberos configuration ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Kerberos configuration sasl.mechanism = GSSAPI security.protocol = SASL_PLAINTEXT sasl.kerberos.service.name = kafka sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; 2. Copy\/move consumerConfig.properties to \/usr\/local\/unravel\/etc. 3. Edit \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. You should find com.unraveldata.ext.kafka.clusters=ClusterName Add the following property using the ClusterName above: com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties 4. Restart the kafka monitor daemon unravel_km. # service unravel_km restart For Multiple Kafka clients Each cluster must have a separate consumerConfig.properties 1. Open \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. The property will be defined with a comma separated list. If there is only one cluster name see above com.unraveldata.ext.kafka.clusters=ClusterName1, ClusterName 2. Create a file named consumerConfigClusterName.properties for each cluster. Add the following properties to each file ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; 3. Copy\/move each file to \/usr\/local\/unravel\/etc. 4. Edit \/usr\/local\/unravel\/unravel.properties. For each cluster add the following property: com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties 5. Restart the kafka monitor daemon unravel_km. # service unravel_km restart KafkaAuthorizations Unravel consumes message to topic __consumer_offsets UnravelOffsetConsumer SentryAuthorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe HOST=*->TOPIC=__consumer_offsets→action=read HOST=*->TOPIC=__consumer_offsets→action=write HOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization Kafka with Ranger Authorization The following privilege must be granted using Ranger for the topic __consumer_offsets Publish Consume Describe For further details, see Security - Create a Kafka Polic References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "Restrict direct access to Web UI", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/restrict-direct-access-to-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Restrict direct access to Web UI", 
"snippet" : "Below we show how to restrict direct access to Web UI. On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.ext.sh vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add \/modify below environmental variable #SET NGUI_HOSTNAME NGUI_HOSTNAME=127.0.0.1 Restart Unravel web UI: # sudo service unravel_n...", 
"body" : "Below we show how to restrict direct access to Web UI. On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.ext.sh vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add \/modify below environmental variable #SET NGUI_HOSTNAME \nNGUI_HOSTNAME=127.0.0.1 Restart Unravel web UI: # sudo service unravel_ngui restart " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "unravel-4-3/advanced-topics/configurations/security-configurations/using-a-private-certificate-authority-with-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with \"sudo \/etc\/init.d\/unravel_all.sh restart\" after making the change. HIGHLIGHTED \/path\/to\/jks_keystore Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts \/usr\/local\/unravel\/etc\/unravel.ext.sh bin\/java \/usr\/java\/jdkl1.8 For example: export JAVA_HOME {\/usr\/java\/jdk1.8} Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts jssecacerts # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for mycompanyca something.cer # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias {mycompanyca} -file {something.cer}\n# sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "unravel-4-3/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties...", 
"body" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties " }, 
{ "title" : "1. Change the Configuration of the Kafka Cluster", 
"url" : "unravel-4-3/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html#UUID-70d9c9dc-df0f-f72f-a5af-4801d26f1b4f_id_ConnectingtoConfigurationofaKafkaStream-1ChangetheConfigurationoftheKafkaCluster", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream \/ 1. Change the Configuration of the Kafka Cluster", 
"snippet" : "Export an available port for JMX_PORT export JMX_PORT=<port_num> The default JMX port for kafka in CDH is 9393 In HDP you would export this parameter under Advanced kafka-env->kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS kafka_run_clas...", 
"body" : " Export an available port for JMX_PORT export JMX_PORT=<port_num> The default JMX port for kafka in CDH is 9393 In HDP you would export this parameter under Advanced kafka-env->kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS kafka_run_class.sh -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Djava.rmi.server.hostname=127.0.0.1\n-Djava.net.preferIPv4Stack=true Not required for HDP Verify the configuration changes on the Kafka cluster. Restart the Kafka broker. " }, 
{ "title" : "2. Configure Unravel Server to Monitor the Kafka Cluster", 
"url" : "unravel-4-3/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html#UUID-70d9c9dc-df0f-f72f-a5af-4801d26f1b4f_id_ConnectingtoConfigurationofaKafkaStream-2ConfigureUnravelServertoMonitortheKafkaCluster", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream \/ 2. Configure Unravel Server to Monitor the Kafka Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "For Unravel Server v4.2.1 or Earlier", 
"url" : "unravel-4-3/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html#UUID-70d9c9dc-df0f-f72f-a5af-4801d26f1b4f_id_ConnectingtoConfigurationofaKafkaStream-ForUnravelServerv421orEarlier", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream \/ For Unravel Server v4.2.1 or Earlier", 
"snippet" : "In \/usr\/local\/unravel\/etc\/unravel_kafka.jmxtrans.json servers In the servers \"port\": \"5005\" \"host\": \"127.0.0.1\" \"alias\": \"kafka-test1\" \"runPeriodSeconds\": 60 In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.ext.kafka.bootstrap_servers=<kafkabrokerIP:ClientPort> For example, com.unravelda...", 
"body" : " In \/usr\/local\/unravel\/etc\/unravel_kafka.jmxtrans.json servers In the servers \"port\": \"5005\"\n\"host\": \"127.0.0.1\"\n\"alias\": \"kafka-test1\"\n\"runPeriodSeconds\": 60 In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.ext.kafka.bootstrap_servers=<kafkabrokerIP:ClientPort> For example, com.unraveldata.ext.kafka.bootstrap_servers=localhost:9092 " }, 
{ "title" : "For Unravel Server v4.2.2 or Later", 
"url" : "unravel-4-3/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html#UUID-70d9c9dc-df0f-f72f-a5af-4801d26f1b4f_id_ConnectingtoConfigurationofaKafkaStream-ForUnravelServerv422orLater", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream \/ For Unravel Server v4.2.2 or Later", 
"snippet" : "The unravel daemon, unravel In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.ext.kafka.clusters com.unraveldata.ext.kafka.<cluster_id>.bootstrap_servers com.unraveldata.ext.kafka.<cluster_id>.jmx_servers com.unraveldata.ext.kafka.<cluster_id>.jmx.<jmx_server_id>.host com.unraveldata.ext....", 
"body" : "The unravel daemon, unravel In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.ext.kafka.clusters\ncom.unraveldata.ext.kafka.<cluster_id>.bootstrap_servers\ncom.unraveldata.ext.kafka.<cluster_id>.jmx_servers\ncom.unraveldata.ext.kafka.<cluster_id>.jmx.<jmx_server_id>.host\ncom.unraveldata.ext.kafka.<cluster_id>.jmx.<jmx_server_id>.port\n clusters: a comma separated list of arbitrary cluster ids. These user-defined names are used to clearly identify the Kafka cluster(s) in the unravel user interface. bootstratp_servers: a list of brokers that is used to retrieve initial information about the kafka cluster. jmx_servers: a comma separated list of aliases for each kafka nodes in the clusters with JMX ports exposed. The Kafka and JMX ports can be found in Cloudera Manager. Navigate to Clusters → Kafka → Configuration → Ports and Addresses. Alternatively, you may lookup up that information in the broker nodes of Zookeeper CLI. For example, com.unraveldata.ext.kafka.clusters=c1,c2\ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093\ncom.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193\ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2\ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105\ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-3/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "unravel-4-3/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-b1697899-1725-7dd6-7775-1e2f8684ba58_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "unravel-4-3/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-b1697899-1725-7dd6-7775-1e2f8684ba58_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "unravel-4-3/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-b1697899-1725-7dd6-7775-1e2f8684ba58_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrat...", 
"body" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires UN Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "unravel-4-3/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "unravel-4-3/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-95c840c3-fe18-7b01-3bed-1776b146505b_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storag...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storage of 20% left to alert contact under \" Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for \" 10 consecutive period(s) Under Actions Send notifications to \n Note: this sns topic should already be setup before you add it. Click \" Create Alarm Now, you will see in \" Alarms ALARM Alarms Click \" Create Alarm \n AWS_CW_RDS_FreeStorageSpace.png \n AWS_CW_RDS_Notify.png \n AWS_CW_RDS_Disk_Used_80.png " }, 
{ "title" : "Creating Application Tags", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags", 
"snippet" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed....", 
"body" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. " }, 
{ "title" : "What is a Tag in Unravel?", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-WhatisaTaginUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ What is a Tag in Unravel?", 
"snippet" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ......", 
"body" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ... " }, 
{ "title" : "How Does Unravel Use Tags?", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-HowDoesUnravelUseTags", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ How Does Unravel Use Tags?", 
"snippet" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights...", 
"body" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights " }, 
{ "title" : "What Types of Tags Are There?", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-WhatTypesofTagsAreThere", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ What Types of Tags Are There?", 
"snippet" : "There are two types of tags: Unravel tags and user-created tags. Unravel Tags All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp User-Created Tags You can create tags based on your use cases....", 
"body" : "There are two types of tags: Unravel tags and user-created tags. Unravel Tags All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp User-Created Tags You can create tags based on your use cases. " }, 
{ "title" : "How Do I Create Tags?", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-HowDoICreateTags", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ How Do I Create Tags?", 
"snippet" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,.....", 
"body" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. Adding Tags to your Application's Configuration Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,... Injecting Tags Through a Python Script You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches a consistent state.For example, Spark receives data from Resource Manager, event log file, YARN aggregated logs, and sensors. Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. " }, 
{ "title" : "Precedence of Tags", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-PrecedenceofTags", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ Precedence of Tags", 
"snippet" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script...", 
"body" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script " }, 
{ "title" : "Sample Use Cases", 
"url" : "unravel-4-3/advanced-topics/creating-application-tags.html#UUID-526ac854-b01a-3450-a903-f9b066e30e3f_id_CreatingApplicationTags-SampleUseCases", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Creating Application Tags \/ Sample Use Cases", 
"snippet" : "Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants...", 
"body" : " Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants " }, 
{ "title" : "EMR Support Matrix", 
"url" : "unravel-4-3/advanced-topics/emr-support-matrix.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ EMR Support Matrix", 
"snippet" : "This support matrix is based on recent test on unravel 4.3.1.4 EMR Version Hive Version Spark Version Hive Support Spark Support 5.15.0 2.3.3 2.3.0 No Yes (Unravel 4.3.1.5 and later ONLY) 5.14.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.13.0 2.3.2 2.3.0 ...", 
"body" : "This support matrix is based on recent test on unravel 4.3.1.4 EMR Version Hive Version Spark Version Hive Support Spark Support 5.15.0 2.3.3 2.3.0 No Yes (Unravel 4.3.1.5 and later ONLY) 5.14.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.13.0 2.3.2 2.3.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes (Unravel 4.3.1.5 and later ONLY) 5.12.1 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.12.0 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.11.1 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.11.0 2.3.2 2.2.1 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.10.0 2.3.1 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.9.0 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.2 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.1 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.8.0 2.3.0 2.2.0 Yes (Unravel 4.3.1.5 and later ONLY) Yes 5.7.0 2.1.1 2.1.1 Yes Yes 5.6.0 2.1.1 2.1.1 Yes Yes 5.5.2 2.1.1 2.1.0 Yes Yes 5.5.1 2.1.1 2.1.0 Yes Yes 5.5.0 2.1.1 2.1.0 Yes Yes 5.4.0 2.1.1 2.1.0 Yes Yes 5.3.1 2.1.1 2.1.0 Yes Yes 5.3.0 2.1.1 2.1.0 Yes Yes 5.2.2 2.1.0 2.0.2 Yes Yes 5.2.1 2.1.0 2.0.2 Yes Yes 5.2.0 2.1.0 2.0.2 Yes Yes 5.1.0 2.1.0 2.0.1 Yes Yes 5.0.3 2.1.0 2.0.1 Yes Yes 5.0.0 2.1.0 2.0.0 Yes Yes 4.9.4 1.0.0 1.6.3 Yes Yes 4.8.3 1.0.0 1.6.3 Yes Yes " }, 
{ "title" : "Hive Metastore", 
"url" : "unravel-4-3/advanced-topics/hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Hive Metastore", 
"snippet" : "Connecting to a Hive Metastore Hive Metastore Configuration...", 
"body" : " Connecting to a Hive Metastore Hive Metastore Configuration " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-3/advanced-topics/hive-metastore/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Hive Metastore \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. For CDH+CM You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain H...", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. For CDH+CM You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/CMGR_HOSTNAME_IP:7180\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore 2. In Unravel Web UI, go to Manage Configuration HIVE Encrypting Passwords in Unravel Properties and Settings \n Hive Metastore URL use the pull-down to specify the type of database mysql or postgresql, your host, port, and name of your hive database. \n Hive Metastore Driver \n Hive Metastore User Name \n Hive Metastore Password 4. Save the information when done: click Save Changes 5. Restart Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh restart 6. After restart, confirm that Hive queries appear in Unravel UI in the \n Applications | Applications For HDP See Step 2: Enable Additional Data Collection \/ Instrumentation for HDP For MapR See Step 2: Enable Additional Data Collection \/ Instrumentation for MapR \n ManageHive.png \n CMGR_rest_api_extracting_HM_pwd.png " }, 
{ "title" : "Hive Metastore Configuration", 
"url" : "unravel-4-3/advanced-topics/hive-metastore/hive-metastore-configuration.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Hive Metastore \/ Hive Metastore Configuration", 
"snippet" : "In order for the Reports | Data Insights must \/usr\/local\/unravel\/etc\/unravel.properties. Hive Metastore Access You must configure the following properties for the Data Insights tab to populate its information correctly. Property Definition Example Default Value javax.jdo.option.ConnectionDriverName ...", 
"body" : "In order for the Reports | Data Insights must \/usr\/local\/unravel\/etc\/unravel.properties. Hive Metastore Access You must configure the following properties for the Data Insights tab to populate its information correctly. Property Definition Example Default Value javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store which contains metadata org.postgresql.Driver - javax.jdo.option.ConnectionPassword Password used to access the data store OIhwSFlavw - javax.jdo.option.ConnectionUserName Username used to access the data store hive - javax.jdo.option.ConnectionURL JDBC connection string for the data store which contains metadata jdbc:postgresql:\/\/congo21.unraveldata.com:7432\/hive - JDBC Configurations You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Default Value com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false False com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. 5 Reference [1] c3p0 project page: http:\/\/www.mchange.com\/projects\/c3p0 " }, 
{ "title" : "Installing MySQL or Compatible Database for Unravel", 
"url" : "unravel-4-3/advanced-topics/installing-mysql-or-compatible-database-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Installing MySQL or Compatible Database for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Compatibility", 
"url" : "unravel-4-3/advanced-topics/installing-mysql-or-compatible-database-for-unravel.html#UUID-846e921d-bccc-18bf-3d2f-f4665be6e17d_id_InstallingMySQLorCompatibleDatabaseforUnravel-Compatibility", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Installing MySQL or Compatible Database for Unravel \/ Compatibility", 
"snippet" : "Unravel's default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server will use the mariadb connector. MySQL versions that are certified by our QA department...", 
"body" : "Unravel's default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server will use the mariadb connector. MySQL versions that are certified by our QA department:: MySQL 5.5.x or AWS RDS or MariaDB or Percona equivalent MySQL versions that are NOT yet certified by our QA department: MySQL 5.6.x MySQL 5.7.x Replication is not required, but it can be used in order to support online backup via mysqldump or LVM or Percona XtraBackup. Unravel will only communicate with the master. " }, 
{ "title" : "Configuration Requirements for MySQL", 
"url" : "unravel-4-3/advanced-topics/installing-mysql-or-compatible-database-for-unravel.html#UUID-846e921d-bccc-18bf-3d2f-f4665be6e17d_id_InstallingMySQLorCompatibleDatabaseforUnravel-ConfigurationRequirementsforMySQL", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Installing MySQL or Compatible Database for Unravel \/ Configuration Requirements for MySQL", 
"snippet" : "Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M thr...", 
"body" : " Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M thread_concurrency = 6 max_connections = 500 max_connect_errors = 2000000000 open_files_limit = 10000 port-open-timeout = 121 expire-logs-days = 2 character_set_server = utf8 collation_server = utf8_unicode_ci innodb_open_files = 2000 innodb_file_per_table = 1 innodb_data_file_path = ibdata1:100M:autoextend innodb_buffer_pool_size = 1G innodb_flush_method = O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency = 20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format = mixed if SSD disk is used, then also set: innodb_io_capacity = 4000 Note The innodb_buffer_pool_size " }, 
{ "title" : "Set Up Steps", 
"url" : "unravel-4-3/advanced-topics/installing-mysql-or-compatible-database-for-unravel.html#UUID-846e921d-bccc-18bf-3d2f-f4665be6e17d_id_InstallingMySQLorCompatibleDatabaseforUnravel-SetUpSteps", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Installing MySQL or Compatible Database for Unravel \/ Set Up Steps", 
"snippet" : "Install the database with a data area sufficiently large for your retention settings. For sizing information see Setting Retention Verify that the database host is reachable from the Unravel server host using telnet to the database port; if you get a timeout, then it is not reachable. Create \" unrav...", 
"body" : " Install the database with a data area sufficiently large for your retention settings. For sizing information see Setting Retention Verify that the database host is reachable from the Unravel server host using telnet to the database port; if you get a timeout, then it is not reachable. Create \" unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c8)\"Z Log into your mysql instance as admin DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into the mysql using the mysql commandline client as user unravel DB_PASSWORD Stop Unravel server # sudo \/etc\/init.d\/unravel_all.sh stop Configure the properties in unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Modify the property lines below so that they reflect your particular values: unravel.jdbc.username=unravel\nunravel.jdbc.password=ENC(QioHJsum9rmqKROG0DRqbU51)\nunravel.jdbc.url=jdbc:mysql:\/\/10.0.0.99:3306\/unravel_mysql_prod\n Use the actual values your set in the steps above. Use an ip address if you want to avoid DNS lookups, but hostname is also okay. The database password can be encrypted Use the schema upgrade utility provided by Unravel server: # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh This step connects to the database and apply schema deltas in-order until the schema is up to date. The stdout indicates success or failure. If table creation privilege was not granted because an internal DBA support group is providing the external database, then request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user 'admin' with the SQL statement emitted by: # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Disable bundled db in Unravel server: # sudo chkconfig unravel_db off\n# sudo chkconfig unravel_pg off Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command will produce an error that can be ignored. Start Unravel server: # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Roles and Role Based Access Control", 
"url" : "unravel-4-3/advanced-topics/roles-and-role-based-access-control.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Roles and Role Based Access Control", 
"snippet" : "Role Based Access Control (RBAC) Supported Roles...", 
"body" : " Role Based Access Control (RBAC) Supported Roles " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-3/advanced-topics/roles-and-role-based-access-control/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Supported Roles", 
"url" : "unravel-4-3/advanced-topics/roles-and-role-based-access-control/supported-roles.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Supported Roles", 
"snippet" : "Unravel supports two roles: Admin Enduser Admin Admins can access all Unravel's pages and features in \"read-write\" mode. Enduser Endusers have \"read-only\" access to Unravel pages and features. Using Role Based Access Control (RBAC) Applications Page applications containing specific tags RBAC If RBAC...", 
"body" : "Unravel supports two roles: Admin Enduser Admin Admins can access all Unravel's pages and features in \"read-write\" mode. Enduser Endusers have \"read-only\" access to Unravel pages and features. Using Role Based Access Control (RBAC) Applications Page applications containing specific tags RBAC If RBAC is: Off On Tags defined (user specific or default) Mode Unravel UIX Access Matrix By Role and RBAC settings RBAC Available Pages Admin (read\/write) Enduser (read-only) Off All Unravel's Pages ✓ ✓ On All Unravel's Pages ✓ Specific tags defined for user? No Yes Extended Mode below Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage Reports | Operational Insights | Chargeback N\/A Pages filtered by default value. See below Pages filtered by specific tags. See Adding Role s Restricted Mode below Application | Applications N\/A Pages filtered by default value. See below Pages filtered by specific tags. See Adding Role s Role Based Access Control General Properties Property Definition Default Value com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off false com.unraveldata.rbac.default Determines how the Enduser's views are filtered when no specific tags are set for enduser. UserName com.unraveldata.ngui.user.mode Determines pages a enduser can access when RBAC is on. extended: Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage, Reports | Operational Insights | Chargeback restricted: Application | Applications extended Using LDAP with Role Based Access Control If you have LDAP you can use it in conjunction with RBAC through the following properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.rbac.ldap.tag.Tag.regex.find Property Definition Default Value Sample com.unraveldata.login.admins.ldap.groups Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. - a-admin,b-admin com.unraveldata.login.admins.readonly.ldap.groups Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. - a-adminRO,b-adminRO com.unraveldata.rbac.ldap.tags A ≈ of the prefix of LDAP group to be used as the PROJECT - proj,dept,div Defines regular expression used to parse LDAP groups for generating the TENANTs PROJECT. Note: PROJECT example 2 - proj-(.*) Generating Projects and Tenants for RBAC from LDAP groups See Adding Roles RBAC tags restrict a enduser to their applications with matching tags. Therefore, the RBAC tags must also be used as Application Tags loaded via setting the below properties are only used if RBAC is turned on. You can turn RBAC via properties ( com.unraveldata.rbac.enabled=true) or in the RBAC Define the tags and regex; below we are adding project and dept with a regex for each tag com.unraveldata.rbac.ldap.tags.find=project,dept com.unraveldata.rbac.ldap.tag.project.regex.find=project-(.*) com.unraveldata.rbac.ldap.tag.dept.regex.find=dept-(.*) Examples When a user logs on, their LDAP groups entry is read and used to create the tags (if any). Using the above properties , User LDAP Groups Tags Project Tenant 1 [“dept-hr”,”dept-sales”,”dept-finance”] { “dept”:[“hr”,”sales”,”finance”]} dept hr, sales, finance 2 [“project-group01”,”project-group02”,”project-group03”] { “project”:[“group01”,”group02”,”group03”]} project group01, group02, group03 3 [“project-group01”,”project-group02”,”project-group03”, “dept-hr”,”dept-sales”,”dept-finance”] { “project”:[“group01”,”group02”,”group03”]} project group01, group02, group03 4 [“division-div01”,”division-div02”,”division-div03”] n\/a n\/a n\/a User 1 User 2 User 3 Role Based Access Control User 4 " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-3/advanced-topics/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "unravel-4-3/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-67a3aebe-a735-ebf7-8efd-d69f92025826_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "unravel-4-3/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-67a3aebe-a735-ebf7-8efd-d69f92025826_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel ...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "unravel-4-3/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-67a3aebe-a735-ebf7-8efd-d69f92025826_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages \n \n \n Package Name \n Location \n \n Benchmarks 1.6.x \n \n https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz \n \n Benchmarks 2.0.x \n \n https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where { LOCATION FNAME # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz\n\n71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , \n cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ \n lib: \n scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh \n src: \n tpch-query-instances: \n cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. \n Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \n \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x \n \n Example \n Description \n Demonstrates \n \n example1 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. \n \n example3 \n A Scala program containing a SparkSQL \n How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. \n \n example4 \n A Scala-based application. This application generates its input and applies multiple transformations to the generated data. \n How Unravel helps to root-cause a failed \n \n example5 \n A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group \n Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only \n Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x \n \n Example \n Description \n Demonstrates \n \n example1 \n see example1 in Benchmarks for Spark 1.6.x \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce \n How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. \n \n example3 example4 example5 \n see example3 - example5 in Benchmarks for Spark 1.6.x \n \n example6 \n A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. \n recommendations.png " }, 
{ "title" : "Unravel APIs", 
"url" : "unravel-4-3/advanced-topics/unravel-apis.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel APIs", 
"snippet" : "Use Case - Auto Actions and Pagerduty...", 
"body" : " Use Case - Auto Actions and Pagerduty " }, 
{ "title" : "Use Case - Auto Actions and Pagerduty", 
"url" : "unravel-4-3/advanced-topics/unravel-apis/use-case---auto-actions-and-pagerduty.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel APIs \/ Use Case - Auto Actions and Pagerduty", 
"snippet" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done...", 
"body" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's Autoactions API. Currently, this action is initiated outside of Unravel's Server. For the integration you need to complete 2 setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerdutyinformation. The python script \n must pagerduty Using pagerduty for notifications Set up a pagerduty service. \n 1 Configuration Services https:\/\/www.pagerduty.com \n 2 Add New Service \n 3 Use our API directly Events API v2 \n 4 Add Service \n 5 I ntegration Key Integrations Run the Unravel API python script on your local computer. \n 1 https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 \n 2 cd unravel-api-demo-v1 # cd unravel-api-demo-v1\n# ls\nREADME.md api-test.py \n 3 # python .\/api-test.py \n 4 Unravel Autoactions API Address The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions UNRAVEL_HOST_IP https:\/\/playground.unraveldata.com. Enter the pagerduty integration key Pagerduty API key \n 5 Start To scroll within list, click within the Autoactions Name \n 6. Navigate to the Auto Actions page ( Manage Auto Actions Creating Auto Actions \n 7. In our example there were 2 active autoactions, \n Kill job hogging the cluster \n Rogue App AA #1. \n Kill job hogging the cluster \n 8. \n \n Kill job hogging the cluster \n Updated Unravel UI \n Updated Unravel API \n Kill job hogging the cluster \n 9 \n Sample sms message \n Sample email The python script \n must pagerduty \n SuccessFailureAPI.png \n PagerDuty-AddService.png \n PagerDuty-AddingNewService.png \n pagerDutyEmail.png \n smsMessages.png \n UI-1Active.png \n scrolldown-unravelAPI.png \n AutoAutosServer-Sorted.png \n Started-unravelAP.png \n filledIn-unravelAPI-boxed.png \n PagerDuty-Key.png \n kk-PagerDuty-AddServiceButton.png \n PagerDuty-Services.png " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "Installing Sensors Unravel Sensor for Individual Applications Submitted Through spark-submit Unravel Sensor for Individual Hive Queries Upgrading the Unravel Server and Sensors Uninstalling Unravel Server Uploading Spark Programs to Unravel...", 
"body" : " Installing Sensors Unravel Sensor for Individual Applications Submitted Through spark-submit Unravel Sensor for Individual Hive Queries Upgrading the Unravel Server and Sensors Uninstalling Unravel Server Uploading Spark Programs to Unravel " }, 
{ "title" : "Installing Sensors", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/installing-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors", 
"snippet" : "Unravel Sensor for Individual Applications Submitted Through spark-submit Unravel Sensor for Individual Hive Queries...", 
"body" : " Unravel Sensor for Individual Applications Submitted Through spark-submit Unravel Sensor for Individual Hive Queries " }, 
{ "title" : "Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/installing-sensors/unravel-sensor-for-individual-applications-submitted-through-spark-submit.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP 1. Obtain the Sensor T...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST_IP http:\/\/UNRAVEL_HOST_IP:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Within this directory, locate the sensor file: unravel-agent-pack-bin.zip 2. Run the Sensor to Intercept Spark Apps Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}3000\/hh\/unravel-agent-pack-bin.zip\n# cd {UNRAVEL_SENSOR_PATH}\n# hdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\n# set UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, substitute your local values for: (UNRAVEL_SENSOR_PATH} unravel-agent-pack-bin.zip (UNRAVEL_SENSOR_PATH} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNRAVEL_SENSOR_PATH={UNRAVEL_SENSOR_PATH}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option B: If You Run Spark Apps in yarn-client Mode To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST If you use multiple hosts as clients, on each client # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip When using yarn-client mode, you need to use the --driver-java-options command line option in your default properties file, due to the timing of driver JVM start-up; see the example below. Define spark.executor.extraJavaOptions spark-submit # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip To use the example below, substitute your local values for: (UNZIPPED_ARCHIVE_DEST} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNZIPPED_ARCHIVE_DEST={UNZIPPED_ARCHIVE_DEST}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --driver-java-options \"-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs=spark-$SPARK_VERSION\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR " }, 
{ "title" : "Unravel Sensor for Individual Hive Queries", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/installing-sensors/unravel-sensor-for-individual-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Unravel Sensor for Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard Map...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; " }, 
{ "title" : "Upgrading the Unravel Server and Sensors", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/upgrading-the-unravel-server-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Upgrading the Unravel Server and Sensors", 
"snippet" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unr...", 
"body" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Lines beginning with '\/\/' are comments. Copy the new RPM to each Unravel host. Stop each host simultaneously \/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host2 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously \/\/ host1\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host2\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host3\n# sudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties You must add license key to unravel.properties After all the RPM upgrades finish, restart Unravel Server on each host simultaneously \/\/ host1\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host2\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh start Complete any deployment-specific upgrade steps. Unravel Sensor Upgrade sensors on CDH cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Check you current sensor version. Log in to the Cloudera Manager and click the parcel ( Check for New Parcels UNRAVEL_SENSOR Click the Download Distribute When activating the new sensors, you will be notified that Hive and Spark services must be restarted. Once new sensor activation is completed, the old version is automatically disabled. Upgrade sensors on HDP cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. \n HIGHLIGHTED \n UNRAVEL_HOST_IP \n SPARK_VERSION _X.Y.Z \n HIVE_VERSION_X.Y.Z Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar Upgrade sensors on MAPR cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar \n image2018-2-5_19-58-45.png \n image2018-2-5_19-55-44.png \n image2018-2-5_19-50-3.png \n image2018-2-5_19-48-34.png \n image2018-2-5_16-51-24.png " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel # sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel\n# sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Uploading Spark Programs to Unravel", 
"url" : "unravel-4-3/advanced-topics/unravel-servers-and-sensors/uploading-spark-programs-to-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading Spark Programs to Unravel", 
"snippet" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their...", 
"body" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: \n HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their location on the spark-submit Example: In yarn-client mode local \n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {Comma separated list of files} \\\n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR The default value of spark.unravel.program.dir Example: In yarn-cluster mode \n --files {comma-separated-list-of-source-files} spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_SOURCE_FILE_DIRECTORY}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {comma-separated-list-of-source-files} \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option 2: Upload a Zip Archive Package all relevant source files into a zip archive. It's advisable to keep the archive small by including only the relevant driver source files. Example: In yarn-client mode local \n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Example: In yarn-cluster mode \n --files $SRC_ZIP_PATH --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $SRC_ZIP_PATH \\\n --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Unravel searches for source files in this order: \n spark.unravel.program.dir Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Applications Program \n program.png " }, 
{ "title" : "Workflows", 
"url" : "unravel-4-3/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Workflows", 
"snippet" : "Tagging Workflows Monitoring Oozie Workflows Monitoring Airflow Workflows Tagging a Hive on Tez Query...", 
"body" : " Tagging Workflows Monitoring Oozie Workflows Monitoring Airflow Workflows Tagging a Hive on Tez Query " }, 
{ "title" : "Tagging Workflows", 
"url" : "unravel-4-3/advanced-topics/workflows/tagging-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Workflows \/ Tagging Workflows", 
"snippet" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: unravel.workflow.name TenantName-ProjectName-WorkflowName unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank space...", 
"body" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: \n unravel.workflow.name TenantName-ProjectName-WorkflowName \n unravel.workflow.utctimestamp \n yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: \n SET unravel.workflow.name=\"ETL-Workflow\"; \n SET unravel.workflow.name=ETL-Workflow; Please note the following: Different runs of the same same unravel.workflow.name Different runs of the same different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name The example below shows a Hive query that is marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\n\nSELECT foo FROM table WHERE … [Rest of Hive Query text goes here] Easy Recipes for Tagging Workflows First, export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow: Export the workflow name: export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow Export the UTC timestamp for this run of the workflow. Here, we use bash's date export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Then follow the appropriate instructions below: \n Tagging a Hive or Beeline query \n Tagging a Sqoop job \n Tagging a direct MapReduce job \n Tagging a Spark job \n Tagging a Pig job \n Tagging an Impala Job How to Tag a Hive Query Using SET Commands in Hive Currently only works for Hive on MR # hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=${env:WORKFLOW_NAME};\nSET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP};\nselect count(1) from lineitem; How to Tag a Beeline Query Using --hiveconf export WORKFLOW_NAME=ETL-Workflow-Beeline\n\nbeeline -u 'jdbc:hive2:\/\/vego1.unraveldatalab.com:10000' \\ \n --hiveconf unravel.workflow.name=${WORKFLOW_NAME} \\\n --hiveconf unravel.workflow.utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') \\\n -e \"use tpcds_text_30; select count(*) from reason;\"\n How to Tag a Sqoop Job Using –D Command Line Parameters # sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes: https:\/\/issues.apache.org\/jira\/browse\/SQOOP-3061 How to Tag a Direct MapReduce Job Using –D Command Line Parameters # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input \/tmp\/soumitra\/data\/small -output \/tmp\/soumitra\/outsmoke How to Tag a Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. \n unravel.workflow.name \n spark.unravel.workflow.name # spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster How to Tag a Pig Job Using –param and SET Commands # pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME;\nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP;\n\nlines = LOAD '\/tmp\/soumitra\/data\/small' using PigStorage('|') AS (line:chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount; How to Tag an Impala Job Using SET Commands # impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql: SET DEBUG_ACTION=\"|unravel.workflow.name::${var:workflowname}|unravel.workflow.utctimestamp::${var:utctimestamp}|\";\nselect * from usstates; How to Tag an Tez Job Using SET Commands Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications Workflow \n 20180525-Workflow.png " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-3/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url UNRAVEL_HOST oozie.server.url UNRAVEL_HOST Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows....", 
"body" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url UNRAVEL_HOST oozie.server.url UNRAVEL_HOST Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows. " }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-3/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel ...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 HIGHLIGHTED Airflow Web UIAccess Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following properties. If you can't find them, add them. Https For Airflow Web UIAccess Set the following three (3) properties. Replace AIRFLOW_WEB_URL http:\/\/ com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true Https For Airflow Web UIAccess Set the following four (4) properties. Replace AIRFLOW_WEB_URL https:\/\/ Replace AIRFLOW_WEB_UI_username AIRFLOW_WEB_UI_password com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true com.unraveldata.airflow.login.name=AIRFLOW_WEB_UI_username com.unraveldata.airflow.login.password= AIRFLOW_WEB_UI_password Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Range By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following property. If you can't find it, add it.Note there’s a “-” (minus sign) in the value. airflow.look.back.num.days=- X Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Below is a sample script, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add Hooks for Unravel Instrumentation. This script below, example-hdp-client.sh spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application only spark-defaults.conf This script can be invoked to submit an Airflow Spark application via spark-submit PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call Configuration Properties Airflow Property Description Set By User Unit Default prepend: available Notes if airflow is currently available false: not available true: available yes boolean - server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ yes url - protocol Type of connection, i.e., https or http https login.name Airflow login username. yes string - login.password Password for login username. yes string - status.timeout.sec Set Airflow workflow status timeout in Unravel. seconds 3600 http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow web UI. Default unlimited. bytes 0 no prepend airflow.look.back.num.days Date range for workflows, specified in days to look back, i.e., -5 is past 5 days, count -5 " }, 
{ "title" : "Tagging a Hive on Tez Query", 
"url" : "unravel-4-3/advanced-topics/workflows/tagging-a-hive-on-tez-query.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Advanced Topics \/ Workflows \/ Tagging a Hive on Tez Query", 
"snippet" : "See for general information see Tagging Workflows The following properties must be set in\/usr\/local\/unravel\/etc\/unravel.properties. You should adjust the script path and method name parameters (highlighted in red) according to your cluster setup. com.unraveldata.tagging.script.enabled=true com.unrav...", 
"body" : "See for general information see Tagging Workflows The following properties must be set in\/usr\/local\/unravel\/etc\/unravel.properties. You should adjust the script path and method name parameters (highlighted in red) according to your cluster setup. com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path=\/usr\/local\/unravel\/etc\/tag_app.py com.unraveldata.app.tagging.script.method.name=get_tags You can create tagged workflows for tez applications in four (4) ways. Use --hiveconf via hive command and enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql. set hive.execution.engine=tez; Select count(*) from my_test_table; Use theglobal python script for application tagging. Assuming the global script is \/tmp\/tag_app.py, Use--hiveconf via beeline command and enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000' --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Use thetez.sql script, then run beeline. You must define these the two (2) workflow tags in tez.sql. set hive.execution.engine=tez; set unravel.workflow.name=my_tez_workflow; unravel.workflow.utctimestamp=20180801T000001Z Select count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000' -f tez.sql\n " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-3/troubleshooting.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Troubleshooting", 
"snippet" : "Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround:...", 
"body" : " Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround: " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "unravel-4-3/troubleshooting/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh Email the bundle to the Unravel support team. " }, 
{ "title" : "Appendices", 
"url" : "unravel-4-3/appendices.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices", 
"snippet" : "Event List Resource Metrics Some Keywords and Error Messages Server Daemon Reference Finding Unravel properties' values in Microsoft's Azure...", 
"body" : " Event List Resource Metrics Some Keywords and Error Messages Server Daemon Reference Finding Unravel properties' values in Microsoft's Azure " }, 
{ "title" : "Event List", 
"url" : "unravel-4-3/appendices/event-list.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Event List", 
"snippet" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception. HIVE Application failure HIveFailureIncorrectHe...", 
"body" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception. HIVE Application failure HIveFailureIncorrectHeaderEvent Indicates that the Hive query fails with \"incorrect header check\" exception. HIVE Application failure HiveFailureReturnCodeEvent Indicates that the Hive query fails and shows return code. HIVE Application failure HiveMapJoinMemoryExhaustionEvent Indicates that the Hive query fails because it is out of memory in map join, and recommends turning off mapjoin. HIVE Application failure HiveOutOfMemoryErrorEvent Indicates that the Hive query fails because it is out of memory. HIVE Informational HiveKillFailEvent Indicates that the Hive query is killed or failed with lots of wasted work. HIVE Informational HiveSuccWithKillFailEvent Indicates that the Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. HIVE Informational HiveShuffleBytesEvent Indicates that the Hive query has lots of data shuffle from map to reduce side. HIVE Informational HiveTimeBreakdownEvent Identifies where time is spent on for the query and points out significant events, including MR-level skew events. HIVE Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true. HIVE Speedup HiveSingleReduceCountDistinct Indicates that the Hive query has a job with a long single reducer because the query has \"count distinct\". HIVE Speedup HiveSingleReduceLongWait Indicates that the Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side. HIVE Speedup HiveSingleReduceOrderBy Indicates that the Hive query has a job with a long single reducer because the query has \"order by\". HIVE Speedup HiveTooFewReduceEvent Indicates that the Hive query is using too few reducers. HIVE Speedup\/ Resource Utilization HiveTooLargeMapEvent Indicates that mappers in the Hive query are requesting too much memory. HIVE Speedup\/ Resource Utilization HiveTooLargeReduceEvent Indicates that reducers in the Hive query are requesting too much memory. HIVE Speedup\/ Resource Utilization HiveTooManyMapEvent Indicates that the Hive query is using too many mappers. HIVE Speedup\/ Resource Utilization HiveTooManyReduceEvent Indicates that the Hive query is using too many reducers. IMPALA Application failure ImpalaFailureEvent Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible). IMPALA Informational\/ Speedup\/ Resource Utilization ImpalaTimeBreakdownEvent Displays the longest phase in the Impala query. If the longest phase is query execution then it displays the longest operator in the Impala query. If applicable, this event shows insights as to why this operator took the most time, and makes tuning suggestions. IMPALA Speedup ImpalaTimeSkewEvent Detects whether one of the fragment instances took much longer than the other instances and indicates the bottleneck node. MR Application Failure MRClassNotFoundEvent Indicates that MR job fails due to a \"class not found\" exception. MR Application Failure MRFailureCompressLibNotAvailable Indicates that MR job fails due to a \"compression library not available\" exception. MR Application Failure MRFailureFileNotFoundEvent Indicates that MR job fails due to a \"file not found\" exception. MR Application Failure MRFailureIllegalArgumentEvent Indicates that MR job fails due to an \"illegal argument\" exception. MR Application Failure MRFailureNumberFormatEvent Indicates that MR job fails due to a \"number format\" exception. MR Application Failure MRFailureTimeOutEvent Indicates that MR job fails due to a \"time out\" exception . MR Application Failure MRGcOverheadLimitExceededMapEvent Indicates that the MR job fails because the GC overhead limit is exceeded on map side. MR Application Failure MRGcOverheadLimitExceededReduceEvent Indicates that the MR job fails because the GC overhead limit is exceeded on reduce side. MR Application Failure MRJavaOutOfMemoryMapEvent Indicates that MR job fails because it is out of memory on map side. MR Application Failure MRJavaOutOfMemoryReduceEvent Indicates that MR job fails because it is out of memory on reduce side. MR Informational MRKillFailEvent Indicates that the MR job is killed or failed with lots of wasted work. MR Informational MRSuccWithKillFailEvent Indicates that the MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. MR Informational MRShuffleBytesEvent Indicates that the MR job has lots of data shuffle from map to reduce side. MR Informational MRTimeBreakdownEvent Identifies where time is spent on for the job and points out significant events. MR Speedup MRLongTasksFromSlowNodeEvent Indicates that the MR job has long-running map\/reduce tasks from slow nodes. MR Speedup MRMapSkewDataIOEvent Indicates that the map phase of the MR job has a time skew with strong correlation with IO. MR Speedup MRReduceSkewDataIOEvent Indicates that the reduce phase of the MR job has a time skew with strong correlation with IO. MR Speedup MRTooFewReduceEvent Indicates that the MR job is using too few reducers. MR Speedup\/ Resource Utilization MRTooLargeMapEvent Indicates that mappers in the MR job are requesting too much memory. MR Speedup\/ Resource Utilization MRTooLargeReduceEvent Indicates that reducers in the MR job are requesting too much memory. MR Speedup\/ Resource Utilization MRTooManyMapEvent Indicates that the MR job is using too many mappers. MR Speedup\/ Resource Utilization MRTooManyReduceEvent Indicates that the MR job is using too many reducers. SPARK Application Failure DriverOomeEvent Indicates that a driver failed with \"OutOfMemory\" error. SPARK Application Failure ExecutorOomeEvent Indicates that an executor failed with \"OutOfMemory\" error. SPARK Application Failure YarnContainerKilledEvent Indicates that there are containers killed by YARN. SPARK Resource Utilization ContainerSizingUnderutilizationEvent Indicates that container resources are underutilized. SPARK Resource Utilization InefficientInputSplitSizeEvent Indicates an inefficient input split size. SPARK Resource Utilization TooFewPartitionsEvent Indicates that there are too few partitions with respect to available parallelism. SPARK Resource Utilization UnderutilizedCpuEvent Indicates that there is low utilization of CPU resources. SPARK Resource Utilization UnderutilizedNodeMemoryEvent Indicates that there is low utilization of memory resources. SPARK Resource Utilization UnderutilizedStorageMemoryEvent Indicates that the Spark storage memory has low utilization. More RDDs can be cached in memory. SPARK Speedup CachingOpportunityEvent Indicates that there is an (unused) opportunity for RDD caching. SPARK Speedup ContendedCpuEvent Indicates that there is contention for CPU resources. SPARK Speedup ExcessiveGcEvent Indicates that there is high garbage collection overhead. SPARK Speedup ExecutorImbalanceEvent Indicates that there is load imbalance among executors. SPARK Speedup ExhaustedStorageEvent Indicates that the Spark storage memory is getting exhausted. SPARK Speedup LightExecutorEvent Indicates that there is large idle time for one or several executors. SPARK Speedup LongStageEvent Indicates that there is load imbalance among tasks for the longest stage of the application. SPARK Speedup ContendedDriverEvent Indicates that the driver is a bottleneck in the application. It monitors data transfers to the driver (i.e., the time spent in fetching the data from executors). TEZ Application Failure TezOutOfMemoryErrorEvent Indicates that the TEZ query ran out of memory. TEZ Resource Utilization\/ Inefficiency TezNoDAGEvent Indicates that the TEZ session was created and TEZ DAG was not submitted. TEZ Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true. TEZ Speedup\/ Resource Utilization MapVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few mappers task. TEZ Speedup\/ Resource Utilization MapVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many mappers task. TEZ Speedup\/ Resource Utilization ReduceVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few reducers task. TEZ Speedup\/ Resource Utilization ReduceVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many reducers task. Note: In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook. WORKFLOW Informational WorkflowTimeBreakdownEvent Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. WORKFLOW Informational\/ Application Failure WorkflowGeneralFailureEvent For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. WORKFLOW Informational\/ Resource Utilization WorkflowIRSummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources. WORKFLOW Informational\/ SLA analysis WorkflowDurationAnomalousEvent If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. WORKFLOW Informational\/ Speedup WorkflowIASummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-3/appendices/resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory avail...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory available for launching new processes avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations committedHeap bytes Committed heap size committedNonHeap bytes Committed non-heap size committedVirtualMemory bytes The committed virtual memory in the operating system currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems edenPeakUsage bytes Maximum memory usage in the eden space freePhysicalMemory bytes The free physical memory in the operating system freeSwap bytes The free swap size fullGcCount COUNT Number of full GC runs fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms initHeap bytes Initial heap size initNonHeap bytes Initial non-heap size maxHeap bytes Maximum heap size maxNonHeap bytes Maximum non-heap size minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC oldPeakUsage bytes Maximum memory usage in the old space processCpuLoad PERCENT Average process CPU load for the last minute (all cores) snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started survivorPeakUsage bytes Maximum memory usage in the survivor space systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) totalPhysicalMemory bytes The total physical memory in the operating system totalSwap bytes The total swap size usedHeap bytes Used heap size usedNonHeap bytes Used non-heap size vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "unravel-4-3/appendices/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Some Keywords and Error Messages", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type....", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. " }, 
{ "title" : "Spark Keywords", 
"url" : "unravel-4-3/appendices/some-keywords-and-error-messages.html#UUID-ec331f04-82af-928f-85ec-57293c096a71_id_SomeKeywordsandErrorMessages-SparkKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Keywords", 
"snippet" : "Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application ...", 
"body" : " Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application on a worker node Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset spark.default.parallelism Default number of partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.executor.memory Related to executor memory spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.sql.shuffle.partitions Number of SparkSQL partitions spark.yarn.executor.memoryOverhead YARN memory overhead SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SparkConf Spark configuration object SQLContext Main Spark SQL entry point StreamingContext Main Spark Streaming entry point " }, 
{ "title" : "Spark Error Messages", 
"url" : "unravel-4-3/appendices/some-keywords-and-error-messages.html#UUID-ec331f04-82af-928f-85ec-57293c096a71_id_SomeKeywordsandErrorMessages-SparkErrorMessages", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Error Messages", 
"snippet" : "Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer....", 
"body" : " Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. " }, 
{ "title" : "MapReduce\/Hive Keywords", 
"url" : "unravel-4-3/appendices/some-keywords-and-error-messages.html#UUID-ec331f04-82af-928f-85ec-57293c096a71_id_SomeKeywordsandErrorMessages-MapReduceHiveKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Some Keywords and Error Messages \/ MapReduce\/Hive Keywords", 
"snippet" : "Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record bo...", 
"body" : " Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Server Daemon Reference", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html#UUID-1f044058-bceb-6b5c-28e8-27a928777d78_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_hostN Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2_N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma_N Metrics Analyzer unravel_ngui aNGular web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s_N Elasticsearch unravel_sw_N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor \\ Impala Daemon unravel_zk_N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html#UUID-1f044058-bceb-6b5c-28e8-27a928777d78_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value general unravel properties com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. ...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Type\/ Description Default Value general unravel properties com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp hdfs properties com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) 30 jdbc properties unravel.jdbc.username MySQL (embedded or external) username for db unravel unravel.jdbc.password MySQL (embedded or external) password for db random generated for bundled MySQL unravel.jdbc.url This is JDBC URL without username and password jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prodc kafka com.unraveldata.kafka.broker_list embedded Kafka 127.0.0.1:4091 mapreduce com.unraveldata.longest.job.duration.days Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) 2 oozie com.unraveldata.oozie.fetch.interval.sec seconds between intervals for fetching Oozie workflow status 120 com.unraveldata.oozie.fetch.num Max number of jobs to fetch during an interval 100 oozie.server.url URL for accessing Oozie to track workflows http:\/\/localhost:11000\/oozie Zookeeper com.unraveldata.zk.quorum embedded Zookeeper ensemble in form host1:port1,host2:port2, … 127.0.0.1:4181 " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html#UUID-1f044058-bceb-6b5c-28e8-27a928777d78_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to jvm to override other settings unset HADOOP_CONF The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml as discovered by running \"hadoop fs -ls \" UNRAVEL_DATA_DIR A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data, zk_data tmp_data com.unraveldata.tmpdir \/srv\/unravel UNRAVEL_LISTEN_PORT The Web UI port on the primary or standalone Unravel installation ( service unravel_ngui 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable Root Environment Settings", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html#UUID-1f044058-bceb-6b5c-28e8-27a928777d78_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference \/ Adjustable Root Environment Settings", 
"snippet" : "The optional file \/etc\/unravel_ctl Env Variable Default if not set Description RUN_AS unravel The \/etc\/init.d\/unravel_* USE_GROUP unravel The primary group membership of the user that runs the daemons...", 
"body" : "The optional file \/etc\/unravel_ctl Env Variable Default if not set Description RUN_AS unravel The \/etc\/init.d\/unravel_* USE_GROUP unravel The primary group membership of the user that runs the daemons " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-3/appendices/server-daemon-reference.html#UUID-1f044058-bceb-6b5c-28e8-27a928777d78_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and ...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~3.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-3/appendices/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Appendices \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Release Notes", 
"url" : "unravel-4-3/release-notes.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes", 
"snippet" : "v4.3.0 v4.3.1.x v4.3.1.0 v4.3.1.1 v4.3.1.2 v4.3.1.3 v4.3.1.4 v4.3.1.5 v4.3.1.6 v4.3.1.7 v4.3.1.8 - not available v4.3.1.9 v4.3.1.10...", 
"body" : " v4.3.0 v4.3.1.x v4.3.1.0 v4.3.1.1 v4.3.1.2 v4.3.1.3 v4.3.1.4 v4.3.1.5 v4.3.1.6 v4.3.1.7 v4.3.1.8 - not available v4.3.1.9 v4.3.1.10 " }, 
{ "title" : "v4.3.0", 
"url" : "unravel-4-3/release-notes/v4-3-0.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-3/release-notes/v4-3-0.html#UUID-45241904-00c0-5fb9-ea06-50c01f5a2a56_id_v430-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0 \/ Software Version", 
"snippet" : "Release Date: 01\/30\/2018 For details on downloading updates see here...", 
"body" : "Release Date: 01\/30\/2018 For details on downloading updates see here " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-3/release-notes/v4-3-0.html#UUID-45241904-00c0-5fb9-ea06-50c01f5a2a56_id_v430-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: Testing is in progress. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the compatibility matrix...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: Testing is in progress. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the compatibility matrix " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-3/release-notes/v4-3-0.html#UUID-45241904-00c0-5fb9-ea06-50c01f5a2a56_id_v430-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional See Improvements and Bug Fixes for list of sensor fixes\/upgrades....", 
"body" : " Optional See Improvements and Bug Fixes for list of sensor fixes\/upgrades. " }, 
{ "title" : "New Features", 
"url" : "unravel-4-3/release-notes/v4-3-0.html#UUID-45241904-00c0-5fb9-ea06-50c01f5a2a56_id_v430-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0 \/ New Features", 
"snippet" : "Significant improvement in speed to UI based on new technology stack (Angular + Node.js). Merged Operation | Reports & Data tab into new Reports Tab to locate all reports together. See Reports. ADD LINK. Added ability to chose the what columns to display in Applications tab. Added ability to filter ...", 
"body" : " Significant improvement in speed to UI based on new technology stack (Angular + Node.js). Merged Operation | Reports & Data tab into new Reports Tab to locate all reports together. See Reports. ADD LINK. Added ability to chose the what columns to display in Applications tab. Added ability to filter apps and resource usage graphs based on tags, e.g., user groups, projects, department. Roles has been expanded to allow the Admin to limit the Endusers to the Applications tab. Admin can further restrict their access to applications based upon tags, i.e., projects, departments. See Role Based Access. ADD LINK. Running Unravel daemons as a single user that is changeable after install Added support for Application Master job counters in Auto Action v2 rules. (UNRAVEL-2621) Added container level aggregation in Auto Action v2. (UNRAVEL-2612) Added support for \"sustained\" violation detection in Auto Action v2. (UNRAVEL-1084) Link to Unravel Documentation on title bar. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-3/release-notes/v4-3-0.html#UUID-45241904-00c0-5fb9-ea06-50c01f5a2a56_id_v430-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.0 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make message more readable. (UNRAVEL-3021) All emails and Slack messages sent from Unravel server now include the organization information as set in Unravel server settings. (UNRAVEL-250...", 
"body" : "Improvements Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make message more readable. (UNRAVEL-3021) All emails and Slack messages sent from Unravel server now include the organization information as set in Unravel server settings. (UNRAVEL-2509) Split \"kill_app\" and \"move_to_queue\" action in conditional and non-conditional. (UNRAVEL-2110) Bug Fixes Fixed very rare but possible ConcurrentModificationException in Auto Action internal metric stream. (UNRAVEL-3173) Fixed aggregates metrics tracking bug caused by aggregator sharing in Auto Action v2 backend. (UNRAVEL-2903) Fixed exception when trying to read Auto Action v1 rule. (UNRAVEL-2891) Fixed “aggregate.Aggregator: found and removed orphan aggregate” error in Auto Action v2 backend. (UNRAVEL-2467) Fixed a bug that caused non-Oozie workflows metrics to be ignored by Auto Action v2 backend. (UNRAVEL-2515) Sensors Allow slowing down metrics collection in sensor agent. (UNRAVEL-2858) Properly close connections when using HttpUrlConnection. (UNRAVEL-2749) Pre-resolve and cache IP address of Unravel server. (UNRAVEL-2015) Sensor support for bringing in job annotations live. (UNRAVEL-2977) Backend support for streaming applications. Changes have been implemented to the StreamingProbe. (UNRAVEL-2819) Probes initialization when profiling apps in trace and debug modes. (UNRAVEL-3182) Upgrading from 4.2.x, Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* logs are moved to \/usr\/local\/unravel\/logs\/ \/srv\/unravel\/log_hdfs\/ directory is removed \/srv\/unravel\/tmp_hdfs\/ is no longer needed, so it is removed (\/srv\/unravel\/tmp\/ is used instead) env vars HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL in \/usr\/local\/unravel\/etc\/unravel.ext.sh no longer used For kerberos, com.unraveldata.kerberos.principal and com.unraveldata.kerberos.keytab.path are used instead After the 4.3.x RPM upgrade, you need to evaluate whether the switch_to_user.sh must be run in order for Unravel Server to load logs via HDFS and to access kerberos-spnego protected REST endpoints. If MapReduce or Spark jobs stop loading after upgrade, that is a strong indication that you need to switch users. This script need only be run once. See Run Unravel Daemons with Custom User " }, 
{ "title" : "v4.3.1.x", 
"url" : "unravel-4-3/release-notes/v4-3-1-x.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x", 
"snippet" : "v4.3.1.0 v4.3.1.1 v4.3.1.2 v4.3.1.3 v4.3.1.4 v4.3.1.5 v4.3.1.6 v4.3.1.7 v4.3.1.8 - not available v4.3.1.9 v4.3.1.10...", 
"body" : " v4.3.1.0 v4.3.1.1 v4.3.1.2 v4.3.1.3 v4.3.1.4 v4.3.1.5 v4.3.1.6 v4.3.1.7 v4.3.1.8 - not available v4.3.1.9 v4.3.1.10 " }, 
{ "title" : "v4.3.1.0", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-0.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.0", 
"snippet" : "Software Version Release Date: 04\/13\/2018 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: Testing is in progress. EMR: Te...", 
"body" : "Software Version Release Date: 04\/13\/2018 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: Testing is in progress. EMR: Testing is in progress. Qubole: Testing is in progress. Unravel Sensor Upgrade Optional See Improvements and Bug Fixes for list of sensor fixes\/upgrades. New Features Significant improvement in speed to UI based on new technology stack (Angular + Node.js) Merged Operation | Reports & Data tab into new Reports Tab to locate all reports together. See Reports Added ability to chose the what columns to display in Applications tables. Added ability to filter apps and resource usage graphs based on tags Roles has been expanded to allow the Admin to limit the Endusers to the Applications tab. Admin can further restrict their access to applications based upon tags, i.e., projects, departments. See Role Based Access Running Unravel daemons as a single user that is changeable after install Added support for Application Master job counters in Auto Action v2 rules. (UNRAVEL-2621) Added container level aggregation in Auto Action v2. (UNRAVEL-2612) Added support for \"sustained\" violation detection in Auto Action v2. (UNRAVEL-1084) Link to Unravel Documentation on title bar. Improvements and Bug Fixes Improvements Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make message more readable. (UNRAVEL-3021) All emails and Slack messages sent from Unravel server now include the organization information as set in Unravel server settings. (UNRAVEL-2509) Split \"kill_app\" and \"move_to_queue\" action in conditional and non-conditional. (UNRAVEL-2110) Bug Fixes Fixed very rare but possible ConcurrentModificationException in Auto Action internal metric stream. (UNRAVEL-3173) Fixed aggregates metrics tracking bug caused by aggregator sharing in Auto Action v2 backend. (UNRAVEL-2903) Fixed exception when trying to read Auto Action v1 rule. (UNRAVEL-2891) Fixed “aggregate.Aggregator: found and removed orphan aggregate” error in Auto Action v2 backend. (UNRAVEL-2467) Fixed a bug that caused non-Oozie workflows metrics to be ignored by Auto Action v2 backend. (UNRAVEL-2515) Sensors Allow slowing down metrics collection in sensor agent. (UNRAVEL-2858) Properly close connections when using HttpUrlConnection. (UNRAVEL-2749) Pre-resolve and cache IP address of Unravel server. (UNRAVEL-2015) Sensor support for bringing in job annotations live. (UNRAVEL-2977) Backend support for streaming applications. Changes have been implemented to the StreamingProbe. (UNRAVEL-2819) Probes initialization when profiling apps in trace and debug modes. (UNRAVEL-3182) Known Issues Kafka Monitoring Tez support Upgrade to 4.2.5 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST evaluate whether the switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh " }, 
{ "title" : "v4.3.1.1", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-1.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.1", 
"snippet" : "Software Version Release Date: 04\/25\/2018 for 4.3.1.1 Upgrade from 4.2.6 and 4.2.7 is supported For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos. HDP: On-premise HDP (up to v2.6) with Kerberos ...", 
"body" : "Software Version Release Date: 04\/25\/2018 for 4.3.1.1 Upgrade from 4.2.6 and 4.2.7 is supported For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos. HDP: On-premise HDP (up to v2.6) with Kerberos enabled + spnego enable. New Installs only. Unravel Sensor Upgrade Optional New Features Significant improvement in speed to UI based on new technology stack (Angular + Node.js). Support for Spark Streaming applications. Support for showing the program for SparkSQL applications. SQL queries can be easily mapped to the corresponding line of code of the program. Backend support for Spark application badges. The application level annotation contains meta-information about each input data source (event log, executor log, resource manager information). If one of the data sources has not been loaded for any reason (e.g., permission issues to download the logs from HDFS), the error message will be present in the application badge. Merged Operation | Reports Tab and Data tab and into the new Reports tab to locate all reports together. Added ability to filter apps and resource usage graphs based on tags, e.g., user groups, projects, department. Added ability to choose what columns to display in APM tables. 4.3.1.3 will add the column selection for the Applications page. Support for scope target transformations in Auto Action rules. (UNRAVEL-3449) Support for target transformation for global cluster\/queue\/user filters in Auto Actions rules. (UNRAVEL-3595) Improvements and Bug Fixes Improvements Automatic Management of Temporary File Spaces Improved Hive’s and MR’s TooManyMapper event to produce more accurate recommendation. (UNRAVEL-2558 & UNRAVEL-2562) Improved Hive’s and MR’s TooManyReducer event to produce more accurate recommendation. (UNRAVEL-2560 & UNRAVEL-2564) Added Hive’s and MR’s TooFewMapper event to detect cases where too few mappers are used and provide recommendation. (UNRAVEL-2559 & UNRAVEL-2563) Improved Hive’s and MR’s TooFewReducer event to produce more accurate recommendation. (UNRAVEL-2561 & UNRAVEL-2565) Added ImpalaNonColumnarTablesEvent to detect tables that are in non-columnar format. Bug Fixes Impala tagging via Python script is overridden by Impala Query Options on CDH 5.13 (IMPALA-20) Shorten jhist file name when its length is longer than limit of local file system. (UNRAVEL-2168) Consolidated Impala time skew event from one per fragment to one event per query. (UNRAVEL-3502) Known Issues HiveTooManyMapEvent, HiveTooManyReduceEvent, MRTooManyMapEvent, and MRTooManyReduceEvent could recommend a new value less than Unravel's temporary folder might have to be deleted manually at re\/start. New Configuration Properties Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem: com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages: com.unraveldata.spark.hadoopFsMulti.useFilteredFiles: com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes: com.unraveldata.spark.time.histogram: Software Upgrade Support Upgrading 4.2.6 or later to 4.3.x Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.2", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-2.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.2", 
"snippet" : "Software Version Release Date: 05\/09\/2018 for 4.3.1.2 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos. HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. Unravel Sensor Upgrade O...", 
"body" : "Software Version Release Date: 05\/09\/2018 for 4.3.1.2 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise CDH (up to v5.13) with Kerberos. HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. Unravel Sensor Upgrade Optional New Features When sorting Kafka topics the sort works across pages not just current page, i.e., the complete topic data is sorted. (UIX-3005) Improvements and Bug Fixes Improvements In Hive queries, if Unravel detects skew and has recommendation for resource configuration, it will highlight skew and direct users to address skew first before applying new resource configuration. In Hive event for tuning the number of map tasks, the property mapreduce.input.fileinputformat.split.minsize In Hive events for tuning the number of map and reduce tasks, the recommended values could sometimes result in apps using too few tasks and having a longer duration instead. The new version will not sacrifice duration while improving resource utilization. Security relevant HTTP headers added on various HTTP services. (UNRAVEL_3408 & UNRAVEL-3419) You can select which columns to display in the Applications | Applications Tab. Bug Fixes Improved stability of unravel_es service. (SENSOR-28) Unravel cleanups temporary folder at startup so no longer needs to be deleted manually. (PLATFORM-203) Cut and paste of job name now works correctly. (CUSTOMER-19) Resolved bytes out unit display in Kafka display. (UIX-360) Spark header now links to parent workflow. (UIX-359 & UIX-377) unravel_hdi_bootstrap script no longer overwrites existing HDP configuration. (CUSTOMER-3) Fixed segv\/sigbus error due to dlopen() error on libjvm.so Label Distribution graph is now displayed on Reports | Data Insights page. (UIX-285) Workflow view is a now disabled for non-admin users. (UIX-396) On Data Page - Details the sorting now works properly. (UIX-382) On Applications | Applications it's now notes if there are no applications. (UIX-382) Hive MetaStore connections in Data page decreased. (CUSTOMER-1) All Operators now shown in operators and query plan. (IMPALA-29) \"IOException: java.io KafkaStringProduce and KafkaStringConsumer are now reading their property files. (PLATFORM-25) Known Issues Reports downloaded in XLS format are currently unsupported on MAC operating systems. Hence the end user will be unable to view reports downloaded for VCores and Cluster MemoryMB from the UI on a MAC operating system.MAC users will be able to download and open reports in PDF, JPEG, PNG, SVG and CSV formats. Windows users will be able to download and open the reports in XLS fo New Configuration Properties None Software Upgrade Support Upgrading 4.2.6 or later to 4.3.x Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.3", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-3.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.3", 
"snippet" : "Software Version Release Date: 05\/21\/2018 for 4.3.1.3 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise (up to v5.14)with Kerberos; includes Spark 2.2.X. HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: O...", 
"body" : "Software Version Release Date: 05\/21\/2018 for 4.3.1.3 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise (up to v5.14)with Kerberos; includes Spark 2.2.X. HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: On-premise 5.2.2 MapR. Unravel Sensor Upgrade Optional New Features Added ability to turn off support\/comments panel. See New Configuration Properties Role Based Accessed Control Added Tagged based support for Reports | Operational Insights | Chargeback Added ability to configure admin and readonly RBAC role using LDAP. See New Configuration Properties Data access information on Reports Data Insights Added storage and trend analysis for HDFS and S3 storage systems based upon historical and current usage. Trend analysis also done in different dimensions: overall storage on the cluster, storage by user, storage by app type. Added default or on-demand small files analysis on the cluster. User can set parameters for the analysis which can be viewed in UI and downloaded as a file. Added ability to interactively sync Unravel with the latest Metastore status, including database, tables and partitions. See New Configuration Properties Heat map support has been added for Clusters ( Reports | Operational Insights | Cluster Workload When Impala queries run as part of an Oozie workflow it is shown as components of Oozie action nodes and Oozie insights reflect the underlying queries. (IMPALA-46) Improvements and Bug Fixes Improvements Reports Overview Reports Data Insights The Last Access Whenever the Hot\/Warm\/Cold policy is changed, the Labels Reports Data Insights Details. Bug Fixes Pig Gantt chart shows correct elapsed time. (Customer-23) Tables Table labels reflect the set policy (definition of Hot, Warm, Cold for display). (DATAPAGE-13, UIX-419, UIX-262, UIX-284) Tables are created after running queries. (DATAPAGE-15) All tables now have ‘dt’ entry. (DATAPAGE-15) Table\/partition counts match on across displays. (DATAPAGE-9, DATAPAGE-14, UIX-256) Fixed user name mis-match between CM and Impalad UI. (IMPALA-51) Auto Actions badge is displayed for Tez apps. (PLATFORM-102) When RBAC is turned on non-admin user can only reach their own apps. (UIX-477) Operations Usage Details Displays Resource graph when weekly Shows correct data when Group by Operations Dashboard Cluster Month and Day view show correct\/same jobs. (UIX-457) Summary reports can be downloaded as csv\/json. (UIX-242) Spark Gantt chart properly reflects time period. (UIX-371) Impala Operations Charts Impala Tool tip in Graph shows correct number of apps. Insights ordered by created timestamp. Fragment or Operators Fragment and Operator view populates correctly. (UIX-470) Merging-Exchange shows Instance View correctly. (UIX-387) Descending sort on time Horizontal Scroll works correctly. (UIX-445) Reports Data Insights Workflow pagination works. (UIX-425) dump_bundled_db.shr fixed for large packet size. (INSTALL-32) Follow symlinks for k_data. (INSTALL-44) Known Issues None New Configuration Properties LDAP properties for Role Based Access Control Please see Supported Roles and Role Based Access Control (RBAC) com.unraveldata.login.admins.ldap.groups: com.unraveldata.login.admins.readonly.ldap.groups: Hive Metastore Please see Hive Metastore Configuration com.unraveldata.metastore.db.c3p0.maxpoolsize: com.unraveldata.metastore.db.c3p0.maxconnectionage: com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections: com.unraveldata.metastore.db.c3p0.breakafteracquirefailure com.unraveldata.metastore.db.c3p0.acquireRetryAttempts: com.unraveldata.metastore.db.c3p0.acquireRetryDelay: Disable Support\/Comment in UI Please see Disabling Support\/Comments Panel com.unraveldata.ngui.support.enabled Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154). Support for upgrade from 4.3.0.4 and 4.3.1.0 Support for upgrade from 4.3.0.4 and 4.3.1.x to 4.3.1. Upgrading 4.2.6 or later to 4.3.x Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.4", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-4.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.4", 
"snippet" : "Software Version Release Date: 06\/18\/2018 for 4.3.1.4 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise5.13 & 5.14 (including Spark 2.2.x) with Kerberos HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: 6....", 
"body" : "Software Version Release Date: 06\/18\/2018 for 4.3.1.4 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise5.13 & 5.14 (including Spark 2.2.x) with Kerberos HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 Unravel Sensor Upgrade Optional New Features Operations Support LDAP search-bind account for login. Integrating LDAP Authentication. Improvements and Bug Fixes Improvements Fixed LDAP Integration issues during upgrade. (CUSTOMER-77) SPARK Logs are now formatted for readability. (UIX-503) The parameters listed in the Event Panel Recommendations are now sorted alphabetically. (UIX-369) WorkFlow APM Compare . Bug Fixes Tez App can now be retrieved on Cloudera. (CUSTOMER-102) RBAC Bug fixes Tag based RBAC now works. (CUSTOMER-75) I OException: java.io . File NotFoundException (No such file or directory) com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path=\/usr\/local\/unravel\/etc\/tag_app.py com.unraveldata.app.tagging.script.method.name Reports Data Insights Details Overview: Operations Usage Details Infrastructure Impala Applications | Applications Correctly displays all applications based on user query and the app count is correct. (UIX-581\/585) Corrected daemon confusion by correcting setting of CDH_CPATH. (INSTALL-35) Corrected Too many CM Sockets in Close_Wait State. (IMPALA-31) Username is now consistent across Impala query run and completion. (IMPALA-51) Known Issues Tez All Task Attempts ID duration is showing incorrect. (UIX-633) Queue and cluster ID's are missing for Hive running on Tez engine. (UIX-718) ActiveController count is shown as zero although active controller count for a cluster is one (1). (UKAFKA-9) Operations Dashboard: Operations Usage Details: New Configuration Properties None Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.4 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;' sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.5", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-5.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.5", 
"snippet" : "Software Version Release Date: 07\/18\/2018 for 4.3.1.5 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise5.13 & 5.14 (including Spark 2.2.x) with Kerberos HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: 6....", 
"body" : "Software Version Release Date: 07\/18\/2018 for 4.3.1.5 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise5.13 & 5.14 (including Spark 2.2.x) with Kerberos HDP: On-premise HDP (up to v2.6) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 Unravel Sensor Upgrade Required if using Hive-2.3.2 or Spark-2.3.0, or to see the Spark Version in Spark APM. New Features Role Based Access Control (RBAC) Reports available when turned on (CUSTOMER-11) Able to generate project and tenant tags from LDAP groups. See configuration properties Pig application status no longer states unknown when not applicable (CUSTOMER-122) Added ability to archive data, i.e., chargeback, estimation models (CUSTOMER-133 Spark - Hive Workflow APM error logs display Oozie jobs less than 90 seconds (CUSTOMER-158) Improvements and Bug Fixes Improvements Data Insights Details Applications Workflow Added HTPP security headers are present (CUSTOMER-139) Ability to see the Spark version in use (CUSTOMER-100) Bug Fixes Removed incorrect error message when running unravel_hdp_setup.sh Charts correctly displaying with no overlaps (CUSTOMER-132) CVS downloads are no longer are vulnerable to JMX (CUSTOMER-138) HDP setup script disables the Impala sensor be default (CUSTOMER-145) Table APM graphs displays information legibly (CUSTOMER-146) Airflow monitoring no longer cause unravel_jc2s Role Based Access Control (RBAC) Supports 2 tags. (CUSTOMER-68) IE browser correctly display tags in Application | Applications (UIX-597) Application | Applications Kafka Consumer Operations Usage Details Infrastructure Infrastructure Jobs Impala Charts can be display using tags (CUSTOMER-144) APM row number consistent across Query Plan and Operator tabs (UIX-827) Reports | Operational Insights Usage Details Chargeback Chargeback Cluster Compare Summary Cluster Workload Cluster Compare Spark Direct access to EMR works (USPARK-73) spark.executor.memory Conf Memory Event Count displays correctly in Events Panel (UIX-847) SparkSQL table entries sorted by duration (USPARK-74) EMR Sensor is no long sending data to spark topic on apps not running on (SENSOR-43) Tez Successful Tez Apps show correct status (TEZLLAP-64) Diagnostics are collected and displayed in App and Dag tiles (TEZLLAP-72) EMR Sensor - metrics data for Tez application are seen (SENSOR-51 ) User Interface Applications | Applications Resolved exception during Yarn parsing (PLATFORM-353) Known Issues PLATFORM-468: YARN logs won't be loaded into Unravel if yarn.log-aggregation.file-formats=IndexedFormat UIX-473: Support chat on app page not supported. UIX-955: Role based access control - com.unraveldata.rbac.prefix config is not completely supported. UIX-942: Role based access control - com.unraveldata.rbac.default=userName is not completely supported. UIX-912: Corner case- Fix Data table plugin popup in the Unravel UI. UIX-903: Intermittently task logs go empty while using Internet Explorer 11. UIX-709: Improve accuracy in pending application count. UIX-886: Improve Chargeback CSV report. UIX-810, DATA PAGE-56: Improve DATA PAGE graphs and table KPI’s . UIX-800, 798: Improvements in Kafka monitor partition page. UIX-609: Improve Cluster workload graphs Hourly\/day view. TEZLLAP-83: Improvements to Hive\/Tez Data Pipeline. Configuration Properties New RBAC Please see Supported Roles and Role Based Access Control (RBAC) com.unraveldata.rbac.enabled: com.unraveldata.rbac.ldap.tags: Project com.unraveldata.rbac.ldap.tag.Tag_Name.regex.find: Tenants Project, Tag_Name com.unraveldata.rbac.ldap.tags Deprecated com.unraveldata.rbac.mode com.unraveldata.rbac.user.operation com.unraveldata.rbac.tag com.unraveldata.rbac.prefix Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.4 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;' sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.6", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-6.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.6", 
"snippet" : "Software Version Release Date: 08\/13\/2018 for 4.3.1.6 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise 5.12, 5.13 & 5.14(including Spark 2.2.x and 2.3.x) with Kerberos HDP: On-premise (up to v2.6.3) with Kerberos + spnego ena...", 
"body" : "Software Version Release Date: 08\/13\/2018 for 4.3.1.6 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix CDH: On-premise 5.12, 5.13 & 5.14(including Spark 2.2.x and 2.3.x) with Kerberos HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 EMR: On-premise 5.13 & 5.14(including Hive 2.3.2 and Spark 2.3.0) Unravel Sensor Upgrade Required if using Hive-2.3.2 or Spark-2.3.0, or to see the Spark Version in Spark APM Recommended for the customers who are on 4.3.1.4 versions or earlier. New Features Added a queue column to Application | Application Tab. (Customer-59) Tags can be added to running applications. (Customer-72) Improvements and Bug Fixes Improvements jsoup timeout can now be set in unravel.properties Added Hive Hook support for HDP 2.3.2 Spark sensor now supports Spark 2.3.0 Added CLI arguments to allow admins choose what to update: Spark, Hive, Tez or MapR. (INSTALL-68) Added Unravel Hive Hook support for HDP 2.1 version. (Sensor-61) Graph tooltips no longer overlap the y-axis. (UIX-1029) Workflow page open apps and workflow instances in the same tab. (UIX-39) Bug Fixes Upgraded ESLint version to 5.3.0 so Unravel no longer vulnerable to CWE400. (CUSTOMER-225) Pushlog.sh creates the diagnostics file. (CUSTOMER-229) Applications| Applications AA\/Tuning suggestions now sort properly. (UIX-918) Username and Queue names are no longer truncated. (UIX-598) Tez Tags work correctly. (CUSTOMER-230) Job status and DAG has finished. (TEZLLAP-69) Task Attempts tab populating correctly. (TEZLLAP-70) Tez app loading correctly. (TEZLLAP-106) Graphs loading correctly. (TEZLLAP-811) Resource x-axis name displays correctly. (UIX-911) Spark Execution tab populates and is displayed correctly. (CUSTOMER-259) Logs populating correctly and scroll bars work. (UIX-1016, UIX-368) All Stages are displayed. (UIX-824) Spark-log driver shows Executor details. (UIX-112) Auto Actions Owner field populates and when selected the owner is notified when AA is triggered.(Customer-161) Notification works properly when Workflow SLA is broken. (CUSTOMER-237) Last run time and history updates correctly. (CUSTOMER-237) AA exception no longer causes RM thread to die. (Platform-522) History Runs Link count displays correctly. (UIX-904) AutoAction email includes link to the violating job. (CUSTOMER-252) MR Available Memory graph populates correctly. (UIX-750) MR\/Hive apps no longer hanging when in Running mode. (PLATFORM-591) Task logs display correctly in Internet Explorer. (UIX-903)Kafka Kafka kafka_create_topics.sh worked properly when RUN_AS and USE_GROUP are different in \/etc\/unravel_ctl Log no longer has duplicated entries. (UKAFKA-6) Impala Usage graphs resizing correctly (UIX-543) Operator search works. (UIX-838) Reports | Operational Insights Cluster Summary and Cluster Compare works correctly (Customer-194) Chargeback report generates report correctly is leading 0 is missing, i.e., .16. (Customer -217) Chargeback: queue list is no longer cut short. (UIX-619) Chargeback: report download has correct memory hours. (UIX-886) Chargeback: download has correct memory hours. (UIX-886) Chargeback: display pagination works for all \"group by\" options. (UIX-887) Cluster Summary: Vcores and Memory tabs sorting corrected. (UIX-948) Cluster Summary: sort by queue works. (UIX-952) Admin Manage Daemon UIX Graph titles no longer overlap graph. (UIX-1015) Close tabs works correctly. (UIX-920) Expand glyph in tiles now continually displays. (UIX-921) Tool tips for minimize and maximize corrected. (UIX-922) Blocks displays correctly when minimized. (UIX-941) Event count shows correct in even panel. (UIX-964) Infrastructure graphs alignment is correct in Internet Explorer. (UIX-967) Known Issues None Configuration Properties New com.unraveldata.airflow.http.timeout.sec. Monitoring Airflow Deprecated None Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.6 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_ migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;' sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.7", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-7.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.7", 
"snippet" : "Software Version Release Date: 09\/14\/2018 for 4.3.1.7 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(inc...", 
"body" : "Software Version Release Date: 09\/14\/2018 for 4.3.1.7 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(including Hive 2.3.2 and Spark 2.3.0) with Kerberos HDI: 3.6 Hadoop 2.7.3 , HDI 3.6 spark 2.1,2.2, 2.3, HDI 3.6 kafka 0.10.0 Unravel Sensor Upgrade No New Features None Improvements and Bug Fixes Improvements New configuration property to retain timezone of history server. (PLATFORM-699) Added option to setup scripts to uninstall unravel instrumentation and sensor. (INSTALL-60) Bug Fixes Backend Reduced unravel_ngui daemon startup time. (CUSTOMER-244) UIX Failed jobs are only shown if Application ID is available. (CUSTOMER-192) Unravel recommendations in Event Panel better formatted and more understandable. (CUSTOMER-204) Cluster Workload Report populates correctly. (CUSTOMER-209, CUSTOMER-211) Added ability to search by app name field on Applications and Workflow page tabs. (CUSTOMER-263) Removed unnecessary fields from from MapReduce Timeline view. (CUSTOMER-279) Auto Refresh updates page correctly. (UIX-1229,UIX-1227) Oozie Spark Tagged Workflow: time stamps, links and tags now populating correctly. (UIX-359) Infrastructure page no longer hanging. (UIX-1228) Metrics data now populates for Apps which are part of Workflows. (UIX-1225) Date Range end time now automatically refreshes. (UIX-1167) Sorting now works for Impala \"Frag Id\". (UIX-1051) Pending count populates correctly on both Operations and Application pages. (UIX-709) Platform Hive-on-MR\/Tez apps that are correctly labeled status unknown when in unknown state. (PLATFORM-664) Tez Various improvements in Tez pipeline efficiency (TEZLLAP-114) Tez tagged workflows are now working correctly. (CUSTOMER-272, CUSTOMER-282) Report App count is consistent and correct across different views in Cluster Workload. (REPORT-94) Known Issues Missing WFI icon within Application tab for MR job associated with workflow. (CUSTOMER-274) Application tab filter for Pending Status makes no sense it should be Accepted instead. (CUSTOMER-278) Add workflow pop up hangs for 2000+ workflows. (REPORT-116) Cluster Summary and Cluster compare hangs for large dataset. (REPORT-117) Date - time picker is missing for reports. (UIX-1201) RBAC: Issue with view by details in infrastructure. (UIX-942) HDI : Logs for Spark jobs are not shown in Cluster running Spark2.3. (USPARK-182) HDI : Hive app is shown only in \"RUNNING\" Stateon Hive APP page. Configuration Properties New com.unraveldata.hdfs.timezone: timezone of HDFS. Deprecated None Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.7 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_ migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;i sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.8 - not available", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-8---not-available.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.8 - not available", 
"snippet" : "This release was not made available for our general users....", 
"body" : "This release was not made available for our general users. " }, 
{ "title" : "v4.3.1.9", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-9.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.9", 
"snippet" : "Software Version Release Date: 11\/05\/2018 for 4.3.1.9 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(inc...", 
"body" : "Software Version Release Date: 11\/05\/2018 for 4.3.1.9 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(including Hive 2.3.2 and Spark 2.3.0) with Kerberos Unravel Sensor Upgrade No New Features None Improvements and Bug Fixes Improvements Special characters are now allowed in Workflow names. (CUSTOMER-377) Added Hive on Spark Support in Airflow. (PLATFORM-770) Fixes for Airflow Monitoring issues under Workflow. (CUSTOMER-304 & PLATFORM-772) Bug Fixes Airflow WorkFlow Auto action policy now triggers only specified workflows. (CUSTOMER-400) Miscellaneous issues with Workflow compare graph issue are fixed. (UIX-1448) AUTOACTION for Airflow Workflow fixes (PLATFORM-852) MR jobs associated with workflow are missing WFI icon\/link in Application tab. (CUSTOMER-274) Application tab filter for Pending Status is irrelevant; accepted status should be added. (CUSTOMER-278) Known Issues Unravel does not support \"Hive-on-Spark\" out of the box, but will show all related Hive and Spark jobs triggered as part of the \"Hive-on-Spark\" job. Some Spark apps are not showing up as part of Airflow workflow (PLATFORM-780) Out Of Memory, When polling large amount of past Airflow instances (PLATFORM-913) Add workflow pop up hangs for 2000+ workflows. (REPORT-116) Cluster Summary and Cluster compare hangs for large dataset. (REPORT-117) Date - time picker is missing for reports. (UIX-1201) RBAC: Issue with view by details in infrastructure. (UIX-942) Workflow Pagination issues (UIX-1339) Add workflow pop up hangs for 2000+ workflows (PLATFORM-882) Configuration Properties New com.unraveldata.airflow.http.max.body.size.byte com.unraveldata.airflow.status.timeout.sec Deprecated None Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.9 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_ migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;i sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.3.1.10", 
"url" : "unravel-4-3/release-notes/v4-3-1-x/v4-3-1-10.html", 
"breadcrumbs" : "Home \/ Unravel 4.3 \/ Release Notes \/ v4.3.1.x \/ v4.3.1.10", 
"snippet" : "Software Version Release Date: 11\/30\/2018 for 4.3.1.10 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(in...", 
"body" : "Software Version Release Date: 11\/30\/2018 for 4.3.1.10 For details on downloading updates see here Certified Platforms Please also review the compatibility matrix HDP: On-premise (up to v2.6.3) with Kerberos + spnego enabled. MapR: 6.0.0 with MapR Expansion Packs 4.1.1 CDH: On-premise 5.13 & 5.14(including Hive 2.3.2 and Spark 2.3.0) with Kerberos Unravel Sensor Upgrade No New Features None Improvements and Bug Fixes Improvements Enabled more logging information in Airflow Sensor (PLATFORM-909) Unravel will update the workflow status to UNKNOWN if it hasn't retrieved the workflow information from Airflow server for hours (PLATFORM-912) Bug Fixes Known Issues Unravel does not support \"Hive-on-Spark\" out of the box, but will show all related Hive and Spark jobs triggered as part of the \"Hive-on-Spark\" job. Some Spark apps are not showing up as part of Airflow workflow (PLATFORM-780) Out Of Memory, When polling large amount of past Airflow instances (PLATFORM-913) Add workflow pop up hangs for 2000+ workflows. (REPORT-116) Cluster Summary and Cluster compare hangs for large dataset. (REPORT-117) Date - time picker is missing for reports. (UIX-1201) RBAC: Issue with view by details in infrastructure. (UIX-942) Configuration Properties New com.unraveldata.airflow.http.max.body.size.byte com.unraveldata.airflow.status.timeout.sec Deprecated None Software Upgrade Support Support for upgrade from 4.2.6 (4.2.6-1128) or 4.2.7 (4.2.7-1154) Support for upgrade from 4.3.0.X to 4.3.1.9 Upgrading from 4.2.6 or 4.2.7 with this RPM causes a background process to run for about 15 minutes. During which data is migrated in the unravel_s_1 \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.out \/usr\/local\/unravel\/logs\/es_ migrate_4.3.1.0_from42.log \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42.sh here nohup. \/usr\/local\/unravel\/install_bin\/es_migrate_4.3.1.0_from42_cleanup.sh here An RPM upgrade triggers a temporary background process so changes can take effect. You can monitor it or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh DONE After upgrade: Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh There should be 47 tables. Check the table count manually with db_access.sh show tables;i sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh. Upgrade to 4.2.6 or later before Upgrade to 4.3.x Upgrading 4.2.x to 4.3.x, One-Time Potential Must-Do Steps Run-As When upgrading from 4.2.x to 4.3.x, you need to be cognizant of the run-as user for Unravel server daemons. Unravel Server 4.2.x and earlier runs daemons with 2 different local users (usually 'unravel' and 'hdfs' or 'mapr'). In 4.3.x, Unravel server simplifies this to run-as the single user 'unravel' by default. During an upgrade from 4.2.x to 4.3.x, all the daemons are converted over to user 'unravel'. Other changes that occur because of this: \/srv\/unravel\/log_hdfs\/* \/usr\/local\/unravel\/logs \/ srv\/unravel\/log_hdfs\/ \/ srv\/unravel\/tmp_hdfs\/ \/srv\/unravel\/tmp\/ env vars HDFS_KEYTAB_PATH HDFS_KERBEROS_PRINCIPAL \/usr\/local\/unravel\/etc\/unravel.ext.sh For kerberos, com.unraveldata.kerberos.principal com.unraveldata. After the first 4.3.x RPM upgrade, you MUST switch_to_user.sh Run Unravel Daemons with Custom User switch_to_user.sh Upgrade Process An RPM upgrade will trigger a temporary background process so changes can take effect. You can monitor or wait for these with \/usr\/local\/unravel\/install_bin\/await_fixups.sh ps The background fixups will produce log file output in: \/tmp\/unravel_post_upgrade_fixups.*.out \/tmp\/unravel_schema.*.out \/usr\/local\/unravel\/logs\/unravel_schema.log \/usr\/local\/unravel\/logs\/es_migrate_4.3.1.0_from42.* \/usr\/local\/unravel\/logs\/unravel_esm.log Please write to us at support@unraveldata.com " }, 
{ "title" : "", 
"url" : "unravel-4-2.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.2 •...", 
"body" : " Unravel 4.2 • " }, 
{ "title" : "Overview", 
"url" : "unravel-4-2/overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. \n The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. \n Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "unravel-4-2/overview.html#UUID-66537773-0467-0b93-956a-9c7aeea9d366_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API....", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. " }, 
{ "title" : "Installation Guides", 
"url" : "unravel-4-2/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides", 
"snippet" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3 (Optional): Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-o...", 
"body" : " Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) Step 1: Install Unravel Server on CDH+CM Step 2: Install Unravel Sensor Parcel on CDH+CM Step 3 (Optional): Enable Impala APM Hortonworks Data Platform (HDP) Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP MapR Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR Amazon Elastic MapReduce (Amazon EMR) and Qubole Clusters Step 1: Install Unravel Server for Amazon EMR or Qubole Cluster Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup Unravel for Azure HDInsight clusters Install Unravel HDInsight app Installation Guide for Unravel VM " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "unravel-4-2/install/install-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "unravel-4-2/install/install-cdh/install-cdh-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check. Configure the host machine. Install the Unravel Server RPM on the host...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check. Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility On-premises CDH up to v5.13, with Spark 2.2.x Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended, or 2x to 4x more RAM Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config. HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH. If Spark is in use, Spark client gateway. LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication. (Open signup by default.) On Unravel Edge-node server, please do not NTP should be running and in-sync with the cluster. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN’s “done dir” in HDFS YARN’s log aggregation directory in HDFS Spark event log directory in HDFS File sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive. 2. Install the Unravel Server RPM on the Host Machine The precise RPM filename will vary. The version has the structure x.y.b b x.y Get the Unravel Server RPM See instructions UPDATE_NEEDED_ADD LINK TO RPM. Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. 3. Configure Unravel Server (Basic\/Core Optional for CDH) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties All values in unravel.properties only optional Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay 1 com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.s3.batch.monitoring.interval.sec Optional 120 If Kerberos is Enabled Add Authentification for HDFS. Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab unravel:unravel chmod this can be the same principal and keytab as (2) if that is more convenient. Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab hdfs:hdfs chmod Tell Unravel Server about it in \/ usr\/local\/unravel\/etc\/unravel.ext.sh export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab\nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal by using ' klist -kt KEYTAB_FILE' If Sentry is Enabled Add These Permissions. For quicker setup, use the named principal. For more narrow privileges, define your own alt principal. Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alt read Spark event log hdfs:\/\/user\/history\/done hdfs or alt read MapReduce logs hdfs:\/\/tmp\/logs hdfs or alt read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alt read Obtain table partition sizes with \"stat\" only Hive Metastore GRANT for select hive or alt read Hive table information Please see Configure Permission for Unravel daemons on CDH Sentry Secured Cluste You can find and verify the principal the keytab by: # klist -kt KEYTAB_FILE\n If you are using KMS and HDFS encryption and are using the hdfs principal, you might need to adjust kms-acls.xml . If you are using \"JNI\" based groups for HDFS (a setting in CM), then you will need to add \" export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native\" to \/usr\/local\/unravel\/etc\/unravel.ext.sh Do Host-Specific Configuration Steps For CDH, there are no host-specific configuration steps. Restart Unravel Server After the edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. 4. Log into Unravel Web UI Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} admin\" \"unraveldata UNRAVEL_HOST_IP. For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 5. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"url" : "unravel-4-2/install/install-cdh/install-cdh-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 2: Install Unravel Sensor Parcel on CDH+CM", 
"snippet" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job...", 
"body" : "Introduction This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. These instructions apply to Unravel Sensor 4.2. Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM \n Workflow Summary Obtain and distribute the parcel from Unravel Server. Put the Hive Hook JAR in AUX_CLASSPATH Configure the gateway automatic deployment of Hive instrumentation. Configure the gateway automatic deployment of Spark instrumentation. \n HIGHLIGHTED When Active Directory Kerberos is used, UNRAVEL_HOST_IP To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 1. Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ X.Y UNRAVEL_HOST_IP unravel_lr UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, you can deactivate it now. Then click Activate 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh no subsitutions HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is Enabled: Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT {ROLE} # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE {ROLE} 3. Configure the Gateway Automatic Deployment of Hive Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use host2's \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip Set the hive-site.xml Snippet in Cloudera Manager, and Deploy the Hive Client Configuration to Gateways In Cloudera Manager (CM): Go to Hive service. Select the Configuration Search for hive-site.xml Add the xml snippet to Hive Client Advanced Configuration Snippet for hive-site.xml View as XML If cluster has been configured with \"Cloudera Navigator\"; the hive.exec.post.hooks hive.exec.post.hooks \n com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger, com.unraveldata.dataflow.hive.hook.HivePostHook \n IMPORTANT! Add the xml snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml View as XML Save the changes with optional comment \"Unravel snippet in hive-site.xml \" Perform action Deploy Hive Client Configuration ) or by using the Actions Restart the Hive service. (Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. ) Again, monitor the situation to see if all Hive queries are failing with a class not found or permission problems. If they are failing hive-site.xml Troubleshooting Check Unravel Web UI If queries are running fine and appearing in Unravel Web UI, then you are done. 4. Configure the Gateway Automatic Deployment of Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for \" spark-defaults In the Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf, On a multi-host Unravel Server deployment, use the fully qualified DNS or logical host2 for UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) \n spark-conf\/ UNRAVEL_HOST_IP SPARK_VERSION \n SPARK_VERSION \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true 5. Save changes. 6. Deploy client configuration by clicking the deploy glyph ( Actions Monitor the situation to see if all Spark queries are failing with a class not found or permission problems. If they are failing spark-defaults.conf 5. Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Set in Cloudera Manager In Cloudera Manager (CM): Go to YARN Select the Configuration Search for ApplicationMaster Java Opts Base ). Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr-Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043 Search for \n MapReduce Client Advanced Configuration Snippet (Safety Valve) for mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property><name>mapreduce.task.profile<\/name><value>true<\/value><\/property>\n<property><name>mapreduce.task.profile.maps<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.reduces<\/name><value>0-5<\/value><\/property>\n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> 6. Save changes. 7. Deploy client configuration by clicking the deploy glyph ( Actions 8. Cloudera Manager will specify a restart which is not necessary to effect these changes. (click Restart Stale Services Monitor the situation and you should see in Unravel UI a Resource Usage tab showing you mappers and reducers when you view the Application page for any completed MRjob. Restart is important for MR sensor to be picked up by queries submitted via Hiveserver2. 6. (Optional) Advanced Configuration Configuration for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Integrating LDAP Authentication for Unravel Web UI Troubleshooting \n \n \n \n Symptom \n \n Problem \n \n Remedy \n \n \n hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ shows directory does not exist \n Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed \n Install Unravel RPM on Unravel service host: \n sudo rpm -U unravel*.rpm* \n OR Verify that unravel \/user\/unravel\/ \n \n \n ClassNotFound com.unraveldata.dataflow.hive.hook.HivePreHook during Hive query execution \n Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ \n Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. \n OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST \n cd \/usr\/local\/unravel\/hive-hook\/; \n cp unravel-hive-HIVE_VER*hook.jar JAR_DEST \n \n Permission denied writing to \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR during Hive query execution \n \n hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR\/* \n \n hadoop fs -chmod 777 \/user\/unravel\/HOOK_RESULT_DIR\/* \n OR Sentry command is needed to give permission \n OR revert to your previous hive-site.xml References \n {+} http:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/cm_mc_hive_udf.html#concept_nc3_mms_lr_unique_2+ " }, 
{ "title" : "Step 3 (Optional): Enable Impala APM", 
"url" : "unravel-4-2/install/install-cdh/install-cdh-part3-impala.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 3 (Optional): Enable Impala APM", 
"snippet" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM Workflow Summary If you want Unravel Server to...", 
"body" : "Introduction This topic explains how to configure Unravel Server to retrieve Impala query data from either ClouderaManager (CM) or Impala daemons ( impalad Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM Workflow Summary If you want Unravel Server to retrieve Impala query data from ClouderaManager, start at Using CM as the Data Source. If you want Unravel Server to retrieve Impala query data from Impala daemons, start at Using Impalad as the Data Source Using CM as the Data Source Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.cloudera.manager.url CM internal URL. Must start with http:\/\/ com.unraveldata.cloudera.manager.port (Optional) CM port number. You only need to specify this if your ClouderaManager is not on port 7180. com.unraveldata.cloudera.manager.username CM username com.unraveldata.cloudera.manager.password CM password com.unraveldata.cloudera.manager.cluster.name.list A comma-separated list of cluster names com.unraveldata.cloudera.manager.version (Optional) CM version. You only need to specify this if your ClouderaManager is earlier than 5.8.0. Note For example, com.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret\ncom.unraveldata.cloudera.manager.username=mycmname\ncom.unraveldata.cloudera.manager.password=mycmpassword\ncom.unraveldata.cloudera.manager.cluster.name.list=cluster1,cluster2,cluster5\ncom.unraveldata.cloudera.manager.port=9997\ncom.unraveldata.cloudera.manager.version=5_7_0 If the Impala service name is not defaulted as \" impala unravel.properties: com.unraveldata.cloudera.manager.service.impala.name=myimpalaservicename Hints: To find out the cluster name: {cm_url}:{cm_port}\/api\/v13\/clusters\/ In the JSON, use the value of \"name\" in the \"items\" list as the cluster name. To find out the service name: {cm_url}:{cm_port}\/api\/v13\/clusters\/{cluster_name}\/services\/\n\n Substitute your particular values for bracketed ClouderaManager (CM) properties, i.e., {cm_port} Using Impalad as the Data Source Use this option if you want to import data from impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html UPDATE_NEEDED_LINK TO https:\/\/docs.google.com\/document\/d\/1KnkIrQ_lCTUU6dtvXbDETUWnIlx-DGWRDTjm8fnmu_c\/edit?ts=591dd404#heading=h.h6gaqdg1wspm Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to unravel.properties Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port For example, com.unraveldata.data.source=impalad\ncom.unraveldata.impalad.nodes=IP:port,IP:port,IP:port By default, the ImpalaSensor unravel.properties com.unraveldata.sensor.tasks.disabled=iw Change the Impala Lookback Window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us References http:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "unravel-4-2/install/install-cdh-4902.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "unravel-4-2/install/install-cdh-4902/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If it is not possible for email to be sent directly to unraveldata.com (blocked SMTP port or unapproved recipient domain), then pick Download Diagnostics and email the produced gz file to support people at UnravelData.com. " }, 
{ "title" : "Unravel HDInsight App Support", 
"url" : "unravel-4-2/install/install-cdh-4902/unravel-hdinsight-app-support.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Unravel HDInsight App Support", 
"snippet" : "For any technical issues in launching the Unravel HDInsight app or using it, please feel free to contact us at azuresupport@unraveldata.com Unravel app Installation Guide Unravel app User Guide For Licensing and guided PoC, please contact our sales team at sales@unraveldata.com Unraveldata Main numb...", 
"body" : "For any technical issues in launching the Unravel HDInsight app or using it, please feel free to contact us at azuresupport@unraveldata.com Unravel app Installation Guide Unravel app User Guide For Licensing and guided PoC, please contact our sales team at sales@unraveldata.com Unraveldata Main number: (650) 741-3442 " }, 
{ "title" : "Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"url" : "unravel-4-2/install/install-cdh-4834.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM)", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/install/install-cdh-4834/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"url" : "unravel-4-2/install/install-cdh-4834/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"snippet" : "Note Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. Introduction This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects informa...", 
"body" : " Note Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. Introduction This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already running), do a \"setup\" procedure: follow the steps 3 to 5 in Hive Bootstrap and Unravel Hive Hook Sensor Setup. HIGHLIGHTED UNRAVEL_HOST_IP UNRAVEL_Hostname_FQDN_Internal_IP Hive Bootstrap and Unravel Hive Hook Sensor Setup: Add a one-time Unravel Hive Bootstrap into Qubole's control panel on the left-hand side in the \"Hive Bootstrap\" section. location_in_s3_where_unravel_jar_folder add jar s3n:\/\/{location_in_s3_where_unravel_jar_folder}\/unravel-hive-0.13.0-hook.jar;\n\nset com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR;\nset hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook;\nset hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.HivePreHook;\nset hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.HivePostHook;\nset hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.HiveFailHook;\nset com.unraveldata.host={UNRAVEL_Hostname_FQDN_Internal_IP};\nset com.unraveldata.hive.hook.tcp=true; Determine the Hive version that Qubole uses, and use that value for HIVE_VERSION__X.Y.Z - target Hive version (e.g. 1.2.0) To determine the Hive version Qubole uses, see http:\/\/docs.qubole.com\/en\/latest\/faqs\/hive\/version-hive-qubole-provide.html HIVE_VERSION_ _X.Y.Z 1.2.0 0.13.0 On the master node of the Qubole Hadoop2\/Hive cluster, check that Unravel Server is reachable. Note Ensure the security group on the master\/slave allows TCP port #3000 and #4043 accessible. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt If the version information is not visible, adjust security groups and routing and try again. SSH Use curl # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_setup.sh > unravel_qubole_setup.sh Ensure wget Installation Directories Hive hook jar - \/usr\/local\/unravel_client Spark jar - \/usr\/local\/unravel-spark Unravel ES - \/usr\/local\/unravel_es On the master node, \/etc\/init.d\/unravel_es # yum install -y wget\n# chmod 755 unravel_qubole_setup.sh \n# sudo .\/unravel_qubole_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 Verify if the setup works in Qubole by invoking Analyze and execute following Hive test query: set hive.on.master=true ;\nselect count(*) from default_qubole_memetracker; " }, 
{ "title" : "Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"url" : "unravel-4-2/install/install-cdh-4834/install-emr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"snippet" : "Introduction Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 4.2+. Highlighted text indicates where you must substitute ...", 
"body" : "Introduction Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 4.2+. Highlighted text indicates where you must substitute your particular values. \n HIGHLIGHTED \n UNRAVEL_HOST_IP Unravel Spark Instrumentation Into NEW Qubole Spark Cluster Configure Unravel s3 read-only credentials into the \/usr\/local\/unravel\/etc\/s3ro.properties Following is an example of s3ro.properties [default]\naws_access_key_id = {ACCESS_KEY_VALUE1}\naws_secret_access_key = {SECRET_KEY_VALUE1}\n\n[profile_name_2]\naws_access_key_id = {ACCESS_KEY_VALUE2}\naws_secret_access_key = {SECRET_KEY_VALUE2} Edit \/usr\/local\/unravel\/etc\/unravel.properties s3ro.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties\ncom.unraveldata.spark.s3.profilesToBuckets=<default>:<s3ro_bucket1>,<profile_name_2>:<s3r0_bucket2> Restart the Unravel ETL daemon(s. # sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Ensure ports 3000 4043 not Verify that port 3000 curl If the version information is not visible (request timeout), then adjust security groups, firewalls, etc. and try again. For VPCs, it might be necessary to add a route. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt Copy the Unravel Spark bootstrap file from Unravel Server to your Spark Qubole cluster, using curl # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/unravel_qubole_bootstrap.sh > unravel_qubole_bootstrap.sh Copy Unravel Spark bootstrap file to your s3:\/\/ Bootstrap_location_for_Spark Qubole cluster # aws s3 cp unravel_qubole_bootstrap.sh s3:\/\/{Bootstrap_location_for_Spark Qubole cluster}\/unravel_qubole_bootstrap.sh In Qubole’s Edit Cluster Setting \n do not unravel_qubole_bootstrap.sh In Qubole, scripts do not take input parameters. Therefore, Unravel bootstrap script takes all the required parameters from the Hadoop configuration. You can customize your Hadoop configuration through specific parameters within the Override Hadoop Configuration Variables unravel-boostrap Note: Separate parameters with commas. Add these settings to unravel-bootstrap unravel-bootstrap=UNRAVEL_SERVER=UNRAVEL_HOST_AND_PORT, SPARK_VER_XYZ={SPARK_VERSION_X.Y.Z} [,SPARK_APP_LOAD_MODE={APP_LOAD_MODE},WRAPPED_SCRIPT={SCRIPT}] where: \n SPARK_VERSION_ The parameters in square brackets [ ] above are optional. \n APP_LOAD_MODE Application Loading Modes \n SCRIPT unravel_qubole_bootstrap.sh. Check the bootstrap log The bootstrap log will contain detailed information about the setup progress and also any possible failures. The log can be found on each cluster node: \/media\/ephemeral0\/unravel\/tmp\/unravel_qubole_bootstrap.sh.out Confirm that the unravel_es Open an SSH Check that the unravel_es # ps -aux | grep unravel_es (Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 Obtain following essential resource metrics sensor file from Unravel Server. # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip Unzip the archive unravel-agent-pack-bin.zip PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS Obtain jar files and snippet script that should be added to the bootstrap action to start the unravel-es # wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unraveldata-clients\/snippets\/run-es.sh\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-emr-sensor.jar\n Ensure that unravel-emr-sensor.jar unravel-emr-sensor.jar PATH_TO_SENSOR_JARS Edit the run-es.sh UNRAVEL_HOST={UNRAVEL_HOST_IP}\nUNRAVEL_EMR_SENSOR_JAR={PATH_TO_SENSOR_JARS} Spark configuration can be provided in theQubole consoleat bootstrap or directly inside spark-defaults.conf spark.unravel.server.hostport {UNRAVEL_HOST_IP}:4043\nspark.driver.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions -javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y} Edit zeppelin-env.sh ZEPPELIN_JAVA_OPTS export ZEPPELIN_JAVA_OPTS=\"-javaagent:{PATH_TO_SENSOR_JARS}\/btrace-agent.jar=bootClassPath=config=driver,libs=spark-SPARK_VERSION\" The Zeppelin configuration is located at \/usr\/lib\/zeppelin\/conf\/zeppelin-env.sh Application Loading Modes for Spark Applications: OPS, DEV, and BATCH There are three modes in which Spark applications can be loaded into Unravel Web UI: OPTS - shows applications in the UI after the application is done. DEV - shows applications in the UI as soon as the first job of the Spark application completes. BATCH - loads applications for which the sensor was not enabled at the time the application has been run. You can specify the application load mode for the bootstrap script by setting SPARK_APP_LOAD_MODE OPS DEV SPARK_APP_LOAD_MODE OPS When to Use Which Mode Unravel recommends using OPS mode as the cluster-side for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false DEV mode is useful during Spark application development. This mode shows a Spark application as soon as the first Spark job of the application finishes. For long running applications, this functionality is useful, as the application is shown in the UI while the application is running. In addition, DEV mode shows applications even when the Spark event log is not being persisted to HDFS or S3. This is advantageous in situations like Spark on Mesos. In a Qubole cluster that is using OPS mode as the default, DEV mode can be obtained for individual Spark applications by overriding spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false BATCH mode is always on and does not interfere with application performance in any way since the loading is entirely outside the application execution path. For details see the next section. Loading Applications in Batch Mode In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is helpful to load all of the applications that have been run in the past, before Unravel Sensor was installed. To enable the batch mode, add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.eventlog.location=hdfs:\/\/NAMENODE_IP_PORT\/user\/spark\/applicationHistory\/ \n Note: Currently, only one event log location is supported. " }, 
{ "title" : "Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"url" : "unravel-4-2/install/install-cdh-4834/step-4--modify-amazon-emr-cluster-bootstrap-setup.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Cloudera Distribution Including Apache Hadoop (CDH), with Cloudera Manager (CM) \/ Step 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"snippet" : "Follow these steps only Introduction This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server. Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amaz...", 
"body" : " Follow these steps only Introduction This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server. \n Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amazon EMR cluster(s). \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients 2. Get the Bootstrap Script(s) To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below policy into the Policy Document: {\n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n { \n \"Sid\": \"getunraveldataclients3files\", \n \"Effect\": \"Allow\", \n \"Action\": [ \n \"s3:ListBucket\", \n \"s3:Get*\" \n ], \n \"Resource\": [ \n \"arn:aws:s3:::unraveldata-clients\/*\" \n ] \n } \n ]\n} Save it by clicking Apply Policy When you create a new Amazon EMR cluster, be sure to add a bootstrap action as shown in the IAM screenshot below. You need to copy and paste the full pathname of the bootstrap action (script) into the Script location Important Note: Do not For guidance on which script to use, see the table below. \n \n \n File \n S3 Bucket \n Local Directory \n Applies To \n \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive \n \n unravel_emr5_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n install_bin\/unraveldata-clients\/ \n Amazon EMR 5.x Hive 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s) Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Follow the steps below to set up an existing cluster. Identify the UNRAVEL_HOST_IP Download s3:\/\/unraveldata-clients\/unravel_emr_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_setup.sh \/tmp hadoop Open an SSH session to the cluster's master node (ssh as user hadoop hadoop # cd \/tmp \n# aws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \n# chmod +x unravel_emr_setup.sh \n# .\/unravel_emr_setup.sh --unravel-server {UNRAVEL_HOST_IP}:3000 To uninstall Hive instrumentation on an Amazon EMR cluster (perhaps because you want to upgrade the instrumentation), you simply run the same install script again with the uninstall # cd \/tmp \n# aws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \n# chmod +x unravel_emr_setup.sh \n# .\/unravel_emr_setup.sh --uninstall Transient Amazon EMR Clusters Hive Applications: This is similar to the previous section on integrating an existing cluster except the script used as a bootstrap step is one of the following files: \n \n \n \n File \n \n S3 Bucket \n \n Local Directory \n \n Applies To \n \n unravel_emr_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 3.x Hive \n \n unravel_emr4_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 4.x Hive \n \n unravel_emr5_bootstrap.sh \n s3:\/\/unraveldata-clients\/ \n \n install_bin\/unraveldata-clients\/ \n Amazon EMR 5.x Hive Spark Applications: The Unravel Server does not load data from a Spark cluster until the cluster is instrumented. Follow the steps below to set up an existing cluster Identify the UNRAVEL_HOST_IP Download s3:\/\/unraveldata-clients\/unravel_emr_spark_setup.sh aws s3 install_bin\/unraveldata-clients scp unravel_emr_spark_setup.sh \/tmp ec2-user Open an ssh ssh user ec2-user ec2- user # cd \/tmp \n# sudo .\/unravel_emr_spark_setup.sh --unravel-server {UNRAVEL_HOST_IP}:3000 --client To use the Spark driver in cluster mode substitute \"– cluster client " }, 
{ "title" : "Hortonworks Data Platform (HDP)", 
"url" : "unravel-4-2/install/install-hdp.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP)", 
"snippet" : "This guide is compatible with: On-premises HDP up to v2.6...", 
"body" : " This guide is compatible with: On-premises HDP up to v2.6 " }, 
{ "title" : "Ordered Steps", 
"url" : "unravel-4-2/install/install-hdp.html#UUID-9add7f98-e1ed-a220-6f23-1128ae38399c_id_HortonworksDataPlatformHDP-OrderedSteps", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Ordered Steps", 
"snippet" : "Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : " Step 1: Install Unravel Server on HDP Step 2A: (For Non Hive-on-Tez Setups) Enable Additional Data Collection \/ Instrumentation for HDP Step 2B: (For Hive-on-Tez Only) Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "unravel-4-2/install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host mach...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.0 on HDP. These instructions apply to Unravel Server 4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel Web UI. (Optional) Configure Unravel Server (advanced options). Pre-Installation Check The following installation requirements must be met for successful installation of Unravel. Platform Compatibility On-premises HDP up to v2.6 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config. HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway. LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication. (Open signup by default.) On Unravel Edge-node server, please do not Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN’s “done dir” in HDFS YARN’s log aggregation directory in HDFS Spark event log directory in HDFS File sizes under HIve warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel Web UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. 2. Install the Unravel Server RPM on the Host Machine The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks as needed to be more selective. Get the Unravel Server RPM See instructions UPDATE_NEEDED_ ADD LINK TO RPM. Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. 3. Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable optional daemons at this point. See Creating Multiple Workers for High Volume Data Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel Web UI's configuration wizard. com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel Web UI's configuration wizard. com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL yarn.resourcemanager.webapp.address=http:\/\/example.localdomain :8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: Create a keytab for unravel for daemons that run as unravel \/etc\/security\/unravel.keytab Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/security\/keytabs\/hdfs.keytab Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): # echo \" \\\nexport HDFS_KEYTAB_PATH=\/etc\/security\/keytabs\/hdfs.keytab \\\nexport HDFS_KERBEROS_PRINCIPAL=unravel\/myhost.mydomain@MYREALM \\\n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh Add properties for Kerberos: # echo \" \\\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM \\ \ncom.unraveldata.kerberos.keytab.path=\/etc\/security\/unravel.keytab \\\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n If Ranger is Enabled: Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/spark-history hdfs read Spark event log hdfs:\/\/spark-history hdfs read Spark2 event log hdfs:\/\/mr-history\/done hdfs read MapReduce logs hdfs:\/\/app-logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information 4. Convert Your Unravel Installation to HDP Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" This completes the basic\/core configuration. 5. Log into Unravel Web UI Using a web browser, navigate to http:\/\/({UNRAVEL_HOST_IP} -f):3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Unravel Web UI displays collected data. For instructions on using Unravel Web UI, see the User Guide 6. (Optional) Configure Unravel Server (Advanced Options) Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2 (Optional): Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel Web UI Configuration Wizard Run the Unravel Web UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/install/install-hdp/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/install/install-hdp/empty-or-missing-topic-4829.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Hortonworks Data Platform (HDP) \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "MapR", 
"url" : "unravel-4-2/install/install-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ MapR", 
"snippet" : "This guide is compatible with MapR 5.1, 5.2 Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR install-mapr...", 
"body" : " This guide is compatible with MapR 5.1, 5.2 Step 1: Install Unravel Server on MapR Step 2: Enable Additional Data Collection \/ Instrumentation for MapR Upgrade Unravel Sensor on MapR install-mapr " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/install/install-mapr/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-2/install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unra...", 
"body" : "Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. 1. Enable Additional Instrumentation on Unravel Server's Host Substitute valid values for: UNRAVEL_HOST_IP SPARK_VERSION_ HIVE_VERSION_ Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client nodes that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Before unravel_mapr_setup.sh wget unzip hive spark-shell Run the shell script unravel_mapr_setup.sh host1 # sudo \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Hive hook jar is installed under: \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2) are made to: \/opt\/mapr\/spark\/spark-2.0.1\/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-env.sh Once the files are present on edge host where Unravel rpm is installed, you can tar these changes\/additions up and put on other hosts, if that is more convenient than running the script. In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). 2. Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server: someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 3. Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> 4. Enable Additional Instrumentation on Other Hosts in the Cluster To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml Run the shell script unravel_mapr_setup.sh Copy the newly edited (in the previous step 4) yarn-site.xml to all nodes. Do a rolling-restart of HiveServer2 " }, 
{ "title" : "Upgrade Unravel Sensor on MapR", 
"url" : "unravel-4-2/install/install-mapr/upgrade-unravel-sensor-on-mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ MapR \/ Upgrade Unravel Sensor on MapR", 
"snippet" : "HIGHLIGHTED UNRAVEL_HOST_IP SPARK_VERSION _X.Y.Z HIVE_VERSION_X.Y.Z Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/ # sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unrav...", 
"body" : " HIGHLIGHTED UNRAVEL_HOST_IP SPARK_VERSION _X.Y.Z HIVE_VERSION_X.Y.Z Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent \/usr\/local\/unravel_client tar scp # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Unravel for Azure HDInsight clusters", 
"url" : "unravel-4-2/install/install-hdi.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters", 
"snippet" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster. Unravel VM vs Unravel...", 
"body" : "Unravel's support on Azure HDInsight cluster comes with two installation options: Option 1: Unravel VM – Install Unravel as a separate Azure VM and monitor multiple HDInsight clusters. Option 2: Unravel App – Install Unravel app during or after creation of an HDInsight cluster. Unravel VM vs Unravel App: To monitor multiple HDInsight clusters on the same virtual network, Unravel VM is a preferable way. On the other hand, to quickly try out Unravel for your on demand HDInsight cluster, you can add the Unravel app during the cluster creation or add the app anytime after a cluster is created. Unravel app resides on the edge node of the target HDInsight cluster, and it can monitor one cluster's activities. " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "unravel-4-2/install/install-hdi.html#UUID-55285332-3fb4-39e8-501c-f479800d98ee_id_UnravelforAzureHDInsightclusters-InstallationGuideforUnravelVM", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Templat...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions Page: Step 6: Updating Unravel Installation " }, 
{ "title" : "Installation Guide for Unravel App", 
"url" : "unravel-4-2/install/install-hdi.html#UUID-55285332-3fb4-39e8-501c-f479800d98ee_id_UnravelforAzureHDInsightclusters-InstallationGuideforUnravelApp", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel App", 
"snippet" : "Page: Install Unravel HDInsight app...", 
"body" : " Page: Install Unravel HDInsight app " }, 
{ "title" : "Install Unravel HDInsight app", 
"url" : "unravel-4-2/install/install-hdi/install-unravel-hdinsight-app.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Install Unravel HDInsight app", 
"snippet" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to inst...", 
"body" : "Unraveldata recently published its HDInsight application on Azure Market Place. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight cluster running on either blob (wasb) or adl (Azure Data Lake) storage Support on other HDInsight clusters will be coming soon. How to to install Unravel HDInsight app on a fresh Spark 2.1 cluster 1. Launch a Spark 2.1 Cluster Login to the Azure port. Click or choose HDInsight service. Create a new cluster; choosing Spark as the cluster type version Next. 2. Setup storage account configuration for the cluster Either create a new storage account or use existing one. Fill in the storage account information for the spark cluster. 3. Find Unravel app on search box or available app listing Enter UNRAVEL Click OK Create After you accepted the \"Terms of Use\" and \"Privacy Policy\" click Next 4. Review the summary and click Create to launch the cluster + Unravel app. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. 5. Access to Unravel app user interface After Unravel app and the spark2 cluster is successfully launched, go to HDinsight service, look for the spark2 cluster, and click on it. Click on Application In most cases, the Unravel HDInsight app user interface is in the following format https:\/\/<clusterName>-unr.apps.azurehdinsight.net\/ 6. Login to the Unravel app Start your browser and navigate to the Unravel app webpage URL https:\/\/clusterName-apps.azurehdinsight.net. The default admin login credential is admin unraveldata. Step 7. Unravel Dashboard When logging into Unravel, you will see the Dashboard. See the User Guide Step 8. Unravel daemons to the Unravel edge node, and check the unravel daemons' process status. ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Step 9. Licensing and support By default Unravel app doesn't contains any license keys, and runs without any issue during the inital 30 days trial period. To continue using Unravel app and technical support, contact our sales. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Step 10. Getting started to use Unravel Please read the Unravel User Guide Getting Started User Guide Step 11. Unravel API (special note for Unravel app) Unravel provides REST api to perform some operations. To try the api, click on the API tab on the dashboard An API page with available api command options are displayed and explained. You can try the API by clicking \"Try it out\" → Execute buttons; it will display the corresponding curl From the Unravel user interface, trying out the api will always has \"TypeError: Failed to fetch\". Because the generated curl command is not using https. Copy the generated curl commands and modify it to include default user credential and using https protocol. ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n The api output will be in JSON format shown below and the long numeric string displayed is the epoch time {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "Installation Guide for Unravel VM", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM", 
"snippet" : "Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Templat...", 
"body" : " Page: Step 1: Install Unravel Server for Azure HDinsight Cluster Page: Step 2: Use Script Action to configure HDinsight cluster for Unravel Page: Step 3: Create Unravel VM using ARM template Page: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions Page: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions Page: Step 6: Updating Unravel Installation " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/install-hdi-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 1: Install Unravel Server for Azure HDinsight Cluster", 
"snippet" : "Introduction This topic explains how to deploy Unravel Server 4.2.X on Azure HDInsight Cluster. Workflow Summary Setup Unravel VM . Install the Unravel Server RPM on the VM. Start Unravel daemons Log into Unravel Web UI Requirements Checklist Platform Compatibility Azure HDinsight 3.3 - 3.6 Hadoop 1...", 
"body" : "Introduction This topic explains how to deploy Unravel Server 4.2.X on Azure HDInsight Cluster. \n Workflow Summary Setup Unravel VM . Install the Unravel Server RPM on the VM. Start Unravel daemons Log into Unravel Web UI Requirements Checklist Platform Compatibility Azure HDinsight 3.3 - 3.6 Hadoop 1.x - 2.x Kerberos (Windows AD) Hive 1.2 Spark 1.6, 2.0, 2.1 Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed \n SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH Open signup or LDAP for Unravel Web UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum OS Disk: \/ Data Disk: \/srv Network Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) HDFS ports open from Hadoop cluster to Unravel Server(s) Hive MetaStore DB port open to Unravel Server(s) for partition reporting For Oozie, port 11000 open to Unravel Server(s) \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Setup Unravel VM Setup Unravel VM on the same VNET and subnet of the target HDInsight cluster Provision a VM. VM size: Standard_E8s_v3 OS: centos7.3 or RHEL 7.4 image Publisher: OpenLogic Setup the VM on the same VNET and Subnet of the HDInsight cluster Start ntpd Security Group Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the head node of each HDInsight cluster to Unravel Server. Add inbound security policy allow ssh and 443 access to the unravel node. The default security policy already allow all access within the VNET. Default rule start with 6500. Configure the environment at first login. Disable selinux # sudo setenforce Permissive\n Edit the file to make sure the setting persists after reboot, be sure SELINUX=permissive. # vi \/etc\/selinux\/config\n Install libaio.x86_64. # sudo yum -y install libaio.x86_64 Install lzop. # sudo yum install lzop.x86_64 Disable the local Firewall. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L 2. Install the Unravel Server RPM on the VM The precise RPM filename will vary. The version has the structure \n x.y.b \n b \n x.y Replace the asterisks as needed to be more selective. Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM See instructions UPDATE_NEEDED_ADD LINK TO RPM. Install the Unravel Server RPM. # sudo rpm -U unravel-4.2.7-Azure-latest.rpm The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password is stored in \/root\/unravel.install.include Grant Access to Unravel Server. By Default Public IP should be assigned to the Unravel VM node. Create a security policy that allows ssh Use of either sshkey password ssh Restriction Do not make Unravel Server UI accessible on the public Internet because doing so would violate your licensing terms. 3. Start Unravel daemons and log into Unravel UI The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.ext.sh unravel.properties Open an SSH Session to Unravel Server Replace somefile.pem # ssh -i {somefile.pem} sshuser@$UNRAVEL_IP Set Correct Permissions on the Unravel Configuration Directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh (Required) # echo \"\nexport CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/* \\\n\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Modify unravel.properties. (Optional) The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust other values in unravel.properties \n \n \n Property \n Description \n Required by HDinsight cluster \n Example Values \n \n \n com.unraveldata.advertised.url \n Defines the Unravel Server URL for HTTP traffic. \n \n http:\/\/LAN_DNS:3000 \n \n \n com.unraveldata.customer.organization \n Identifies your installation for reporting purposes. \n \n Company_and_org \n \n \n com.unraveldata.tmpdir \n Location where Unravel's temp file will reside \n \n srv\/unravel\/tmp \n \n \n com.unraveldata.history.maxSize.weeks \n Sets retention for search data. \n \n 26 \n \n \n com.unraveldata.login.admins \n Unravel UI admin \n \n com.unraveldata.login.admins=admin \n \n \n com.unraveldata.hdinsight.storage-account-name-1 \n Optional for Spark when HDinsight using blob storage storage account name for the HDinsight cluster \n using blob \n \n fs.azure.account.key. STORAGEACCOUNT \n \n \n com.unraveldat \n Primary storage account key \n using blob \n Ondaq2aYMpJf8pCdvtFJ\/zARJLhFr4Vf94PPJvMP1EsoFzBKp \n \n \n com.unraveldat a.hdinsight \n Optional for Spark when HDinsight using blob storage Storage account name for the HDinsight cluster (same as account-name-1 \n using blob \n \n fs.azure.account.key. STORAGEACCOUNT \n \n \n com.unraveldat a.hdinsight \n Secondary storage account key \n using blob \n aL3MFZ\/5hP4k0LxA+tn5\/NM6EkM1AZkFZzKmWjgEMqe0o6F \n \n \n com.unraveldat a.adl.accountFQDN \n The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net \n using Data Lake \n \n datalake0001.azuredatalakestore.net \n \n \n com.unraveldat a.adl.clientld \n An application ID. An application registration has to be created in the Azure Active Directory \n using Data Lake \n 5d19877f-3eb5-413a-9a41-7ae8a0048cfk \n \n \n com.unraveldat a.adl.clientKey \n An application access key which can be created after registering an application \n using Data Lake \n 6FMzo61+cKIRPFZRxzUxiLSuWc5YEsdZzYbtU5rMyUg= \n \n \n com.unraveldat a.adl.accessTokenEndpoint \n The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal \n using Data Lake \n \n https:\/\/login.microsoftonline.com\/bc745a0d-f282-4e99-b95f-1ecb477a209g\/oauth2\/token \n \n \n com.unraveldat a.adl.clientRootPath \n It is the path in the Data lake store where the target cluster has been given access. \n using Data Lake \n \/clusters\/CLUSTERNAME \n \n \n com.unraveldat a.ext.kafka.clusters \n Name of kafka cluster. The display name show on the Unravel UI to define kafka cluster. Other Unravel kafka properties depends on this name CLUSTERNAME \n \n kafka \n udkafka \n \n \n com.unraveldat a.ext.kafka.CLUSTERNAME.bootstrap_servers \n Kafka cluster bootstrap server and port (usually are two worker nodes) \n \n kafka \n wn0-UDKAFK:9092,wn1-UDKAFK:9092 \n \n \n com.unraveldat a.ext.kafka. \n Define kafka cluster broker servers names \n \n kafka \n broker-1,broker-2,broker-3 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 host \n \n kafka \n wn0-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-1 port \n \n kafka \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 host \n \n kafka \n wn1-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-2 port \n \n kafka \n 9999 \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 host \n \n kafka \n wn2-UDKAFK \n \n \n com.unraveldat a.ext.kafka. \n Define jmx broker-3 port \n \n kafka \n 9999 For HDinsight cluster using blob storage Please find and append the following properties in the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.unravelstorage03.blob.core.windows.net\ncom.unraveldata.hdinsight.primary-access-key=Ondaq2aYMpJf8pCdvtFJ\/zAR1LhFr4Vf94PPJvMP1DsoFzBKp\/\/4DVQi+hcL5+XsW2XFNI7p\ncom.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.unravelstorage03.blob.core.windows.net\ncom.unraveldata.hdinsight.secondary-access-key=aL3MFZ\/5hP4k0LxA+tn5\/NM6EkM1AZkFZzCmWjgEMqe0o6F33gJZxwfQABLaynxpatWY71 You will need to update the above blob Storage account name and access key For HDinsight cluster using Data Lake storage Please find and append the following properties in the \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.adl.accountFQDN=datalake0001.azuredatalakestore.net\ncom.unraveldata.adl.clientId=5d19877f-3eb5-413k-9a41-7ae8a0048cfa\ncom.unraveldata.adl.clientKey=5FMzo61+cKIRPFZRxzUxiLSuWc5YEsdZzYbtU5r\ncom.unraveldata.adl.accessTokenEndpoint=https:\/\/login.microsoftonline.com\/bc745a0d-f282-4e99-b95f-1ecb477a209e\/oauth2\/token\ncom.unraveldata.adl.clientRootPath=\/clusters\/{CLUSTERNAME} You will need to update the above data like account name, clientId, clientKey, accessTokenEndpoint and clientRootPath Find the Data Lake storage account from HDinsight cluste Properties Find the Data Late clientRootPath \n HDinsight cluste Properties | Storage Accounts Find the user (Azure principal) that has access to the Data Lake storage path or folder from Azure portal. \n HDinsight cluste Properties | Storage Accounts Data Lake Data Explorer \n \n Access. Finding the clientId, clientKey, and accessTokenEndpoint On Azure portal → click Azure Active Directory | App Registrations Click the account principal and look for the Application ID and that is the client ID. Click the Keys key name expiration date client key SAVE Go back to App Registration screen and click Endpoints OAUTH 2.0 TOKEN ENDPOINT. \n \n Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" 4. Log into Unravel Web UI. Create a SSH # ssh -i somefile.pem sshuser@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin\" \"unraveldata For the free trial version, use the Chrome web browser. \n Unravel Web UI Login Screen Congratulations! Unravel Server is up and running. Please proceed to Step2 For instructions on using Unravel Web UI, see the User Guide " }, 
{ "title" : "Step 2: Use Script Action to configure HDinsight cluster for Unravel", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/install-hdi-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 2: Use Script Action to configure HDinsight cluster for Unravel", 
"snippet" : "1. Create a new Spark HDInsight cluster with Unravel's Script Action script. Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access, download the following files into your private blob store...", 
"body" : "1. Create a new Spark HDInsight cluster with Unravel's Script Action script. Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access, download the following files into your private blob store that are accessible by your secure HDInsight cluster. the unravel_hdi_spark_bootstrap_3.0.sh \n http:\/\/central.maven.org\/maven2\/org\/anarres\/lzo\/lzo-core\/1.0.5\/lzo-core-1.0.5.jar You use the Azure Portal to create a Spark 2.1 HDInsight Cluster. In Summary Confirm configuration On the Advanced settings When entering Virtual Network settings you must enter the VNET and Subnet where Unravel VM is located. Defining Script Actions On the Advanced setting screen click on the arrow to the right of script actions to bring up the Submit script action screen: Select Script type Name The Bash script URI uses the github URL https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh The Unravel script for Script Action is located on the Unravel VM \/usr\/local\/unravel\/webapps\/ROOT\/hh\/unraveldata-clients\/unravel_hdi_bootstrap.sh above instruction use github to store the Unravel script; and Azure script action script can also be uploaded to Azure blob storage with https URI Check the boxes “Head”, “Worker” and \"Edge\" ; so this custom script action will apply on both Head and worker nodes. In the parameter field enter the string \" --unravel-server\" follow by unravel node’s private IP address:port and spark or hive version ( see below); then click Create \n --unravel-server 10.10.1.10:3000 --spark-version 2.1.0 Azure portal will validate the script; if the script is validated you will be returned to you initial screen. validations it returns to the same screen and click Select. Now return back to Step 5 and Script Action is shown Configured; then click Next Other supported script arguments are --spark-version and --hive-version accepting the X.Y.Z format of the system version - eg. --spark-version 2.1.0 --hive-version 1.2.0 Summary configuration is displayed on Step 6; click Create The HDInsight cluster is shown on Azure dashboard with the status of Deploying. The Cluster creation process takes approximately 20-25 minutes. 2. Checking Script Action process Once the HDInsight cluster creation completed: check the cluster information from Azure dashboard Find out the Ambari URL and ssh access to first head node. And inspect script action if running success or not. Ambari URL = https:\/\/<Cluster_Name>. azurehdinsight.net ssh access to first head node = ssh sshuser@<Cluster_Name>- ssh.azurehdinsight.net Check Script Actions, by clicking the “Script actions” icon on the above window; and should be seeing the Script Action run “Succeeded” Login to Ambari UI and check the Ambari tasks; and there should be three main tasks and corresponding log files Customscript Action task Unravel call to stop Spark service Unravel call to start Spark service 3. Run Unravel script action on existing HDInsight cluster Script actions can be invoked after the cluster is up and running. In Azure portal of the cluster page select “Script actions”, “Submit new”. Enter the script action details. After the settings have been entered, and the settings saved the action will be run on the cluster nodes. Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, just submit a new script action script. Choose Script type: \"- Custom\" Name: Enter a name for this script Bash script URI: https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh If your HDInsight cluster has no public internet access please download the above script action script and upload to the blob storage that the HDInsight cluster has access to it. Node type: For Spark, and Hadoop HDInsight cluster, check \"Head\", \"Worker\" and \"Edge\" node if you have configured with edge node. For Kafka HDInsight cluster, the script action only applies to \"Head\" node. Script action also been applied to HDInsight cluster using Azure 1.0 command line, the following is an example of using Azure 1.0 CLI Install Azure 1.0 CLI use docker container is the quickest way described in this Azure online doc azure hdinsight script-action create estspk2rh74 -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_a.sh -p 'unravel-server 10.10.1.15:3000 --spark-version 2.1.0' -t \"headnode;workernode;edgenode\" The command structure is below azure hdinsight script-action create <CLUSTERNAME> -g <RESOURCEGROUP> -n <NAME> -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_a.sh -g = Resource Group name -n = Name of this script action task -u = script path -p = paramaters -t = node types \n Example screen capture is below The result of running this script action script via Azure 1.0 command line can also be checked from Azure portal Submitting script action for Kafka cluster azure hdinsight script-action create $CLUSTER_NAME -g $RESOURCE_GROUP -n $NAME_FOR_TASK -u $PATH_FOR_KAFKA_SCRIPT_ACTION -p '$UNRAVEL_VM_IP:3000' -t \"headnode\" For no internet access script action for kafka cluster please see this page under the section \"Using Script Actions with No Internet access cluster\" \n Example of screen capture shown below " }, 
{ "title" : "Step 3: Create Unravel VM using ARM template", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/step-3--create-unravel-vm-using-arm-template.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 3: Create Unravel VM using ARM template", 
"snippet" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or cr...", 
"body" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or create your own template files. This example template creates an Azure \"standard E8s V3\" VM in the existing VNET and subnet, and it adds a data disk on the VM for \"\/ srv You need You can change data disk size; it is currently set to 500G. If you change the disk size, update unravel-setup.sh For Centos 7.3 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.parameters.json For Redhat 7.4 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.parameters.json The parameter files have to be modified to fit your Azure environment; you have to fill in the blob storage account, access keys or ADLS account information. See here To download and install the Unravel RPM, you must download the ARM template which is embedded in the Azure Extension script. You can obtain the extension script here: Extension Script for CentOS 7.3 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/unravel-setup.sh Extension Script for CentOS 7.4 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/unravel-setup.sh The custom extension script fixes most of the basic unravel configuration; however, you must manually edit \/usr\/local\/unravel\/etc\/unravel.properties See Step 1 unravel.properties here If Unravel VM is deploying in a closed environment, download the unravel rpm file from UPDATE_NEEDED_ADD LINK TO RPM.4.2.7. unravel-setup.sh Below is the content of this extension script # Download unravel rpm\n\/usr\/bin\/wget http:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.2.7-Azure-latest.rpm\n\n\/usr\/bin\/sleep 5\n\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=1p6ed4s492012j5rb242rq3x3w702z1l455g501z2z4o2o4lo675555u3h\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start Download the ARM template and parameter JSON files into your configured Azure CLI workstation Azure CLI to deploy Unravel VM using this template and parameters json file # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json To Validate template before deployment # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually start unravel daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/step-4--arm-template-for-spark2-hdinsight-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"snippet" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template will also run Unravel's Script Actions script to setup unravel sensors and configuration on header, worker and edge nodes. Th...", 
"body" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template will also run Unravel's Script Actions script to setup unravel sensors and configuration on header, worker and edge nodes. This ARM template uses the existing VNET, Subnet and Storage Account on the same resource group. You will need to update those values in parameter, variables to reflect your Azure environment. A Spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use Spark edge node for accessing the cluster, testing your client applications, and hosting your client applications. You will need to deploy unravel VM and update the script action parameters h--unravel-server UNRAVEL_IP:3000 --spark-version 2.1.0 You can change the VM size of header, worker and edge nodes; and currently they are all using VM size of \"Standard_D3_v2\" \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Unravel script action requires Internet Access to download the script action script and other packages. For secured HDInsight cluster without public Internet access. Please download the no dependency script below into your blob store that accessible by the HDInsight cluster. \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0_nodep.sh After modify this template please validate it before applying and HDInsight cluster creation takes about 15 - 25 minutes Download the ARM template and parameter JSON files into your configured Azure CLI workstation. Validate template before deployment. az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Use the Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Apply Unravel Script Actions scripts manually on an existing Spark2 cluster Optionally, if you already have an existing spark2 cluster, you can apply Unravel's spark2 script via Azure portal. From Azure portal, click the resource of the target spark2 cluster under your resource group and click Script actions On script actions dialog box: Click Submit new Select script type \"- Custom\" Enter a Name for this script e.g. \" unravel-spark-setup. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Input parameters: --unravel-server UNRAVEL_VM_IP_address Check the box Persist this script action to rerun ... Click Create You can upload the unravel_hdi_bootstrap.sh Script Action will validate the script and then process it. Monitor the Azure portal until Scriptactions is completed. Then login to Ambari and check the Ambari task status. The checkbox for \"Persist this script action to rerun when new nodes are added ..\" will only affect new worker nodes when scale up the worker nodes. The Unravel script action script will not automaticallt apply because of checking this check box for newly added edge node from ARM template. Install additional edge node on existing HDinsight cluster ARM template for install edge node with Unravel Script Action only \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json ARM template to install the edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example) In this example, an edge node will be created first. Next, it runs emptynode-setup.sh unravel_hdi_bootstrap.sh \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires change in scriptActionUri path and application name in variables and also parameters for cluster name. Please adjust the ARM templates for your setup and validate it before using. " }, 
{ "title" : "Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/step-5--arm-template-for-kafka-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"snippet" : "Unravel script action requires Internet Access to download the script action No Internet access cluster This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. It runs Unravel's Script Actions script to detect the Kafka cluster bootstrap server...", 
"body" : " Unravel script action requires Internet Access to download the script action No Internet access cluster This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. It runs Unravel's Script Actions script to detect the Kafka cluster bootstrap servers and jmx broker nodes; and update this information into the unravel.properties The Unravel script actions script for Kafka is specifically for Kafka cluster and is needed to run on header node only. This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. The worker nodes in this Kafka cluster uses two data disks per node. You will need to update those values (VNET, Subnet, Storage Account, Cluster name, ...etc ) in parameter, variables to reflect your Azure environment. You need to deploy unravel VM and update the script action parameters UNRAVEL_IP You can change the VM size of header, worker nodes; currently they are all using VM size of \"Standard_D3_v2\" After modifying this template please validate it before applying. The HDInsight cluster creation takes about 15 - 25 minutes Using Script Actions With Internet Access Download the ARM template and parameter JSON files into your configured Azure CLI workstation. To Validate template before deployment az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Use the Azure CLI to deploy Kafka cluster using the template and parameters JSON file. az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json After the Kafka cluster is successfully created, the unravel script actions script should apply the Kafka configuration on Unravel's VM \/user\/local\/unravel\/etc\/unravel.properties The following is the sample of lines appended to unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 And once unravel.properties unravel_km \/etc\/init.d\/unravel_km restart Unravel kafka ScriptAction scripts manually on existing kafka cluster Optionally, if you already have an existing kafka cluster, you can apply Unravel's kafka script via Azure portal. From Azure portal, click the resource of the target Kafka cluster under your resource group and click \"Script actions\" On script actions windows: Click Submit new Select script type, e.g., \"- Custom\" Enter a Name for this script e.g. \"unravel-kafka-setup\" Enter the script path from above e.g. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Input parameters: UNRAVEL_VM_IP_address:3000 Check the box \"Persist this script action to rerun ...\" you can upload the unravel_hdi_kafka_bootstrap.sh Script Action will validate the script and then process it. Monitor the Azure portal Script until it completes. Then login to Ambari of the cluster and check the Ambari task status. Login to unravel VM and restart the unravel Kafka monitor daemon, unravel_km \/etc\/init.d\/unravel_km restart Using Script Actions with No Internet access cluster For Kafka cluster script actions, it requires to get the jq package from azure content at ubuntu.com So for secured kafka cluster, you can access to a public VM (ubuntu) and do # apt-get install -d jq It will download the package of jq and dependency packages libonig2 on \/var\/cashe\/apt\/archives \n tar scp you can also get this jq and libonig2 deb files from unravel public blob store, you need to download it to your workstation then upload to hn0 and hn1 \n https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/jq_and_libonig2.tar Extract the jq and libonig2 files on the \/tmp folder of hn0 and hn1 # cd \/tmp\/\n# tar -xvf jq_and_libonig2.tar\n# sudo dpkg -i libonig2_5.9.6-1_amd64.deb jq_1.5+dfsg-1_amd64.deb Download the new kafka script actions https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap_nodep.sh The above kafka script action Submit the script actions \n Optional step script actions Edit the downloaded unravel_hdo_kafka_bootstrap_nodep.sh. ######################################################################################################\n# #\n# Get the jq from your own blob content #\n# You need to download and jq and libonig2 deb packages from #\n# https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/jq_and_libonig2.tar #\n# update and uncomment the following lines of script for jq install from your blob store #\n# #\n######################################################################################################\n#\n# wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar\n# tar -xvf \/tmp\/jq_and_libonig2.tar -C \/tmp\n# sudo dpkg -i \/tmp\/libonig2_5.9.6-1_amd64.deb\n# sudo dpkg -i \/tmp\/jq_1.5+dfsg-1_amd64.deb\n#\n#######################################################################################################\n# #\n# End of installing jq, uncomment above for jq installation from blob store #\n# #\n####################################################################################################### Uncomment and update the the https path for your jq_and_libonig2.tar wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar Uncomment the four lines below: wget https:\/\/<BLOBSTORE_ACCOUNT>.blob.core.windows.net\/<BLOBSTORE_NAME>\/jq_and_libonig2.tar -O \/tmp\/jq_and_libonig2.tar\ntar -xvf \/tmp\/jq_and_libonig2.tar -C \/tmp\nsudo dpkg -i \/tmp\/libonig2_5.9.6-1_amd64.deb\nsudo dpkg -i \/tmp\/jq_1.5+dfsg-1_amd64.deb You can now submit script actions must jq_and_libonig2.tar Script action can be submitted from Azure portal or using Azure Command Line 1.0 Submitting script action for kafka cluster azure hdinsight script-action create $CLUSTER_NAME -g $RESOURCE_GROUP -n $NAME_FOR_TASK -u $PATH_FOR_KAFKA_SCRIPT_ACTION -p '$UNRAVEL_VM_IP:3000' -t \"headnode\" -g = Resource Group name -n = Name of this script action task -u = script path -p = parameters -t = node types \n Example of screen capture shown below Testing your Kafka configuration You can follow the Azure documentation Download this two jar files from unravel public blob into your HDInsight cluster wget https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/kafka-streaming-1.0-SNAPSHOT.jar\nwget https:\/\/unravelstorage01.blob.core.windows.net\/unravel-app-blob-2018-04-13\/kafka-producer-consumer-1.0-SNAPSHOT.jar Ssh into your kafka cluster headnode and running the following commands to define the variables CLUSTERNAME, KAFKAZKHOSTS, KAFKABROKERS. export CLUSTERNAME=<your_kafka_clustername>\n\nexport KAFKAZKHOSTS=`curl -sS -u admin:<ambari_password> -G https:\/\/$CLUSTERNAME.azurehdinsight.net\/api\/v1\/clusters\/$CLUSTERNAME\/services\/ZOOKEEPER\/components\/ZOOKEEPER_SERVER | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2`\n\nexport KAFKABROKERS=`curl -sS -u admin:<ambari_password> -G https:\/\/$CLUSTERNAME.azurehdinsight.net\/api\/v1\/clusters\/$CLUSTERNAME\/services\/KAFKA\/components\/KAFKA_BROKER | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2` Create the kafka topics by running the following commands on kafka headnode \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic test --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic wordcounts --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic RekeyedIntermediateTopic --zookeeper $KAFKAZKHOSTS\n\/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic wordcount-example-Counts-changelog --zookeeper $KAFKAZKHOSTS Verify created kafka topics \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-topics.sh --list --zookeeper $KAFKAZKHOSTS Start the streaming application as background java -jar kafka-streaming-1.0-SNAPSHOT.jar $KAFKABROKERS $KAFKAZKHOSTS & Send records to test topics java -jar kafka-producer-consumer-1.0-SNAPSHOT.jar producer $KAFKABROKERS at this moment observe the unravel UI → Operation page → Chart → Kafka; you should observe some metric data is being collected. Once the producer completes, use this command to view information stored in wordcounts topic. \/usr\/hdp\/current\/kafka-broker\/bin\/kafka-console-consumer.sh --bootstrap-server $KAFKABROKERS --topic wordcounts --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --from-beginning Kill this process or command after a while, then observe unravel UI again " }, 
{ "title" : "Step 6: Updating Unravel Installation", 
"url" : "unravel-4-2/install/install-hdi/installation-guide-for-unravel-vm/step-6--updating-unravel-installation.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Installation Guides \/ Unravel for Azure HDInsight clusters \/ Installation Guide for Unravel VM \/ Step 6: Updating Unravel Installation", 
"snippet" : "From time to time, Unravel Data will release new package with new features and improvement for customers to download and update their existing installations. Updating Unravel VM on Azure is simple; just download the new unravel RPM package and install it. Text with brackets ( { } ) indicate where yo...", 
"body" : "From time to time, Unravel Data will release new package with new features and improvement for customers to download and update their existing installations. Updating Unravel VM on Azure is simple; just download the new unravel RPM package and install it. Text with brackets ( { } ) indicate where you must substitute your particular values for the text including the brackets. 1. Download unravel rpm file # wget http:\/\/preview.unraveldata.com\/img\/{NEW-RPM-FILENAME.rpm} 2. Install new rpm file on unravel VM # rpm -U {NEW-RPM-FILENAME.rpm} Update unravel installation should not affect the connected HDinsight cluster operation and can be done at any time. However some unravel rpm updates require the sensor upgrade and this will require you to re-submit the unravel script action script to head, worker, and edge nodes. " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-2/user-guide.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started The Operations Tab The Applications Tab The Data Tab Setting Up Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Perf...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started The Operations Tab The Applications Tab The Data Tab Setting Up Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-2/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "Case 1: How to Search for Applications and Optimize\/Tune a Hive Application Case 2: How to \"Root\" Cause\" Issues with a Workflow that Missed its SLA Case 3: How to Debug Failed Applications Case 4: How to Review Spark Applications and Identify Areas for Performance Improvements...", 
"body" : "[video] Case 1: How to Search for Applications and Optimize\/Tune a Hive Application Case 2: How to \"Root\" Cause\" Issues with a Workflow that Missed its SLA Case 3: How to Debug Failed Applications Case 4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin " }, 
{ "title" : "Setting Up Access to Big Data Components", 
"url" : "unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-SettingUpAccesstoBigDataComponents", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Setting Up Access to Big Data Components", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"url" : "unravel-4-2/user-guide/getting-started.html#UUID-52065521-19a6-3ac3-867f-306517b9725f_id_GettingStarted-CreatingUsersandSettingUpEmailSMTPLDAPKerberos", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Getting Started \/ Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "The Operations Tab", 
"url" : "unravel-4-2/user-guide/the-operations-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Operations Tab", 
"snippet" : "The Operations Tab provides synopsis of your clusters and its activities, and the ability to generate reports on the cluster activities. It has three sub-tabs: Dashboard, Charts and Reports. Typically you can configure the date range time period cluster(s)...", 
"body" : "The Operations Tab provides synopsis of your clusters and its activities, and the ability to generate reports on the cluster activities. It has three sub-tabs: Dashboard, Charts and Reports. Typically you can configure the date range time period cluster(s) " }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Dashboard", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, workflows, application inefficiencies, etc. By default the Dashboard is configured hourly all clusters 7 days Dashboard Tiles Fi...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, workflows, application inefficiencies, etc. By default the Dashboard is configured hourly all clusters 7 days Dashboard Tiles Finished and Running YARN applications, and Resources The first two tiles display the number of yarn applications and their states, and the third tile cluster resource usage. Clicking on the Open Section Finished YARN Application Detail This view is equivalent to Applications Finding Applications Finding Applications Running YARN Application Detail Displays the chart Operations Charts Jobs Resources Detail Displays the chart Operations Charts Resources Workflow Missing SLA Each row in this tile lists the workflow missing SLA and pertinent details including some KPIs. Clicking on the application name brings up the workflow manager. Inefficient Applications Its three sub-tabs, HIVE, MapReduce, and Spark Jobs each contain a list of inefficiencies that have occurred for jobs of that type. Click on the sub-tab to change the application type (1) or the event name to bring up the list of applications that experienced the event (2). Clicking on the application name brings up the application's information (3). Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. Selecting the event\/alerts brings up a Cluster Resource view ( Charts Resources Cluster View in Auto Actions add a new Auto Action or Alert " }, 
{ "title" : "Charts", 
"url" : "unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Charts", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Charts", 
"snippet" : "To view charts, click Operations Charts One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the various applications running in the clusters. Through Charts For example, Unravel can pinpoint the applications causing a sudden ...", 
"body" : "To view charts, click Operations Charts One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the various applications running in the clusters. Through Charts For example, Unravel can pinpoint the applications causing a sudden a spike in the total VCores or memory MB usage. This allows you to easily you drill down into these applications to understand their behavior. Whenever possible, Unravel provides recommendations and insights Common Features of Charts\/Graphs See the Resources chart below for an example of the following attributes. If is the graph is expandable it has Show More Most graphs\/charts allow you to chose how to display them (1). Typically your options are line, stacked area or grouped bar chart. When available, a chart glyph is displayed in the upper right corner of the graph\/chart. Click on the chart type to change the display. The first four charts: Resources, Jobs, Nodes and Impala: initially display their graphs in line mode, and, have relevant applications or queries listed at the bottom. When present, clicking on the application's name brings up its details. On the Resources, Jobs, and Impala tab you can chose how to Group by Placing your cursor within a chart\/graph brings up the information for that point in time in a text format (3). Clicking on the graph shows applications\/queries running at that point in time on the cluster. Click on the application name to bring up its details (4). Resources Vcores and memory MB are graphed by available\/allocated and allocated across app\/user\/queue. You can group the graphs by App Type User Queue Jobs Graphs the accepted, running, new, and pending jobs as applicable. You can Group State App Type User Queue Nodes This chart graphs Nodes by Total Active Lost Unhealthy Decommissioned Rebooted the Healthy Active nodes, and Active: the Unhealthy Active nodes. Unhealthy: Impala Graphs memory MB consumption and the number of queries. Queries can Grouped User Queue Services Graphs the system parameters. Use the Host Narrow Wide Kafka Lists all the configured Kafka clusters. Selecting the cluster name brings up the cluster view " }, 
{ "title" : "Reports", 
"url" : "unravel-4-2/user-guide/the-operations-tab.html#UUID-89d2d837-b268-10e9-0d31-7324b10f1e1a_id_TheOperationsTab-Reports", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Operations Tab \/ Reports", 
"snippet" : "To view reports, click Operations Reports This tab allows you to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Chargeback You can generate ChargeBack Application Type, User Queue Use the Group by User Application Type VCore\/Hour Memory MB\/Hour Update ...", 
"body" : "To view reports, click Operations Reports This tab allows you to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Chargeback You can generate ChargeBack Application Type, User Queue Use the Group by User Application Type VCore\/Hour Memory MB\/Hour Update Report The report consists of three sections: pie charts showing the top results, the chargeback report and a complete listing of the YARN applications contributing to the underlying report. The chargeback report (6) and applications (5) can be sorted in ascending or descending order on various reported metrics. Cluster Summary The Cluster Summary Applications User Queue VCore Memory Applications VCore Seconds Cluster Compare To compare clusters chose the initial Time Range Compare With Range Group By User Queue Any deviation in metrics across the time ranges is highlighted (4). A green red Time Compare With Group By " }, 
{ "title" : "The Applications Tab", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab", 
"snippet" : "The Applications cron Map-Reduce Hive (on Map-Reduce) Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL Impala Pig Cascading Kafka Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, sche...", 
"body" : "The Applications cron Map-Reduce Hive (on Map-Reduce) Tez Hive (on Tez) Spark Native Spark Streaming SparkSQL Impala Pig Cascading Kafka Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. It takes significant expertise and effort to get to the root cause(s) of an application's problems. Unravel's Intelligence Engine provides insights into your application's run to help resolve it's problems\/inefficiencies. These insights are called events Event Panel Examples The Applications Tab has three views you can chose using Show , Applications and Templates, Workflows. " }, 
{ "title" : "Common Features across Tiles, Panels, Tabs, etc.", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-APMCommonCommonFeaturesacrossTilesPanelsTabsetc", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Common Features across Tiles, Panels, Tabs, etc.", 
"snippet" : "When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descendi...", 
"body" : " When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( Clicking on the app name\/id usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. The application status is color coded: There is an Auto Actions column ( When more information glyph ( A block glyph ( Graphs Frequently a pull-down menu, i.e., If you can chose how to display the graph, i.e., area, line, the options are displayed in the upper right corner ( Hovering within the chart\/graph or on a diagram item, i.e., line, execution, brings up the information for that point\/item in time in a text format ( " }, 
{ "title" : "Applications", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Applications", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Applications", 
"snippet" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filtering by app name app type status queue user cluster duration number of events By time period, Initially search results are ordered by the most recent start time. To...", 
"body" : "Finding Applications You can search for your application(s) in a variety of ways: By full job ID, user name, table name cluster ID Filtering by app name app type status queue user cluster duration number of events By time period, Initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Search results list individual jobs and their IDs. If the job is part of a Hive query, Pig script, or a Workflow, a link to that Hive query\/Pig script\/workflow page is noted in the job's Go To GoTo " }, 
{ "title" : "Application Managers Basic Layout", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-ApplicationManagersBasicLayout", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Application Managers Basic Layout", 
"snippet" : "Below we are using the Spark Application Manager as the example. Many other tiles follow this format, MapReduce, Workflow, Fragments, Hive, etc. A black title bar notes the type of tile (Spark, Impala, MapReduce, Fragment, etc) and the job ID. The right side of the title bar are glyphs for adding a ...", 
"body" : "Below we are using the Spark Application Manager as the example. Many other tiles follow this format, MapReduce, Workflow, Fragments, Hive, etc. A black title bar notes the type of tile (Spark, Impala, MapReduce, Fragment, etc) and the job ID. The right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. Unravel's Intelligence Engine provides insights into an application and may provide recommendation, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains the Key Performance Indicators (KPIs) and general job information. [empty] : notes the number of events the job had. If there were no events Event icon No Events Event Panel Examples notes the job type and status. The box is colored code the same as the application status. Job icon: user, queue, start, stop time, etc. Job information: these vary by job type. KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation Common Sub-Tabs: Application Managers and views frequently contain the following sub-tabs. The contents will vary by job type. The below examples are again from the Spark Application Manager. Lists all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted. \"No errors found\" when there are none. Errors for each application are categorized by severity type and also include keywords and details associated with each. Keywords extract important details from the errors messages\/log data that can help developers\/operators quickly root cause issue. Examples of keywords include Oozie errors code(s), Java run time error(s), etc. Errors: : A list available logs. Click on the name to view the log information. \"No logs are available for this job.\" is listed if there are none. Logs : List the configuration parameters for the task\/job being displayed and their values. The parameters vary according to task\/job. Conf " }, 
{ "title" : "[empty]", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Application-SpecificManager", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ [empty]", 
"snippet" : "Spark Application Manager See the Spark Application Manager Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: resolve ineffici...", 
"body" : "Spark Application Manager See the Spark Application Manager Hive Application Manager The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: resolve inefficiencies, bottlenecks and reasons for failure within applications. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O : The number of YARN apps making up the Hive query. Number of YARN apps Sub-Tabs By default the Hive APM opens showing the Navigation and Query tabs. The left sub-tabs are: : List all the MapReduce jobs associate with the query. Click on the job name bring up job in the Navigation MapReduce Application Manager : Shows detailed information about the MapReduce jobs and their relationship with one another. This view helps identify bottlenecks and inefficiencies. Execution Graph This graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other and some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). To close the box click close (2) or scroll within the tab. Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. : See the Gantt Chart here : Exceptions, errors, and warnings associated with this application. See Errors here The right sub-tabs are: : Shows the Hive Query. See the Hive Application Manager tab above for an example Hive Query window. Query A list of all the accessed Tables. Tables: Clicking on the table bring up the Table Detail. See here : Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the Task Attempts entire : Graphs the Map and Reduce tasks over the duration of the job. Attempts MapReduce Application Manager The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O Sub-Tabs By default the MapReduce APM opens in the Graphs | Attempts : Has four (4) sub tabs. Graphs : Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in on the Map\/Reduce slot duration is noted textually. Attempts andMemory: Graphs utilization of slot containers, Vcores, and memory over time. Containers, Vcores, : Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. It's divided into two (2) sections, a Timeline Distribution Timeline Selected Tasks Timeline Map Reduce Killed\/Failed Selected Tasks : The metrics, their definitions and values. Metrics : Logs for the driver and executors of this application. See Logs here The defined parameters and their values. See Configuration: here : Graphs JVM-level metrics at the executor and driver level. Resource Usage Initially only ten (10) of the series are graphed. You can select one or more series to use (1) and you can choose to show more or less of the series (3). Clicking on a name causes the graph to only display information for that series. Use the METRIC Get Data Resource Metrics : Exceptions, errors, and warnings associated with this application. See Errors here Impala Application Manager The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Operators Sub-Tabs By default the Impala APM opens showing the Fragments and Query tabs. The left sub-tabs are: : Displays a table with information about each fragment associated with this query. The Fragments Coordinator Operators More : Displays a list of all operators for all fragments. Operators You can search the operators name. Click on the operator to display its details. Scan HDFS details Aggregate Details Exchange Details : Charts the fragments. Gannt Chart : Shows the query plan in fragment or operator view. Query Plan Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. The right sub-tabs are: : Shows the query plan code. Click on Query Copy to copy the query. (See 3 in Impala APM view above.) Query : Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. Mem Usage [empty] The Kafka Application Manager provides Multi-Cluster support for monitoring : Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. | Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out\/sec Messages in\/sec Total Fetch Requests per \/sec Number of Active Controller Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafa Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators Number of Topics Number of Partitions The Topic lists displays the KPIs; when details are available a more info You can chose both the Partition Metric th offset Partition Details' The Kafka View has two tabs, Topic Detail Partition Detail Consumer Details' Kafka Topic Detail By default the Kafka Topic Detai Topic Detail Kafka Partition Detail You can chose both the Partition Metric th offset Unravel Insights for Kafka Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is if: Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. if: Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: OK, the consumer is working, but falling behind, or Warning: : the consumer has stopped or stalled. Error Tez Application Manager The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/srv\/unravel\/log_hdfs\/unravel_us_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O Sub-Tabs By default the Tez APM opens showing the Navigation and Program Tabs. The left sub-tabs are: : List the Dag jobs with KPIs, Duration and I\/O. Navigation The DAG detail has six tabs: Displays the query. Query: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Graph: : Lists all the relevant counters for the Tez-DAG and their values. Counter Vertex Timeline Wall Clock Total Run All Vertices : List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task Id; Tasks containing the string will be displayed. All Task : List all attempts, their status (failed, success, etc.), vertex name and other relevant information. All Task Attempts : Lists all relevant parameters and their value. Changed Configuration : List the configuration parameters and their values. Configuration The right sub-tabs are: : Displays the query. Program : Has three (3) sub tabs. Graphs andMemory: Graphs utilization of slot containers, Vcores, and memory over time. Containers, Vcores, : Graphs the resources consumed. Resources By default the Resource systemCpuLoad Select series Metric Get Data " }, 
{ "title" : "Event Panel Examples", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-EventPanelEventPanelExamples", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Event Panel Examples", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. Recommendations Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. Efficiency 2: Tez DAG Resulted in one recommendation (#1). Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Templates", 
"url" : "unravel-4-2/user-guide/the-applications-tab.html#UUID-e58a9cba-a53f-0d3c-cded-62c85fbde605_id_TheApplicationsTab-Templates", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Templates", 
"snippet" : "Shows the templates in use. The templates with the highest average duration Showing Templates with the highest average duration Templates using most number of map tasks Templates using most number of reduce tasks Templates doing most DFS read I\/O Templates doing most DFS write I\/O Templates using mo...", 
"body" : "Shows the templates in use. The templates with the highest average duration Showing Templates with the highest average duration Templates using most number of map tasks Templates using most number of reduce tasks Templates doing most DFS read I\/O Templates doing most DFS write I\/O Templates using most reduce time Templates using most reduce time Workflow Manager The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications | Applications Key Performance Indicators : The number, if any, of Unravel insights for this query. See the Events Event Panel Examples : Total time taken by the query Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps Sub-Tabs The APM opens showing the Navigation Compare The left sub-tabs : Provides an easy way to understand the breakdown of the workflow the applications which comprise the Workflow, i.e., Hive, Spark, MapReduce, Oozie. Click on Navigation More Type : Exceptions, errors, and warnings associated with this application. See Errors here The right sub-tabs : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events : Displays charts for Map Task sand Reduce Tasks, broken down by success, failed, and killed as appropriate. Task Attempts : Graphs the attempts over the time interval in Wall Clock time and list the Map and Reduce Slot Duration in total time across all tasks. Attempts " }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-2/user-guide/the-applications-tab/spark-application-manager.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Applications Tab \/ Spark Application Manager", 
"snippet" : "Overview The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use it to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark executors, Detect and fix poor partitioning, Detect and...", 
"body" : "Overview The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use it to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. There are multiple Spark applications types, for instance SQL, Streaming, PySpark, Shell, and Notebook. Currently Unravel's UI Spark Application Manager distinguishes between SQL, Streaming, and the remaining (PySpark, Shell and Notebook). The information contained with the Spark Application Manager can vary by the type of job. But, as with all Applications Managers, no matter what the job, the layout is similar and there are common tabs\/information across all types. Common Features across Tiles, Panels, Tabs, etc. When no information is relevant\/available, it's usually explicitly stated. For example, when there are no events, the text No Events When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Tables can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( ). Click on a column name to use it for the sort. Clicking on a column already being used reverses the sort order. Clicking on the application name\/id usually brings up the application information, i.e., the Spark Application Manager or Job information. The application status is color coded ( , , , and ) when applicable. The Auto Action\/Tuning column ( )) notes if Unravel has tuning suggestions ( ) or the application has triggered an Auto Action\/alert ( ). There is an Auto Actions column ( ) when relevant. The number of auto actions (0-n) triggered is noted. A information glyph ( ) indicates when more information is available. Clicking on it brings up a new block or view as appropriate. A block glyph ( ) notes when the applications\/task\/job is open. Green indicates the block is open\/displayed, grey closed. Graphs Frequently a pull down menu, e.g., , above the graph offers display options to sort\/display the graph on, i.e., various metrics, filters, types, etc. If you can chose how to display the graph the options are shown in the upper right corner ( ). The current display will be highlighted in blue. Click on a graph type to change the display. Hovering within the chart\/graph\/diagram brings up the information for that point\/item in time in a text box ( ). When tables or lists are spread across multiple screen a page glyph ( )will be displayed. Click on the number to switch to that page. Spark Application Managers Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). The right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If it has a link to the parent, there will be an up arrow ( Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions or insights into how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Examples The next section contains the Key Performance Indicators (KPIs) and general job information. \n Event icon No Events Event Panel Examples \n Job icon: application status \n Job information: \n Key Performance Indicators (KPIs): The last section, typically divided into two, has specific information related to the application\/job. The sections for a specific Spark Application (e.g Streaming) go into more detail. If the job is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Spark Application Managers Regardless of the type of Spark job, every Application Manager view is split into two sides. Every application type has the following three tabs on the left: \n Errors: \n Logs Below is a excerpt of the executor-20 log. \n Conf When choosing a configuration item to display, it is possible that some configurations parameters apply to other configurations. If that is the case, the applicable configurations are highlighted. In the example below, memory was selected. The memory's configuration also contains parameters relevant to: driver (spark.driver.memory), executor (spark.executor.memory) and resources (yarn.nodemanager.resource.memory-mb). Every job has the following four tabs on the right: \n Program \n Task Attempts The donut graphs shows the percentage of successful (green) and of failed (orange) tasks. The legend on the right lists the number of successes or failure. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination of failed and successful jobs. Hovering over the chart tells you the percentage of each. \n Graphs \n Attempt \n Containers \n Vcores \n Memory \n Resource By default the Resource systemCpuLoad Select series name Metric Get Data Below is executor-2's data displayed using JSON. Spark Streaming Enabling Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf. Spark 1.6.x After the Unravel sensor has been deployed and installed on the cluster open spark-defaults.conf spark.driver.extraJavaOptions -javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-1.6. Key Performance Indicators Unlike other Spark applications the Spark-Streaming APM lists no KPI's other than: \n Events Event Panel Examples The Application Manager has a Stream tab Graph: Attempts The left tabs are: Stream Tab This tab displays the core of an Streaming Application and from here you drill down into the batches, The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Total Delay Metric Processing Time Scheduling Delay. This graph is composed of two sections. By default, the upper and lower section (1) display the entire run. The table lists the stream batches relevant to the time period selected in the lower section of the graph as shown in the screenshot below. The table lists only the first three batches, but you can page through the table (2). By default the streams are sorted on start time in ascending order. The batch table is initially sorted by time in ascending order and lists KPIs for the batches, e.g., Input Records, Scheduling\/Processing\/Total Delaty and the number of Stream operations. You can chose the time window to display by clicking and dragging over a region of the lower section of the graph. You can expand\/contract the section to display by pulling\/pushing the edge ( The Stream Batch table is updated to reflect the time period selected. In this example the right three batch groups were selected from the graph above. In the Spark APM window above there is 43 pages of stream batches; with the narrowed time selection there are now only 10 pages of streams. In the example below, the right most 3 batches were selected, note the batches have decreased from 43 to 13. Clicking on a batch brings up the Spark Stream Batch. The batch window lists all the jobs associated with the batch and the batch's metadata. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. It has two tabs, Output Operations Metadata duration processing delay scheduling delay total delay Output Operations descriptions duration Job ID duration status The Metadata Clicking on a job id brings up the Spark Job window in the same block thereby replacing the Spark Stream Batch information. The title bar notes it is a Spark Job and part of a Spark Streaming application. In the title bar there is an up arrow glyph ( Duration # of Stages Stages Metadata Tasks The metadata and its associated values are shown. Click on the Batch ID Click on the Stage to bring up the Spark Stage panel. The Spark Stage is displayed in the same block thereby replacing the Spark Job information. The Spark Stage view is not specific to the job type. See Stage Details Execution Tab Currently no data is displayed for streaming applications since it would display thousands of nodes rendering the information useless. In Unravel v4.3.1 the Unravel UI will display the execution information at the stage level. Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf The right tabs are: Program Currently there is no information under this tab. Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resources Spark SQL-Query Key Performance Indicators \n Events Event Panel Examples \n Duration \n Data I\/O \n Number of Stages The left tabs are: \n Navigation Lists the application's jobs with their relevant KPIs: Status Start Time Duration Paritions\/Tasks Read Write # Stages Start Time The job block lists the KPIs Duration # of Stages Stages Metadata Status Start Time Duration Partitions\/Tasks Read Write Input Output Start Time Spark Stage Details \n Execution A execution graph of the query. There are times when the DAG is too large to display and it will be noted. In v4.3.1 execution graphs will be displayed in the stage block. Hover over a stage for more information about the stage. \n Gantt Char Displays the stages using a Gantt Chart. The table is sorted on Start Time \n Errors, Log and Conf Tabs For an explanation of these tabs see Errors Logs Conf The right tabs are: \n Program This tab connects all the pieces of a SQL query in one location. The table lists all queries with significant KPI's and, at most, the top five stages having the longest duration. For a particular query the SQL query text, the Program, the stages with the longest duration and KPIs are linked. The lower section contains two tabs, SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID to display it's SQL and program. Click on the stage to display its Spark Stage Detail. See Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. \n Task Attempts, Graphs and Resources For an explanation of these tabs see Task Attempts Graphs Resources Other Spark Applications All other Spark Applications have a APM like the Spark SQL except there is no Query Spark Stage Details The Spark Stage block displays two KPIs, duration Data IO Graphs Timeline Attempts Task Attempts Program The default view is Graph Task Attempts Attempts. Program Details Clicking on the Source File Nam Call Site Line Number org.apache.spark.streaming.dstream.DStream.print(DStream.scala:757). \n \n Timeline Block The Timeline tab has two sections, \n Distribution \n Timeline Timeline Breakdown Selected Tasks Distribution Charts The Distribution Charts ShuffleMap Seconds Input KB Output KB Disk Bytes Spilled Memory Bytes Spilled Records Read Timeline You can chose the time period to display by clicking and dragging to select a time period. You can expand\/contract the section by pulling\/pushing the edge, or move the box by clicking on it and sliding it to the new location. Only the series relevant to that time period are displayed. In this example, the time period from 24-48 seconds has been selected, during which no series was running, so the Timeline \n \n Timeline The Timeline Tab has three sub-tabs: Timeline Timeline Breakdown Selected Tasks \n Timeline The timeline is a Gantt chart of the series which makes up the stage. See Distribution Charts above for an example of the graph. Only the series relevant to the time selected are displayed. You can filter the display by Tasks Killed\/Failed Show All Example Text Box T imeline Breakdown This is useful to identify bottlenecks. For each executor used in the current stage multiple metrics are graphed: Scheduler Delay, Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Executor Computing Time performing actual work thrashing, or waiting for scheduling. In this example, there is only one executor. \n Selected Tasks A list of tasks, if any, for the stage. " }, 
{ "title" : "The Data Tab", 
"url" : "unravel-4-2/user-guide/the-data-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab", 
"snippet" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. It has two tabs, Overview Details...", 
"body" : "The Data tab provides a snapshot of tables and partitions over the last 24 hours within a historical context. It has two tabs, Overview Details " }, 
{ "title" : "Common Features across Tiles, Panels, Tabs, etc.", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-CommonFeaturesacrossTilesPanelsTabsetc", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Common Features across Tiles, Panels, Tabs, etc.", 
"snippet" : "When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Table\/Application list can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highligh...", 
"body" : " When there are multiple tabs, clicking on the tab will display it. Lists (applications, tables, stages, etc.) Table\/Application list can be sorted by a column in ascending or descending order, i.e., READ I\/O, Memory GB. The column currently used will have the arrow indicating the sort order highlighted ( ). Click on a column name to use it for the sort. Clicking on a column already being used alters the sort order. When more information glyph ( ) appears clicking on it displays the information in a new view\/tile, i.e., APM, job block, etc. A check box is left of the table\/partition name if it must be selected in order to be graphed or make the more information glyph active. Clicking on the app name\/id usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. The table label is color coded, , Hot ), Warm ), or Co ld ) when applicable. The Auto Action\/Tuning column ( ) notes if Unravel has tuning suggestions ( ) or the application has triggered an Auto Action\/alert ( ). Graphs Frequently a pull down menu, i.e., , above the graph offers options to sort the graph on, i.e., various metrics, filters, types, etc. Hovering within the chart\/graph or on a diagram item, i.e., line, execution, brings up the information for that point\/item in time in a text format ( ). " }, 
{ "title" : "Overview", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-Overview", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Overview", 
"snippet" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of 3 sections. Tables Partitions More Info The 24 hour time period is noted in the upper right corner of the dashboard. Tables & Partitions Tiles The Tables Partitions Line Charts : Numb...", 
"body" : "The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of 3 sections. Tables Partitions More Info The 24 hour time period is noted in the upper right corner of the dashboard. Tables & Partitions Tiles The Tables Partitions Line Charts : Number of Tables\/Partitions accessed. # Accessed : Number of Tables\/Partitions created. # Created : Size of Tables\/Partitions created. Size Created : Total Number of Tables\/Partitions currently in the system. Total Number Each line chart has the value of the metric for the last 24 hours overlaid on line graph which shows the historical values up to that point in time. Hovering over the line brings up a text box containing the value for that point in time. Donut Charts These charts display their information both in absolute values and as a percentage of the whole. : Displays Current Label Distribution Hot Warm, Cold policy configuration The Partitions : Total number of partitions and reclaimable partitions, and Partition Count : Total size of partitions and reclaimable amount. Partition Size The number and size of Reclaimable Partitions are calculated thusly: Reclaimable Partitions Found " }, 
{ "title" : "More Info", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-MoreInfo", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ More Info", 
"snippet" : "This section has three graphs. : Total number of queries accessing the tables. Accessed Queries : Total Read IO due to accessing the table. Total Read IO : Total number of users accessing the tables. Click on the information glyph to see the list of users. Number of Users...", 
"body" : "This section has three graphs. : Total number of queries accessing the tables. Accessed Queries : Total Read IO due to accessing the table. Total Read IO : Total number of users accessing the tables. Click on the information glyph to see the list of users. Number of Users " }, 
{ "title" : "Details Tab", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-DetailsTab", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Details Tab", 
"snippet" : "The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In the example below, the first three tables have been selected and are displayed. Choose the table(s) by selecting the check box (1) next to the table. Use the Metric Read IO, Total Users...", 
"body" : "The details tab has two sections, a graph and a table list. By default the graph uses the Read IO Read IO Graph In the example below, the first three tables have been selected and are displayed. Choose the table(s) by selecting the check box (1) next to the table. Use the Metric Read IO, Total Users Total Attempts, Total Size Reset Graph Table List You can Search Show Hot Warm Cold All Warm Read IO More Info Table Detail . Configuration Policy The sortable columns are: Hover over the name to see the full name. Table Name: : The table’s owner. Owner : Date\/Time of the last access to the table. Last Access : Date\/Time the table was created Created : Total table size. Size : Total data read by all applications that accessed the table. Read IO : Total number of attempts by all applications that accessed the table. Attempts : Total number of applications that accessed the table. Apps : Number of partitions in the table. Partitions : Number of reclaimable partitions in the table. RP (Count) : Total size of reclaimable partitions in the table. RP (size) : Total number ofusersthat accessed the table.Hovering over the user number lists the user(s) names. Users : Tables used along side this table as part of an application. Hovering over the table number lists the table name(s). Other Tables The two additional colums are: : Table age\/last access, Labels Hot Warm Cold warm cold. : A link to the More Info Table Detail " }, 
{ "title" : "Table Detail", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-TableDetail", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Table Detail", 
"snippet" : "This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the ...", 
"body" : "This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed, Read IO User # Apps, Attempts There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Read IO, Total Users, Total Attempts, Total Size Application Detail Application Tab page Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition S ize MR jobs By default the 100 latest partitions are loaded, and the first partition listed is graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View " }, 
{ "title" : "Configuration", 
"url" : "unravel-4-2/user-guide/the-data-tab.html#UUID-dfb98723-b4a8-4bfc-9983-d595304d1c61_id_TheDataTab-ConfigConfiguration", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ The Data Tab \/ Configuration", 
"snippet" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data Details From the pull down menus...", 
"body" : "This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold Current Label Distribution Details tab While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access the Policy Configuration Data Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save " }, 
{ "title" : "Setting Up Auto Actions", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "About Auto Actions", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-AboutAutoActions", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ About Auto Actions", 
"snippet" : "Unravel's Auto Actions The Unravel Server processes Auto Actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical ex...", 
"body" : "Unravel's Auto Actions The Unravel Server processes Auto Actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate this rule in order to detect violation, and Action(s) for Unravel Server to execute whenever it detects a violation of the rule. " }, 
{ "title" : "How to Create Auto Actions", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-CreateAAHowtoCreateAutoActions", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ How to Create Auto Actions", 
"snippet" : "Click the Admin pull-down menu, and select Manage. On the Manage page, select the Auto Actions tab. The Auto Actions To add a new action, click ADD NEW AUTO ACTION. The Add New Action Map Reduce Jobs Long running MR job Too many mappers for an MR job Too many reducers for an MR jo Resource contentio...", 
"body" : " Click the Admin pull-down menu, and select Manage. On the Manage page, select the Auto Actions tab. The Auto Actions To add a new action, click ADD NEW AUTO ACTION. The Add New Action Map Reduce Jobs \n Long running MR job \n Too many mappers for an MR job \n Too many reducers for an MR jo Resource contention \n Resource contention in cluster \n Resource contention in queu Rogue Identification \n Rogue user \n Rogue application Long Running Jobs \n Long running YARN application \n Long running Hive query \n Long running workflow Expert mode This is a very powerful mode of operation. You can take full advantage of all the Unravel Auto Actions engine's features running the scene via defining rules and actions in “free form” using the Auto Actions JSON language. Consult with the Unravel team before attempting to use the Expert Mode. In this mode you must specify prerequisite conditions, defining conditions, and actions: \n Prerequisite conditions \n Defining conditions \n Actions Define an Auto Action Using predefined templates. Select an Auto Action Template and click NEXT The Auto Action Template Enter the name of the rule and a description. The name is required and is used to identify the rule in the UI. The name is included in every message the Auto Action sends\/posts upon violation. The name should be short and clearly identify the purpose of the rule definition. The description is optional but highly recommended. In it you can provide an explanation of what the rule is designed to do, i.e. monitor metrics and actions of a user. The description is mainly for your reference but can be included in Auto Actions messages. Specify the rule and its applicable scope. A rule has two conditions: \n Prerequisite conditions \n Defining conditions: \n User \n Queue \n Cluster Note: this rule is applicable only to multi-cluster Unravel monitoring configuration. \n Application Name \n Time Specify the Auto Actions executed for rule violations. Once a violation is triggered, you have a choice of actions to take. The Auto Actions engine automatically logs and flags violations As with the rules, the actions vary between templates. Typically, you have the following available: \n Send email email \n HTTP post HTTP endpoint \n Move to queue \n Kill application Caution: This is a destructive action that may affect the cluster performance and availability to the users. \n Send an Email Select the Send Email ADD RECIPIENT INCLUDE OWNER email cluster history charts dashboard \n HTTP post sent to Slack This action requires support through Slack “hooks”. Select the HTTP Post ADD URL HTTP post \n Kill the Application Select the Kill App . \n \n Caution: This is a destructive action that may affect the cluster performance and availability to the users. Applications targeted to be killed must have: Directly caused the rule violation, and Allocated resources, i.e. in allocated or running states. \n Kill App Move app to queue Move app Kill App . \n Move the application to another queue Select the Move app to queue \n Caution: While a non-destructive action that should not affect the cluster performance and its availability to the user, we suggest moving with caution. Applications targeted to be moved must have: Directly caused the rule violation, and Allocated resources, i.e. in allocated or running state. \n Move app to queue Kill App Move app Kill App Using theExpert ModeTemplate. The Auto Actions engine is capable of much more than is available through the templates. Using the Auto Actions Expert Mode Section 4.1 Expert Mode \n Caution: This flexiblity and power also makes this mode dangerous and capable of wreaking havoc. Consult with Unravel team before attempting to use the Expert Mode. Select Expert Mode from the Auto Actions template screen. Use the JSON language enter the rule(s) definition(s) in the Rule text box and any Actions in the Actions text box. See Sample Auto Actions " }, 
{ "title" : "5. Click SAVE AUTO ACTION.", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-5ClickSAVEAUTOACTION", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ 5. Click SAVE AUTO ACTION.", 
"snippet" : "A link is generated to cluster history charts dashboard at the point when this violation occurred +\/- 5 minutes (a 10 minute time slice). The link is included in every message (email or HTTP post) and violation log event. A \"bell\" badge is placed the UI next to each application that triggered a viol...", 
"body" : "A link is generated to cluster history charts dashboard at the point when this violation occurred +\/- 5 minutes (a 10 minute time slice). The link is included in every message (email or HTTP post) and violation log event. A \"bell\" badge is placed the UI next to each application that triggered a violation. Example cluster history charts dashboard Example Email Auto Action policy \"ROGUE APPLICATION #1\" violation detected.\n\nPolicy description: \"Identify applications that are using too much of the cluster resources\"\n\nApplication \"application_1498514199803_2411\" has 1 violation:\n\n1. Sum of memory in MB allocated to containers is 1GB >= 1MB\n\nTimestamp: 07\/19\/2017 08:41:35 +0000\n\nReach Unravel server at http:\/\/localhost:3000\/\n\nSee cluster history at http:\/\/localhost:3000\/ops_dashboard\/charts\/resources?from=1499817549571&to=1499817669570&at=1499817609571&interval=1m Example HTTP Post Example Event Log: " }, 
{ "title" : "Understanding the Snooze Feature", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions.html#UUID-8b1e2dee-d646-3567-1d58-d29f53d699f5_id_SettingUpAutoActions-UnderstandingtheSnoozeFeature", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Understanding the Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions being repeated for the same violation context during a specified period of time If, and only if, the action adds no further information to the violation, i.e., is essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert...", 
"body" : "The snooze function prevents automatic actions being repeated for the same violation context during a specified period of time If, and only if, the action adds no further information to the violation, i.e., is essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert adds no new context the user A's violation. Snooze is set the first time the user violates the rule. Though actions are snoozed for some users, the Auto Action continues to run uninterrupted and will take action for those not \"snoozing\" at the time of violation. Snooze is irrelevant if the action is Kill App Move App For example: the rule\/action: if user uses memory > 1 GB send email two users: A & B snooze time: 30 minutes \n at 20:00 user A > 1GB → email is sent + snooze set (runs until 20:30). user B < 1GB → nothing is done. \n at 20:10 user A > 1GB → 'snoozing', no action is taken. user B > 1GB → email is sent + snooze set (runs until 20:40). \n at 20:20 user A > 1GB → 'snoozing', no action is taken user B > 1GB → 'snoozing', no action is taken \n at 20:35 user A > 1GB → email is sent + snooze set (runs until 21:05) user B > 1GB → 'snoozing', no action is taken The snooze property, com.unraveldata.auto.action.snooze.period.sec \/usr\/local\/unravel\/etc\/unravel.properties . Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions/running-auto-action-demos.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with Auto Actions and their triggering. Example Auto Actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED U...", 
"body" : "The Demos program provides you a way to understand and experiment with Auto Actions and their triggering. Example Auto Actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". HIGHLIGHTED Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory. DEMO_PATH auto-actions-demos # ls auto-actions-demos\ndemos\/ setup\/ Go to DEMO_PATH Open .\/settings Execute the .\/setup-all script. # .\/setup-all The Auto Action rules that include time specification will be automatically adjusted to the current time period, i.e. from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Admin Manage Auto Actions You should see all the Auto-Actions demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag, e.g., AA-Spark-1c, Map-1b, AA-Hive-1a. Executing the demos Go to DEMO_PATH\/demos directory. For each Auto Actions rules listed in Admin->Manage->Auto Actions there is a corresponding script in the demo's directory. Each script will trigger the corresponding Auto Action demo. For example, in the UI you will see an Auto Action named AA_Spark-1c. demo-Spark-1c. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Go to DEMO_PATH\/demos directory. For each Auto Actions rules listed in Admin->Manage->Auto Actions there is a corresponding script in the demo's directory. Each script will trigger the corresponding Auto Action demo. For example, in the UI you will see an Auto Action named AA_Spark-1c. demo-Spark-1c. # ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute “.\/demo-tag” script to trigger the corresponding “AA-tag” rule. Each script is designed to simulate violation conditions for the corresponding Auto Action on the target Hadoop cluster, i.e., to trigger AA-Spark-1c you run the demo-Spark-1c script. Some Auto Actions demo scripts cause multiple Auto Actions to trigger. While this a side effect that is unavoidable, it is also what can happen when running your defined Auto Actions. This is because defined Auto Actions can have overlapping definitions. . Cleaning up demos Go to DEMO_PATH\/setup directory. Run .\/clean-all script. # .\/clean-all This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 Auto Actions Demos List Application type Use case Auto Action Triggering Script Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vcores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Will also trigger MR-1a as well. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. AA-MR-2b Demo-MR-2b Will also trigger MR-2a as well. Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. Spark Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark application has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark application allocated vcores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark application is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Alert if a workflow is reading more data than expected. Related articles Page: Setting Up Auto Actions Page: Running Auto Action Demos " }, 
{ "title" : "Sample Auto Actions", 
"url" : "unravel-4-2/user-guide/setting-up-auto-actions/sample-auto-actions.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Setting Up Auto Actions \/ Sample Auto Actions", 
"snippet" : "Supported cluster metrics AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master also maintains various counter. Users can use these metrics and the counter...", 
"body" : "Supported cluster metrics AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master also maintains various counter. Users can use these metrics and the counters when defining Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on: \"live\" running applications allowing to take proactive actions when violations are detected, and on MapReduce AM metrics only when the user specifies a rule requiring the polling\/aggregation of a metric. YARN Resource Manager metrics appCount - total number of applications elapsedTime - total elapsed time of containers in milliseconds allocatedMB - sum of memory in MB allocated to containers allocatedVCores - sum of virtual cores allocated to containers runningContainers - number of containers currently running memorySeconds - amount of allocated memory in MB-seconds vcoreSeconds - amount of allocated CPU resources in virtual core-seconds For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API MapReduce Application Master metrics elapsedAppTime - time since the application was started in milliseconds mapsCompleted - number of completed maps reducesTotal - total number of reduces reducesCompleted - number of completed reduces mapsPending - number of maps still to be run mapsRunning - number of running maps reducesPending - number of reduces still to be run reducesRunning - number of running reduces newReduceAttempts - number of new reduce attempts runningReduceAttempts - number of running reduce attempts failedReduceAttempts - number of failed reduce attempts killedReduceAttempts - number of killed reduce attempts successfulReduceAttempts - number of successful reduce attempts newMapAttempts - number of new map attempts runningMapAttempts - number of running map attempts failedMapAttempts - number of failed map attempts killedMapAttempts - number of killed map attempts successfulMapAttempts - number of successful map attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduce Application Master counters \n Shuffle Errors badId - total number of errors related with the interpretations of IDs from shuffle headers connection - total number of established network connections ioError - total number of errors related with reading and writing intermediate data wrongLength - total number of errors related to compression and decompression of intermediate data wrongMap - total number of errors related to duplication of the mapper output data wrongReduce - total number of errors related to the attempts of shuffling data for wrong reducer \n File System Counters fileBytesRead - mount of data read from local file system fileBytesWritten - amount of data written to local file system fileReadOps - number of read operations from local file system fileLargeReadOps - number of read operations of large files from local file system fileWriteOps - number of write operations from local file system hdfsBytesRead - amount of data read from HDFS hdfsBytesWritten - amount of data written to HDFS hdfsReadOps - number of read operations from HDFS hdfsLargeReadOps - number of read operations of large files from HDFS hdfsWriteOps - number of write operations to HDFS \n Map-Reduce Framework Counters mapInputRecords - total number of records processed by all of the mappers mapOutputRecords - total number of records produced by by all of the mappers mapOutputBytes - total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes - amount of (compressed) data which was actually written to disk splitRawBytes - amount of data consumed for metadata representation during splits combineInputRecords - total number of records processed by combiners combineOutputRecords - total number of records produced by combiners reduceInputGroups - total number of unique keys reduceShuffleBytes - of data processed in shuffle and reduce phase reduceInputRecords - total number of records processed by all reducers reduceOutputRecords - total number of records produced by all reducers spilledRecords - total number of map and reduce records that were spilled to disk shuffledMaps - total number of mappers which undergone through shuffle phase failedShuffle - total number of mappers which failed to undergo through shuffle phase mergedMapOutputs - total number of mapper output files undergone through shuffle phase gcTimeMillis - wall time spent in Java Garbage Collection cpuMilliseconds - cumulative CPU time for all tasks physicalMemoryBytes - total physical memory used by all tasks including spilled data virtualMemoryBytes - total virtual memory used by all tasks committedHeapBytes - total amount of memory available for JVM \n Job Counters totalLaunchedMaps - total number of launched map tasks totalLaunchedReduces - total number of launched reduce tasks dataLocalMaps - number of map tasks which were launched on the nodes containing required data slotsMillisMaps - total time spent by all executing maps in occupied slots slotsMillisReduces - total time spent by all executing reduces in occupied slots millisMaps - total time spent by all map tasks millisReduces - total time spent by all reduce tasks vcoresMillisMaps - total vcore-seconds taken by all map tasks vcoresMillisReduces - total vcore-seconds taken by all reduce tasks mbMillisMaps - total megabyte-seconds taken by all map tasks mbMillisReduces - total megabyte-seconds taken by all reduce tasks \n File Input Format Counters bytesRead - amount of data read by every tasks for every filesystem \n File Output Format Counters bytesWritten - amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Spark Metrics In addition to the metric set supported by MapReduce applications, Spark application, Spark applications can be polled on: inputRecords outputRecords outputToInputRecordsRatio totalJoinInputRowCount totalJoinOutputRowCount inputPartitions outputPartitions Hive\/Workflow Metrics duration - total time taken by the application totalDfsBytesRead totalDfsBytesWritten Information on Demo Auto Actions can be found here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Alert for Rogue application - any application which is consuming a major portion of cluster resources. a. If any application (not sla bound) is consuming more than certain vcores\/memory at midnight, move it to a quarantine queue You can use the Rogue Application or memory Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Set Time rule condition as: Set Move app rule as: b. If any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application or memory Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n}\n or as below for memory. {\n \"scope\": \"multi_app\",\n \"metric\":\"allocated_mb\",\n \"compare\": \">\",\n \"value\": [X]\n} Set Queue rule conditions as: Set Move app action as: Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. a. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. b. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. a. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n b. Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. a. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n UserAlert for Rogue User - Any user consuming a major portion of cluster resources. \n a. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} b. Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Workflow Alert if a workflow is taking longer than expected. a. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} b. Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-2/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-2/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Go to Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Go to Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster_the spike's timestamp,_the bottom of the page. When you see many applications in the ACCEPTED RUNNING RUNNING ACCEPTED Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-2/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster_the spike's timestamp,_the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "unravel-4-2/user-guide/use-cases/optimizing-the-performance-of-spark-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web ...", 
"body" : "Unravel Web UI makes it easy for you to identify underperforming Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec. In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec. Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Kafka Insights", 
"url" : "unravel-4-2/user-guide/use-cases/kafka-insights.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, \n OK \n Lagging \n Stalled A Topic's status is set to the lowest status among it's Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Examples 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Note: Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric offset, test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI’s. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-2/advanced-topics.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics", 
"snippet" : "Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery, and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting ...", 
"body" : " Autoscaling HDInsight Spark Cluster using Unravel API Backing-up, Disaster Recovery, and Reverting to Prior Version Cluster Wide Report Configurations Custom Configurations Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Integrating LDAP Authentication for Unravel Web UI Using a Private Certificate Authority with Unravel Connecting to\/Configuration of a Kafka Stream Connecting to a Hive Metastore Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Creating Application Tags Running Verification Scripts and Benchmarks Supported Roles Unravel Servers and Sensors Installing Sensors Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Installing Unravel Sensor for Individual Hive Queries Uninstalling Unravel Server Upgrading the Unravel Server and Sensors Uploading the Spark Program(s) to Unravel Using an External MySQL or Compatible Database for Unravel Workflows Monitoring Airflow Workflows Monitoring Oozie Workflows Tagging Workflows " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/advanced-topics/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Backing-up, Disaster Recovery, and Reverting to Prior Version", 
"url" : "unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version", 
"snippet" : "It is best practice to create a snapshot (backup) of your Unravel Server and to test your snapshot on a failover server daily or at a minimum, weekly; in addition, it is absolutely essential to create a snapshot before any upgrade in case you need to revert to the previous Unravel version....", 
"body" : "It is best practice to create a snapshot (backup) of your Unravel Server and to test your snapshot on a failover server daily or at a minimum, weekly; in addition, it is absolutely essential to create a snapshot before any upgrade in case you need to revert to the previous Unravel version. " }, 
{ "title" : "Dump of Unravel Server's State", 
"url" : "unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-DumpofUnravelServersState", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Dump of Unravel Server's State", 
"snippet" : "Creating a snapshot of your Unravel Server involves dumping its state and databases to a specified location. You create a snapshot by running the dump_unravel.sh # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh $DEST The result is a subdirectory under $DEST unravel_yyyymmddthhmmZ If you use an external...", 
"body" : "Creating a snapshot of your Unravel Server involves dumping its state and databases to a specified location. You create a snapshot by running the dump_unravel.sh # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh $DEST The result is a subdirectory under $DEST unravel_yyyymmddthhmmZ If you use an external database in production, that database is not dumped; you must replicate or back up the externally managed database separately. " }, 
{ "title" : "Load of Unravel Server's State", 
"url" : "unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-LoadofUnravelServersState", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Load of Unravel Server's State", 
"snippet" : "You can load a saved state into Unravel Server by specifying a source directory, $SRC, on the invocation of the load_unravel.sh script replaces Very important: this operation wipes out existing data in the installed Unravel Server on the current host. Data in an external database is unaffected. # su...", 
"body" : "You can load a saved state into Unravel Server by specifying a source directory, $SRC, on the invocation of the load_unravel.sh script replaces Very important: this operation wipes out existing data in the installed Unravel Server on the current host. Data in an external database is unaffected. # sudo \/usr\/local\/unravel\/bin\/load_unravel.sh $SRC The result is that the saved state $SRC " }, 
{ "title" : "Disaster Recovery Scenario", 
"url" : "unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-DisasterRecoveryScenario", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Disaster Recovery Scenario", 
"snippet" : "For disaster recovery, you can prepare a cold failover or alternate server, and periodically update it with a snapshot. In this case, you would create a snapshot periodically on your normally live Unravel Server, and load that snapshot onto your alternate server. For a cold failover (to the alternat...", 
"body" : "For disaster recovery, you can prepare a cold failover or alternate server, and periodically update it with a snapshot. In this case, you would create a snapshot periodically on your normally live Unravel Server, and load that snapshot onto your alternate server. For a cold failover (to the alternate server), there is no need to start all the Unravel daemons until switching over–in other words, after the disaster in the normal data center. " }, 
{ "title" : "Revert to Prior Version Scenario", 
"url" : "unravel-4-2/advanced-topics/backing-up,-disaster-recovery,-and-reverting-to-prior-version.html#UUID-7e8e8e6b-7095-e59e-364e-68320a84ae64_id_Backing-upDisasterRecoveryandRevertingtoPriorVersion-ReverttoPriorVersionScenario", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Backing-up, Disaster Recovery, and Reverting to Prior Version \/ Revert to Prior Version Scenario", 
"snippet" : "Best practice is to execute rolling upgrades of Unravel Server: first upgrade the normal server, then upgrade the alternate server. In this fashion you always have an easy way to completely revert an upgrade to the prior version. To revert, do the following steps. If you use an external database in ...", 
"body" : "Best practice is to execute rolling upgrades of Unravel Server: first upgrade the normal server, then upgrade the alternate server. In this fashion you always have an easy way to completely revert an upgrade to the prior version. To revert, do the following steps. If you use an external database in production, you cannot revert it with this procedure; you might need to revert it separately. Create a snapshot (saved state). Dump Unravel Server state $DEST before upgrade. For example, # sudo \/usr\/local\/unravel\/bin\/dump_unravel.sh \/mnt\/someplace Note the backup subdirectory created. For example, \/mnt\/someplace\/unravel_yyyymmddthhmmZ\/ Try the new version of Unravel Server. Upgrade Unravel Server Start Unravel Server: # \/etc\/init.d\/unravel_all.sh start (Optional) Revert to the previous version of Unravel Server. Stop Unravel Server: # \/etc\/init.d\/unravel_all.sh stop Uninstall the RPM: # rpm -e unravel This command's output is a list of all data files. Remove all data files listed in the output of the \" rpm -e Install the previous version of Unravel Server: # rpm -U unravel*rpm* Load the snapshot (saved state) you saved in $DEST #\/usr\/local\/unravel\/bin\/load_unravel.sh \/mnt\/someplace\/unravel_yyyymmddthhmmZ\/ Start Unravel Server: # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Cluster Wide Report", 
"url" : "unravel-4-2/advanced-topics/cluster-wide-report.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of the recommendations revolve around these parameters: MapSplitSizeParams MHiveExecReducersBytesParam MHiveExecParallelParam , and MMapReduceSlowStartParam . MMapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "unravel-4-2/advanced-topics/cluster-wide-report.html#UUID-45a459b7-2296-b3c5-1a23-15babf5d95c2_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSe...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/ usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/ usr\/local\/unravel\/install_bin\/ClusterReport. cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz to cd dbin Input.txt # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report # sudo -u hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "unravel-4-2/advanced-topics/configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel...", 
"body" : " Creating Multiple Workers for High Volume Data Configuring Multiple Hosts for Unravel Server Defining a Custom TC Port Setting Retention Time in Unravel Server Setting Up Email for Auto Actions and Collaboration Spark Properties for Spark Work daemon @ Unravel " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # ...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/configuring-multiple-hosts-for-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig 1. Stop Unravel Server On each Unravel host, run this command: sudo \/etc\/init.d\/unravel_all.sh stop 2. Modify unravel.properties on host1 Pick a machine to be host1 If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties host1 Replace 127.0.0.1 3316 unravel_mysql_prod To find your fully qualified hostname, type hostname -I unravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod 2. Copy host1's unravel.properties to Other Hosts Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh host1 host2 host3 # host1\n# scp \/usr\/local\/unravel\/etc\/unravel.properties host2:\/usr\/local\/unravel\/etc\/\n# scp \/usr\/local\/unravel\/etc\/unravel.ext.sh host2:\/usr\/local\/unravel\/etc\/\n# scp \/etc\/unravel_ctl host2:\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel Important Note The scripts invoked below will make an identical change to the unravel.properties 3. Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host): # host1\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh\\ \nhost1 host2 \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh\\ \nhost1 host2 For a 3-host ensemble (substitute host): # host1 sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh\\ \nhost1 host2 host3\n# host2 sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh\\ \nhost1 host2 host3\n# host3 sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh\\ \nhost1 host2 host3 These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id 4. Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order, on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: # host1\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host3\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: # host1 \nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15sec for Zookeeper quorum to settle: sleep 15 Start up Kafka ensemble: # host1\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-k Wait 10sec for Kafka coordination: sleep 10 Create the Kafka topics (only on one host): # host1 \nsudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh 5. Start Unravel Server Finish multi-host installation by starting up Unravel Server: # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host1\necho \"http:\/\/$(hostname -f):3000\/\" \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start 6. Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 7. Snapshot unravel.properties " }, 
{ "title" : "Defining a Custom TC Port", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/defining-a-custom-tc-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom TC Port", 
"snippet" : "These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 # echo 'export UNRAVEL_LISTEN_PORT={18080}' \\ >>\/usr\/local\/unravel\/etc\/unravel.ext.sh...", 
"body" : " These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 # echo 'export UNRAVEL_LISTEN_PORT={18080}' \\\n>>\/usr\/local\/unravel\/etc\/unravel.ext.sh " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration Core Retention Manage Configuration unravel.properties will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-r...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration Core Retention Manage Configuration unravel.properties will be ignored When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space. That means about 1000 jobs per 1GB of disk. In Unravel Web UI, select the Manage Configuration Core \/usr\/local\/unravel\/etc\/unravel.properties The TIME SERIES RETENTION DAYS unravel.properties: com.unraveldata.retention.max.days=30 This setting is the most significant factor in controlling disk space usage in the database used by Unravel. The WEEKS TO SHOW FOR SEARCH RESULTS unravel.properties: com.unraveldata.recent.maxSize.weeks=2 This value should be no larger than the next setting minus 1. If recent activity in Unravel gives you too many results, consider making this smaller. It should not be less than 2. The WEEKS TO SHOW FOR DEEP SEARCH RESULTS unravel.properties: com.unraveldata.history.maxSize.weeks=4 This value should be at least 1 week more than the setting immediately above, so that com.unraveldata.recent.maxSize.weeks < com.unraveldata.history.maxSize.weeks is true. After changing any of the settings above, restart unravel_td sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/setting-up-email-for-auto-actions-and-collaboration.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. \n \n \n Web UI \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 25 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n false \n Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw \n \n START TLS \n mail.smtp.starttls.enable \n false \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n null \n Username for SMTP authentication \n \n USER PASSWORD \n mail.smtp.pw \n null \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n l \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone @example.com \n Use a From: \n \n LOCALHOST \n mail.smtp.localhost \n localhost.local \n A domain name for apparent sender; must have at least one dot (e.g. organization.com) \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. GMail SMTP Example These settings are for our internal use. Do not \n \n \n Config Label \n Property \n Value \n Description \n \n PORT \n mail.smtp.port \n 587 \n Port \n \n AUTHENTICATE \n mail.smtp.auth \n true \n Enable SMTP authentication? \n \n START TLS \n mail.smtp.starttls.enable \n true \n Use start-TLS? \n \n SSL ENABLE \n mail.smtp.ssl.enable \n false \n Use SSL right from the start? \n \n USER \n mail.smtp.user \n \n s \n Username for SMTP authentication \n \n USER PASSWORD \n \n mail.smtp.pw \n ******** \n Password for SMTP authentication \n \n HOST \n mail.smtp.host \n \n smtp.gmail.com \n Host for SMTP server \n \n FROM USER \n mail.smtp.from \n someone@example.com \n This sets the From header \n \n LOCALHOST \n mail.smtp.localhost \n \n e \n A domain name for apparent sender; must have at least one dot \n \n DEBUG \n mail.smtp.debug \n false \n Enable debug mode? Set to true (temporarily) to see more details in logs. Debug mode under \"Advance SMTP\" section Unravel daemons to restart after email setup Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_tc restart \nsudo \/etc\/init.d\/unravel_all.sh stop-etl\nsudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n--> enter password from dist.unraveldata.com\n--> should see following output in terminal mode and if you see \"result is = null\", then, setup is correct.\n:\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add following properties to \/usr\/local\/unravel\/unravel.properties on Unravel Server: mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Disable unneeded daemons: sudo service unravel_os3 stop\nsudo chkconfig unravel_os3 off Restart daemons: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Spark Properties for Spark Work daemon @ Unravel", 
"url" : "unravel-4-2/advanced-topics/configurations/custom-configurations/spark-properties-for-spark-work-daemon---unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Work daemon @ Unravel", 
"snippet" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an ap...", 
"body" : "Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:_1,_1.lz4,_1.snappy,_1.inprogress,,.lz4,.snappy, .inprogress,_2,_2.lz4,_2.snappy,_2.inprogress com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) Executor log processing Property Definition Default com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) Tagging Property Definition Default com.unraveldata.tagging.enabled Enables tagging functionality. True com.unraveldata.tagging.script.enabled Enables tagging. False com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False EMR\/HDInsight specific properties Property Definition Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False True Wasb block storage specific properties (for HDInsight): The following properties are set with information obtained from Microsoft Azure. MiFor a storage account with the name storagename storagename blob.core.windows.net Property Definition Default com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Login in to Azure. How to locate your storage account name At the Azure Dashboard Select Storage Accounts Copy the Storage Name ( STORAGE_NAME) fs.azure.account.key. STORAGE_NAME blob.core.windows.net com.unraveldata.hdinsight.storage-account-name-1 com.unraveldata.hdinsight.storage-account-name-2 How to locate your access keys. From the home page select Storage Accounts Access Keys. key1 com.unraveldata.hdinsight.primary-access-ke key2 com.unraveldata.hdinsight.secondary-access-key =<key2>. S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g. , \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_ke aws_secret_access_key access_key\/secretKey - Data Lake (ADL) specific data properties The following properties are set with information obtained from the ADL properties in Microsoft Azure. For further information on how to retrieve these Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster “spk21utj02” has been given access to “\/clusters\/spk21utj02” on Data Lake store. retrieve from Microsoft Azure Login in to Azure. How to locate your account Fully Qualified Domain Name Select Data Lake Store | Data Explorer Data Explorer Click on Folder properties Properties PATH FQDN com.unraveldata.adl.accountFQDN=<FQDN> How to locate your OAUTH 2.0 Token Endpoint, Application ID and Application Key. From Home page, select Azure Active Director App Registration. Select Endpoints. OAUTH 2.0 Token Endpoint com.unraveldata. adl.accessTokenEndpoint=<OAUTH 2.0 Token Endpoint> Locate Application ID and Application Key. Select View all applications, If your application is listed select it. If you need to create a new application select + New application registration Create Whether your selected an existing application or created a new one, you will see the the registered App Application ID com.unraveldata.adl.clientID=<Application ID>. Select Settings Keys. Description Duration. Save. Copy and save the key Value . Set com.unraveldata.adl.clientKey=<Key Value>. How to grant access to a newly create application. Select Data Lake Store | Data Explorer Select Access +Add Access Click OK Select Grant the permissions OK Your app is now listed under Assigned Permission " }, 
{ "title" : "Security Configurations", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/adding-ssl-and-tls-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_tc which listens on port 3000 in Enabling TLS to Unravel Web UI Directly These ste...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_tc which listens on port 3000 in Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4. Install needed packages. # sudo yum install httpd mod_ssl Note: There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf unravelhost_FQDN SSLCertificate* <VirtualHost *:80>\n ServerName unravelhost_FQDN unravelhost_FQDN unravelhost_FQDN \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt \/etc\/certs\/wildcard_unravelhost_RSA_private.key\n \/etc\/certs\/IntermediateCA.crt Adjust or add com.unraveldata.advertised.url \/usr\/local\/unravel\/etc\/unravel.properties :port# com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_tc # sudo service unravel_tc restart Start the httpd # sudo service httpd start Visit https:\/\/unravelhost_FQDN Troubleshooting To enable verbose logging in Apache2, add these lines. ProxyHTMLLogVerbose On\nLogLevel debug\n where LogLevel debug trace1 trace8 Note: Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ add this line after ServerName line in the virt. host httpd RequestHeader set X-FORWARDED-PROTO 'https'\n " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/alternate-kerberos-principal-for-cluster-access-on-cdh.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in Step 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in Step 2 of the installation guide. HIGHLIGHTED 1. Check HDFS Default umask For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see subdirectories and read files. Check the value of dfs.umaskmode and make sure the middle digit is 2 or 0. This controls the group mask and ACLs are masked using this default group mode. 2. Enable ACL Inheritance In Cloudera Manager, HDFS Configuration, search for namenode advanced configuration snippet dfs.namenode.posix.acl.inheritance.enabled hdfs-site.xml https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 3. Restart Cluster When you are ready, restart the cluster to effect a change in dfs.namenode.posix.acl.inheritance.enabled 4. Change ACL of Target HDFS Directories Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented). The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # set ACL for future directories\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse\n\n# set ACL for existing directories\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse 5. Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target users 'unravel' and 'hdfs', with a valid kerberos ticket corresponding to keytab principal (substitute your values for KEYTAB_FILE KEYTAB_FILE2 PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n\n# sudo -u hdfs kdestroy\n# sudo -u hdfs kinit -kt {KEYTAB_FILE2} {PRINCIPAL}\n# sudo -u hdfs hadoop fs -ls \/user\/history\n# sudo -u hdfs hadoop fs -ls \/tmp\/logs\n# sudo -u hdfs hadoop fs -ls \/user\/hive\/warehouse " }, 
{ "title" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/configure-permission-for-unravel-daemons-on-cdh-sentry-secured-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Permission for Unravel daemons on CDH Sentry Secured Cluster", 
"snippet" : "HDFS Permission The following instruction applies to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl expor...", 
"body" : "HDFS Permission The following instruction applies to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl export RUN_AS=$user_name HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH please see the Cloudera Documentation HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. \/\/ For TLS secured CM\n# curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\"\n\n\/\/ For no TLS secured CM\n# curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL Login to database host that hosting the hive metastore database as admin or root user who has grant privilege, and create a new database user and grant select privilege on the hive metastore database For MySQL, the following is the mysql commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, the following is the psql commands to create unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI Go to Manage Configuration HIVE : JDBC URL Hive Metastore URL use the pull-down to specify the type of database mysql or postgresql, your host, port, and name of your hive database. : JDBC driver Hive Metastore Driver : JDBC User name Hive Metastore User Name : JDBC password Hive Metastore Password " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/enabling-tls-to-unravel-web-ui-directly.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Here is how to directly enable TLS (SSL) to unravel_tc which is listening on port 3000. Alternatively, you can add an apache2 reverse proxy in Adding SSL and TLS to Unravel Web UI These steps work with Unravel 4.2.x and earlier. In 4.3 the mechanism will change. Note that you must take steps to reap...", 
"body" : "Here is how to directly enable TLS (SSL) to unravel_tc which is listening on port 3000. Alternatively, you can add an apache2 reverse proxy in Adding SSL and TLS to Unravel Web UI These steps work with Unravel 4.2.x and earlier. In 4.3 the mechanism will change. Note that you must take steps to reapply these changes after an upgrade, so be sure to save copies of the changed and added files. Insert this snippet of xml into \/usr\/local\/unravel\/conf\/server.xml.orig in place of the commented-out one containg <Connector port=\"8443\" : <Connector port=\"3003\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" SSLEnabled=\"true\"\n scheme=\"https\" secure=\"true\"\n address=\"0.0.0.0\"\n connectionTimeout=\"20000\" \n maxHttpHeaderSize=\"8192\"\n executor=\"tomcatThreadPool\"\n URIEncoding=\"UTF-8\" server=\"Unravel\/4.2\"\n enableLookups=\"false\" disableUploadTimeout=\"true\"\n socket.appWriteBufSize=\"32768\"\n acceptCount=\"90\" socket.processorCache=\"10\" socket.bufferPoolSize=\"1048576\" socket.bufferPool=\"10\" socket.keyCache=\"10\"\n allowTrace=\"false\" compression=\"on\" compressionMinSize=\"16384\"\n noCompressionUserAgents=\"gozilla, traviata\"\n compressableMimeType=\"text\/html,text\/xml,text\/plain\"\n clientAuth=\"false\" sslProtocol=\"TLSv1.2\"\n sslEnabledProtocols=\"TLSv1.2\" \n ciphers=\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDH_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDH_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDH_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDH_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDH_RSA_WITH_AES_256_CBC_SHA,TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDH_RSA_WITH_AES_128_CBC_SHA,TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA\"\n keyAlias=\"unravel.example.com\" \n keystoreFile=\"\/usr\/local\/unravel\/etc\/keystore.p12\" \n keystoreType=\"PKCS12\"\n keystorePass=\"verysecret\"\n \/> Note: you need to provide the PKCS12 keystore file in this example, specify the key store password you plan to set below, and specify the port you want used (3003 for example). Listening on port 443 is not supported directly, but could be implemented via iptables prerouting and redirect. The ciphers are listed explicitly in order to avoid allowing deprecated, weak encryption. Change the \/usr\/local\/unravel\/conf\/server.xml.orig Here we create a PKCS12 keystore, the most widely used keystore format. The inputs are expected to be the CA chain, the signed certificate, and the private server key in PKCS8 format (if in base64 \"PEM\" encoding, it will have \"BEGIN ENCRYPTED PRIVATE KEY\"). In this example, we refer to an imaginary domain unravel.example.com with a suggested file naming convention. Create a keystore.p12 file using openssl cat my_root_ca.cert IntermediateCA.crt unravel_example_com_certificate.crt > combined.crt\n\nopenssl pkcs12 -export -in unravel_example_com_certificate.crt \\\n -inkey unravel_example_com_RSA_private.key -out \/usr\/local\/unravel\/etc\/keystore.p12 \\\n -name unravel.example.com -CAfile combined.crt -caname root -chain\n\nchown unravel.unravel \/usr\/local\/unravel\/etc\/keystore.p12 Note: Substitute local values for my_root_ca.cert, IntermediateCA.crt unravel_example_com_certificate.crt example.com unravel_example_com_RSA_private.key as needed. For additional info see https:\/\/knowledge.rapidssl.com\/support\/ssl-certificate-support\/index?page=content&actp=CROSSLINK&id=SO17070 Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/unravel.example.com:3003 Restart Unravel web UI: sudo service unravel_tc restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/encrypting-passwords-in-unravel-properties-and-settings.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you pre...", 
"body" : "A command-line utility is provided that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Here is an example run of pw_encrypt.sh sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter (Return key), it will emit something like: ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties Manage Configuration How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/integrating-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use Hi...", 
"body" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use HiveServer2 LDAP, then follow the steps below. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties Change QA ldap:\/\/LDAP_HOST For QA For ldap:\/\/LDAP_HOST ldaps:\/\/LDAP_HOST unravel\/jre\/ ldap:\/\/ldap_host:9999 For Active Directory (AD), example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.Domain=QA Change LDAP_HOST and QA (use the Microsoft \"domain\", not DNS) to appropriate value for your installation. For Active Directory (AD) with base DN defined, example 2: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldaps:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myou,dc=domain,dc=com\nhive.server2.authentication.ldap.guidKey=uid Change LDAP_HOST, ou=myou,dc=domain,dc=com to appropriate value for your installation. For guidKey, the default username attribute in windows AD is uid but could also be sAMAccountName . For Open LDAP, example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 2: In this example, we expect a typical DN to be uid=%s,ou=myunit,dc=example,dc=com where %s is the login name as typed in the login form. In some cases 'cn' is used in place of 'uid'. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 3: In this example, we use a manager or admin account to bind, and then search for user logins. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.search_bind_authentication=true\ncom.unraveldata.ldap.base=ou=myunit,dc=example,dc=com\ncom.unraveldata.ldap.bind_dn=cn=Manager,dc=example,dc=com\ncom.unraveldata.ldap.bind_pw=????????\ncom.unraveldata.ldap.url=ldap:\/\/localhost:389\ncom.unraveldata.ldap.user_name_attr=uid Change the example ou=myunit,dc=example,dc=com , bind_pw, url, bind_dn and base to appropriate values for your installation. 2. Restart unravel_tc Restart unravel_tc # sudo \/etc\/init.d\/unravel_tc restart Advanced Hive Properties Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The process of authentication is described next. Authentication Process for Active Directory (AD) Bind as username + at sign + domain, using the given password, with simple LDAP auth mode verbose log will show Connecting Connected If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group filter is specified, it is checked if a user filter is specified, it is checked Authentication Process for Open LDAP Bind as cn or uid =username + baseDN using the given password, with simple LDAP auth mode the guidKey property determines whether cn or uid is used if userDNPattern is used, it takes precedence over baseDN, and each pattern is tried If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group pattern or filter is specified, it is checked if a user filter is specified, it is checked Property Description Example Value hive.server2.authentication.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. REQUIRED for advanced query except when setting custom query or groupDNPattern. uid or CN hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). hiveuser,impalauser,hiveadmin,hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member, uniqueMember or memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group, groupOfNames or groupOfUniqueNames com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_tc_webapp.log file under \/usr\/local\/unravel\/logs\/ ; when enabled, user names and group names can appear in this log, but raw passwords are not logged. Can be true or false or not set; default is false " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "unravel-4-2/advanced-topics/configurations/security-configurations/using-a-private-certificate-authority-with-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with sudo \/etc\/init.d\/unravel_all.sh restart Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: chmod 444 \/path\/to\/jks_keystore\nln -s \/path\/to\/jks_keystore \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Substitute the path for your local settings for \/path\/to\/jks_keystore and ensure that the target file is updated whenever your CA certificates are updated. Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts or jssecacerts file it contains is up-to-date. If this is convenient, you can edit \/usr\/local\/unravel\/etc\/unravel.ext.sh For example: export JAVA_HOME \/usr\/java\/jdk1.8 Substitute your local settings for \/usr\/java\/jdk1.8 Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts to jssecacerts so that an upgrade of Unravel will preserve your change: cd \/usr\/local\/unravel\/jre\/lib\/security\nsudo cp -p cacerts jssecacerts List contents of the jssecacerts keystore: sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias mycompanyca -file something.cer Substitute your local values for mycompanyca and something.cer when you execute this command. " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "unravel-4-2/advanced-topics/connecting-to-configuration-of-a-kafka-stream.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI....", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. " }, 
{ "title" : "For CDH+CM", 
"url" : "unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForCDHCM", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For CDH+CM", 
"snippet" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the RES...", 
"body" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/CMGR_HOSTNAME_IP:7180\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore In Unravel Web UI, on the top right-hand corner, click Admin Manage On the left tab, click Hive Encrypting Passwords in Unravel Properties and Settings HIVE METASTORE URL HIVE METASTORE DRIVER HIVE METASTORE USER NAME HIVE METASTORE PASSWORD Save the information when done: click Save Changes Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in the Application " }, 
{ "title" : "For HDP", 
"url" : "unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForHDP", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For HDP", 
"snippet" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "For MapR", 
"url" : "unravel-4-2/advanced-topics/connecting-to-a-hive-metastore.html#UUID-6aa93491-2cc2-a769-8364-282fc0d9f6bd_id_ConnectingtoaHiveMetastore-ForMapR", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For MapR", 
"snippet" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : "See Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "unravel-4-2/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-6722163d-6640-62dc-b702-c6cbf4e619b8_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrat...", 
"body" : "Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires Uncheck Password must be changed. Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "unravel-4-2/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "unravel-4-2/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-36fe406d-0d0c-e524-e55f-71f023a65295_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storag...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for \" Alarms\" Click on \" Create Alarm On the right-hand section under \" RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database FreeStorageSpace Next In \" Alarm Threshold \" Name \" Description Add free storage of 20% left to alert contact under \" Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for \" 10 consecutive period(s) Under Actions Send notifications to Note: this sns topic should already be setup before you add it. Click \" Create Alarm Now, you will see in \" Alarms ALARM Alarms Click \" Create Alarm advanced-topics-create-aws-rds-cloudwatch-alarm-for-free-storage-space " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/advanced-topics/empty-or-missing-topic-4887.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "UPDATE_NEEDED_FIX_EXTERNAL LNKS_TO_4.2 This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "\n UPDATE_NEEDED_FIX_EXTERNAL LNKS_TO_4.2 This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel ...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "unravel-4-2/advanced-topics/running-verification-scripts-and-benchmarks.html#UUID-ecf079fb-aa8d-28be-42c2-aeb42e425f9d_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages \n \n \n Package Name \n Location \n \n Benchmarks 1.6.x \n \n https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz \n \n Benchmarks 2.0.x \n \n https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where {LOCATION} is full pathname of the benchmark (see above) and {FNAME} is the package name. # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz\n\n71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , \n cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ \n lib: \n scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh \n src: \n tpch-query-instances: \n cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. \n Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \n \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. \n \n example3 \n A Scala program containing a SparkSQL \n How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. \n \n example4 \n A Scala-based application. This application generates its input and applies multiple transformations to the generated data. \n How Unravel helps to root-cause a failed \n \n example5 \n A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group \n Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only \n Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x \n \n \n Example \n Description \n Demonstrates \n \n example1 \n see example1 in Benchmarks for Spark 1.6.x \n \n example2 \n A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce \n How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. \n \n example3 example4 example5 \n see example3 - example5 in Benchmarks for Spark 1.6.x \n \n example6 \n A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. \n How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. " }, 
{ "title" : "Supported Roles", 
"url" : "unravel-4-2/advanced-topics/supported-roles.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel currently supports the following roles:", 
"url" : "unravel-4-2/advanced-topics/supported-roles.html#UUID-6445e693-14e6-dcf5-6e8d-4949d84c672c_id_SupportedRoles-Unravelcurrentlysupportsthefollowingroles", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles \/ Unravel currently supports the following roles:", 
"snippet" : "Full Admin This role is for operators. It provides access to all features and the ability to invoke every action within Unravel, including setting up Auto Actions, moving jobs to different queues, etc. You have full and complete access to the \"Admin\" pages. Read-only This role is for end-users, e.g....", 
"body" : "Full Admin This role is for operators. It provides access to all features and the ability to invoke every action within Unravel, including setting up Auto Actions, moving jobs to different queues, etc. You have full and complete access to the \"Admin\" pages. Read-only This role is for end-users, e.g., Data Scientists\/Engineers. Access to Unravel features is available in \"read-only\" mode. The \"Admin\" pages are not accessible. " }, 
{ "title" : "Adding Users and Specifying Roles", 
"url" : "unravel-4-2/advanced-topics/supported-roles.html#UUID-6445e693-14e6-dcf5-6e8d-4949d84c672c_id_SupportedRoles-AddingUsersandSpecifyingRoles", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Supported Roles \/ Adding Users and Specifying Roles", 
"snippet" : "To add Admins see Adding More Admins to Unravel Web UI. To set up integration with your LDAP in order to easily specify a user's role see Integrating LDAP Authentication of Unravel Web UI....", 
"body" : "To add Admins see Adding More Admins to Unravel Web UI. To set up integration with your LDAP in order to easily specify a user's role see Integrating LDAP Authentication of Unravel Web UI. " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel Sensors", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/installing-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/installing-sensors/installing-unravel-sensor-for-individual-applications-submitted-through-spark-submit.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute you...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Within this directory, locate the sensor file: unravel-agent-pack-bin.zip 2. Run the Sensor to Intercept Spark Apps Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp mkdir $UNRAVEL_SENSOR_PATH \ncd $UNRAVEL_SENSOR_PATH \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip\ncd $UNRAVEL_SENSOR_PATH \nhdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\nset UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace # UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR : Parent directory of the Unravel Sensor .zip file, #UNRAVEL_SENSOR_PATH unravel-agent-pack-bin.zip UNRAVEL_SENSOR_PATH : IP address and port of the #UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. #SPARK_EVENT_LOG_DIR : #PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit #SPARK_VERSION:Spark version to be instrumented. Valid options are 1.3 1.5 1.6 2.0 For example, export UNRAVEL_SENSOR_PATH=#UNRAVEL_SENSOR_PATH\nexport UNRAVEL_SERVER_IP_PORT=#UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=#SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=#PATH_TO_SPARK_EXAMPLE_JAR\nexport SPARK_VERSION=#SPARK_VERSION\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. Option B: If You Run Spark Apps in yarn-client Mode To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/unravel-agent-pack-bin.zip\nunzip unravel-agent-pack-bin.zip Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit To use the example below, be sure to replace # UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR # UNRAVEL_SENSOR_PATH UNRAVEL_SENSOR_PATH # UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 # SPARK_EVENT_LOG_DIR # PATH_TO_SPARK_EXAMPLE_JAR Absolute spark-submit spark-examples.jar #UNZIPPED_ARCHIVE_DEST: Directory on the local file system that contains the unzipped content of unravel-agent-pack-bin.zip unravel-agent-pack-bin.zip #SPARK_VERSION:Spark version to be instrumented. Valid options are 1.3 1.5 1.6 2.0 For example, export UNZIPPED_ARCHIVE_DEST=#UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=#UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=#SPARK_EVENT_LOG_DIR\nexport PATH_TO_SPARK_EXAMPLE_JAR=#PATH_TO_SPARK_EXAMPLE_JAR\nexport SPARK_VERSION=#SPARK_VERSION\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Note that a full blank line separates lengthy lines that are wrapped, except for the spark-submit that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Hive Queries", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/installing-sensors/installing-unravel-sensor-for-individual-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Sensors \/ Installing Unravel Sensor for Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip text and text with brackets ( { } ), unless otherwise noted, indicates where you must...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set \nmapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043; " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel\nsudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Unravel Servers and Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Uploading the Spark Program(s) to Unravel", 
"url" : "unravel-4-2/advanced-topics/unravel-servers-and-sensors/uploading-the-spark-program-s--to-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading the Spark Program(s) to Unravel", 
"snippet" : "Unravel has the capability to upload Spark program(s) to the Unravel UI. Currently we support the uploading of Java, Scala and Python source code. We do not currently support JVM byte code. HIGHLIGHTED In order to upload a program you must: Package all relevant sources\/program files into a zip archi...", 
"body" : "Unravel has the capability to upload Spark program(s) to the Unravel UI. Currently we support the uploading of Java, Scala and Python source code. We do not currently support JVM byte code. HIGHLIGHTED In order to upload a program you must: Package all relevant sources\/program files into a zip archive. It is advisable to keep the archive small by including only the relevant Driver source files. Provide the following information in a spark-submit zip.archive \/home\/adrian\/benchmarks\/scripts\/sources.zip --files zip.archive --conf spark.unravel.program.zip zip.archive the path the to Spark Example Jar Example: FULLY_QUALIFIED_ZIP_PATH FULLY_QUALIFIED_JAR_PATH export PROGRAM_FILES_ZIP={FULLY_QUALIFIED_ZIP_PATH} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $PROGRAM_FILES_ZIP \\\n --conf \"spark.unravel.program.zip=$PROGRAM_FILES_ZIP\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR After the Spark application has completed, we can visualize the Spark program(s) under the Program tab, which is situated on the right hand side of the application page. Unravel also highlights the line of code corresponding to a given RDD node of the execution graph when a RDD node is selected (by clicking on it). In the illustrative example shown below, MapPartitionsRDD was evaluated on line 324 of QueryDriver.scala. " }, 
{ "title" : "Using an External MySQL or Compatible Database for Unravel", 
"url" : "unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Compatibility", 
"url" : "unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-Compatibility", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Compatibility", 
"snippet" : "Unravel default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server uses the mariadb connector. The following versions of MySQL are known to work: MySQL 5.5...", 
"body" : "Unravel default installation uses a bundled database for part of it's storage. For production or logistical or support reasons, it can be desirable to use an external database. We recommend MySQL. Unravel server uses the mariadb connector. The following versions of MySQL are known to work: MySQL 5.5.x or AWS RDS or MariaDB or Percona equivalent The following MySQL versions are NOT yet certified by our QA department: MySQL 5.6.x MySQL 5.7.x Replication is not required, but it can be used in order to support online backup via mysqldump or LVM or Percona XtraBackup. Unravel will only communicate with the master. " }, 
{ "title" : "Configuration Requirements for MySQL", 
"url" : "unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-ConfigurationRequirementsforMySQL", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Configuration Requirements for MySQL", 
"snippet" : "Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size= 64M thre...", 
"body" : " Innodb storage engine with file-per-table (set below) if replication is used, set MIXED bin log type (set below) mysqld section of cnf file (possibly \/etc\/my.cnf.d\/server.cnf) or equivalent in the cloud key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size= 64M thread_concurrency = 6 max_connections = 500 max_connect_errors=2000000000 open_files_limit=10000 port-open-timeout=121 expire-logs-days=2 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_open_files=2000 innodb_file_per_table=1 innodb_data_file_path = ibdata1:100M:autoextend innodb_buffer_pool_size = 1G innodb_flush_method=O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency=20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format=mixed if SSD disk is used, then also set: innodb_io_capacity = 4000 Note The innodb_buffer_pool_size depends on load and cluster size. On a dedicated machine, it can be 50% of the RAM size. Using 1G is absolute minimum. For a large cluster we use 48G. " }, 
{ "title" : "Set Up Steps", 
"url" : "unravel-4-2/advanced-topics/using-an-external-mysql-or-compatible-database-for-unravel.html#UUID-88b9621d-c34b-b587-9527-1f58253780b8_id_UsinganExternalMySQLorCompatibleDatabaseforUnravel-SetUpSteps", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Using an External MySQL or Compatible Database for Unravel \/ Set Up Steps", 
"snippet" : "Install the database Setting Retention Verify that the database host Create \" unravel \" user Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c8)\" Log into your mysql instance as admin\/master user and do the following commands, substituting above { DB_...", 
"body" : " Install the database Setting Retention Verify that the database host Create \" unravel \" user Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c8)\" Log into your mysql instance as admin\/master user and do the following commands, substituting above { DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into the mysql using the mysql commandline client as user unravel DB_PASSWORD Stop Unravel server: # sudo \/etc\/init.d\/unravel_all.sh stop Configure unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Modify the property lines below so that they reflect your particular values: unravel.jdbc.username=unravel\nunravel.jdbc.password=ENC(QioHJsum9rmqKROG0DRqbU51)\nunravel.jdbc.url=jdbc:mysql:\/\/10.0.0.99:3306\/unravel_mysql_prod\n Use the actual values your set in the steps above. Use an ip address if you want to avoid DNS lookups, but hostname is also okay. The database password can be encrypted Dump Bundled DB with Schema On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \\\n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \\\n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \\\n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \\\n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into MySQL Load the initial db with schema into the MySQL instance, substituting HOST unravel # gunzip --stdout {DEST_FILE} | \/usr\/local\/unravel\/mysql\/bin\/mysql --host={HOST} -u unravel -p --port=3306 --force unravel_mysql_prod Disable bundled db in Unravel server: # sudo chkconfig unravel_db off Start Unravel server: # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Workflows", 
"url" : "unravel-4-2/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-2/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web ser...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web service are in the same cluster. If You Use Http For Airflow Web UIAccess Add the following 3 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace { airflow_web_url com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= airflow_web_url com.unraveldata.airflow.available=true Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart If You Use Https For Airflow Web UIAccess Add the following 4 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace { airflow_web_url com.unraveldata.airflow.server.url= airflow_web_url com.unraveldata.airflow.available=true Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart If You Enabled login security For Airflow Web UIAccess com.unraveldata.airflow.login.name= airflow_web_UI_UserName com.unraveldata.airflow.login.password={airflow_web_UI_password} Then restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Range of Monitoring By default, Unravel Server ingests all the workflows that started within the last 5 days. If you wish to change the date range to the last {X} Add the following configuration to \/usr\/local\/unravel\/etc\/unravel.properties airflow.look.back.num.days=-{X} Restartthe unravel_jcs2 # sudo \/etc\/init.d\/unravel_jcs2 restart " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-2/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import PythonOperator from da...", 
"body" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Oozie\/Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n 1. Add Hooks for Unravel Instrumentation The example below shows the contents of a bash script, example-hdp-client.sh spark-submit spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport Setting these parameters on a per-application spark-defaults.conf This script references the following variables, which you would need to edit: PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR 2. Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call 2016-08-30 3. Monitor the Workflow To see the new Oozie workflow in Unravel Web UI, select APPLICATIONS Workflows Add Workflow " }, 
{ "title" : "Empty or missing topic", 
"url" : "unravel-4-2/advanced-topics/workflows/empty-or-missing-topic.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Advanced Topics \/ Workflows \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Appendices", 
"url" : "unravel-4-2/appendices.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices", 
"snippet" : "Event List Resource Metrics Some Keywords and Error Messages Unravel Server Reference...", 
"body" : " Event List Resource Metrics Some Keywords and Error Messages Unravel Server Reference " }, 
{ "title" : "Event List", 
"url" : "unravel-4-2/appendices/event-list.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Event List", 
"snippet" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception HIVE Application failure HIveFailureIncorrectHea...", 
"body" : "The table below lists the events generated by Unravel. App Type Event Category Event Name Event Type and Description of When it Occurs HIVE Application failure HiveFailureBrokenPipeEvent Indicates that the Hive query fails with \"broken pipe\" exception HIVE Application failure HIveFailureIncorrectHeaderEvent Indicates that the Hive query fails with \"incorrect header check\" exception HIVE Application failure HiveFailureReturnCodeEvent Indicates that the Hive query fails and shows return code HIVE Application failure HiveMapJoinMemoryExhaustionEvent Indicates that the Hive query fails because it is out of memory in map join, and recommends turning off mapjoin HIVE Application failure HiveOutOfMemoryErrorEvent Indicates that the Hive query fails because it is out of memory HIVE Informational HiveKillFailEvent Indicates that the Hive query is killed or failed with lots of wasted work HIVE Informational HiveSuccWithKillFailEvent Indicates that the Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources HIVE Informational HiveShuffleBytesEvent Indicates that the Hive query has lots of data shuffle from map to reduce side HIVE Informational HiveTimeBreakdownEvent Identifies where time is spent on for the query and points out significant events, including MR-level skew events HIVE Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true HIVE Speedup HiveSingleReduceCountDistinct Indicates that the Hive query has a job with a long single reducer because the query has \"count distinct\" HIVE Speedup HiveSingleReduceLongWait Indicates that the Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side HIVE Speedup HiveSingleReduceOrderBy Indicates that the Hive query has a job with a long single reducer because the query has \"order by\" HIVE Speedup HiveTooFewReduceEvent Indicates that the Hive query is using too few reducers HIVE Speedup\/Resource Utilization HiveTooLargeMapEvent Indicates that mappers in the Hive query are requesting too much memory HIVE Speedup\/Resource Utilization HiveTooLargeReduceEvent Indicates that reducers in the Hive query are requesting too much memory HIVE Speedup\/Resource Utilization HiveTooManyMapEvent Indicates that the Hive query is using too many mappers HIVE Speedup\/Resource Utilization HiveTooManyReduceEvent Indicates that the Hive query is using too many reducers IMPALA Informational\/ Speedup\/ Resource Utilization BottleneckOperatorEvent Displays the longest phase in the Impala query Displays the longest operator in the Impala query. If applicable, this event also shows insights as to why this operator takes the most time, and makes tuning suggestions IMPALA Application failure ImpalaFailureEvent Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible) IMPALA Speedup ImpalaSkewExecutionEvent Indicates whether one of the instances takes much longer time than the other instances for the same operator IMPALA Speedup ImpalaUnderestimatedCounfOfRowsEvent Indicates that an estimate is outdated and should be refreshed MR Application Failure MRClassNotFoundEvent Indicates that MR job fails due to a \"class not found\" exception MR Application Failure MRFailureCompressLibNotAvailable Indicates that MR job fails due to a \"compression library not available\" exception MR Application Failure MRFailureFileNotFoundEvent Indicates that MR job fails due to a \"file not found\" exception MR Application Failure MRFailureIllegalArgumentEvent Indicates that MR job fails due to an \"illegal argument\" exception MR Application Failure MRFailureNumberFormatEvent Indicates that MR job fails due to a \"number format\" exception MR Application Failure MRFailureTimeOutEvent Indicates that MR job fails due to a \"time out\" exception MR Application Failure MRGcOverheadLimitExceededMapEvent Indicates that the MR job fails because the GC overhead limit is exceeded on map side MR Application Failure MRGcOverheadLimitExceededReduceEvent Indicates that the MR job fails because the GC overhead limit is exceeded on reduce side MR Application Failure MRJavaOutOfMemoryMapEvent Indicates that MR job fails because it is out of memory on map side MR Application Failure MRJavaOutOfMemoryReduceEvent Indicates that MR job fails because it is out of memory on reduce side MR Informational MRKillFailEvent Indicates that the MR job is killed or failed with lots of wasted work MR Informational MRSuccWithKillFailEvent Indicates that the MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources MR Informational MRShuffleBytesEvent Indicates that the MR job has lots of data shuffle from map to reduce side MR Informational MRTimeBreakdownEvent Identifies where time is spent on for the job and points out significant events MR Speedup MRLongTasksFromSlowNodeEvent Indicates that the MR job has long-running map\/reduce tasks from slow nodes MR Speedup MRMapSkewDataIOEvent Indicates that the map phase of the MR job has a time skew with strong correlation with IO MR Speedup MRReduceSkewDataIOEvent Indicates that the reduce phase of the MR job has a time skew with strong correlation with IO MR Speedup MRTooFewReduceEvent Indicates that the MR job is using too few reducers MR Speedup\/Resource Utilization MRTooLargeMapEvent Indicates that mappers in the MR job are requesting too much memory MR Speedup\/Resource Utilization MRTooLargeReduceEvent Indicates that reducers in the MR job are requesting too much memory MR Speedup\/Resource Utilization MRTooManyMapEvent Indicates that the MR job is using too many mappers MR Speedup\/Resource Utilization MRTooManyReduceEvent Indicates that the MR job is using too many reducers SPARK Application Failure DriverOomeEvent Indicates that a driver failed with \"OutOfMemory\" error SPARK Application Failure ExecutorOomeEvent Indicates that an executor failed with \"OutOfMemory\" error SPARK Application Failure YarnContainerKilledEvent Indicates that there are containers killed by YARN SPARK Resource Utilization ContainerSizingUnderutiliztionEvent Indicates that container resources are underutilized SPARK Resource Utilization InefficientInputSplitSizeEvent Indicates an inefficient input split size SPARK Resource Utilization TooFewPartitionsEvent Indicates that there are too few partitions with respect to available parallelism SPARK Resource Utilization UnderutilizedCpuEvent Indicates that there is low utilization of CPU resources SPARK Resource Utilization UnderutilizedNodeMemoryEvent Indicates that there is low utilization of memory resources SPARK Resource Utilization UnderutilizedStorageMemoryEvent Indicates that the Spark storage memory has low utilization. More RDDs can be cached in memory SPARK Speedup CachingOpportunityEvent Indicates that there is an (unused) opportunity for RDD caching SPARK Speedup ContendedCpuEvent Indicates that there is contention for CPU resources SPARK Speedup ExcessiveGcEvent Indicates that there is high garbage collection overhead SPARK Speedup ExecutorImbalanceEvent Indicates that there is load imbalance among executors SPARK Speedup ExhaustedStorageEvent Indicates that the Spark storage memory is getting exhausted SPARK Speedup LightExecutorEvent Indicates that there is large idle time for one or several executors SPARK Speedup LongStageEvent Indicates that there is load imbalance among tasks for the longest stage of the application Workflow Informational WorkflowTimeBreakdownEvent Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. Workflow Informational\/ Application Failure WorkflowGeneralFailureEvent For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. Workflow Informational\/ Resource Utilization WorkflowIRSummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources Workflow Informational\/SLA analysis WorkflowDurationAnomalousEvent If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. Workflow Informational\/ Speedup WorkflowIASummaryEvent Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time TEZ Resource Utilization\/Inefficiency TezNoDAGEvent Indicates that the TEZ session was created and TEZ DAG was not submitted. TEZ Speedup HiveExecuteParallelEvent Detects that hive.exec.parallel false true TEZ Speedup\/Resource Utilization MapVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few mapper task TEZ Speedup\/Resource Utilization MapVertexTooManyTaskEvent Indicates that the TEZ DAG is using too few mapper task TEZ Speedup\/Resource Utilization ReduceVertexTooFewTaskEvent Indicates that the TEZ DAG is using too few reducers task TEZ Speedup\/Resource Utilization ReduceVertexTooManyTaskEvent Indicates that the TEZ DAG is using too many reducers task Note: In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-2/appendices/resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time w...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started totalPhysicalMemory bytes The total physical memory in the operating system freePhysicalMemory bytes The free physical memory in the operating system committedVirtualMemory bytes The committed virtual memory in the operating system freeSwap bytes The free swap size availableMemory bytes An estimate of memory available for launching new processes vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process totalSwap bytes The total swap size processCpuLoad PERCENT Average process CPU load for the last minute (all cores) systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) fullGcCount COUNT Number of full GC runs minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms allocatedBytes bytes Accumulated number of allocated bytes edenPeakUsage bytes Maximum memory usage in the eden space survivorPeakUsage bytes Maximum memory usage in the survivor space oldPeakUsage bytes Maximum memory usage in the old space avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms initHeap bytes Initial heap size maxHeap bytes Maximum heap size usedHeap bytes Used heap size committedHeap bytes Committed heap size initNonHeap bytes Initial non-heap size maxNonHeap bytes Maximum non-heap size usedNonHeap bytes Used non-heap size committedNonHeap bytes Committed non-heap size currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "unravel-4-2/appendices/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type....", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. " }, 
{ "title" : "Spark Keywords", 
"url" : "unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-SparkKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Keywords", 
"snippet" : "Spark Key Term Explanation spark.executor.memory Related to executor memory spark.default.parallelism Default number of partitions spark.yarn.executor.memoryOverhead YARN memory overhead spark.sql.shuffle.partitions Number of SparkSQL partitions spark.dynamicAllocation.enabled Enables dynamic alloca...", 
"body" : " Spark Key Term Explanation spark.executor.memory Related to executor memory spark.default.parallelism Default number of partitions spark.yarn.executor.memoryOverhead YARN memory overhead spark.sql.shuffle.partitions Number of SparkSQL partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Executor Process launched by the application on a worker node Driver Process that coordinates the application execution SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SQLContext Main Spark SQL entry point SparkConf Spark configuration object StreamingContext Main Spark Streaming entry point " }, 
{ "title" : "Spark Error Messages", 
"url" : "unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-SparkErrorMessages", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ Spark Error Messages", 
"snippet" : "Spark Error Messages Explanation java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" sho...", 
"body" : " Spark Error Messages Explanation java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow " }, 
{ "title" : "MapReduce\/Hive Keywords", 
"url" : "unravel-4-2/appendices/some-keywords-and-error-messages.html#UUID-8a389c18-dbb6-cb25-5e27-6744794543ef_id_SomeKeywordsandErrorMessages-MapReduceHiveKeywords", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Some Keywords and Error Messages \/ MapReduce\/Hive Keywords", 
"snippet" : "Key Term Explanation mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into hive.exec.reducers.bytes.per.reducer Size per reducer mapreduce.job.reduces Default num...", 
"body" : " Key Term Explanation mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into hive.exec.reducers.bytes.per.reducer Size per reducer mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. hive.exec.parallel Whether to execute jobs in parallel. " }, 
{ "title" : "Unravel Server Reference", 
"url" : "unravel-4-2/appendices/unravel-server-reference.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference", 
"snippet" : "install-hdi-unravel-vm This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "install-hdi-unravel-vm This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_ngui REST API unravel_db bundled db (on a custom port) unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_ngui REST API unravel_db bundled db (on a custom port) unravel_zk_N bundled Zookeeper (on a custom port) unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_hhs Hive Hook Sensor unravel_hhwe Hive Hook Worker EMR unravel_jcs1 Job Collector Sensor MRv1 unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw1_N Job Collector Sensor Worker MRv1 unravel_jcw2_N Job Collector Sensor Worker YARN unravel_lr Log Receiver unravel_ja \"Job Analyzer\" summarizes jobs unravel_s_N Search engine unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_os3 Oozie v3 Sensor unravel_os4 Oozie v4 Sensor unravel_tw Table Worker unravel_pw Partition Worker unravel_ew_N Event Worker unravel_sw_N Spark Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/u...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/local\/unravel\/logs Do not set directly; set UNRAVEL_LOG_DIR in etc\/unravel.ext.sh instead and this property will be derived from that com.unraveldata.zk.quorum 127.0.0.1:4181 embedded Zookeeper ensemble in form host1:port1,host2:port2, … com.unraveldata.kafka.broker_list 127.0.0.1:4091 embedded Kafka unravel.jdbc.username unravel MySQL (embedded or external) username for db unravel.jdbc.password random generated for bundled MySQL MySQL (embedded or external) password for db unravel.jdbc.url jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod This is JDBC URL without username and password com.unraveldata.hdfs.interactive.monitoring.interval.sec 30 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) com.unraveldata.hdfs.batch.monitoring.interval.sec 300 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) com.unraveldata.longest.job.duration.days 2 Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) oozie.server.url http:\/\/localhost:11000\/oozie URL for accessing Oozie to track workflows com.unraveldata.oozie.fetch.num 100 Max number of jobs to fetch during an interval com.unraveldata.oozie.fetch.interval.sec 120 seconds between intervals for fetching Oozie workflow status " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset L...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm JAVA_EXT_OPTS unset Last chance arguments to jvm to override other settings HADOOP_CONF as discovered by running \"hadoop fs -ls \" The directory containing the hadoop config files core-site.xml, hdfs-site.xml, and mapred-site.xml UNRAVEL_DATA_DIR \/srv\/unravel A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data (db_data, k_data, zk_data) and also tmp data if property com.unraveldata.tmpdir is not set. UNRAVEL_LISTEN_PORT 3000 The Web UI port on the primary or standalone Unravel installation (service unravel_tc) which listens on 0.0.0.0 ; the property com.unraveldata.tc UNRAVEL_LOG_DIR \/usr\/local\/unravel\/logs A destination dir. owned by user unravel for log files UNRAVEL_TC_SHUTDOWN_PORT 3005 An unoccupied port used for cleanly stopping the Web UI (service unravel_tc) which listens on 127.0.0.1 " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-2/appendices/unravel-server-reference.html#UUID-5f6d115d-9eb4-570a-92cd-1e5d50e38fef_id_UnravelServerReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Appendices \/ Unravel Server Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs L...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~1.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file to preserve traceability \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME or other settings as shown in table above n\/a Optional; example syntax: export JAVA_HOME=\/path \/srv\/unravel\/log_hdfs log directory for daemons that run as user hdfs (for YARN, when applicable) <2GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR \/srv\/unravel\/tmp_hdfs tmp directory for daemons that run as user hdfs (for YARN, when applicable) <1GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR " }, 
{ "title" : "Unravel 4.2.x", 
"url" : "unravel-4-2/release-notes--4-2-x.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x", 
"snippet" : "Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7...", 
"body" : " Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7 " }, 
{ "title" : "Release Notes: Version 4.2", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Software Version", 
"snippet" : "On-premise RPM EMR RPM...", 
"body" : " On-premise RPM EMR RPM " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ New Features", 
"snippet" : "Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD...", 
"body" : " Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD " }, 
{ "title" : "Tested Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-TestedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Improvements\/Bugfixes", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2.html#UUID-28da5d0f-5313-5df4-67af-206bd5d80c15_id_ReleaseNotesVersion42-KnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Known Issues", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Release Notes: Version 4.2.3", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Software Version", 
"snippet" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM...", 
"body" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-HowtoDownload", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec " }, 
{ "title" : "Tested Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-TestedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional...", 
"body" : " Optional " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ New Features", 
"snippet" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed ac...", 
"body" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed acyclic graph (DAG). DAG details include query text, DAG graphs, and information on vertices, tasks, and attempts. For more information, see the User Guide Spark Pipeline This release includes an improved pipeline for collecting data from Spark applications. The pipeline provides: Additional metadata Stage level updates: Previously, the Spark APM in Unravel Web UI updated the application's page only once, when the application was finished. With this release, as soon as a stage completes, the Spark APM in Unravel Web UI shows stage level information. This improvement allows you to interact with the Spark APM more often. Smaller memory footprint Smarter events: The event generation algorithm has been updated with additional triggers. Events are generated only if an application's original suggested Workflow tagging: Unravel Web UI now displays tagged workflows identically, irrespective of the mode in which the Spark applications were loaded. Supported loading modes remain the same: OPS mode BATCH mode Applications are tagged even when the event log files are unavailable. Consistent display of information about each Spark application, even if the event log file is not available. At a minimum, a Spark application's page displays the application metadata, taken from the resource manager. Multi-Cluster Kafka Support This release supports data collection for multi-cluster Kafka topics, and Kafka application performance management through Unravel Web UI. Kafka support includes: Multi cluster support Multi cluster metrics monitoring Multi cluster consumer offset\/lag monitoring Consumer groups Single view for CG status\/stats across topics it consumes Additional metrics added for brokers Insights Tunable sliding window algorithm Consumer group lagging\/stalled " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Improvements and Bug Fixes", 
"snippet" : "Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries ...", 
"body" : " Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries insights improvements Improved time breakdown event Improved suggestions on hash joins Fixed the issue “Unravel not loading eventlog.inprogress file” Fixed the issue “Impala queries not coming up in UI” Fixed the issue “Page refresh changes the sorting order” Fixed the issue “High latency in loading MR jobs”.JCS2 now uses “hdfs ls” as opposed to “du” Fixed the issue “Issue with unravel_us_1 demon” ES migration script to migrate ES mappings for Impala (runs during upgrade of 4.1 or older version to 4.2) " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-KnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Known Issues", 
"snippet" : "Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support...", 
"body" : " Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support " }, 
{ "title" : "What's New", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-9d7d4a61-1948-9b92-f977-859825bf0b77_id_ReleaseNotesVersion423-WhatsNew", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ What's New", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Release Notes: Version 4.2.4", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Software Version", 
"snippet" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM...", 
"body" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-HowtoDownload", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4 " }, 
{ "title" : "Tested Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-TestedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Unravel Sensor Upgrade", 
"snippet" : "Yes ( See details below)...", 
"body" : " Yes ( See details below) " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-300ed05d-5940-1fe3-a8d5-53e289321799_id_ReleaseNotesVersion424-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Improvements and Bug Fixes", 
"snippet" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creat...", 
"body" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creating rules for Tez Apps AutoActions now can read Application Master job counter metrics and use it in Expert Auto Action rules (ref - https:\/\/hadoop.apache.org\/docs\/r2.4.1\/hadoop-yarn\/hadoop-yarn-site\/MapredAppMasterRest.html#Job_Counters_API Sensors FD leak and HttpUrlConnection issues fixed in spark sensor Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394 ) " }, 
{ "title" : "Release Notes: Version 4.2.5", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional One improvement in sensors; details under Improvements and Bug Fixes....", 
"body" : " Optional One improvement in sensors; details under Improvements and Bug Fixes. " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ New Features", 
"snippet" : "UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides reten...", 
"body" : " UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides retention for long running Spark apps (streaming, shells, notebooks) by keeping a maximum number of annotation entries inside the DB and ES. UNRAVEL-2429, UNRAVEL-2430: Support for showing both the Spark program and SQL query in the UI for a SparkSQL query.For applications with multiple queries added a table with KPIs sorted by query runtime to help identify the longest running query and the longest stage of a query. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-edbd4ceb-b056-7bde-04c6-9837b3d4fde7_id_ReleaseNotesVersion425-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server s...", 
"body" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server settings. Bug Fixes UNRAVEL-3070: Fixed memory leak in Spark Worker. UNRAVEL-3030: init script for mysql leaves a process owned by root running. UNRAVEL-3173: Fixed very rare but possible ConcurrentModificationException in Auto Action internal metric stream. UNRAVEL-3427: Fixed. In the cases that query memory metrics are not available, Impala event generator produces NPE. UNRAVEL-3229: Fixed Airflow http connection problem. Sensors UNRAVEL-2858 : Allows slowing down metrics collection in sensor agent. Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394). " }, 
{ "title" : "Release Notes: Version 4.2.6", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Software Version", 
"snippet" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Unravel Sensor Upgrade", 
"snippet" : "Not Required, recommended for high volume...", 
"body" : " Not Required, recommended for high volume " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ New Features", 
"snippet" : "UNRAVEL-3576: SLES Parcels support - beta...", 
"body" : " UNRAVEL-3576: SLES Parcels support - beta " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-37ab17a8-a7a4-e640-4ca1-ee29261aa8d9_id_ReleaseNotesVersion426-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM us...", 
"body" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM usage UNRAVEL-3552: collect only minimal set of resource usage metrics by default UNRAVEL-3543: Improved reliability in Unravel Impala worker Bug Fixes UNRAVEL-3520: For N\/A type Impala query, fixed NPE in missing fields in query profile Security Audit Issues UNRAVEL-3416: Security Fixes to prevent SQL injection UNRAVEL-3418: XSS - Cross-Site Scripting UNRAVEL-3444: State of settings from db lost in running processes if properties edited by hand UNRAVEL-3583: Email related fixes in Unravel Auto-Action UNRAVEL-3597: Improvements in Auto-Action page loading UNRAVEL-3536: Prevent jackson lib conflict in hive-hook sensor UNRAVEL-3546: Prevent re-submitting application data from unravel_es Known Issues Secure cookies and headers (UNRAVEL-3419, UNRAVEL-3420) " }, 
{ "title" : "Release Notes: Version 4.2.7", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Software Version", 
"snippet" : "Release 04\/11\/2018 For details on downloading updates see here...", 
"body" : "Release 04\/11\/2018 For details on downloading updates see here " }, 
{ "title" : "Certified Platforms", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix...", 
"body" : " CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Unravel Sensor Upgrade", 
"snippet" : "Recommended. See bug fixes below for further information....", 
"body" : " Recommended. See bug fixes below for further information. " }, 
{ "title" : "New Features", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ New Features", 
"snippet" : "TEZLLAP-5: TEZ APM support on CDH Platform...", 
"body" : " TEZLLAP-5: TEZ APM support on CDH Platform " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by defau...", 
"body" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by default. TEZLLAP-19: Hive Template Changes: Failed and killed Tez DAGs are linked with hive now. Bug Fixes Stability and Robustness Fixes across features Sensor Fixes for Robustness SENSOR-20: Improved sensor configuration problem detection routine SENSOR-17: Fixed RSS calculation for process tree USPARK-15, USPARK-16, USPARK-38 & USPARK-39: Spark Scalability Bug Fixes. Optimized configuration settings for Spark worker daemon. See New Configuration Settings below. UIX-7,UIX-8, UIX-14, UIX-20: Miscellaneous UI Bug Fixes dor stability and correctness. TEZLLAP-6, TEZLLAP-7, TEZLLAP-19 : UNRAVEL-3583, UNRAVEL-3598:Auto Action Bug Fixes Impala tagging via Python script is overridden by Impala Query Options on CDH 5.13 Repackaged embedded Jackson libs " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-KnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Known Issues", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "New configuration settings", 
"url" : "unravel-4-2/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-0793a0bd-202f-ea8b-8070-a7c3cfc62711_id_ReleaseNotesVersion427-Newconfigurationsettings", 
"breadcrumbs" : "Home \/ Unravel 4.2 \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ New configuration settings", 
"snippet" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveld...", 
"body" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveldata.spark.time.histogram " }, 
{ "title" : "", 
"url" : "unravel-4-0-4-1.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Unravel 4.0-4.1 •...", 
"body" : " Unravel 4.0-4.1 • " }, 
{ "title" : "Overview", 
"url" : "unravel-4-0-4-1/overview.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Overview", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Where Does Unravel Server Reside?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhereDoesUnravelServerReside", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Overview \/ Where Does Unravel Server Reside?", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. The Unravel Server on a Gateway\/Edge Node in a Hadoop Cluster " }, 
{ "title" : "What Does a Basic Deployment Provide?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhatDoesaBasicDeploymentProvide", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Overview \/ What Does a Basic Deployment Provide?", 
"snippet" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event stre...", 
"body" : "Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS, analyze this information, and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. The figure below illustrates the flow of information from the Hadoop cluster, to Unravel Server, to Unravel Web UI. Information Flow from the Hadoop Cluster to Unravel Server to Unravel Web UI " }, 
{ "title" : "What Are Advanced Deployment Options?", 
"url" : "unravel-4-0-4-1/overview.html#UUID-0e84ca9a-9b02-c616-0e95-74bb5a6ec45f_id_Overview-WhatAreAdvancedDeploymentOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Overview \/ What Are Advanced Deployment Options?", 
"snippet" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Unravel Sensor for Hive...", 
"body" : "After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Unravel Sensor for Hive " }, 
{ "title" : "Installation Guides", 
"url" : "unravel-4-0-4-1/install.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cloudera", 
"url" : "unravel-4-0-4-1/install/cloudera.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). This guide is compatible with CDH 4.5 - 5.10 Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 " }, 
{ "title" : "Part 1: Install Unravel Server on CDH+CM", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Conf...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For ...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility CDH 4.5 - 5.10 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions For CDH, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties All values in unravel.properties can be modified through the in Unravel UI's configuration wizard, for CDH. Furthermore, some properties can only Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 If Kerberos is Enabled: Add authentication for HDFS... (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties You can find the principal by using 'klist -kt KEYTAB_FILE' If Sentry is Enabled: Add these permissions... Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs read Spark event log hdfs:\/\/usr\/history\/done hdfs read MapReduce logs hdfs:\/\/tmp\/logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information Do Host-Specific Configuration Steps For CDH, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part1.html#UUID-38315e01-d866-1a7d-2357-153362c39700_id_Part1InstallUnravelServeronCDHCM-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server on CDH+CM \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel UI Configuration Wi...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Install Unravel Sensor Parcel on CDH+CM Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure the cluster. Install the Unravel Server RPM on the cluster. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Enable additional data collection\/instrumentation. " }, 
{ "title" : "Requirements Checklist", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-RequirementsChecklist", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Requirements Checklist", 
"snippet" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in P...", 
"body" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH If Spark is in use, Spark client gateway Open signup or LDAP for Unravel UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/ Disk: \/srv For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Network Port 3000 (or 4020) for Unravel UI access HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) unless Hive-hook via HDFS option is used For Oozie, port 11000 open to Unravel Server(s) " }, 
{ "title" : "1. Configure the Cluster", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-1ConfiguretheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 1. Configure the Cluster", 
"snippet" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because t...", 
"body" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because the EBS partition you create\/mount below must be in the same AZ. [HVD] Security Group Unravel Server works with multiple EMR clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the master node of each EMR cluster to Unravel Server. Simplest approach it to make Unravel server a member of the security group ElasticMapReduce-master when the instance running Unravel Server is first launched. If Unravel Server is already started or if you prefer more security groups, open ports 3000 (TCP) and 4041-4043 (TCP and UDP) from the ElasticMapReduce-master group to a new server security group called 'unravel'. Start ntpd Configure the Environment at First Login Disable selinux # sudo setenforce Permissive\n Edit the file to make sure setting persists after reboot. Make sure SELINUX=permissive # vi \/etc\/selinux\/config\n Install libaio.x86_64 # sudo yum -y install libaio.x86_64 Install lzop # sudo yum install lzop.x86_64 Configure Ephemeral Storage Find the available ephemeral storage (also called instance storage lsblk It can be convenient to use \/srv Find block devices with no mount point: # lsblk For each unmounted area do the following steps, substituting correct value for Z EPHEMERAL # sudo mkfs.ext4 \/dev\/xvd{Z}\n If necessary, create mount point and check if mounted, # mkdir $EPHEMERAL \n# echo \"\/dev\/xvd{Z} $EPHEMERAL ext4 defaults,noatime,nodiratime 1 2\" | sudo tee -a \/etc\/fstab \n# sudo mount -a\n# df -h $EPHEMERAL Make a note of the path to the mounted ephemeral store, referred to as UNRAVEL_EPHEMERAL Configure Durable Storage [HVD] In a PoC or test install, this step can be skipped if there is sufficient disk space (at least 100GB) on \/ or under \/srv mounted from an ephemeral ('instance storage') disk area. Here we create a \"Provisioned IOPS\" EBS volume, setting the maximum IOPS, based on the size 300GB. In AWS EC2 console, Volumes in the same AZ as the Unravel server On Unravel server, find the letter Z lsblk Use dd Z # sudo dd if=\/dev\/zero of=\/dev\/xvd{Z} bs=1M Reference:[ | http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html {+} http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html+ Format the volume as ext4 Z # sudo mkfs.ext4 \/dev\/xvd Z Mount the new volume (for example, as \/mnt\/unravel_durable UNRAVEL_DURABLE \/etc\/fstab mount -a Z # echo \"\/dev\/xvd Z UNRAVEL_DURABLE Check if the volume is mounted: # df -h $ UNRAVEL_DURABLE " }, 
{ "title" : "2. Install the Unravel Server RPM on the Cluster", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-2InstalltheUnravelServerRPMontheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 2. Install the Unravel Server RPM on the Cluster", 
"snippet" : "The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: ...", 
"body" : " The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: # scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm\" . For the enterprise version: # scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm.$timestamp . Install the Unravel Server RPM # sudo rpm -U unravel-4.0*.x86_64.EMR.rpm* The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password is stored in \/root\/unravel.install.include Grant Access to Unravel Server Assign elastic IP to Unravel Server using AWS Console [unless you use VPC] Adjust internal DNS for new IP [for access via a browser] Request reverse DNS change from AWS [if you plan on adding SSL] Restriction Do not make Unravel Server accessible on the public Internet because doing so would violate your licensing terms. Supply Credentials Needed for EMR and EC2 Create IAM User(s) and Credentials Open the AWS IAM (Identity and Access Management) console with your browser and then make the following users and credentials so that Unravel Server can access EMR logs stored in S3 and use EMR read-only permission to find EMR clusters for efficient data loading. These credentials are described separately, but can be combined into one user if desired. Multiple accounts can also be created per access kind here if Unravel Server is going to monitor multiple accounts; just create multiple credential files. S3 Read-Only Access Create a group named s3ro AmazonS3ReadOnlyAccess Create a user named s3unravel Add user s3unravel s3ro. EMR API Read-Only Access Create a group named emrro AmazonElasticMapReduceReadOnlyAccess Create a user named emrunravel Add user emrunravel emrro Cloudwatch API Read-Only Access Create a group named cwro AmazonCloudWatchReadOnlyAccess Create a user named cwunravel Add user cwunravel cwro " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc # scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unrav...", 
"body" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem cwro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc where: is your RSA key, rsa.pem is the LOCAL_IP of the Unravel Server, and root@xx.xxx.xx.xxx , s3ro.properties emrro.properties cwro.properties [default]\naws_access_key_id = {your access key}\naws_secret_access_key = {your secret key} You can create multiple credentials of the same type for multiple accounts by creating multiple files with the same base name and appending .1 .2 \/usr\/local\/unravel\/etc\/ s3ro.properties.1\nemrro.properties.1\ncwro.properties.1 All three files are required for each account. Open an SSH Session to Unravel Server Replace somefile.pem - must be a fully qualified path. UNRAVEL_HOST_IP # ssh -i {somefile.pem} root@$UNRAVEL_IP Set Correct Permissions on the Unravel Configuration Directory # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties \n# sudo chmod 644 *.properties Modify unravel.properties The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the following values in unravel.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \ncom.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \ncom.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties Adjust other values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.s3.profile.config.file.path Location of Unravel s3 read-only access & secret key filename s3ro.properties. com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties com.unraveldata.emr.profile.config.file.path Location of Unravel EMR read-only access & secret key filename emrro.properties. com.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties com.unraveldata.cloudwatch.profile.config.file.path Location of Unravel Cloud Watch read-only access & secret key filename cwro.properties. com.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties com.unraveldata.spark.s3.profilesToBuckets s3 profile associated to the s3 bucket For 1 bucket, follow example as follows: com.unraveldata.spark.s3.profilesToBuckets=<default>:<s3_bucket> For 2 buckets follow below example: com.unraveldata.spark.s3.profilesToBuckets=<s3_profile>:<bucket>,<s3_2nd_profile>:<2nd_s3_bucket> com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 # sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 # sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_hhw_2 \n# sudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_hhw_3 \n# sudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_hhw_4 \n# sudo chkconfig --add unravel_jcw2_4 Enable Collection from Hive Metastore If you have a central Hive Metastore, you can inform Unravel Server to enable more monitoring and analysis: For MySQL use avax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver instead of the postgresql driver Substitute thecorrect values for your site. # echo \" \n javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/10.0.0.10:7432\/hive_nqz \n javax.jdo.option.ConnectionDriverName=org.postgresql.Driver \n javax.jdo.option.ConnectionUserName=hive_nqz \n javax.jdo.option.ConnectionPassword=123456789abc \n \" | sudo tee -a \/usr\/local\/unravel\/unravel.properties Adjust Storage Locations [HVD] Prepare symlinks from \/srv\/unravel UNRAVEL_EPHEMERAL UNRAVEL_DURABLE \/srv Make sure daemons are stopped [HVD] # sudo \/etc\/init.d\/unravel_all.sh stop Check that all Unravel daemons are stopped: # ps -U unravel -f If any processes are owned by Unravel, stop them with a kill command. Check that destination areas are present [HVD] # df -h $ UNRAVEL_EPHEMERAL UNRAVEL_DURABLE Move files and create symlinks [HVD] # sudo \/bin\/mv \/srv\/unravel\/k_data $ UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_DURABLE This zk # sudo \/bin\/mv \/srv\/unravel\/zk_3_data $ UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_DURABLE UNRAVEL_DURABLE UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL UNRAVEL_EPHEMERAL Start Unravel Server # sudo \/etc\/init.d\/unravel_all.sh start Set Up an External DB [HVD] For performance and ease of management, using an RDS MySQL instead of the bundled mysql is recommended. Set Up RDS MySQL [HVD] RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group Create db instance Select multi-AZ Select db.m3.large Select provisioned IOPS 1000 Select SSD size (capacity depends on activity level) 770GB for 180 days retention (number of days set in unravel.properties) 1.54TB for 360 days retention No read-only replicas needed Prefer overlap with Unravel Server AZ Retain 7 days of snapshots Specify unravel ElasticMapReduce-master Name db instance unravel X Use MySQL 5.5.42 Disable auto-minor-upgrade Define a parameter group unravel key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Modify unravel X Use unravel Take effect immediately Create the unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c10)\" Log into RDS mysql instance from Unravel Server as admin\/master user and do the following commands, substituting the DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${ DB_PASSWORD Log into RDS mysql as user unravel DB_PASSWORD Dump Bundled DB with Schema [HVD] On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into RDS MySQL [HVD] Load the initial db with schema into the RDS MySQL instance, substituting RDS_HOST unravel # gunzip --stdout $DEST_FILE | \/usr\/local\/unravel\/mysql\/bin\/mysql --host=$ RDS_HOST Configure to Use RDS MySQL [HVD] Configure unravel.properties Edit the file: # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust existing properties to point to new RDS MySQL. Change example values as needed: unravel.jdbc.username=unravel \nunravel.jdbc.url=jdbc:mysql:\/\/unravelrds.something.REGION.rds.amazonaws.com:3306\/unravel_mysql_prod \nunravel.jdbc.password=****** Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the Us...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part2.html#UUID-db7adaea-b885-78d1-d7f3-44fac882db94_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-5OptionalEnableAdditionalDataCollectionInstrumentation", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"snippet" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup...", 
"body" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup " }, 
{ "title" : "Part 3: Additional Topics for CDH", 
"url" : "unravel-4-0-4-1/install/cloudera/install-cdh-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Cloudera \/ Part 3: Additional Topics for CDH", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Sending Diagnostics to Unravel Support...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Sending Diagnostics to Unravel Support " }, 
{ "title" : "Hortonworks", 
"url" : "unravel-4-0-4-1/install/hortonworks.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). This guide is compatible with: HDP 2.2-2.6...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). This guide is compatible with: HDP 2.2-2.6 " }, 
{ "title" : "Part 1: Install Unravel Server on HDP", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Workflow Summary Pre-...", 
"body" : "This topic explains how to deploy Unravel Server 4.0-4.1 on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR j...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility HDP 2.2-2.6 Hadoop 1.x - 2.x Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server....", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access For HDP, use Ambari Web UI to create a Gateway node configuration. The hadoop command must be present on the Unravel target server. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.u...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised....", 
"body" : "Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel UI's configuration wizard. com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel UI's configuration wizard. com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL yarn.resourcemanager.webapp.address=http:\/\/example.localdomain :8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: (a) Create a keytab for unravel for daemons that run as unravel \/etc\/keytabs\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct realm and \/hostname, if applicable): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=hdfs\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/etc\/keytabs\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n You can find the principal by using 'klist -kt KEYTAB_FILE' If Ranger is Enabled: Resource Principal Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs read Spark event log hdfs:\/\/usr\/history\/done hdfs read MapReduce logs hdfs:\/\/tmp\/logs hdfs read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs read Obtain table partition sizes Hive Metastore GRANT hive read Hive table information " }, 
{ "title" : "4. Convert Your Unravel Installation to HDP", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-4ConvertYourUnravelInstallationtoHDP", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 4. Convert Your Unravel Installation to HDP", 
"snippet" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades....", 
"body" : "Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh Note: This change will stick after RPM upgrades. " }, 
{ "title" : "5. Update Site-Specific HDP Properties", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-5UpdateSite-SpecificHDPProperties", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 5. Update Site-Specific HDP Properties", 
"snippet" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base=...", 
"body" : "The following site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh \/usr\/local\/unravel\/etc\/unravel.properties # Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/user\/spark\/applicationHistory\/\n \n# Add Hive Metastore database information for Unravel Hive Config \njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/{hostname}:3306\/{database_name} \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName={HiveMetastoreUserName} \njavax.jdo.option.ConnectionPassword={HiveMetastorePassword} Verify above site-specific values using the Ambari Web UI (AWU): Verify com.unraveldata.job.collector.done.log.base In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Take note of the value of mapreduce.jobhistory.done-dir unravel.properties com.unraveldata.job.collector.done.log.base= Verify com.unraveldata.job.collector.log.aggregation.base In AWU, on the left-hand side, click YARN Configs Advanced Node Manager Take note of the value of yarn.nodemanager.remote-app-log-dir unravel.properties com.unraveldata.job.collector.log.aggregation.base= Verify Hive Metastore javax.jdo.option.Connection In AWU, on the left-hand side, click Hive Configs Advanced Hive Metastore Take note of following properties and their values to be entered into unravel.properties Database URL Database Host JDBC Driver Class Database Name Database Username Database Password Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\necho \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part1.html#UUID-0bb8d995-b747-714e-4115-a7a572063098_id_Part1InstallUnravelServeronHDP-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 1: Install Unravel Server on HDP \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for HDP Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ Introduction", 
"snippet" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1....", 
"body" : "This guide describes how to install the Unravel Sensor for Hive Hook and Spark on HDP 2.2.x-2.5.x clusters using Ambari Web UI (AWU). Unravel Hive Hook is used to collect information about Hive queries in Hadoop. The information here applies to Hive versions 0.10.0 through 2.1.x and Unravel 4.0-4.1. Highlighted text indicates where you must substitute your particular values. " }, 
{ "title" : "1. Start Unravel Server", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-1StartUnravelServer", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 1. Start Unravel Server", 
"snippet" : "Note: Unravel needs to be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start...", 
"body" : " Note: Unravel needs to be up for the next step to complete. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-2InstallUnravelHiveHookandSparkSensorOntoHDPServers", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 2. Install Unravel Hive Hook and Spark Sensor Onto HDP Servers", 
"snippet" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps # ensure wget is install and use below script to install sensors # \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh # from Unravel server (eg. edge node) # run...", 
"body" : "Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows. # login as root to do below steps\n# ensure wget is install and use below script to install sensors \n# \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/unravel_hdp_setup.sh \n# from Unravel server (eg. edge node)\n# run on each server that will use instrumentation:\nyum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\nsudo .\/unravel_hdp_setup.sh install -y --unravel-server UNRAVEL_HOST_IP:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION Substitute valid values for: - fully qualified host name or IP address UNRAVEL_HOST_IP - target Spark version SPARK_VERSION - target Hive version HIVE_VERSION This installation creates the following files: Hive hook jar in \/usr\/local\/unravel_client\/ Spark jar in \/usr\/local\/unravel-spark\/jars\/ Once the files are installed under \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-spark\/ " }, 
{ "title" : "3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-3AddUnravelHiveHookhive-siteSettingstoAllofHDPsServersintheClusterUsingAWU", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 3. Add Unravel Hive Hook hive-site Settings to All of HDP's Servers in the Cluster Using AWU", 
"snippet" : "Completion of this step requires a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, Hive jobs could fail with a ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. Add AUX_CLASSPATH hive...", 
"body" : " Completion of this step requires a restart of all affected Hive services in Ambari UI. If the env steps below are not deployed or use incorrect paths, Hive jobs could fail with a ClassNotFoundException when the hive-site change takes effect after the Hive service is restarted. Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env AUX_CLASSPATH=$ AUX_CLASSPATH Hold off on restarting any services until the next step. Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar You can optionally restart the services in Ambari, as prompted, at this point in order to verify that the environment change is done correctly. After the restart, look at hadoop-env.sh and hive-env.sh on an edge node and check the path to the jar file. Hint: find the files with sudo find \/etc -name '*env.sh' -newerct '1 hour ago' Add the contents of \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip UNRAVEL_HOST_IP Custom hive-site Hive In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.HiveDriverHook \ncom.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR \ncom.unraveldata.hive.hook.tcp=true \ncom.unraveldata.host= UNRAVEL_HOST_IP You should restart the services in Ambari now, as prompted, to test whether Hive queries can succeed after the above configuration change. If you get ClassNotFoundException during a query, then make corrections or revert. " }, 
{ "title" : "4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-4IfPossibleEnsurethathiveexecutionengineisSettoMapReduceinyourHivequery", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 4. If Possible, Ensure that hive.execution.engine is Set to MapReduce in your Hive query", 
"snippet" : "set hive.execution.engine=mr;...", 
"body" : "set hive.execution.engine=mr; " }, 
{ "title" : "5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-5OptionallyforSparkonYARNEnableUnravelSparkInstrumentationonAllofHDPsServersintheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 5. Optionally for Spark on YARN, Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster", 
"snippet" : "Completion of this step will require a restart of all affected Spark services in Ambari UI. Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.se...", 
"body" : " Completion of this step will require a restart of all affected Spark services in Ambari UI. Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults Inside Custom spark-defaults Add Property Bulk property add mode spark.unravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP " }, 
{ "title" : "6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part2.html#UUID-4f381ca1-12c0-612e-4e50-eb6c77e04540_id_Part2EnableAdditionalDataCollectionInstrumentationforHDP-6OptionallyforMapReduce2MRJVMSensorCluster-Wide", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 2: Enable Additional Data Collection \/ Instrumentation for HDP \/ 6. Optionally for MapReduce2 (MR) JVM Sensor Cluster-Wide", 
"snippet" : "IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size Leave a white space in-between the current and the following new property) On the top notification banner, click Save -javaagent:\/usr\/local\/unravel-spark\/jars\/btrace...", 
"body" : " IMPORTANT In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size Leave a white space in-between the current and the following new property) On the top notification banner, click Save -javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site: Inside Custom mapred-site Add Property Bulk property add mode: On the top notification banner, click Save mapreduce.task.profile=true\nmapreduce.task.profile.maps=0-5\nmapreduce.task.profile.reduces=0-5\nmapreduce.task.profile.params=-javaagent:\/usr\/local\/unravel-spark\/jars\/btrace-agent.jar=systemClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-sys.jar,bootClassPath=\/usr\/local\/unravel-spark\/jars\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport= UNRAVEL_HOST_IP Notice that in this code block, the blank lines separate single full lines of text that are wrapped due to length. Also, ensure you replace UNRAVEL_HOST_IP Following instructions are for Unravel rpm 4.0.x and 4.1.x Propagate Unravel MR JAR onto all the servers in the cluster as follows : Ensure you have already installed unzip curl ssh to the Unravel gateway host server and use guided steps to unzip the Unravel MR jars. With root or sudo access, change directory to \/usr\/local\/unravel-spark and run the below curl get command: cd \/usr\/local\/unravel-spark\ncurl http:\/\/localhost:3000\/hh\/unravel-sensor-for-mapreduce-bin.zip -o unravel-sensor-for-mapreduce-bin.zip\nunzip -d jars unravel-sensor-for-mapreduce-bin.zip Now, tar up the \/usr\/local\/unravel-spark cd \/usr\/local\/\ntar -cvf unravel-spark.tar .\/unravel-spark Copy unravel-spark.tar \/usr\/local unravel-spark " }, 
{ "title" : "Part 3: Additional Topics for HDP", 
"url" : "unravel-4-0-4-1/install/hortonworks/install-hdp-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Hortonworks \/ Part 3: Additional Topics for HDP", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Custom Configurations Adding More Admins to Unravel Web UI Sending Diagnostics to Unravel Support...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Custom Configurations Adding More Admins to Unravel Web UI Sending Diagnostics to Unravel Support " }, 
{ "title" : "MapR", 
"url" : "unravel-4-0-4-1/install/mapr.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on MapR. This guide is compatible with MapR 5.1...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on MapR. This guide is compatible with MapR 5.1 " }, 
{ "title" : "Part 1: Install Unravel Server on MapR", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on the MapR platform. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on the MapR platform. Workflow Summary Pre-installation check Configure the host machine. Install the Unravel Server RPM on the host machine. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Configure Unravel Server (advanced options). " }, 
{ "title" : "Pre-Installation Check", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-Pre-InstallationCheck", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000...", 
"body" : "The following installation requirements must be met for successful installation of Unravel. Platform Compatibility MapR 5.1 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway LDAP (AD or Open LDAP) compatible for Unravel UI user authentication (Open signup by default) Don't install Unravel Server on the same edge node that hosts Zookeeper. Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done dir\" in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network Port 3000 (or 4020) from users and entire cluster to Unravel UI HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP port 4043 open from entire cluster to Unravel Server(s) For Oozie, port 11000 open from Unravel Server(s) to the Oozie server Resource Manager (RM) port 8032 from Unravel Server(s) to the RM server(s) Port 4176, 4181 through 4189, 3316, 4091 must be available for localhost communication between Unravel daemons or services " }, 
{ "title" : "1. Configure the Host Machine", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-1ConfiguretheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 1. Configure the Host Machine", 
"snippet" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel hadoop fs -mkdir \/user\/unravel hadoop fs -chown unravel:mapr \/user\/unravel If MapR...", 
"body" : "Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access Install mapr-client hadoop fs Configure the Host Before installing the RPM Run the following commands on Unravel Server as root sudo useradd -g mapr unravel\nhadoop fs -mkdir \/user\/unravel\nhadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr \/etc\/unravel_ctl Check available RAM to ensure availability: free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 (only change this setting on the Unravel gateway\/client machine). And the restart mfs. " }, 
{ "title" : "2. Install the Unravel Server RPM on the Host Machine", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-2InstalltheUnravelServerRPMontheHostMachine", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 2. Install the Unravel Server RPM on the Host Machine", 
"snippet" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist...", 
"body" : "Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.rpm\" . For the enterprise version: scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.rpm.$timestamp . The precise RPM filename will vary. The version has the structure x.y.b b x.y Install the Unravel Server RPM Replace the asterisks as needed to be more selective. sudo rpm -U unravel-4.0*.x86_64.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Do Host-Specific Post-Installation Actions Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off...", 
"body" : "Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw sudo chkconfig unravel_sw off If you have 10000-20000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4 Modify unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs com.unraveldata.job.collector.done.log.base=\/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process com.unraveldata.job.collector.log.aggregation.base=\/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=maprfs:\/\/\/apps\/spark yarn.resourcemanager.webapp.address Resource Manager web app address yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie If Kerberos is Enabled: (a) Create a keytab for unravel for daemons that run as unravel \/usr\/local\/unravel\/etc\/unravel.keytab (b) Create a keytab for hdfs for the Unravel daemons that run as user hdfs \/etc\/keytabs\/hdfs.keytab (c) Tell Unravel Server about it (env var for hdfs keytab location; substitute correct hostname): echo \"export HDFS_KEYTAB_PATH=\/etc\/keytabs\/hdfs.keytab \nexport HDFS_KERBEROS_PRINCIPAL=unravel\/myhost.mydomain@MYREALM \n\" | sudo tee -a \/usr\/local\/unravel\/etc\/unravel.ext.sh (d) Add properties for Kerberos: echo \"\ncom.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab\n\" | sudo tee -a \/usr\/local\/unravel\/unravel.properties\n\n If Sentry is Enabled: Resource Principal Access hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR * read+write Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60\n echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. Check Unravel UI for MR jobs loading: on the Applications M...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. Unravel UI displays collected data. Check Unravel UI for MR jobs loading: on the Applications Map Reduce For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Configure Unravel Server (Advanced Options)", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part1.html#UUID-2bcdc5b5-d969-d439-ea52-ea45c23af8a9_id_Part1InstallUnravelServeronMapR-5OptionalConfigureUnravelServerAdvancedOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 1: Install Unravel Server on MapR \/ 5. (Optional) Configure Unravel Server (Advanced Options)", 
"snippet" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unrave...", 
"body" : "Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Part 2: Enable Additional Data Collection \/ Instrumentation for MapR Run the Unravel UI Configuration Wizard Run the Unravel UI configuration wizard to choose additional configuration options. For instructions on configuring advanced options, see the Unravel UI User Guide " }, 
{ "title" : "Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ Introduction", 
"snippet" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's ...", 
"body" : "This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. Workflow Summary Enable additional instrumentation on Unravel Server's host. Enter correct value for Hive Metastore, Resource Manager and Oozie properties. Confirm that Unravel Web UI shows additional data. Confirm and adjust the settings in yarn-site.xml Enable additional instrumentation on other hosts in the cluster. " }, 
{ "title" : "1. Enable Additional Instrumentation on Unravel Server's Host", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-1EnableAdditionalInstrumentationonUnravelServersHost", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 1. Enable Additional Instrumentation on Unravel Server's Host", 
"snippet" : "Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_s...", 
"body" : " Best practice is to enable instrumentation on Unravel Server itself for testing, practice, and demonstration purposes, before enabling instrumentation on other gateway\/edge\/client machines that do real client work (Hive queries, Map-Reduce jobs, Spark, and so on). Run the shell script unravel_mapr_setup.sh host1 sudo yum install -y wget\ncd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\nsudo .\/unravel_mapr_setup.sh install -y --unravel-server $UNRAVEL_HOST:3000 --spark-version SPARK_VERSION --hive-version HIVE_VERSION In the code above, substitute valid values for: UNRAVEL_HOST_IP - fully qualified host name or IP address SPARK_VERSION - target Spark version HIVE_VERSION - target Hive version " }, 
{ "title" : "2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-2EntercorrectvalueforHiveMetastoreResourceManagerandOozieproperties", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 2. Enter correct value for Hive Metastore, Resource Manager, and Oozie properties", 
"snippet" : "Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore # ### Uncomment and enter correct HM java.jdo.option.Connection properties ### #javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive #javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_...", 
"body" : " Use vi \/usr\/local\/unravel\/etc\/unravel.properties # hive metastore\n#\n### Uncomment and enter correct HM java.jdo.option.Connection properties ###\n#javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/example.localdomain:3306\/hive\n#javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/example.localdomain:7432\/hive_zzzzzz\n#javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\n#javax.jdo.option.ConnectionDriverName=org.postgresql.Driver\n#javax.jdo.option.ConnectionUserName=hiveuser?\n#javax.jdo.option.ConnectionPassword=???????\n\n# optional selectivity of databases to analyze in metastore\n#com.unraveldata.metastore.databasePattern=s*|t*|d*\n\n# Resource Manager (RM)\n#\n# Enable https access to Resource Manager\n#https.protocols=TLSv1.2\n#\n### Uncomment and enter correct below properties RM ###\n#yarn.resourcemanager.webapp.address=http:\/\/example.localdomain:8088 \n\n# Resource Manager username to login\n#yarn.resourcemanager.webapp.username=foo\n\n# Resource Manager password to login\n#yarn.resourcemanager.webapp.password=?????\n\n#\n# oozie\n#\n### Uncomment below oozie properties when oozie is used ###\n# oozie.server.url=http:\/\/<oozie-hostname-IP-address>:11000\/oozie Restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "3. Confirm that Unravel Web UI Shows Additional Data", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-3ConfirmthatUnravelWebUIShowsAdditionalData", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 3. Confirm that Unravel Web UI Shows Additional Data", 
"snippet" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and ...", 
"body" : "Run a Hive job using a test script provided by Unravel Server: This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. Replace $ someUser sudo -u $ someUser This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. This script runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. " }, 
{ "title" : "4. Confirm and Adjust the Settings in yarn-site.xml", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-4ConfirmandAdjusttheSettingsinyarn-sitexml", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 4. Confirm and Adjust the Settings in yarn-site.xml", 
"snippet" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property> <name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value> <source>yarn-site.xml<\/source> <\/property> yarn.log-aggregation-enable <...", 
"body" : "Check specific properties only Unravel srv in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml yarn.resourcemanager.webapp.address <property>\n<name>yarn.resourcemanager.webapp.address<\/name>\n<value>10.0.0.110:8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name>\n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> " }, 
{ "title" : "5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part2.html#UUID-a37291a6-cfad-f86f-687c-55249119b120_id_Part2EnableAdditionalDataCollectionInstrumentationforMapR-5EnableAdditionalInstrumentationonOtherHostsintheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 2: Enable Additional Data Collection \/ Instrumentation for MapR \/ 5. Enable Additional Instrumentation on Other Hosts in the Cluster", 
"snippet" : "To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar ...", 
"body" : " To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml, yarn-site.xml and user defined function jars, you can add the changes and jar from Unravel to your existing mechanism. Run the shell script unravel_mapr_setup.sh Copy the newly edited (in the previous step 4) yarn-site.xml to all nodes. Do a rolling-restart of HiveServer2 " }, 
{ "title" : "Part 3: Additional Topics for MapR", 
"url" : "unravel-4-0-4-1/install/mapr/install-mapr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ MapR \/ Part 3: Additional Topics for MapR", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Amazon EMR and Qubole", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Amazon Elastic MapReduce (Amazon EMR) and Qubole clusters. This guide is compatible with Amazon EMR 3.4 - 4.7...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Amazon Elastic MapReduce (Amazon EMR) and Qubole clusters. This guide is compatible with Amazon EMR 3.4 - 4.7 " }, 
{ "title" : "Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Introduction", 
"snippet" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure...", 
"body" : "This topic explains how to deploy Unravel Server 4.0 on Amazon EMR or Qubole. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Configure the cluster. Install the Unravel Server RPM on the cluster. Configure Unravel Server (basic\/core options). Log into Unravel UI. (Optional) Enable additional data collection\/instrumentation. " }, 
{ "title" : "Requirements Checklist", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-RequirementsChecklist", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ Requirements Checklist", 
"snippet" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in P...", 
"body" : "Platform Compatibility Amazon EMR 3.4 - 4.7 Hadoop 1.x - 2.x Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 1.6.x Software Operating System: RedHat\/Centos 6.4 - 7.3 libaio.x86_64 installed should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN client\/gateway, hadoop and hive commands in PATH If Spark is in use, Spark client gateway Open signup or LDAP for Unravel UI user authentication Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum Disk: \/ Disk: \/srv For 10,000+ MR jobs per day, two or more gateway\/edge nodes are recommended Network Port 3000 (or 4020) for Unravel UI access HDFS ports open from Hadoop cluster to Unravel Server(s) For MR1, TaskTracker port open from Hadoop cluster to Unravel Server(s) For MR1\/YARN, Hive Metadata DB port open to Unravel Server(s) for partition reporting UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server(s) unless Hive-hook via HDFS option is used For Oozie, port 11000 open to Unravel Server(s) " }, 
{ "title" : "1. Configure the Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-1ConfiguretheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 1. Configure the Cluster", 
"snippet" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because t...", 
"body" : "Provision an Instance Instance Type: r3.2xlarge OS: centos7 Server must be \"Optimized for EBS\" if EBS durable storage is used Set boot partition to 12GB or larger You must Must be in same region as the EMR clusters Unravel Server will monitor Note the Availability Zone (AZ) of the instance because the EBS partition you create\/mount below must be in the same AZ. [HVD] Security Group Unravel Server works with multiple EMR clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the master node of each EMR cluster to Unravel Server. Simplest approach it to make Unravel server a member of the security group ElasticMapReduce-master when the instance running Unravel Server is first launched. If Unravel Server is already started or if you prefer more security groups, open ports 3000 (TCP) and 4041-4043 (TCP and UDP) from the ElasticMapReduce-master group to a new server security group named unravel Start ntpd Configure the Environment at First Login Disable selinux # sudo setenforce Permissive Edit the file to make sure setting persists after reboot. Make sure SELINUX=permissive # vi \/etc\/selinux\/config Install libaio.x86_64 # sudo yum -y install libaio.x86_64 Install lzop # sudo yum install lzop.x86_64 Configure Ephemeral Storage Find the available ephemeral storage (also called instance storage lsblk It can be convenient to use \/srv Find block devices with no mount point: # lsblk For each unmounted area do the following steps, substituting correct value for Z EPHEMERAL # sudo mkfs.ext4 \/dev\/xvd Z If necessary, create mount point and check if mounted, # mkdir $ EPHEMERAL Z EPHEMERAL EPHEMERAL Make a note of the path to the mounted ephemeral store, referred to as UNRAVEL_EPHEMERAL Configure Durable Storage [HVD] In a PoC or test install, this step can be skipped if there is sufficient disk space (at least 100GB) on \/ or under \/srv mounted from an ephemeral ('instance storage') disk area. Here we create a \"Provisioned IOPS\" EBS volume, setting the maximum IOPS, based on the size 300GB. In AWS EC2 console, Volumes in the same AZ as the Unravel server On Unravel server, find the letter Z lsblk Use dd Z # sudo dd if=\/dev\/zero of=\/dev\/xvd{Z} bs=1M Reference:[ | http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html {+} http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ebs-prewarm.html+ Format the volume as ext4 Z # sudo mkfs.ext4 \/dev\/xvd Z Mount the new volume (for example, as \/mnt\/unravel_durable UNRAVEL_DURABLE \/etc\/fstab mount -a Z # echo \"\/dev\/xvd Z UNRAVEL_DURABLE Check if the volume is mounted: # df -h $ UNRAVEL_DURABLE " }, 
{ "title" : "2. Install the Unravel Server RPM on the Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-2InstalltheUnravelServerRPMontheCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 2. Install the Unravel Server RPM on the Cluster", 
"snippet" : "The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: ...", 
"body" : " The precise RPM filename will vary. The version has the structure x.y.b b x.y Replace the asterisks below as needed to be more selective. Get the Unravel Server RPM Copy the RPM from the Unravel distribution server to the host machine using the username and password given to you by Unravel Support: For the free trial version: # scp \"unraveltrial@trial.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm\" . For the enterprise version: # scp $USER@dist.unraveldata.com:unravel-4.0-*.x86_64.EMR.rpm.$timestamp . Install the Unravel Server RPM # sudo rpm -U unravel-4.0*.x86_64.EMR.rpm* The installation does the following: It creates the directory \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ It creates the Unravel user, unravel It puts an initial internal database and other durable state in \/srv\/unravel\/ \/root\/unravel.install.include It puts scripts for controlling Unravel services into \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh It includes support for YARN by default. It creates an HDFS directory for Hive Hook information collection. Grant Access to Unravel Server Restriction Do not make Unravel Server accessible on the public Internet because doing so would violate your licensing terms. Assign elastic IP to Unravel Server using AWS Console [unless you use VPC] Adjust internal DNS for new IP [for access via a browser] Request reverse DNS change from AWS [if you plan on adding SSL] Supply Credentials Needed for EMR and EC2 Create IAM User(s) and Credentials Open the AWS IAM (Identity and Access Management) console with your browser and then make the following users and credentials so that Unravel Server can access EMR logs stored in S3 and use EMR read-only permission to find EMR clusters for efficient data loading. These credentials are described separately, but can be combined into one user if desired. Multiple accounts can also be created per access kind here if Unravel Server is going to monitor multiple accounts; just create multiple credential files. S3 Read-Only Access Create a group named s3ro AmazonS3ReadOnlyAccess Create a user named s3unravel Add user s3unravel s3ro. EMR API Read-Only Access Create a group named emrro AmazonElasticMapReduceReadOnlyAccess Create a user named emrunravel Add user emrunravel emrro Cloudwatch API Read-Only Access Create a group named cwro AmazonCloudWatchReadOnlyAccess Create a user named cwunravel Add user cwunravel cwro " }, 
{ "title" : "3. Configure Unravel Server (Basic\/Core Options)", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-3ConfigureUnravelServerBasicCoreOptions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 3. Configure Unravel Server (Basic\/Core Options)", 
"snippet" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc # scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unrav...", 
"body" : "The Unravel Server's configuration directory is located at \/usr\/local\/unravel\/etc unravel.properties Copy S3 and EMR Credential Files to Unravel Server # scp -i ~\/rsa.pem s3ro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem emrro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc \n# scp -i ~\/rsa.pem cwro.properties root@xx.xxx.xx.xxx:\/usr\/local\/unravel\/etc where: is your RSA key, rsa.pem is the LOCAL_IP of the Unravel Server, and root@xx.xxx.xx.xxx , s3ro.properties emrro.properties cwro.properties [default]\naws_access_key_id = {your access key}\naws_secret_access_key = {your secret key} You can create multiple credentials of the same type for multiple accounts by creating multiple files with the same base name and appending \".1\" for the second account, \".2\" for the third account, and so on. For example, using the file naming convention suggested above, copy these additional files into \/usr\/local\/unravel\/etc\/ s3ro.properties.1\nemrro.properties.1\ncwro.properties.1 All three files are required for each account. Open an SSH Session to Unravel Server Replace somefile.pem - must be a fully qualified path. UNRAVEL_HOST_IP # ssh -i somefile.pem Set Correct Permissions on the Unravel Configuration Directory # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties \n# sudo chmod 644 *.properties Modify unravel.properties The settings file \/usr\/local\/unravel\/etc\/unravel.properties Open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Edit the following values in unravel.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties \ncom.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties \ncom.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties Adjust other values in unravel.properties Property Description Example com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. com.unraveldata.advertised.url=http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. com.unraveldata.customer.organization=Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside com.unraveldata.tmpdir=\/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. com.unraveldata.history.maxSize.weeks=26 com.unraveldata.hive.hook.topic.num.threads . Defines the number of threads. Default is 1. Depending on job volume, increase this property to Optional N N ThousandJobsPerDay com.unraveldata.hive.hook.topic.num.threads=1 com.unraveldata.s3.profile.config.file.path Location of Unravel s3 read-only access & secret key filename s3ro.properties. com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties com.unraveldata.emr.profile.config.file.path Location of Unravel EMR read-only access & secret key filename emrro.properties. com.unraveldata.emr.profile.config.file.path=\/usr\/local\/unravel\/etc\/emrro.properties com.unraveldata.cloudwatch.profile.config.file.path Location of Unravel Cloud Watch read-only access & secret key filename cwro.properties. com.unraveldata.cloudwatch.profile.config.file.path=\/usr\/local\/unravel\/etc\/cwro.properties com.unraveldata.spark.s3.profilesToBuckets s3 profile associated to the s3 bucket For 1 bucket, follow example as follows: com.unraveldata.spark.s3.profilesToBuckets=<default>:<s3_bucket> For 2 buckets follow below example: com.unraveldata.spark.s3.profilesToBuckets=<s3_profile>:<bucket>,<s3_2nd_profile>:<2nd_s3_bucket> com.unraveldata.login.admins Defines the usernames that can access Unravel UI's admin pages. Default is admin com.unraveldata.login.admins=admin com.unraveldata.s3.batch.monitoring.interval.sec . Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. Optional com.unraveldata.s3.batch.monitoring.interval.sec=120 com.unraveldata.spark.eventlog.location Where to find Spark event logs com.unraveldata.spark.eventlog.location=hdfs:\/\/example.localdomain:8020\/spark-history\/ oozie.server.url Oozie URL oozie.server.url=http:\/\/example.localdomain :11000\/oozie Enable\/Disable Optional Daemons Depending on your workload volume or kind of activity, you can enable or disable optional daemons at this point. If you are not using Oozie, disable unravel_os3 # sudo chkconfig unravel_os3 off If you are not using Spark, disable unravel_sw_1 # sudo chkconfig unravel_sw_1 off If you have 10000-20000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_hhw_2 \n# sudo chkconfig --add unravel_jcw2_2 If you have 20000-30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_hhw_3 \n# sudo chkconfig --add unravel_jcw2_3 If you have more than 30000 jobs per day, enable these workers: # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_hhw_4 \n# sudo chkconfig --add unravel_jcw2_4 Enable Collection from Hive Metastore If you have a central Hive Metastore, you can inform Unravel Server to enable more monitoring and analysis: For MySQL use avax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver instead of the postgresql driver Substitute thecorrect values for your site. # echo \" \n javax.jdo.option.ConnectionURL=jdbc:postgresql:\/\/10.0.0.10:7432\/hive_nqz \n javax.jdo.option.ConnectionDriverName=org.postgresql.Driver \n javax.jdo.option.ConnectionUserName=hive_nqz \n javax.jdo.option.ConnectionPassword=123456789abc \n \" | sudo tee -a \/usr\/local\/unravel\/unravel.properties Adjust Storage Locations [HVD] Prepare symlinks from \/srv\/unravel $UNRAVEL_EPHEMERAL $UNRAVEL_DURABLE \/srv Make sure daemons are stopped [HVD] # sudo \/etc\/init.d\/unravel_all.sh stop Check that all Unravel daemons are stopped: # ps -U unravel -f If any processes are owned by Unravel, stop them with a kill command. Check that destination areas are present [HVD] # df -h $UNRAVEL_EPHEMERAL \n# df -h $UNRAVEL_DURABLE Move files and create symlinks [HVD] # sudo \/bin\/mv \/srv\/unravel\/k_data $UNRAVEL_DURABLE\/k_data \n# sudo ln -s $UNRAVEL_DURABLE\/k_data \/srv\/unravel\/k_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_1_data $UNRAVEL_DURABLE\/zk_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_1_data \/srv\/unravel\/zk_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/zk_2_data $UNRAVEL_DURABLE\/zk_2_data \n# sudo ln -s $UNRAVEL_DURABLE\/zk_2_data \/srv\/unravel\/zk_2_data This zk # sudo \/bin\/mv \/srv\/unravel\/zk_3_data $UNRAVEL_EPHEMERAL\/zk_3_data \n# sudo ln -s $UNRAVEL_EPHEMERAL\/zk_3_data \/srv\/unravel\/zk_3_data \n\n# sudo \/bin\/mv \/srv\/unravel\/db_data $UNRAVEL_DURABLE\/db_data \n# sudo ln -s $UNRAVEL_DURABLE\/db_data \/srv\/unravel\/db_data \n\n# sudo \/bin\/mv \/srv\/unravel\/s_1_data $UNRAVEL_DURABLE\/s_1_data \n# sudo ln -s $UNRAVEL_DURABLE\/s_1_data \/srv\/unravel\/s_1_data \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp $UNRAVEL_EPHEMERAL\/tmp \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp \/srv\/unravel\/tmp \n\n# sudo \/bin\/mv \/srv\/unravel\/log_hdfs $UNRAVEL_EPHEMERAL\/log_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/log_hdfs \/srv\/unravel\/log_hdfs \n\n# sudo \/bin\/mv \/srv\/unravel\/tmp_hdfs $UNRAVEL_EPHEMERAL\/tmp_hdfs \n# sudo ln -s $UNRAVEL_EPHEMERAL\/tmp_hdfs \/srv\/unravel\/tmp_hdfs Start Unravel Server # sudo \/etc\/init.d\/unravel_all.sh start Set Up an External DB [HVD] For performance and ease of management, using an RDS MySQL instead of the bundled mysql is recommended. Set Up RDS MySQL [HVD] RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group Create db instance Select multi-AZ Select db.m3.large Select provisioned IOPS 1000 Select SSD size (capacity depends on activity level) 770GB for 180 days retention (number of days set in unravel.properties) 1.54TB for 360 days retention No read-only replicas needed Prefer overlap with Unravel Server AZ Retain 7 days of snapshots Specify unravel ElasticMapReduce-master Name db instance \" unravelX Use MySQL 5.5.42 Disable auto-minor-upgrade Define a parameter group unravel key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Modify unravelX Use unravel Take effect immediately Create the unravel Pick a db password for user unravel DB_PASSWORD=\"$(cat \/dev\/urandom | tr -cd 'a-zA-Z0-9' | head -c10)\" Log into RDS mysql instance from Unravel Server as admin\/master user and do the following commands, substituting above DB_PASSWORD CREATE DATABASE unravel_mysql_prod; \n COMMIT; \n CREATE USER 'unravel'@'%' IDENTIFIED BY 'changeme'; \n GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; \n FLUSH PRIVILEGES; \n UPDATE user SET Password=PASSWORD('${DB_PASSWORD}') WHERE user.User='unravel'; \n FLUSH PRIVILEGES; \n COMMIT; \n QUIT; Log into RDS mysql as user unravel DB_PASSWORD Dump Bundled DB with Schema [HVD] On Unravel Server, do the following to dump the db with schema: # sudo \/etc\/init.d\/unravel_all.sh stop \n# sudo \/etc\/init.d\/unravel_db start \n RPW=$(grep 'DB_ROOT_PASSWORD' \/root\/unravel.install.include | awk -F= '{ print $2 }') \n [ ! \"$RPW\" ] && echo \"could not find Unravel bundled db root password\" \n DEST_FILE=\/tmp\/unravel.backup.$(export TZ=UTC;date '+%Y%m%dt%H%MZ').sql.gz \n \/usr\/local\/unravel\/mysql\/bin\/mysqldump --host=127.0.0.1 -u root --port=3316 --opt \\ \n --complete-insert --tz-utc --skip-comments --single-transaction --insert-ignore \\ \n unravel_mysql_prod -p$RPW | gzip > $DEST_FILE Load DB with Schema Into RDS MySQL [HVD] Load the initial db with schema into the RDS MySQL instance, substituting $RDS_HOST unravel # gunzip --stdout $DEST_FILE | \/usr\/local\/unravel\/mysql\/bin\/mysql --host=$RDS_HOST -u unravel -p --port=3306 --force unravel_mysql_prod Configure to Use RDS MySQL [HVD] Configure unravel.properties Edit the file: # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Adjust existing properties to point to new RDS MySQL. Change example values as needed: unravel.jdbc.username=unravel \nunravel.jdbc.url=jdbc:mysql:\/\/unravelrds.something.REGION.rds.amazonaws.com:3306\/unravel_mysql_prod \nunravel.jdbc.password=****** Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" " }, 
{ "title" : "4. Log into Unravel UI", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-4LogintoUnravelWebUI", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 4. Log into Unravel UI", 
"snippet" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the Us...", 
"body" : "Using a web browser, navigate to http:\/\/($hostname -f):3000 admin unraveldata For the free trial version, use the Chrome browser. Unravel UI Login Screen Congratulations! Unravel Server is up and running. You should begin to see information collected. For instructions on using Unravel UI, see the User Guide " }, 
{ "title" : "5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part1.html#UUID-c288f2d4-6f6f-37c7-8aa4-c3e09a2282dd_id_Part1InstallUnravelServerforAmazonEMRorQuboleCluster-5OptionalEnableAdditionalDataCollectionInstrumentation", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 1: Install Unravel Server for Amazon EMR or Qubole Cluster \/ 5. (Optional) Enable Additional Data Collection\/Instrumentation", 
"snippet" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup...", 
"body" : "For instructions, see Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup " }, 
{ "title" : "Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part2.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster", 
"snippet" : "Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values....", 
"body" : " Follow these steps only if you have a Qubole-based Hadoop2\/Hive cluster. Highlighted text indicates where you must substitute your particular values. " }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part2.html#UUID-e74e34e8-930f-2c37-7305-8e186e2d77ec_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Introduction", 
"snippet" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already run...", 
"body" : "This topic explains how to install Unravel Sensor (Hive Hook) on Qubole-based Hadoop2\/Hive clusters. Unravel Sensor collects information about Hive queries in Hadoop and pushes it to Unravel Server. Workflow Summary Step 1 is a one-time only step. If the Qubole cluster already exists (is already running), do a \"setup\" procedure: follow the steps 3 to 5 in Hive Bootstrap and Unravel Hive Hook Sensor Setup. " }, 
{ "title" : "Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part2.html#UUID-e74e34e8-930f-2c37-7305-8e186e2d77ec_id_Part2InstallUnravelHiveSensoronQuboleHadoop2HiveCluster-HiveBootstrapandUnravelHiveHookSensorSetup", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 2: Install Unravel Hive Sensor on Qubole Hadoop2\/Hive Cluster \/ Hive Bootstrap and Unravel Hive Hook Sensor Setup:", 
"snippet" : "- is the s3 location where the unravel hive hook jar will be accessed location_in_s3_where_unravel_jar_folder - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Unravel_Hostname_FQDN_Internal_IP Add a one-time Unravel Hive ...", 
"body" : " - is the s3 location where the unravel hive hook jar will be accessed location_in_s3_where_unravel_jar_folder - is the Unravel server's fully qualified internal IP address, preferably the LAN (private) IP if in the same availability zone Unravel_Hostname_FQDN_Internal_IP Add a one-time Unravel Hive Bootstrap into Qubole's control panel on the left-hand side in the \"Hive Bootstrap\" section, as follows: add jar s3n:\/\/ location_in_s3_where_unravel_jar_folder Unravel_Hostname_FQDN_Internal_IP Determine the Hive version that Qubole uses, and use that value for HIVE_VER To determine the Hive version Qubole uses, see http:\/\/docs.qubole.com\/en\/latest\/faqs\/hive\/version-hive-qubole-provide.html must be a Hive version that Unravel Server supports: either HIVE_VER 1.2.0 0.13.0 On the master node of the Qubole Hadoop2\/Hive cluster, check that Unravel Server is reachable. Ensure the security group on the master\/slave allows TCP port 3000 and 4043 accessible. # curl http:\/\/{ Unravel_Hostname_FQDN_Internal_IP If the version information is not visible, adjust security groups and routing and try again. SSH to the master server of the existing Qubole Hadoop2\/Hive cluster, run the following commands: Use curl # curl http:\/\/ Unravel_Hostname_FQDN_Internal_IP Ensure wget The installation creates these files: Hive hook jar is \/usr\/local\/unravel_client\/ Spark jar is \/usr\/local\/unravel-spark\/ Unravel ES is \/usr\/local\/unravel_es\/ On the master node, \/etc\/init.d\/unravel_es service # yum install -y wget\n# chmod 755 unravel_qubole_setup.sh \n# sudo .\/unravel_qubole_setup.sh install -y --unravel-server Unravel_Hostname_FQDN_Internal_IP Verify if the setup works in Qubole by invoking Analyze and execute following Hive test query: set hive.on.master=true ;\nselect count(*) from default_qubole_memetracker; " }, 
{ "title" : "Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Introduction", 
"snippet" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your parti...", 
"body" : "Unravel Sensor can collect information about Spark applications. This guide describes how to install Unravel Sensor for Qubole-based Spark clusters. The information here applies to Spark versions 1.5.x through 2.0.x and Unravel 3.4-4.1. Highlighted text indicates where you must substitute your particular values. text and text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text. HIGHLIGHTED must be a fully qualified DNS or the IP address. UNRAVEL_HOST_IP : is the absolute path to the location of the sensor jars. PATH_TO_SENSOR_JARS " }, 
{ "title" : "Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AddUnravelSparkInstrumentationtoNewQuboleSparkCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Add Unravel Spark Instrumentation to New Qubole Spark Cluster", 
"snippet" : "Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties Substitute actual values for ACCESS_KEY_VALUEx SECRET_KEY_VALUEx [default] aws_access_key_id = ACCESS_KEY_VALUE1 SECRET_KEY_VALUE1 ACCESS_KEY_VALUE2 SECRET_KEY_VALUE2 Edit \/usr\/local\/unra...", 
"body" : " Configure Unravel s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties This sample s3ro.properties Substitute actual values for ACCESS_KEY_VALUEx SECRET_KEY_VALUEx [default]\naws_access_key_id = ACCESS_KEY_VALUE1 SECRET_KEY_VALUE1 ACCESS_KEY_VALUE2 SECRET_KEY_VALUE2 Edit \/usr\/local\/unravel\/etc\/unravel.properties s3ro.properties com.unraveldata.s3.profile.config.file.path=\/usr\/local\/unravel\/etc\/s3ro.properties\ncom.unraveldata.spark.s3.profilesToBuckets=<default>:<s3ro_bucket1>,<profile_name_2>:<s3r0_bucket2> Restart the Unravel ETL daemon(s): # sudo \/etc\/init.d\/unravel_all.sh stop-etl\n# sudo \/etc\/init.d\/unravel_all.sh start Ensure ports 3000 (for web UI access) and 4043 (from cluster) are open for incoming traffic. These should not Verify that port 3000 is open by running a curl If the version information is not visible (request timeout), then adjust security groups, firewalls, etc. and try again. For VPCs, it might be necessary to add a route. # curl http:\/\/{UNRAVEL_HOST_IP}:3000\/version.txt Copy the Unravel Spark bootstrap file from Unravel Server to your Spark Qubole cluster, using curl Substitute the actual host for UNRAVEL_HOST_IP # curl http:\/\/ UNRAVEL_HOST_IP Copy Unravel Spark bootstrap file to your s3:\/\/ Bootstrap_location_for_Spark Qubole Cluster aws s3 cp unravel_qubole_bootstrap.sh s3:\/\/{Bootstrap_location_for_Spark Qubole cluster}\/unravel_qubole_bootstrap.sh In Qubole's Edit Cluster Setting do not unravel_qubole_bootstrap.sh In Qubole, scripts do not take input parameters. Therefore, Unravel's bootstrap script takes all the required parameters from the Hadoop configuration. You can customize your Hadoop configuration through specific parameters within the Override Hadoop Configuration Variables unravel-bootstrap Separate parameters with commas. Add these settings to unravel-bootstrap The parameters in square brackets [ ] below are optional. Their meanings are: SPARK_APP_LOAD_MODE Appendix WRAPPED_SCRIPT unravel_qubole_bootstrap.sh unravel-bootstrap=UNRAVEL_SERVER= UNRAVEL_HOST_AND_PORT SPARK_VER_XYZ SPARK_APP_LOAD_MODE WRAPPED_SCRIPT SPARK_VERSION_X.Y.Z Confirm that the unravel_es Open an SSH session to the Qubole master node. Run this command to check that the unravel_es service has been started: # ps -aux | grep unravel_es " }, 
{ "title" : "(Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-OptionalAddUnravelSparkInstrumentationtoanExistingQuboleSparkCluster", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ (Optional) Add Unravel Spark Instrumentation to an Existing Qubole Spark Cluster", 
"snippet" : "Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP To obtain Spark version 2.0.x zip file: # ...", 
"body" : " Ensure that you have configured s3 read-only credentials in \/usr\/local\/unravel\/etc\/s3ro.properties steps 1 and 2 above Obtain following essential Spark version files from Unravel Server: To obtain Spark version 1.6.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP To obtain Spark version 2.0.x zip file: # wget http:\/\/ UNRAVEL_HOST_IP Unzip the archive unravel-sensor-for-spark-bin-1.6.zip PATH_TO_SENSOR_JARS Obtain EMR jar files and snippet script that should be added to the bootstrap action to start the unravel_es process: \n#wget http:\/\/ UNRAVEL_HOST_IP UNRAVEL_HOST_IP Ensure that unravel-emr-sensor.jar unravel-emr-sensor.jar PATH_TO_SENSOR_JARS Edit run-es.sh # Replace following two parameters into your Qubole Spark environment #\nUNRAVEL_HOST= UNRAVEL_HOST_IP PATH_TO_SENSOR_JARS Spark configuration can be provided in theQubole consoleat bootstrap or directly inside spark-defaults.conf spark.unravel.server.hostport UNRAVEL_HOST_IP PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS Edit zeppelin-env.sh ZEPPELIN_JAVA_OPTS export ZEPPELIN_JAVA_OPTS=\"-Dcom.sun.btrace.FileClient.flush=-1 -javaagent: PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS PATH_TO_SENSOR_JARS The Zeppelin configuration is located at \/usr\/lib\/zeppelin\/conf\/zeppelin-env.sh " }, 
{ "title" : "Appendix", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-AppendixAppendix", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Appendix", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Application Loading Modes for Spark Applications: OPS, DEV, and BATCH", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-ApplicationLoadingModesforSparkApplicationsOPSDEVandBATCH", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Application Loading Modes for Spark Applications: OPS, DEV, and BATCH", 
"snippet" : "There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark application completes. BATCH mode loads applications for which the sen...", 
"body" : "There are three modes in which Spark applications can be loaded into Unravel Web UI: OPS mode shows applications in the UI after the application is done. DEV mode shows applications in the UI as soon as the first job of the Spark application completes. BATCH mode loads applications for which the sensor was not enabled at the time the application has been run. You can specify the application load mode for the bootstrap script by setting SPARK_APP_LOAD_MODE OPS DEV SPARK_APP_LOAD_MODE OPS " }, 
{ "title" : "When to Use Which Mode", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-WhentoUseWhichMode", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ When to Use Which Mode", 
"snippet" : "Unravel recommends using OPS mode as the cluster-side default for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOpt...", 
"body" : " Unravel recommends using OPS mode as the cluster-side default for all Qubole clusters. The OPS mode has been rigorously benchmarked to have less than 1.3% CPU and memory overhead. Both the OPS and DEV modes use the Unravel Spark sensor, which is enabled via modifications to spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false DEV mode is useful during Spark application development. This mode shows a Spark application as soon as the first Spark job of the application finishes. For long running applications, this functionality is useful, as the application is shown in the UI while the application is running. In addition, DEV mode shows applications even when the Spark event log is not being persisted to HDFS or S3. This is an advantage in situations like Spark on Mesos. In a Qubole cluster that is using OPS mode as the default, DEV mode can be obtained for individual Spark applications by overriding spark.driver.extraJavaOptions -Dcom.unraveldata.spark.sensor.disableLiveUpdates=false BATCH mode is always on and does not interfere with application performance in any way since the loading is entirely outside the application execution path. For details see the next section. " }, 
{ "title" : "Loading Applications in Batch Mode", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part3.html#UUID-5ec11343-7c26-ad21-307c-c8ef8280eb65_id_Part3InstallUnravelSparkSensoronNeworExistingQuboleSparkCluster-LoadingApplicationsinBatchMode", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 3: Install Unravel Spark Sensor on New or Existing Qubole Spark Cluster \/ Loading Applications in Batch Mode", 
"snippet" : "In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is ...", 
"body" : "In order to load Spark apps in BATCH mode, Unravel Server must pull the Spark event log file either from S3 or from HDFS. The data collected in the UI is less detailed than when Unravel Sensor is enabled (for instance, detailed resource usage metrics are unavailable). The BATCH mode of operation is helpful to load all of the applications that have been run in the past, before Unravel Sensor was installed. To enable the batch mode, add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.eventlog.location=hdfs:\/\/ NAMENODE_IP_PORT Currently, only one event log location is supported. " }, 
{ "title" : "Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part4.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Introduction", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part4.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-Introduction", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ Introduction", 
"snippet" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workf...", 
"body" : "This topic explains how to modify and Amazon EMR cluster's bootstrap\/setup for Unravel Server 4.0. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Steps marked [HVD] are for production high volume and durable installations can be skipped for PoCs. Workflow Summary Add your AWS account number(s) to the Unravel Data main s3 bucket policy. Get the bootstrap script(s). Integrate the bootstrap script(s) into your Amazon EMR cluster(s). " }, 
{ "title" : "1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part4.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-1AddYourAWSAccountNumberstotheUnravelDataMainS3BucketPolicy", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 1. Add Your AWS Account Number(s) to the Unravel Data Main S3 Bucket Policy", 
"snippet" : "Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients...", 
"body" : " Prior to doing below steps, ensure that your AWS account number(s) is added to the Unravel Data main s3 bucket policy for s3:\/\/unraveldata-client \/usr\/local\/unravel\/install_bin\/unraveldata-clients " }, 
{ "title" : "2. Get the Bootstrap Script(s)", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part4.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-2GettheBootstrapScripts", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 2. Get the Bootstrap Script(s)", 
"snippet" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/unraveldata-clients\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy...", 
"body" : "To gain access to the s3 bucket that contains the bootstrap scripts ( s3:\/\/unraveldata-clients\/ EMR_DefaultRole In the IAM management console on the left-hand side, click Roles EMR_DefaultRole Click Inline Policies Create Role Policy Custom Policy Click Select Enter the policy name as you wish. Copy and paste below policy into the Policy Document: {\n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n { \n \"Sid\": \"getunraveldataclients3files\", \n \"Effect\": \"Allow\", \n \"Action\": [ \n \"s3:ListBucket\", \n \"s3:Get*\" \n ], \n \"Resource\": [ \n \"arn:aws:s3:::unraveldata-clients\/*\" \n ] \n } \n ]\n} Save it by clicking Apply Policy When you create a new Amazon EMR cluster, be sure to add a bootstrap action as shown in the IAM screenshot below. You need to copy and paste the full pathname of the bootstrap action (script) into the Script location Important Note: Do not For guidance on which script to use, see the table below. File S3 Bucket Local Directory Applies To unravel_emr_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 3.x Hive unravel_emr4_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 4.x Hive " }, 
{ "title" : "3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part4.html#UUID-f01e3951-c89c-a2cd-0ead-53b6a4996c79_id_Part4ModifyAmazonEMRClusterBootstrapSetup-3IntegratetheBootstrapScriptsintoYourAmazonEMRClusters", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 4: Modify Amazon EMR Cluster Bootstrap\/Setup \/ 3. Integrate the Bootstrap Script(s) into Your Amazon EMR Cluster(s)", 
"snippet" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unrave...", 
"body" : "For Persistent (\"Long-Running\" or \"Existing\") Amazon EMR Clusters Hive Applications: Unravel does not load data from a cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unraveldata-clients\/unravel_emr_setup.sh aws s3 install_bin\/unraveldata-clients Copy unravel_emr_setup.sh \/tmp scp ssh hadoop Open an SSH session to the cluster's master node ( ssh hadoop hadoop cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --unravel-server LOCAL_IP To uninstall Hive instrumentation on an Amazon EMR cluster (perhaps because you want to upgrade the instrumentation), you simply run the same install script again with the uninstall cd \/tmp \naws s3 cp s3:\/\/unraveldata-clients\/unravel_emr_setup.sh . \nchmod +x unravel_emr_setup.sh \n.\/unravel_emr_setup.sh --uninstall For Transient Amazon EMR Clusters Hive Applications: This is similar to the previous section on integrating an existing cluster except the script used as a bootstrap step is one of the following files: File S3 Bucket Local Directory Applies To unravel_emr_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 3.x Hive unravel_emr4_bootstrap.sh s3:\/\/unraveldata-clients\/ install_bin\/unraveldata-clients\/ Amazon EMR 4.x Hive Spark Applications: The Unravel Server does not load data from a Spark cluster until the cluster is instrumented. Use the steps here to set up an existing cluster. Identify the the local IP address of your Unravel Server ( LOCAL_IP Download s3:\/\/unraveldata-clients\/unravel_emr_spark_setup.sh aws s3 install_bin\/unraveldata-clients Copy unravel_emr_setup.sh \/tmp scp ssh ec2-user Open an SSH session to the cluster's master node ( ssh ec2-user ec2-user cd \/tmp \nsudo .\/unravel_emr_spark_setup.sh --unravel-server \\ \n $LOCAL_IP:3000 --client Change --client --cluster " }, 
{ "title" : "Part 5: Additional Topics for EMR or Qubole", 
"url" : "unravel-4-0-4-1/install/amazon-emr-and-qubole/install-emr-part5.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Installation Guides \/ Amazon EMR and Qubole \/ Part 5: Additional Topics for EMR or Qubole", 
"snippet" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Custom Configurations Adding More Admins to Unravel Web UI Sending Diagnostics to Unravel Support...", 
"body" : " Creating Active Directory Kerberos Principals and Keytabs for Unravel Installing Unravel Sensor for Individual Applications Submitted Through spark-submit Custom Configurations Adding More Admins to Unravel Web UI Sending Diagnostics to Unravel Support " }, 
{ "title" : "User Guide", 
"url" : "unravel-4-0-4-1/user-guide.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide", 
"snippet" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations....", 
"body" : "The key value proposition of Unravel is to help you analyze, optimize, and troubleshoot big data applications and operations. " }, 
{ "title" : "Getting Started", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase1HowtoSearchforApplicationsandOptimizeTuneaHiveApplication", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #1: How to Search for Applications and Optimize\/Tune a Hive Application", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase2HowtoRootCauseIssueswithaWorkflowthatMissedItsSLA", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #3: How to Debug Failed Applications", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase3HowtoDebugFailedApplications", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #3: How to Debug Failed Applications", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-UseCase4HowtoReviewSparkApplicationsandIdentifyAreasforPerformanceImprovements", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Use Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Running the Configuration Wizard", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-RunningtheConfigurationWizard", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Running the Configuration Wizard", 
"snippet" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for faile...", 
"body" : "After you install the Unravel Server RPM, you can run Unravel Web UI's configuration wizard to: Set up access to various Big Data components (HDFS, Hive, Oozie, and so on). Create users Set up email\/SMTP, LDAP, Kerberos The configuration wizard informs you of errors and continuously checks for failed and incorrect settings. To start the configuration wizard, click Admin Manage Configuration The Unravel Web UI configuration wizard is available only for the admin " }, 
{ "title" : "Setting Up Access to Big Data Components", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-SettingUpAccesstoBigDataComponents", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Setting Up Access to Big Data Components", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"url" : "unravel-4-0-4-1/user-guide/getting-started.html#UUID-e4556521-68b0-45c3-541b-be128a3c5e4a_id_GettingStarted-CreatingUsersandSettingUpEmailSMTPLDAPKerberos", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Getting Started \/ Creating Users and Setting Up Email\/SMTP, LDAP, Kerberos", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "The Operations Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Dashboard", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Dashboard", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Dashboard", 
"snippet" : "To view the dashboard, click Operations Dashboard The dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a spe...", 
"body" : "To view the dashboard, click Operations Dashboard The dashboard provides a good overview of all activities in the cluster. It contains tiles that display cluster KPI time series, application summaries, and highlights of inefficient applications and workflows missing SLAs. To see details within a specific tile, click that tile. To configure the dashboard for a specific time range or cluster, select options from the pull-down menus in the banner. " }, 
{ "title" : "Charts", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Charts", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Charts", 
"snippet" : "To view charts, click Operations Charts Charts To see KPIs for resources, jobs, nodes, or services, select the appropriate secondary tab in the Charts To see a detailed view of any of KPI, click the tile for that KPI. To configure Charts...", 
"body" : "To view charts, click Operations Charts Charts To see KPIs for resources, jobs, nodes, or services, select the appropriate secondary tab in the Charts To see a detailed view of any of KPI, click the tile for that KPI. To configure Charts " }, 
{ "title" : "Reports", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Reports", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Reports", 
"snippet" : "To view reports, click Operations Reports...", 
"body" : "To view reports, click Operations Reports " }, 
{ "title" : "Chargeback", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-Chargeback", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Chargeback", 
"snippet" : "Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback...", 
"body" : "Unravel provides an easy way to create chargeback reports for multi-tenant cluster usage. You can generate usage reports categorized by application type, queue, users, and so on. To configure Chargeback " }, 
{ "title" : "Cluster Summary", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-ClusterSummary", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Cluster Summary", 
"snippet" : "The Cluster Summary Reports...", 
"body" : "The Cluster Summary Reports " }, 
{ "title" : "Cluster Compare", 
"url" : "unravel-4-0-4-1/user-guide/the-operations-tab.html#UUID-76e842b1-76c3-a0b6-83c9-616def6286e6_id_TheOperationsTab-ClusterCompare", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Operations Tab \/ Cluster Compare", 
"snippet" : "The Cluster Compare Reports...", 
"body" : "The Cluster Compare Reports " }, 
{ "title" : "The Applications Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab", 
"snippet" : "The Applications The performance and reliability of an application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort t...", 
"body" : "The Applications The performance and reliability of an application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems of an application. Unravel provides insights into an application. These insights are called events Key Performance Indicators " }, 
{ "title" : "Finding Applications", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-FindingApplications", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Applications", 
"snippet" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property,...", 
"body" : "You can search on various dimensions (app type, status, queue, user, cluster, duration, and so on) to find your application(s). Use the date pull-down menu to limit results to a specific time range. Search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. You can also use the global search bar in the top banner to search by full job ID, user name, table name and cluster ID. Search results list individual jobs, and job IDs. If the job is part of a Hive query, Pig script, and\/or a workflow, then a link to that Hive query\/Pig script\/workflow page appears on the same line. To go to the job-specific page, click the job ID. To go to the application-specific manager for a query\/script\/job\/workflow, click the icon under its GoTo " }, 
{ "title" : "Application-Specific Managers", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-Application-SpecificManagers", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Application-Specific Managers", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Spark Application Manager", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-SparkApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Spark Application Manager", 
"snippet" : "The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor partitioning Detect and fix ...", 
"body" : "The Spark Application Manager provides a detailed view into the behavior of Spark applications. You can use this view to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications Optimize resource allocation for Spark executors Detect and fix poor partitioning Detect and fix inefficient and failed Spark apps Tune JVM settings for Spark drivers\/executors Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Spark application. The Spark Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Spark application depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Spark application. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of stages that make up the Spark application and their status Number of Stages Sub-Tabs The Spark Application Manager contains multiple sub-tabs, each of which is described below. : A table of the stages associated with this application. To see details about a stage, click its row in the table. This displays the Spark Stage view, as illustrated in the image below. Navigation The Spark Stage view provides detailed information about each Spark stage. It includes: : General task\/slot statistics of the stage Graphs : Timeline and histogram of task attempts, duration, and bytes shuffled\/spilled. Timeline This information is very useful for identifying data skew. : Key\/value pairs of configuration settings for the application Configuration Click here to see sample screenshots... : Displays both the stage view and RDD view associated with this application. Execution Graph : A timeline of application stages Gantt Chart : The logical plan of the SparkSQL query Query Plan : Exceptions, errors, and warnings associated with this application Errors : Logs for the driver and executors of this application, and a skyline of task attempts within the stage Logs : Source code of a general purpose Spark application, or the SQL query for a SparkSQL query Program : Statistics about the task attempts that are executed as part of the current application Task Attempts : Utilization of slot containers over time Containers : Utilization of slot vcores over time Vcores : Utilization of slot memory over time Memory : Graphs of JVM-level metrics at the executor and driver level. To show the graph for a specific metric, select that metric from the Resource METRIC Get Data Resource Metrics " }, 
{ "title" : "Hive Application Manager", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-HiveApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Hive Application Manager", 
"snippet" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. You can use this view to resolve inefficiencies, bottlenecks and reasons for failure within applications. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). The Hive Application ...", 
"body" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. You can use this view to resolve inefficiencies, bottlenecks and reasons for failure within applications. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). The Hive Application Manager uses Unravel's Intelligence Engine to automatically identify certain inefficiencies with the application and to provide recommendations on how to improve efficiency. Key Performance Indicators (KPIs) The key performance indicators (KPIs) at the top provide the most important information about the Hive query. The Hive Application Manager displays the following KPIs: : The number of Unravel recommendations or insights for this query. To see details, click the Events Events The performance and reliability of your Hive query depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, and so on. Therefore, it takes significant expertise and effort to get to the root cause of problems in a Hive query. Unravel Events are designed to save you time and effort by automatically providing insights into the application. These events capture reasons for failed and killed queries as well as provide recommendations to improve application performance. : Total time taken by the application to complete execution Duration : Total data read and written by the application Data I\/O : The number of YARN apps that make up the Hive query Number of YARN apps Sub-Tabs The Hive Application Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the application and drill down into MapReduce jobs that make up the application. It includes information about MapReduce jobs such as duration, I\/O, resources, events, job ID, and job status. When you select a MapReduce job, its details are shown in a pane below it. This detailed view includes the MapReduce timeline, MapReduce task attempts, and slot usage graph. Navigation : Provides insights into the execution of the application. It shows detailed information about MapReduce stages and their relationship with one another. The execution view is split into two areas; the execution plan and the expanded info area. Execution Graph : Shows the relationship between MapReduce stages and high level information about each stage. The information shown in each stage box includes MapReduce job ID, base table name, time taken by the stage, percentage of total run time taken by the stage, and execution status. Gantt Chart Errors : Shows detailed information about a stage when selected. The expanded info shows each of the map reduce functions, tables usage information, timings of each function and input paths used by the stage. Query : Provides an easy way to understand efficiency and status of MapReduce task attempts by breaking down attempted tasks by successful, failed and killed. Task Attempts Attempts : Shows slot usage by Map and Reduce jobs over time. Slot Usage " }, 
{ "title" : "MapReduce Application Manager", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-MapReduceApplicationManager", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ MapReduce Application Manager", 
"snippet" : "The MapReduce Application Manager provides a detailed view into the behavior of MapReduce applications. It is used by Hadoop DBAs or application owners (engineers, BI team, analysts) to resolve inefficiencies, bottlenecks and reasons for failure within applications. The MapReduce Application Manager...", 
"body" : "The MapReduce Application Manager provides a detailed view into the behavior of MapReduce applications. It is used by Hadoop DBAs or application owners (engineers, BI team, analysts) to resolve inefficiencies, bottlenecks and reasons for failure within applications. The MapReduce Application Manager uses the Unravel intelligence engine to automatically identify certain inefficiencies with the application and provides solutions on how to the fix the problem. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Sub-Tabs The MapReduce Application Manager contains multiple sub-tabs, each of which is described below. : Provides a detailed view into a MapReduce job execution. The Timeline shows execution of each MapReduce task on the machine that those tasks ran on and allows drill-down into each task to obtain further information. The timeline view comes with filters which can be used to display only map tasks, reduce tasks and killed\/failed tasks. The histograms above the Timeline show the distribution of MapReduce tasks along time and data size. This histogram can also be used as a filter to zoom in on specific tasks. Timeline : Provides comprehensive information about each application including task and job logs. The logs section intelligently selects interesting tasks and presents its logs in an organized manner. Logs : Provides a complete list of configuration settings used during the application execution. Configuration " }, 
{ "title" : "Finding Workflows", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-FindingWorkflows", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Finding Workflows", 
"snippet" : "To find workflows, select SHOW Workflow...", 
"body" : "To find workflows, select SHOW Workflow " }, 
{ "title" : "", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Workflow Manager", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-WorkflowManager", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Workflow Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automaticall...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager uses the Unravel intelligence engine to automatically identify inefficiencies with the workflow and provides solutions on how to the fix the problem. The Workflow Manager helps pipeline owners easily maintain SLAs. The workflow header provides primary information about the workflow such as name, user name, queue that the workflow was submitted on, start time and tags that the workflow has been given. Key Performance Indicators (KPIs) : The number of Unravel recommendations or insights for this workflow. To see details, click the Events Events : Total time taken by the workflow to complete execution Duration : Total data read and written by the workflow Data I\/O Resources : The number of apps that make up this workflow Number of Apps Sub-Tabs The Workflow Manager contains multiple sub-tabs, each of which is described below. : Provides an easy way to understand the breakdown of the workflow and drill down into the Hive queries, Spark jobs, and MapReduce jobs that make up the application. It includes information about duration, I\/O, resources, events, job IDs, and job status. Navigation : Shows the dependency between the various components and the time taken by each. It helps you to identify stuck or incomplete components which could be affecting the overall completion of the workflow. Gantt Chart : Clearly shows the dependencies between various components of the workflow and the status of components. To see details about status, hover over a status box to see the tool tips. DAG View : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer over instances displays top KPIs such as duration, data I\/O, resources, and the number of jobs in that instance. Clicking on a point on the chart loads that particular instance for inspection. Compare " }, 
{ "title" : "Events", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-EventsEvents", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Events", 
"snippet" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events...", 
"body" : "Unravel Events are designed to save you time and effort by automatically providing recommendations insights Example: Hive Recommendations Example: Hive Insights Example: Spark Recommendations Example: Spark Insights Example: Workflow Events " }, 
{ "title" : "Error View", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab.html#UUID-4ac59250-f39d-0e67-bf5c-f478d28826ab_id_TheApplicationsTab-ErrorView", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Error View", 
"snippet" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details f...", 
"body" : "This new feature allows to quickly identify errors affecting your applications. Error views are available for MR, Hive and Oozie applications. Errors for each application are categorized by Severity type and also include Keywords and details associated with each. Keywords extract important details from the errors messages\/log data that can help developers\/operators quickly root cause issue. Examples of keywords include Oozie errors code(s), Java run time error(s) etc. " }, 
{ "title" : "Resource Metrics", 
"url" : "unravel-4-0-4-1/user-guide/the-applications-tab/resource-metrics.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Applications Tab \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time w...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description snapshotTs TIMESTAMP (milliseconds) The time the metric was read startTs TIMESTAMP (milliseconds) The time when the collection process started totalPhysicalMemory BYTES The total physical memory in the operating system freePhysicalMemory BYTES The free physical memory in the operating system committedVirtualMemory BYTES The committed virtual memory in the operating system freeSwap BYTES The free swap size availableMemory BYTES An estimate of memory available for launching new processes vmRss BYTES The resident set size of the complete process tree vmRssDir BYTES The resident set size of the process totalSwap BYTES The total swap size processCpuLoad PERCENTS Average process CPU load for the last minute (all cores) systemCpuLoad PERCENTS Average system CPU load for the last minute (all cores) fullGcCount COUNT Number of full GC runs minorGcCount COUNT Number of minor GC runs minorGcTime DURATION (nanoseconds) Accumulated time spent in minor GC fullGcTime DURATION (nanoseconds) Accumulated time spent in full GC gcEdenSurvivedAvg BYTES Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcSurvivorPromotedAvg BYTES Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg BYTES Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms allocatedBytes BYTES Accumulated number of allocated bytes edenPeakUsage BYTES Maximum memory usage in the eden space survivorPeakUsage BYTES Maximum memory usage in the survivor space oldPeakUsage BYTES Maximum memory usage in the old space avgMinorInterval DURATION (nanoseconds) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations avgFullGcInterval DURATION (nanoseconds) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms gcOldLiveAvg BYTES Average number of bytes alive in the old generation. Might not be available for particular GC algorithms initHeap BYTES Initial heap size maxHeap BYTES Maximum heap size usedHeap BYTES Used heap size committedHeap BYTES Committed heap size initNonHeap BYTES Initial non-heap size maxNonHeap BYTES Maximum non-heap size usedNonHeap BYTES Used non-heap size committedNonHeap BYTES Committed non-heap size currentThreadCpuTime DURATION (nanoseconds) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime DURATION (nanoseconds) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems " }, 
{ "title" : "The Data Tab", 
"url" : "unravel-4-0-4-1/user-guide/the-data-tab.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Table\/Partition Usage", 
"url" : "unravel-4-0-4-1/user-guide/the-data-tab.html#UUID-252ec35e-09cb-026a-9479-a05cb52df60e_id_TheDataTab-TablePartitionUsage", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ The Data Tab \/ Table\/Partition Usage", 
"snippet" : "The Data Details On the Details Click on any column to sort by that metric such as: Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read I\/O - to find tables which have performed most I\/O Attempts - to find tables which have had most number of read\/write att...", 
"body" : "The Data Details On the Details Click on any column to sort by that metric such as: Last Access - to find tables used most recently Created At - to sort oldest\/newest tables Read I\/O - to find tables which have performed most I\/O Attempts - to find tables which have had most number of read\/write attempts on them Apps - to find tables which have most number of applications using it Users - to find tables which have the most number of users using it Other Tables - shows other tables commonly used along with a particular table in applications Labels - view tables which have been labeled Hot, Warm or Cold Hover on columns like Users and Other Tables to know respectively which users accessed this table and which other tables are accessed along with this table. View trends of table usage and access by accessing the drop-down menu on the top right corner. Metrics available are Read I\/O, Total Users, Total Apps, Total Attempts as shown below. You can also search for a particular table by typing part of the table name in the table search box as shown below. In our example we are looking for tables with the word customer in it. Table Details To drill down into a particular table click its More Info This opens a Table Detail You can view different metric trends on this pane including: Read IO Number of Users Number of Apps Number of Attempts It also allows you to view the historical usage and access information about this table. The table detail view also shows a list of applications and users that accessed the table in the given time range. This list of applications and users is also sortable for easy search and browse capabilities. Partition Details To view detailed information about partitions associated with a table, click the Partition Detail Table Detailed View Partition Detail The partition histogram displays the access age of partitions (including the max access age). Unravel uses this information to calculate the number and size of reclaimable partitions as follows: Reclaimable partitions found = number of partitions with last access time < (current time - max access age of partitions). In the example above: No partition is accessed by any application more than 4 days 14 hours after it is created. Thus, the partitions created on Dec 1, 2015, will not be accessed after Dec 5, 2015. In Unravel, the max access age of partitions for the above table will be computed as 4 days 14 hrs based on the history of accesses to partitions in the above table. So on Dec 15, 2015, when you look at the table above: Reclaimable Partitions found = number of Partitions with Last Access Time < (Dec 14 2015 - 4days 14 hours) i.e., (Dec 10, 2015). Thus, the partitions created on any day before Dec 10, 2015 will be marked as a Reclaimable Partition. In this case, it will be all the 26 partitions. The page also list all partitions by key KPIs, including: Last access date\/time Create date\/time Current size Number of users accessing the partition Unravel provides an easy way to label tables\/partitions as HOT WARM COLD Once you have the rule configured as above, click SAVE RULES While the HOT WARM COLD As illustrated above, HOT rule can be \"Last Access <= 120 days\", WARN rule can be \"Last Access <= 175 days and > 120 days\" and COLD rule can be \"Last Access >= 176 days\" These rules are checked periodically (every 24 hours) and tables\/partition are labeled and classified accordingly. The Data overview page provides a quick summary information based on these labels. Operators can use this information to identify unused or frequently used tables\/partitions and take appropriate actions. " }, 
{ "title" : "Setting Up Auto Actions (Alerts)", 
"url" : "unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Auto Actions", 
"url" : "unravel-4-0-4-1/user-guide/setting-up-auto-actions--alerts-.html#UUID-9fa98f37-b271-7594-bd19-35bf82f977ce_id_SettingUpAutoActionsAlerts-AutoActions", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Setting Up Auto Actions (Alerts) \/ Auto Actions", 
"snippet" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin M...", 
"body" : "An Auto Action is basically a policy such that when it is violated, an action is taken automatically. You can use an Auto Action to alert you to a situation that needs manual intervention, such as resource contention, stuck jobs, failed queries, and so on. To create an Auto Action: Click the Admin Manage On the Manage Auto Actions Click ADD NEW AUTO ACTION Select the desired Auto Action type, and then click Next Enter a name for the new Auto Action, and specify its prerequisite conditions, defining conditions, and actions: Prerequisite conditions: A set of boolean conditions such that when they are all met, Unravel will evaluate the defining conditions of the Auto Action. Examples include: whether this Auto Action should be evaluated during a given time, whether this Auto Action should be evaluated for a job belonging to a given user, etc. Defining conditions: A set of boolean conditions defining the Auto Action. When they are all met, the corresponding action defined as part of the Auto Action will be taken automatically. Examples include: Is this job running for too long? Does it use too many mappers? Actions: A set of actions to be taken when the defining conditions are all evaluated to be true. Examples include: send an email to admin (i.e., “alerting”), kill the job, etc. Click SAVE AUTO ACTION " }, 
{ "title" : "Use Cases", 
"url" : "unravel-4-0-4-1/user-guide/use-cases.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "unravel-4-0-4-1/user-guide/use-cases/detecting-resource-contention-in-the-cluster.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention in Cluster Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "unravel-4-0-4-1/user-guide/use-cases/identifying-rogue-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Discovering Inefficient Applications", 
"url" : "unravel-4-0-4-1/user-guide/use-cases/discovering-inefficient-applications.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ User Guide \/ Use Cases \/ Discovering Inefficient Applications", 
"snippet" : "The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the f...", 
"body" : "The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). Save up to 9 minutes by caching atPetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Unravel Server Reference", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel Server Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zooke...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_tc bundled TomCat (port 3000), Web UI unravel_db bundled db (on a custom port) unravel_zk_N bundled Zookeeper (on a custom port) unravel_k bundled Kafka (on a custom port) unravel_hhs Hive Hook Sensor unravel_hhw Hive Hook Worker unravel_hhwe Hive Hook Worker EMR unravel_hvw Hive Worker unravel_jcs1 Job Collector Sensor MRv1 unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw1_N Job Collector Sensor Worker MRv1 unravel_jcw2_N Job Collector Sensor Worker YARN unravel_lr Log Receiver unravel_mrw Map Reduce Worker unravel_ja \"Job Analyzer\" summarizes jobs unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_os3 Oozie v3 Sensor unravel_os4 Oozie v4 Sensor unravel_tw Table Worker unravel_pw Partition Worker unravel_ew_N Event Worker unravel_sw_N Spark Worker " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name Default Value Description com.unraveldata.tmpdir \/srv\/unravel\/tmp $UNRAVEL_DATA_DIR\/tmp Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. com.unraveldata.logdir \/usr\/local\/unravel\/logs Do not set directly; set UNRAVEL_LOG_DIR in etc\/unravel.ext.sh instead and this property will be derived from that com.unraveldata.zk.quorum 127.0.0.1:4181 embedded Zookeeper ensemble in form host1:port1,host2:port2, ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ com.unraveldata.kafka.broker_list 127.0.0.1:4091 embedded Kafka unravel.jdbc.username unravel MySQL (embedded or external) username for db unravel.jdbc.password random generated for bundled MySQL MySQL (embedded or external) password for db unravel.jdbc.url jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod This is JDBC URL without username and password com.unraveldata.hdfs.interactive.monitoring.interval.sec 30 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) com.unraveldata.hdfs.batch.monitoring.interval.sec 300 Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) com.unraveldata.longest.job.duration.days 2 Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) oozie.server.url http:\/\/localhost:11000\/oozie URL for accessing Oozie to track workflows com.unraveldata.oozie.fetch.num 100 Max number of jobs to fetch during an interval com.unraveldata.oozie.fetch.interval.sec 120 seconds between intervals for fetching Oozie workflow status " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java JAVA_EXT_OPTS unset Last chance a...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source Env Variable Default if not set Description JAVA_HOME Should use update-alternatives to make correct Java first choice The standard way to specify the home dir. of Oracle Java so that $JAVA_HOME\/bin\/java JAVA_EXT_OPTS unset Last chance arguments to jvm to override other settings HADOOP_CONF as discovered by running hadoop fs -ls The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml UNRAVEL_DATA_DIR \/srv\/unravel A base directory owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data zk_data UNRAVEL_LISTEN_PORT 3000 The Unravel UI port on the primary or standalone Unravel installation (service unravel_tc 0.0.0.0 com.unraveldata.tc.port UNRAVEL_LOG_DIR \/usr\/local\/unravel\/logs A destination dir. owned by user unravel for log files UNRAVEL_TC_SHUTDOWN_PORT 3005 An unoccupied port used for cleanly stopping the Unravel UI (service unravel_tc 127.0.0.1 " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "unravel-4-0-4-1/unravel-server-reference.html#UUID-91ae4d7b-8d28-1df8-05c6-5436e1af1505_id_UnravelServerReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel Server Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs L...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path Purpose Expected Size Notes \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~1.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file to preserve traceability \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/srv\/unravel\/log_hdfs log directory for daemons that run as user hdfs <2GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR \/srv\/unravel\/tmp_hdfs tmp directory for daemons that run as user hdfs <1GB Needed for YARN. Owned by user hdfs; not applicable for MRv1 or EMR " }, 
{ "title" : "Advanced Topics", 
"url" : "unravel-4-0-4-1/advanced-topics.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Define HOST Variable for Unravel Server as an FQDN", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineHOSTVariableforUnravelServerasanFQDN", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define HOST Variable for Unravel Server as an FQDN", 
"snippet" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST...", 
"body" : "(Replace UNRAVEL_HOST HOST=$UNRAVEL_HOST " }, 
{ "title" : "Define REALM Variable", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-DefineREALMVariable", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Define REALM Variable", 
"snippet" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM...", 
"body" : "(Use upper case for all; replace EXAMPLEDOTCOM REALM=$EXAMPLEDOTCOM " }, 
{ "title" : "Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-active-directory-kerberos-principals-and-keytabs-for-unravel.html#UUID-27942b0f-0489-fe23-9146-a5723738861a_id_CreatingActiveDirectoryKerberosPrincipalsandKeytabsforUnravel-CreatetheActiveDirectoryADKerberosPrincipalsandKeytabs", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel \/ Create the Active Directory (AD) Kerberos Principals and Keytabs", 
"snippet" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Adminis...", 
"body" : "Use the two variables you defined above to replace the magenta text below. Verify that Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel hdfs Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires Un-select Password must be changed Select Password cannot be changed Right-click the created user, click Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: The commands to run in a cmd or powershell are the following: setspn -A unravel\/HOSTunravel setspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab HOST \/etc\/keytabs\/ sudo chmod 700 \/etc\/keytabs\/* sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Sensors", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-applications-submitted-through-spark-shell.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions ...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/ UNRAVEL_HOST VERSION To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION 2.0 1.6 1.5 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps executed via the spark-shell To intercept Spark apps executed via the spark-shell, you need to unzip the Unravel Sensor .zip file on the client node at a location on the local file system that is readable by all users, referred to as UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark- VERSION Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-shell always executed in client mode To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 SPARK_EVENT_LOG_DIR UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\n\n\/usr\/lib\/spark\/bin\/spark-shell \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n<<EOF\n\/\/ your spark-shell code snippet will follow here\n\/\/ For exemplifying, we use a snippet of RDDRelation below\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\ncase class Record(key: Int, value: String)\n\n val df = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\"))).toDF()\n\n \/\/ Any RDD containing case classes can be registered as a table. The schema of the table is\n \/\/ automatically inferred using scala reflection.\n df.registerTempTable(\"records\")\n\n \/\/ Once tables have been registered, you can run SQL queries over them.\n println(\"Result of SELECT *:\")\n sqlContext.sql(\"SELECT * FROM records\").collect().foreach(println)\n\n \/\/ Aggregation queries are also supported.\n val count = sqlContext.sql(\"SELECT COUNT(*) FROM records\").collect().head.getLong(0)\n println(s\"COUNT(*): $count\")\n\nexit\n\nEOF\n\n Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-applications-submitted-through-spark-shell-8218.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Applications Submitted Through spark-shell", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications executed via spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications executed via spark-shell per-application profiling cluster-wide profiling btrace-agent.jar btrace-boot.jar unravel-boot.jar, and unravel-sys.jar spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-submit The information here applies to Spark versions 1.3.x through 2.0.x. Brown text indicates where you must substitute your particular values. 1. Obtain the Sensor The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST http:\/\/ UNRAVEL_HOST VERSION To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/spark-VERSION\/ VERSION 2.0 1.6 1.5 1.3 Within this directory, locate the sensor file: unravel-sensor-for-spark-bin.zip 2. Run the Sensor to Intercept Spark Apps executed via the spark-shell To intercept Spark apps executed via the spark-shell, you need to unzip the Unravel Sensor .zip file on the client node at a location on the local file system that is readable by all users, referred to as UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark If you use multiple hosts as clients, do this step for each client mkdir $UNZIPPED_ARCHIVE_DEST\ncd $UNZIPPED_ARCHIVE_DEST \nwget http:\/\/$UNRAVEL_HOST:3000\/hh\/spark-VERSION\/unravel-sensor-for-spark-bin.zip\nunzip unravel-sensor-for-spark-bin.zip Important Please keep the original unravel-sensor-for-spark-bin.zip UNZIPPED_ARCHIVE_DEST Define spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark-shell always executed in client mode To use the example below, be sure to replace UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT SPARK_EVENT_LOG_DIR PATH_TO_SPARK_EXAMPLE_JAR UNRAVEL_SENSOR_PATH unravel-sensor-for-spark-bin.zip UNRAVEL_SENSOR_PATH UNRAVEL_SERVER_IP_PORT unravel_lr IP:PORT 10.0.0.142:4043 SPARK_EVENT_LOG_DIR UNZIPPED_ARCHIVE_DEST unravel-sensor-for-spark-bin.zip unravel-sensor-for-spark-bin.zip For example, export UNZIPPED_ARCHIVE_DEST=$UNZIPPED_ARCHIVE_DEST\nexport UNRAVEL_SERVER_IP_PORT=$UNRAVEL_SERVER_IP_PORT\nexport SPARK_EVENT_LOG_DIR=$SPARK_EVENT_LOG_DIR\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:$UNZIPPED_ARCHIVE_DEST\/btrace-agent.jar=bootClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-boot.jar,systemClassPath=$UNZIPPED_ARCHIVE_DEST\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\"\n\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-sensor-for-spark-bin.zip\/btrace-agent.jar=bootClassPath=unravel-sensor-for-spark-bin.zip\/unravel-boot.jar,systemClassPath=unravel-sensor-for-spark-bin.zip\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\"\n\n\n\n\/usr\/lib\/spark\/bin\/spark-shell \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-sensor-for-spark-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n<<EOF\n\/\/ your spark-shell code snippet will follow here\n\/\/ For exemplifying, we use a snippet of RDDRelation below\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.functions._\n\nimport sqlContext.implicits._\n\ncase class Record(key: Int, value: String)\n\n val df = sc.parallelize((1 to 100).map(i => Record(i, s\"val_$i\"))).toDF()\n\n \/\/ Any RDD containing case classes can be registered as a table. The schema of the table is\n \/\/ automatically inferred using scala reflection.\n df.registerTempTable(\"records\")\n\n \/\/ Once tables have been registered, you can run SQL queries over them.\n println(\"Result of SELECT *:\")\n sqlContext.sql(\"SELECT * FROM records\").collect().foreach(println)\n\n \/\/ Aggregation queries are also supported.\n val count = sqlContext.sql(\"SELECT COUNT(*) FROM records\").collect().head.getLong(0)\n println(s\"COUNT(*): $count\")\n\nexit\n\nEOF\n\n Note that a full blank line separates lengthy lines that are wrapped, except for the spark-shell that uses line continuation backslashes. " }, 
{ "title" : "Installing Unravel Sensor for Individual Hive Queries", 
"url" : "unravel-4-0-4-1/advanced-topics/sensors/installing-unravel-sensor-for-individual-hive-queries.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Sensors \/ Installing Unravel Sensor for Individual Hive Queries", 
"snippet" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar W...", 
"body" : "The MapReduce JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-sensor-for-mapreduce-bin.zip btrace-agent.jar btrace-boot.jar unravel-mr-boot.jar unravel-mr-sys.jar When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set \nyarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=systemClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-sys.jar,bootClassPath=\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Part 2: Install Unravel Sensor Parcel on CDH+CM Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive: set mapreduce.job.cache.archives=path_in_hdfs\/unravel-sensor-for-mapreduce-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set \nmapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: SetUNRAVEL_HOST_IPof your Unravel gateway server. Port #4043 is where Unravel LR server is running. set mapreduce.task.profile.params=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; Enable the JVM agent for application master. set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-sensor-for-mapreduce-bin.zip\/btrace-agent.jar=systemClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-sys.jar,bootClassPath=unravel-sensor-for-mapreduce-bin.zip\/unravel-mr-boot.jar,scriptOutputFile=\/dev\/null -Dunravel.server.hostport=<UNRAVEL_HOST_IP>:4043 -Dcom.sun.btrace.FileClient.flush=-1; " }, 
{ "title" : "Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web ser...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so that you can see them in Unravel Web UI. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 Before you start, ensure that the Unravel Server host and the server that runs Airflow web service are in the same cluster. If You Use Http For Airflow Web UIAccess Add the following 3 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.protocol=http\ncom.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart If You Use Https For Airflow Web UIAccess Add the following 4 settings to \/usr\/local\/unravel\/etc\/unravel.properties Replace {airflow web url} with the full URL, starting with http:\/\/ com.unraveldata.airflow.server.url={airflow web url}\ncom.unraveldata.airflow.available=true\ncom.unraveldata.airflow.login.name={airflow web UI username}\ncom.unraveldata.airflow.login.password={airflow web UI password} Then restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Range of Monitoring By default, Unravel Server ingests all the workflows that started within the last 5 days. If you wish to change the date range to the last x Add the following configuration to \/usr\/local\/unravel\/etc\/unravel.properties Don't omit the “-” (minus sign) in the value. airflow.look.back.num.days=-x Restartthe unravel_jcs2 sudo \/etc\/init.d\/unravel_jcs2 restart " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators import PythonOperator from da...", 
"body" : "Enabling Oozie In unravel.properties oozie.server.url sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Enabling AirFlow The script below, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Oozie\/Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n 1. Add Hooks for Unravel Instrumentation The example below shows the contents of a bash script, example-hdp-client.sh spark-submit spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport Setting these parameters on a per-application spark-defaults.conf This script references the following variables, which you would need to edit: PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR 2. Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call 2016-08-30 3. Monitor the Workflow To see the new Oozie workflow in Unravel Web UI, select APPLICATIONS Workflows Add Workflow " }, 
{ "title" : "Tagging Workflows", 
"url" : "unravel-4-0-4-1/advanced-topics/workflows/tagging-workflows.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Workflows \/ Tagging Workflows", 
"snippet" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utct...", 
"body" : "About Unravel Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: [Wrong usage] SET unravel.workflow.name=\"ETL-Workflow\"; [Correct usage] SET unravel.workflow.name=ETL-Workflow; Please note the following: Different runs of the same same unravel.workflow.name Different runs of the same different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name The example below shows a Hive query that is marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\n\nSELECT foo FROM table WHERE … [Rest of Hive Query text goes here] Easy Recipes for Tagging Workflows First, export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow: Export the workflow name: export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow Export the UTC timestamp for this run of the workflow. Here, we use bash's date export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Then follow the appropriate instructions below: Tagging a Hive query Tagging a Sqoop job Tagging a direct MapReduce job Tagging a Spark job Tagging a Pig job Tagging a Impala query How to Tag a Hive Query Using SET Commands in Hive hive -f hive\/simple_wf.hql In hive\/simple_wf.hql SET unravel.workflow.name=${env:WORKFLOW_NAME};\nSET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP};\nselect count(1) from lineitem; How to Tag a Sqoop Job Using –D Command Line Parameters sqoop export \\\n *-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar How to Tag a Direct MapReduce Job Using –D Command Line Parameters hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n*-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" * \\\n-p \/wordcount.properties -input \/tmp\/soumitra\/data\/small -output \/tmp\/soumitra\/outsmoke How to Tag a Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name spark-submit \\\n * --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" *\n * --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" *\n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster How to Tag a Pig Job Using –param and SET Commands pig \\\n*-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP * \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME;\nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP;\n\nlines = LOAD '\/tmp\/soumitra\/data\/small' using PigStorage('|') AS (line:chararray);\nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount; How to Tag a Impala Job Using –-query_options and SET Command Set DEBUG_ACTION in Impala script to pass the workflow tags into impala queries: SET DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}; According to this CDH article: https:\/\/www.cloudera.com\/documentation\/enterprise\/5-14-x\/topics\/impala_query_options.html impala-shell --query_option=DEBUG_ACTION=|unravel.workflow.name::{$WORKFLOW_NAME}|unravel.workflow.utctimestamp::{$WORKFLOW_TIMESTAMP}| -f impala.script Note: the tagging option should be turned on for this tagging to work. ie, the followings need to be set in unravel.properties: # Tagging\ncom.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path=\/tmp\/app_tag.py\ncom.unraveldata.app.tagging.script.method.name=get_tags Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Application Workflow " }, 
{ "title" : "Custom Configurations", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/adding-more-admins-to-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/configuring-multiple-hosts-for-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The i...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 The internal DNS or IP address of a host is specific to your installation. Each host is assigned unique roles identified by daemon names that start with unravel_ Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc host1 Port 4043 unravel_lr host2 If you do not use an external database (db), unravel_db host1 unravel_db \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig 1. Stop Unravel Server On each Unravel host, run this command: sudo \/etc\/init.d\/unravel_all.sh stop 2. Modify unravel.properties on host1 Pick a machine to be host1 If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties host1 Replace 127.0.0.1 3316 unravel_mysql_prod To find your fully qualified hostname, type hostname -I unravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod 2. Copy host1's unravel.properties to Other Hosts Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh host1 host2 host3 # host1\nscp \/usr\/local\/unravel\/etc\/unravel.properties host2:\/usr\/local\/unravel\/etc\/\n# host1\nscp \/usr\/local\/unravel\/etc\/unravel.ext.sh host2:\/usr\/local\/unravel\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel Important Note The scripts invoked below will make an identical change to the unravel.properties 3. Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. For a 2-host ensemble (substitute host): # host1\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh\\ \nhost1 host2 \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh\\ \nhost1 host2 For a 3-host ensemble (substitute host): # host1 sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh\\ \nhost1 host2 host3\n# host2 sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh\\ \nhost1 host2 host3\n# host3 sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh\\ \nhost1 host2 host3 These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id 4. Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order, on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: # host1\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host2\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh \n# host3\nsudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: # host1 \nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-zk \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15sec for Zookeeper quorum to settle: sleep 15 Start up Kafka ensemble: # host1\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start-k \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start-k Wait 10sec for Kafka coordination: sleep 10 Create the Kafka topics (only on one host): # host1 \nsudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh 5. Start Unravel Server Finish multi-host installation by starting up Unravel Server: # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host1\necho \"http:\/\/$(hostname -f):3000\/\" \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start 6. Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 7. Snapshot unravel.properties as new golden file " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/creating-multiple-workers-for-high-volume-data.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 sudo chkconfig --add unravel_hhw_2 sudo ch...", 
"body" : " These instructions apply to single host Unravel deployments only; for multihost deployments, please contact Unravel Support If you have 10000-20000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_hhw_2 \nsudo chkconfig --add unravel_jcw2_2\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_2 If you have 20000-30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_hhw_3 \nsudo chkconfig --add unravel_jcw2_3\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_3 If you have more than 30000 jobs per day, run these commands on Unravel Server to enable these workers: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_hhw_4 \nsudo chkconfig --add unravel_jcw2_4\n# If Spark is in use:\nsudo chkconfig --add unravel_sw_4 Start Unravel Server Run the following command to start the additional daemons you enabled above: sudo \/etc\/init.d\/unravel_all.sh start\n " }, 
{ "title" : "Defining a Custom TC Port", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/defining-a-custom-tc-port.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Defining a Custom TC Port", 
"snippet" : "These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\ >>\/usr\/local\/unravel\/etc\/unravel.ext.sh...", 
"body" : " These instructions apply to any deployment. Run the following command on Unravel Server host1 18080 echo 'export UNRAVEL_LISTEN_PORT=18080' \\\n>>\/usr\/local\/unravel\/etc\/unravel.ext.sh " }, 
{ "title" : "Integrating LDAP Authentication for Unravel Web UI", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/integrating-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Integrating LDAP Authentication for Unravel Web UI", 
"snippet" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use Hi...", 
"body" : "To configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel Web UI, use settings identical to the settings for HiveServer2. In other words, if you have HiveServer2 set up for AD-based LDAP, use the same values for Unravel Web UI. If you do not use HiveServer2 LDAP, then follow the steps below. 1. Modify unravel.properties Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties Change QA ldap:\/\/LDAP_HOST For QA For ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST unravel\/jre\/ ldap:\/\/ ldap_host For Active Directory (AD): com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.Domain=QA Change LDAP_HOST and QA to appropriate value for your installation. For Open LDAP, example 1: com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.baseDN=ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. For Open LDAP, example 2: In this example, we expect a typical DN to be uid=%s,ou=myunit,dc=example,dc=com where %s is the login name as typed in the login form. In some cases 'cn' is used in place of 'uid'. com.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.use_jndi=true\nhive.server2.authentication.ldap.url=ldap:\/\/LDAP_HOST\nhive.server2.authentication.ldap.guidKey=uid\nhive.server2.authentication.ldap.userDNPattern=uid=%s,ou=myunit,dc=example,dc=com Change the example ou=myunit,dc=example,dc=com to appropriate value for your installation. 2. Restart unravel_tc Restart unravel_tc sudo \/etc\/init.d\/unravel_tc restart Advanced Hive Properties Below is a list of advanced properties that narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The process of authentication is described next. Authentication Process for Active Directory (AD) Bind as username + at sign + domain, using the given password, with simple LDAP auth mode verbose log will show Connecting and then Connected when bind is successful If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group filter is specified, it is checked if a user filter is specified, it is checked Authentication Process for Open LDAP Bind as cn or uid =username + baseDN using the given password, with simple LDAP auth mode the guidKey property determines whether cn or uid is used if userDNPattern is used, it takes precedence over baseDN, and each pattern is tried If there are no more qualifications for groups or filtering or custom query, then login to Unravel is deemed successful if bind succeeds If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful verbose log will show results list and the match arguments in effect user or group filters will be ignored if custom query is used If a group pattern or filter is specified, it is checked if a user filter is specified, it is checked Property Description Example Value hive.server2.authentication.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) hive.server2.authentication.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. REQUIRED for advanced query except when setting custom query or groupDNPattern. uid or CN hive.server2.authentication.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). , hiveuser impalauser hiveadmin hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. member uniqueMember memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. group groupOfNames groupOfUniqueNames com.unraveldata.ldap.verbose enables verbose logging. Grep for \"Ldap\" entries in the unravel_tc_webapp.log file under \/usr\/local\/unravel\/logs\/ ; when enabled, user names and group names can appear in this log, but raw passwords are not logged. Can be true or false or not set; default is false " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties admin Manage Configuration Core Retention unravel.properties When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires abo...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties admin Manage Configuration Core Retention unravel.properties When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 500KB of disk space. That means about 2000 jobs per 1GB of disk. In Unravel Web UI, select the Manage Configuration Core The TIME SERIES RETENTION DAYS unravel.properties: com.unraveldata.retention.max.days=90 The WEEKS TO SHOW FOR SEARCH RESULTS unravel.properties com.unraveldata.history.maxSize.weeks=7 This value should be no larger than the next setting minus 1. The WEEKS TO SHOW FOR DEEP SEARCH RESULTS unravel.properties com.unraveldata.recent.maxSize.weeks=14 This value should be at least 1 week more than the setting immediately above. After changing any of the settings above, restart unravel_td sudo \/etc\/init.d\/unravel_td restart " }, 
{ "title" : "Setting Up Email for Auto Actions and Collaboration", 
"url" : "unravel-4-0-4-1/advanced-topics/custom-configurations/setting-up-email-for-auto-actions-and-collaboration.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Custom Configurations \/ Setting Up Email for Auto Actions and Collaboration", 
"snippet" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternativ...", 
"body" : "You can specify an SMTP server for Unravel Server so that it can send reports, alerts, and collaboration emails. Several examples are shown below. Adapt the one that is most similar to your environment. You can set these values through Unravel Web UI's Manage Configuration Email Web UI An alternative to using Unravel Web UI's Manage \/usr\/local\/unravel\/etc\/unravel.properties Property If you specify a saved email setting in Unravel Web UI, that setting overrides the corresponding setting in the unravel.properties Defaults When you do not specify properties or configuration settings, Unravel Server tries to use the default 'classic' SMTP setting at localhost:25 ; this sometimes works for customers that set up SMTP spooling with sendmail or postfix, but it might block emails to external domains (for anti-spam reasons). On EC2, this sometimes works for small emails, but significant use is blocked for anti-spam reasons. Web UI Property Value Description PORT mail.smtp.port 25 Port AUTHENTICATE mail.smtp.auth false Enable SMTP authentication? If true, then USER (mail.smtp.user) and USER PASSWORD ( mail.smtp.pw START TLS mail.smtp.starttls.enable false Use start-TLS? SSL ENABLE mail.smtp.ssl.enable false Use SSL right from the start? USER mail.smtp.user null Username for SMTP authentication USER PASSWORD mail.smtp.pw null Password for SMTP authentication HOST mail.smtp.host l Host for SMTP server FROM USER mail.smtp.from someone @example.com Use a From: LOCALHOST mail.smtp.localhost localhost.local A domain name for apparent sender; must have at least one dot (e.g. organization.com) DEBUG mail.smtp.debug false Enable debug mode? Set to true (temporarily) to see more details in logs. Unravel daemons to restart after email setup Run the following commands on Unravel Server: sudo \/etc\/init.d\/unravel_tc restart \nsudo \/etc\/init.d\/unravel_all.sh stop-etl\nsudo \/etc\/init.d\/unravel_all.sh start Verify email setup works Run the following commands on Unravel Server: sudo -u unravel \/usr\/local\/unravel\/install_bin\/diag_email.sh someone@example.com \n--> enter password from dist.unraveldata.com\n--> should see following output in terminal mode and if you see \"result is = null\", then, setup is correct.\n:\n:\nresult is = null\nAt least one smtp pathway worked\nfor log output see \/usr\/local\/unravel\/logs\/test_email.log See the stdout. It will test smtp settings (either from unravel.properties or defaults or in settings table in db or command line overrides). It will also test \"smtp2\" email which is compiled-in as a backup for alerts to Unravel Support. Customer reports are not Email setup for Auto-Actions After above email setup has been completed in Unravel UI under Email Config Wizard, next, please do below steps to configure Auto-Actions. Add these properties to \/usr\/local\/unravel\/unravel.properties mail.smtp.from=someone@example.com\ncom.unraveldata.report.user.email.domain=example.com Disable unneeded daemons: sudo service unravel_os3 stop\nsudo chkconfig unravel_os3 off Restart daemons: sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI....", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data Page in Unravel Web UI. " }, 
{ "title" : "For CDH+CM", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForCDHCM", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For CDH+CM", 
"snippet" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the RES...", 
"body" : "You can enable instrumentation for the Hive metastore through Unravel Web UI's configuration wizard, but first you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API From CDH version 5.5 onward, use the REST API \" http:\/\/ CMGR_HOSTNAME_IP Look at the response body, a JSON-like text format as in the image below. Search the response body for \" metastore In Unravel UI, on the top right corner, click admin Manage On the left tab, click Hive HIVE METASTORE URL HIVE METASTORE DRIVER HIVE METASTORE USER NAME HIVE METASTORE PASSWORD Save the information when done: click Save Changes Restart Unravel Server: sudo \/etc\/init.d\/unravel_all.sh restart 6. After restart, confirm that Hive queries appear in Unravel UI in the Application " }, 
{ "title" : "For HDP", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForHDP", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For HDP", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "For MapR", 
"url" : "unravel-4-0-4-1/advanced-topics/connecting-to-a-hive-metastore.html#UUID-0c705d7e-ea15-32ef-6aab-65ffbe456247_id_ConnectingtoaHiveMetastore-ForMapR", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Connecting to a Hive Metastore \/ For MapR", 
"snippet" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR...", 
"body" : "See Part 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html#UUID-11c5f57d-7413-5d9d-c486-9a999fb8376e_id_CreatinganAWSRDSCloudWatchAlarmforFreeStorageSpace-ThisguideistoconfigureanAWSRDSCloudWatchAlarmforDiskFreeStorageSpaceMetricsaspartofRDSmonitoring", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace \/ This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring:", 
"snippet" : "Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database you wish to monitor for FreeStorageSpace Next In the Alarm Threshold - for this Database ...", 
"body" : " Go to AWS Cloud Watch Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier, select the database you wish to monitor for FreeStorageSpace Next In the Alarm Threshold - for this Database Metrics (e.g. RDS_FreeStorageSpace_for_MySQL-A) Name - describe what the above database metrics name you entered (e.g. Disk space monitor of RDS MySQL-A) Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for 10 consecutiveperiod(s) Under Actions Send notifications to Note: this sns topic should already be setup before you add it. Click Create Alarm In Alarms Alarms To to create alarm metrics for RDS monitoring on Storage Space, click Create Alarm " }, 
{ "title" : "Creating Application Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags", 
"snippet" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed....", 
"body" : "Annotating applications with tags (key-value pairs) allows you to search, group, and charge back based on tags. It also allows you to track Unravel's insights based on tags.Thus, understanding how tags work in Unravel is crucial. Application tags are immutable: once created they cannot be changed. " }, 
{ "title" : "What is a Tag in Unravel?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-WhatisaTaginUnravel", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What is a Tag in Unravel?", 
"snippet" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ......", 
"body" : "A tag is key-value pair <k,v> A Thus, application A <k1,v1>, <k2,v2>, ... " }, 
{ "title" : "How Does Unravel Use Tags?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-HowDoesUnravelUseTags", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Does Unravel Use Tags?", 
"snippet" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights...", 
"body" : "Unravel Server and Web UI use tags to: Group applications for chargeback reports Provide access control for different users Search\/group applications using tags Group applications for insights " }, 
{ "title" : "What Types of Tags Are There?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-WhatTypesofTagsAreThere", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ What Types of Tags Are There?", 
"snippet" : "There are two types of tags: Unravel tags and user-created tags....", 
"body" : "There are two types of tags: Unravel tags and user-created tags. " }, 
{ "title" : "Unravel Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-UnravelTags", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Unravel Tags", 
"snippet" : "All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp...", 
"body" : "All the tag names starting with unravel unravel.workflow.name unravel.workflow.utctimestamp " }, 
{ "title" : "User-Created Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-User-CreatedTags", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ User-Created Tags", 
"snippet" : "You can create tags based on your use cases....", 
"body" : "You can create tags based on your use cases. " }, 
{ "title" : "How Do I Create Tags?", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-HowDoICreateTags", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ How Do I Create Tags?", 
"snippet" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them....", 
"body" : "There are two ways to create tags: by adding them to the configuration of your application, or by writing a Python script to inject them. " }, 
{ "title" : "Adding Tags to your Application's Configuration", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-AddingTagstoyourApplicationsConfiguration", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Adding Tags to your Application's Configuration", 
"snippet" : "Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,......", 
"body" : "Add tags to your application's configuration (configuration file or setConf key1,value1,key2,value2,key3,value3,... " }, 
{ "title" : "Injecting Tags Through a Python Script", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-InjectingTagsThroughaPythonScript", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Injecting Tags Through a Python Script", 
"snippet" : "You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches ...", 
"body" : "You can write Python script which is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. Unravel receives metadata about applications from different sources, and that metadata can be received out of order, but it is merged and eventually reaches a consistent state.For example, Spark receives data from Resource Manager, event log file, YARN aggregated logs, and sensors. Your Python script must be idempotent, in other words, it must produce the same result over multiple invocations with different input (metadata) for the same application. " }, 
{ "title" : "Precedence of Tags", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-PrecedenceofTags", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Precedence of Tags", 
"snippet" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script...", 
"body" : "Unravel gives precedence to tags in this order (low to high): Unravel tags defined in application configuration Tags extracted by Python script " }, 
{ "title" : "Sample Use Cases", 
"url" : "unravel-4-0-4-1/advanced-topics/creating-application-tags.html#UUID-c92d3633-2ac9-09a1-5f25-23aa3fe48661_id_CreatingApplicationTags-SampleUseCases", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Creating Application Tags \/ Sample Use Cases", 
"snippet" : "Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants...", 
"body" : " Differentiate between job types such as pipeline and workflow Differentiate between projects, queues, departments, groups,users, and tenants " }, 
{ "title" : "Troubleshooting", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting", 
"snippet" : "Provide solutions for commonly encountered problems. If you can't reach Unravel Server, ping LANS_DNS....", 
"body" : "Provide solutions for commonly encountered problems. If you can't reach Unravel Server, ping LANS_DNS. " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are worki...", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (â€œSmoke Testsâ€?) Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run thespark_test_via_parcel.shscript. This script runs a Spark app. Itâ€™s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, even before you install and configure Unravel Sensor. You should be able to see the data generated by this Spark app on Unravel Web UI. \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server <unravel_host_IP_address> Note: You can run this script without installing and configuring Unravel Sensor. After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 Running Benchmarks We provide sample Spark, MapReduce, Hive, and WF apps that you can download from preview.unraveldata.com Spark Download our sample Spark app: curl https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz -o spark-benchmarks1.tgz This .tgz Run md5sum md5sum spark-benchmarks1.tgz Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 spark-benchmarks1.tgz Uncompress the .tgz tar -zxvf spark-benchmarks1.tgz Run the samples. Instructions on how to run the samples are included in the package itself, inside demo-benchmarks-for-spark\/benchmarks\/README After running the samples, check the Program Execution Graph Execution Graph " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "unravel-4-0-4-1/advanced-topics/troubleshooting/running-verification-scripts-and-benchmarks-8237.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Troubleshooting \/ Running Verification Scripts and Benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working...", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. Why Run Verification Tests or Benchmarks? Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. Running Verification Tests (“Smoke Tests”) Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run thespark_test_via_parcel.shscript. This script runs a Spark app. It’s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, even before you install and configure Unravel Sensor. You should be able to see the data generated by this Spark app on Unravel Web UI. \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server <unravel_host_IP_address> You can run this script without installing and configuring Unravel Sensor. After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark, run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 Running Benchmarks We provide sample Spark, MapReduce, Hive, and WF apps that you can download from preview.unraveldata.com Spark Download our sample Spark app: curl https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz -o spark-benchmarks1.tgz This .tgz Run md5sum md5sum spark-benchmarks1.tgz Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 spark-benchmarks1.tgz Uncompress the .tgz tar -zxvf spark-benchmarks1.tgz Run the samples. Instructions on how to run the samples are included in the package itself, inside demo-benchmarks-for-spark\/benchmarks\/README. After running the samples, check the Program and the Execution Graph tabs in Unravel Web UI. Click an RDD in the Execution Graph to see the corresponding line of code in the app. " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/uninstalling-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel sudo rpm -e unravel\nsudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Upgrading Unravel Server", 
"url" : "unravel-4-0-4-1/advanced-topics/upgrading-unravel-server.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Advanced Topics \/ Upgrading Unravel Server", 
"snippet" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1 sudo \/etc\/init.d\/unravel_all.sh stop...", 
"body" : "This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Copy the new RPM to each Unravel host. Stop each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host2\nsudo \/etc\/init.d\/unravel_all.sh stop \n# host3\nsudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously # host1\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host2\nsudo rpm -U unravel-4.*.x86_64.rpm* \n# host3\nsudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties You must enter add license key to unravel.properties Run \/usr\/local\/unravel\/install_bin\/await_fixups.sh sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh After all the RPM upgrades finish, restart Unravel Server on each host simultaneously # host1\nsudo \/etc\/init.d\/unravel_all.sh start \n# host2\nsudo \/etc\/init.d\/unravel_all.sh start \n# host3\nsudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Unravel 4.0.x", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "4.1.941", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-SoftwareVersion", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Software Version", 
"snippet" : "On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z...", 
"body" : " On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z " }, 
{ "title" : "New Features", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-NewFeatures", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ New Features", 
"snippet" : "Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics...", 
"body" : " Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics " }, 
{ "title" : "Tested Platforms", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-TestedPlatforms", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Improvements\/Bugfixes", 
"snippet" : "Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\"...", 
"body" : " Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\" " }, 
{ "title" : "Robustness\/Reliability", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-RobustnessReliability", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Robustness\/Reliability", 
"snippet" : "Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common setti...", 
"body" : " Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common settings are all included and don't need to be specified by the user Improved Unravel Resource Metrics Sensor performance " }, 
{ "title" : "Spark Support", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-SparkSupport", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Spark Support", 
"snippet" : "Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configur...", 
"body" : " Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configuration from MySQL Accessing S3 log files fixes: Mapping multiple S3 buckets to the same S3 profile Set the S3 region to a custom one. Scripts and DSL API extension to generate large Oozie workflows programmatically " }, 
{ "title" : "Workflow Support", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-WorkflowSupport", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Workflow Support", 
"snippet" : "Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this fe...", 
"body" : " Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this feature, the Oozie workflows won't show up; instead, the selected (tagged) applications will be grouped into a single tagged workflow and presented in the Workflows tab. Airflow improvements " }, 
{ "title" : "MR Insights", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-MRInsights", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ MR Insights", 
"snippet" : "Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs...", 
"body" : " Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs " }, 
{ "title" : "Known Issues", 
"url" : "unravel-4-0-4-1/release-notes--4-0-x/release-notes--4-1-941.html#UUID-ca9bddc7-bfc8-7b76-d620-018aadef560f_id_ReleaseNotesVersion41941-KnownIssues", 
"breadcrumbs" : "Home \/ Unravel 4.0-4.1 \/ Unravel 4.0.x \/ 4.1.941 \/ Known Issues", 
"snippet" : "Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (so...", 
"body" : " Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (source - unravel_emr_sensor.log in EMR cluster) Improve\/Rewrite EMR bootstrap script to avoid all manual steps. " }, 
{ "title" : "", 
"url" : "release-notes.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Release Notes...", 
"body" : " Release Notes " }, 
{ "title" : "Unravel 4.0.x", 
"url" : "release-notes/release-notes--4-0-x.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "4.1.941", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Software Version", 
"snippet" : "On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z...", 
"body" : " On-premise RPM unravel-4.1-941.x86_64.rpm.20170725T0407Z EMR RPM unravel-4.1-941.x86_64.EMR.rpm.20170725T0239Z " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ New Features", 
"snippet" : "Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics...", 
"body" : " Application data from Resource Manager is published to jc topic and then processed by JCW2 daemon to create entry in jobs table, and create hitdoc. This ensures that kill apps (jhist and job conf files are not available in HDFS) are available in Unravel. Cluster Analytics " }, 
{ "title" : "Tested Platforms", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-TestedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Improvements\/Bugfixes", 
"snippet" : "Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\"...", 
"body" : " Fixed the issue of \"Done directory setting via UI\" Fixed the issue of \"Config wizard not showing the value of spark event log location\" " }, 
{ "title" : "Robustness\/Reliability", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-RobustnessReliability", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Robustness\/Reliability", 
"snippet" : "Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common setti...", 
"body" : " Improved bootstrap scripts for Qubole and EMR Improved setup scripts for HDP and MapR Improved experience deploying and configuring the Unravel Resource Metrics Sensor One single distribution package - no need to download separate ZIP file for MR and Spark Simplified configuration - the common settings are all included and don't need to be specified by the user Improved Unravel Resource Metrics Sensor performance " }, 
{ "title" : "Spark Support", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-SparkSupport", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Spark Support", 
"snippet" : "Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configur...", 
"body" : " Tag interesting Spark apps, usability improvements, time breakdown event Put caps on the maximum number of executors in recommendations Differentiate among actionable events in Spark using dynamic ranks as in Hive and MR Bounding executor logs + efficient log processing, API to access Spark configuration from MySQL Accessing S3 log files fixes: Mapping multiple S3 buckets to the same S3 profile Set the S3 region to a custom one. Scripts and DSL API extension to generate large Oozie workflows programmatically " }, 
{ "title" : "Workflow Support", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-WorkflowSupport", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Workflow Support", 
"snippet" : "Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this fe...", 
"body" : " Oozie workflow pipeline: Improve backend storage efficiency of Oozie workflows Test and pass changes on large Oozie workflows Tagged workflow change: Enable using Python script to tag any Unravel supported applications Enable user to tag Oozie workflow via similar approach. If a user enables this feature, the Oozie workflows won't show up; instead, the selected (tagged) applications will be grouped into a single tagged workflow and presented in the Workflows tab. Airflow improvements " }, 
{ "title" : "MR Insights", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-MRInsights", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ MR Insights", 
"snippet" : "Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs...", 
"body" : " Improvement to Hive\/MR time breakdown event Mark interesting events to be shown on front end Get ApplicationMaster log for app performance tuning for MR jobs " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/release-notes--4-0-x/release-notes--4-1-941.html#UUID-f59358b8-e725-d777-b5f0-7eaff3cf3c2d_id_ReleaseNotesVersion41941-KnownIssues", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.0.x \/ 4.1.941 \/ Known Issues", 
"snippet" : "Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (so...", 
"body" : " Considerable delay in MR jobs to appear on UI. delay is more than 15 to 25 minutes. in EMR Need serious investigation on MR pipeline to identify if there is issues with long SQL queries, non-indexed column or JCW. Running Nested oozie workflow since 4 hours. jhist files for many jobs are missing (source - unravel_emr_sensor.log in EMR cluster) Improve\/Rewrite EMR bootstrap script to avoid all manual steps. " }, 
{ "title" : "Unravel 4.2.x", 
"url" : "release-notes/release-notes--4-2-x.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x", 
"snippet" : "Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7...", 
"body" : " Release Notes: Version 4.2 Release Notes: Version 4.2.3 Release Notes: Version 4.2.4 Release Notes: Version 4.2.5 Release Notes: Version 4.2.6 Release Notes: Version 4.2.7 " }, 
{ "title" : "Release Notes: Version 4.2", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html#UUID-b4791178-aa86-09f0-292c-e0bf390a5d81_id_ReleaseNotesVersion42-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Software Version", 
"snippet" : "On-premise RPM EMR RPM...", 
"body" : " On-premise RPM EMR RPM " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html#UUID-b4791178-aa86-09f0-292c-e0bf390a5d81_id_ReleaseNotesVersion42-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ New Features", 
"snippet" : "Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD...", 
"body" : " Impala APM 1.0 (Beta): This release supports application performance management (APM) for Impala queries on CDH clusters. Kafka APM 1.0 (Beta): This release supports application performance management (APM) for Kafka queries. Smart Auto Actions (Beta): TBD Unravel API 1.0 (Beta): TBD " }, 
{ "title" : "Tested Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html#UUID-b4791178-aa86-09f0-292c-e0bf390a5d81_id_ReleaseNotesVersion42-TestedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Tested Platforms", 
"snippet" : "CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster...", 
"body" : " CDH with Kerberos HDP with Kerberos EMR 5.2.X Qubole Spark type cluster " }, 
{ "title" : "Improvements\/Bugfixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html#UUID-b4791178-aa86-09f0-292c-e0bf390a5d81_id_ReleaseNotesVersion42-ImprovementsBugfixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Improvements\/Bugfixes", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2.html#UUID-b4791178-aa86-09f0-292c-e0bf390a5d81_id_ReleaseNotesVersion42-KnownIssues", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2 \/ Known Issues", 
"snippet" : "TBD...", 
"body" : " TBD " }, 
{ "title" : "Release Notes: Version 4.2.3", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Software Version", 
"snippet" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM...", 
"body" : "Date: 11\/1\/2017 On-premise RPM unravel-4.2.3.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-HowtoDownload", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.3.x86_64.rpm MD5 for unravel.x86_64.rpm: 3701dca005208365b3490f3b0390b5ec " }, 
{ "title" : "Tested Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-TestedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.5) with Kerberos enabled EMR: testing is in progress Qubole: testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional...", 
"body" : " Optional " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ New Features", 
"snippet" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed ac...", 
"body" : "Tez Support This release supports the monitoring of applications run within the Tez framework. This includes: Hive on Tez Pig on Tez Cascading on Tez The Tez application performance manager (APM) in Unravel Web UI provides a detailed view into the behavior of \"Tez framework\" queries as a directed acyclic graph (DAG). DAG details include query text, DAG graphs, and information on vertices, tasks, and attempts. For more information, see the User Guide Spark Pipeline This release includes an improved pipeline for collecting data from Spark applications. The pipeline provides: Additional metadata Stage level updates: Previously, the Spark APM in Unravel Web UI updated the application's page only once, when the application was finished. With this release, as soon as a stage completes, the Spark APM in Unravel Web UI shows stage level information. This improvement allows you to interact with the Spark APM more often. Smaller memory footprint Smarter events: The event generation algorithm has been updated with additional triggers. Events are generated only if an application's original suggested Workflow tagging: Unravel Web UI now displays tagged workflows identically, irrespective of the mode in which the Spark applications were loaded. Supported loading modes remain the same: OPS mode BATCH mode Applications are tagged even when the event log files are unavailable. Consistent display of information about each Spark application, even if the event log file is not available. At a minimum, a Spark application's page displays the application metadata, taken from the resource manager. Multi-Cluster Kafka Support This release supports data collection for multi-cluster Kafka topics, and Kafka application performance management through Unravel Web UI. Kafka support includes: Multi cluster support Multi cluster metrics monitoring Multi cluster consumer offset\/lag monitoring Consumer groups Single view for CG status\/stats across topics it consumes Additional metrics added for brokers Insights Tunable sliding window algorithm Consumer group lagging\/stalled " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Improvements and Bug Fixes", 
"snippet" : "Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries ...", 
"body" : " Unravel Sensor has been upgraded to improve performance related to DNS issues. This upgrade is optional. For help with upgrading Unravel Sensor, contact Unravel Support Impala improvements and insights Impala OPs support Daemon memory consumption graph Number of queries running graph Impala queries insights improvements Improved time breakdown event Improved suggestions on hash joins Fixed the issue “Unravel not loading eventlog.inprogress file” Fixed the issue “Impala queries not coming up in UI” Fixed the issue “Page refresh changes the sorting order” Fixed the issue “High latency in loading MR jobs”.JCS2 now uses “hdfs ls” as opposed to “du” Fixed the issue “Issue with unravel_us_1 demon” ES migration script to migrate ES mappings for Impala (runs during upgrade of 4.1 or older version to 4.2) " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-KnownIssues", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ Known Issues", 
"snippet" : "Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support...", 
"body" : " Data collection related to Impala queries causes the number of TCP connections to increase over time. There are issues with multi-host installations of Unravel Server. For help, contact Unravel Support " }, 
{ "title" : "What's New", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-3.html#UUID-86af4c67-95a9-2e76-5657-14c6b5995b38_id_ReleaseNotesVersion423-WhatsNew", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.3 \/ What's New", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Release Notes: Version 4.2.4", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Software Version", 
"snippet" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM...", 
"body" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-HowtoDownload", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4 " }, 
{ "title" : "Tested Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-TestedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Unravel Sensor Upgrade", 
"snippet" : "Yes ( See details below)...", 
"body" : " Yes ( See details below) " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4.html#UUID-36f57696-b19e-5232-f447-4544b3ba1608_id_ReleaseNotesVersion424-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Improvements and Bug Fixes", 
"snippet" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creat...", 
"body" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creating rules for Tez Apps AutoActions now can read Application Master job counter metrics and use it in Expert Auto Action rules (ref - https:\/\/hadoop.apache.org\/docs\/r2.4.1\/hadoop-yarn\/hadoop-yarn-site\/MapredAppMasterRest.html#Job_Counters_API Sensors FD leak and HttpUrlConnection issues fixed in spark sensor Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394 ) " }, 
{ "title" : "Release Notes: Version 4.2.4", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Software Version", 
"snippet" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM...", 
"body" : "Release Date: 12\/12\/2017 On-premise RPM unravel-4.2.4.x86_64.rpm EMR RPM " }, 
{ "title" : "How to Download", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-HowtoDownload", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ How to Download", 
"snippet" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4...", 
"body" : "# curl -v http:\/\/preview.unraveldata.com\/img\/unravel.x86_64.rpm -o unravel-4.2.4.x86_64.rpm Note:- MD5SUM ce260b2e3e79a7742dcb74b6d7d6c1c4 " }, 
{ "title" : "Tested Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-TestedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Tested Platforms", 
"snippet" : "CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress...", 
"body" : " CDH: On-premise CDH (up to version v5.12) with Kerberos enabled HDP: On-premise HDP (up to version v2.6) with Kerberos enabled MapR: Testing is in progress EMR: Testing is in progress Qubole: Testing is in progress " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Unravel Sensor Upgrade", 
"snippet" : "Yes ( See details below)...", 
"body" : " Yes ( See details below) " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ New Features", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-4-11273.html#UUID-4a8687cc-0fec-7168-6991-0edc95e13cd5_id_ReleaseNotesVersion424-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.4 \/ Improvements and Bug Fixes", 
"snippet" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creat...", 
"body" : "Bug Fixes Impala - Number of TCP connections increases over time Fixed close connection when using HttpUrlConnection New way to add schema_migrations in core-unravel Support Tez app type in AutoActions Added support for application master job counter AutoActions AutoActions feature now support creating rules for Tez Apps AutoActions now can read Application Master job counter metrics and use it in Expert Auto Action rules (ref - https:\/\/hadoop.apache.org\/docs\/r2.4.1\/hadoop-yarn\/hadoop-yarn-site\/MapredAppMasterRest.html#Job_Counters_API Sensors FD leak and HttpUrlConnection issues fixed in spark sensor Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394 ) " }, 
{ "title" : "Release Notes: Version 4.2.5", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-1d2ed0b4-8943-cbf5-8cca-654d03225c70_id_ReleaseNotesVersion425-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 01\/21\/2017 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-1d2ed0b4-8943-cbf5-8cca-654d03225c70_id_ReleaseNotesVersion425-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2)with Kerberos enabled. EMR: Testing is in progress. Qubole: Testing is in progress. Please also review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-1d2ed0b4-8943-cbf5-8cca-654d03225c70_id_ReleaseNotesVersion425-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Unravel Sensor Upgrade", 
"snippet" : "Optional One improvement in sensors; details under Improvements and Bug Fixes....", 
"body" : " Optional One improvement in sensors; details under Improvements and Bug Fixes. " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-1d2ed0b4-8943-cbf5-8cca-654d03225c70_id_ReleaseNotesVersion425-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ New Features", 
"snippet" : "UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides reten...", 
"body" : " UNRAVEL-2836: Support for \"adl\" protocol when using MS Data Lake on HDInsight. UNRAVEL-2977: Shows jobs in the Navigation tab instead of Stages. Stages are shown upon selecting a job. UNRAVEL-2135, UNRAVEL-2819: Customized Spark streaming page for streaming applications. UNRAVEL-2616: Provides retention for long running Spark apps (streaming, shells, notebooks) by keeping a maximum number of annotation entries inside the DB and ES. UNRAVEL-2429, UNRAVEL-2430: Support for showing both the Spark program and SQL query in the UI for a SparkSQL query.For applications with multiple queries added a table with KPIs sorted by query runtime to help identify the longest running query and the longest stage of a query. " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-5.html#UUID-1d2ed0b4-8943-cbf5-8cca-654d03225c70_id_ReleaseNotesVersion425-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.5 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server s...", 
"body" : "Improvements UNRAVEL-3021: Improved Auto Action email \/ Slack message subject, removed misleading “only for <state>” phrase and make it cleaner and more readable. UNRAVEL-2509: All emails and Slack messages sent from Unravel server now include organization information as it’s set in Unravel server settings. Bug Fixes UNRAVEL-3070: Fixed memory leak in Spark Worker. UNRAVEL-3030: init script for mysql leaves a process owned by root running. UNRAVEL-3173: Fixed very rare but possible ConcurrentModificationException in Auto Action internal metric stream. UNRAVEL-3427: Fixed. In the cases that query memory metrics are not available, Impala event generator produces NPE. UNRAVEL-3229: Fixed Airflow http connection problem. Sensors UNRAVEL-2858 : Allows slowing down metrics collection in sensor agent. Known Issues Multi-host Unravel Install issues (internal ref:UNRAVEL-2394). " }, 
{ "title" : "Release Notes: Version 4.2.6", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-1a83004f-0fd3-962e-d024-ee66b62b3138_id_ReleaseNotesVersion426-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Software Version", 
"snippet" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM...", 
"body" : "Release Date: 03\/05\/2018 For details on downloading updates see UPDATE_NEEDED_LINK_TO_RPM " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-1a83004f-0fd3-962e-d024-ee66b62b3138_id_ReleaseNotesVersion426-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX...", 
"body" : " CDH: On-premise CDH (up to v5.13) with Kerberos enabled. HDP: On-premise HDP (up to v2.6) with Kerberos enabled. MapR: (up to version MapR5.2) with Kerberos enabled. Please review the UPDATE_NEEDED_LINK_TO_COMPAT_MATRIX " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-1a83004f-0fd3-962e-d024-ee66b62b3138_id_ReleaseNotesVersion426-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Unravel Sensor Upgrade", 
"snippet" : "Not Required, recommended for high volume...", 
"body" : " Not Required, recommended for high volume " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-1a83004f-0fd3-962e-d024-ee66b62b3138_id_ReleaseNotesVersion426-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ New Features", 
"snippet" : "UNRAVEL-3576: SLES Parcels support - beta...", 
"body" : " UNRAVEL-3576: SLES Parcels support - beta " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-6.html#UUID-1a83004f-0fd3-962e-d024-ee66b62b3138_id_ReleaseNotesVersion426-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.6 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM us...", 
"body" : "Improvements Security Audit Issues UNRAVEL-3421: Session Timeout set to 1 hr now UNRAVEL-3426: Tomcat version disclosure in headers, now says \"unravel\/4.x\" High Volume Cluster Support UNRAVEL-3679: Reduce disk and RAM overhead UNRAVEL-3512: Increase File descriptor limits UNRAVEL-3555: Adjust RAM usage UNRAVEL-3552: collect only minimal set of resource usage metrics by default UNRAVEL-3543: Improved reliability in Unravel Impala worker Bug Fixes UNRAVEL-3520: For N\/A type Impala query, fixed NPE in missing fields in query profile Security Audit Issues UNRAVEL-3416: Security Fixes to prevent SQL injection UNRAVEL-3418: XSS - Cross-Site Scripting UNRAVEL-3444: State of settings from db lost in running processes if properties edited by hand UNRAVEL-3583: Email related fixes in Unravel Auto-Action UNRAVEL-3597: Improvements in Auto-Action page loading UNRAVEL-3536: Prevent jackson lib conflict in hive-hook sensor UNRAVEL-3546: Prevent re-submitting application data from unravel_es Known Issues Secure cookies and headers (UNRAVEL-3419, UNRAVEL-3420) " }, 
{ "title" : "Release Notes: Version 4.2.7", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Software Version", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-SoftwareVersion", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Software Version", 
"snippet" : "Release 04\/11\/2018 For details on downloading updates see here...", 
"body" : "Release 04\/11\/2018 For details on downloading updates see here " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-CertifiedPlatforms", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Certified Platforms", 
"snippet" : "CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix...", 
"body" : " CDH: On-premise CDH (up to v5.13 incl. Spark 2.2.x) HDP: On-premise HDP (up to v2.6) Please review the compatibility matrix " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-UnravelSensorUpgrade", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Unravel Sensor Upgrade", 
"snippet" : "Recommended. See bug fixes below for further information....", 
"body" : " Recommended. See bug fixes below for further information. " }, 
{ "title" : "New Features", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-NewFeatures", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ New Features", 
"snippet" : "TEZLLAP-5: TEZ APM support on CDH Platform...", 
"body" : " TEZLLAP-5: TEZ APM support on CDH Platform " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-ImprovementsandBugFixes", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Improvements and Bug Fixes", 
"snippet" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by defau...", 
"body" : "Improvements UNRAVEL-3686 & UNRAVEL-3685: Set default intervals to last 24 hours and hourly in OPS dashboard and Applications page. UKAFKA-1: Slowing down and reducing metrics collections for Kafka monitor which can be controlled using config file. USPARK-36: Added support for Spark tagging by default. TEZLLAP-19: Hive Template Changes: Failed and killed Tez DAGs are linked with hive now. Bug Fixes Stability and Robustness Fixes across features Sensor Fixes for Robustness SENSOR-20: Improved sensor configuration problem detection routine SENSOR-17: Fixed RSS calculation for process tree USPARK-15, USPARK-16, USPARK-38 & USPARK-39: Spark Scalability Bug Fixes. Optimized configuration settings for Spark worker daemon. See New Configuration Settings below. UIX-7,UIX-8, UIX-14, UIX-20: Miscellaneous UI Bug Fixes dor stability and correctness. TEZLLAP-6, TEZLLAP-7, TEZLLAP-19 : UNRAVEL-3583, UNRAVEL-3598:Auto Action Bug Fixes Impala tagging via Python script is overridden by Impala Query Options on CDH 5.13 Repackaged embedded Jackson libs " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-KnownIssues", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ Known Issues", 
"snippet" : "None...", 
"body" : " None " }, 
{ "title" : "New configuration settings", 
"url" : "release-notes/release-notes--4-2-x/release-notes--version-4-2-7.html#UUID-6b9aff06-bde4-9a58-1766-c59204a4c174_id_ReleaseNotesVersion427-Newconfigurationsettings", 
"breadcrumbs" : "Home \/ Release Notes \/ Unravel 4.2.x \/ Release Notes: Version 4.2.7 \/ New configuration settings", 
"snippet" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveld...", 
"body" : "Please see Spark Properties for Spark Worker daemon @ Unravel com.unraveldata.onprem com.unraveldata.spark.live.pipeline.enabled com.unraveldata.spark.live.pipeline.maxStoredStages com.unraveldata.spark.hadoopFsMulti.useFilteredFiles com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes com.unraveldata.spark.time.histogram " }, 
{ "title" : "", 
"url" : "downloads.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Downloads Documentation Downloads Release Notes Unravel 4.0-4.1 Unravel 4.2 4.2 | 4.2.3 | 4.2.4 | 4.2.5 | 4.2.6 | 4.2.7 Unravel 4.3 4.3.0 | 4.3.1.0 | 4.3.1.1 | 4.3.1.2 | 4.3.1.3 | 4.3.1.4 | 4.3.1.5 | 4.3.1.6 | 4.3.1.7 | 4.3.1.9 | 4.3.1.10 Unravel 4.4 4.4 | 4.4.1 | 4.4.2.0 | 4.4.2.1 Unravel 4.5 4.5.0...", 
"body" : " Downloads Documentation Downloads Release Notes Unravel 4.0-4.1 Unravel 4.2 4.2 | 4.2.3 | 4.2.4 | 4.2.5 | 4.2.6 | 4.2.7 Unravel 4.3 4.3.0 | 4.3.1.0 | 4.3.1.1 | 4.3.1.2 | 4.3.1.3 | 4.3.1.4 | 4.3.1.5 | 4.3.1.6 | 4.3.1.7 | 4.3.1.9 | 4.3.1.10 Unravel 4.4 4.4 | 4.4.1 | 4.4.2.0 | 4.4.2.1 Unravel 4.5 4.5.0 | 4.5.0.1* | 4.5.0.2 | 4.5.0.3 | 4.5.0.4 " }, 
{ "title" : "Compatibility Matrix", 
"url" : "compatibility-matrix.html", 
"breadcrumbs" : "Home \/ Compatibility Matrix", 
"snippet" : "...", 
"body" : " " }, 
{ "title" : "", 
"url" : "newsletters.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "Newsletters...", 
"body" : " Newsletters " }, 
{ "title" : "", 
"url" : "faqs.html", 
"breadcrumbs" : "Home \/ ", 
"snippet" : "FAQs...", 
"body" : " FAQs " }
]
$(document).trigger('search.ready');
});